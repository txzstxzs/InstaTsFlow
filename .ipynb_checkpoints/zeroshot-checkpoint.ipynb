{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4b405c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "from pypots.data import load_specific_dataset, mcar, masked_fill\n",
    "from pypots.imputation import MRNN,BRITS,SAITS\n",
    "from pypots.utils.metrics import cal_mse,cal_rmse,cal_mae\n",
    "from sklearn.preprocessing import MinMaxScaler,StandardScaler\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rc('font',family='Times New Roman') # 设置全局字体\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f86ab1de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 30, 6])\n",
      "tensor([0.7323,    nan,    nan, 0.7433, 0.7333,    nan], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "'第一个样本'\n",
    "oneshot_data = np.loadtxt('AAL.csv',delimiter = \",\")\n",
    "norm_data = MinMaxScaler().fit_transform(oneshot_data)\n",
    "\n",
    "zero_shot = np.array(norm_data[160:190])    # 40-70     seed1 100-130\n",
    "zero_shot = torch.from_numpy(zero_shot).float().cuda()\n",
    "zero_shot = zero_shot.unsqueeze(dim=0)\n",
    "print(zero_shot.shape)\n",
    "\n",
    "\n",
    "np.random.seed(4)\n",
    "# torch.manual_seed(4)\n",
    "# torch.cuda.manual_seed_all(4)\n",
    "\n",
    "X_ori, X_missed_1, missing_mask, indicating_mask = mcar(zero_shot, 0.3, np.nan)\n",
    "print(X_missed_1[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1233d1f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                            | 0/6 [00:00<?, ?it/s]2025-06-01 14:49:32 [INFO]: No given device, using default device: cuda\n",
      "2025-06-01 14:49:32 [WARNING]: saving_path not given. Model files and tensorboard file will not be saved.\n",
      "2025-06-01 14:49:32 [INFO]: Model initialized successfully with the number of trainable parameters: 597,780\n",
      "2025-06-01 14:49:32 [INFO]: epoch 0: training loss 1.4826\n",
      "2025-06-01 14:49:32 [INFO]: epoch 1: training loss 0.9485\n",
      "2025-06-01 14:49:32 [INFO]: epoch 2: training loss 0.8625\n",
      "2025-06-01 14:49:32 [INFO]: epoch 3: training loss 0.6549\n",
      "2025-06-01 14:49:32 [INFO]: epoch 4: training loss 0.6101\n",
      "2025-06-01 14:49:32 [INFO]: epoch 5: training loss 0.6452\n",
      "2025-06-01 14:49:32 [INFO]: epoch 6: training loss 0.6819\n",
      "2025-06-01 14:49:32 [INFO]: epoch 7: training loss 0.5962\n",
      "2025-06-01 14:49:32 [INFO]: epoch 8: training loss 0.4290\n",
      "2025-06-01 14:49:32 [INFO]: epoch 9: training loss 0.4204\n",
      "2025-06-01 14:49:32 [INFO]: epoch 10: training loss 0.3579\n",
      "2025-06-01 14:49:32 [INFO]: epoch 11: training loss 0.3482\n",
      "2025-06-01 14:49:32 [INFO]: epoch 12: training loss 0.3850\n",
      "2025-06-01 14:49:32 [INFO]: epoch 13: training loss 0.3769\n",
      "2025-06-01 14:49:32 [INFO]: epoch 14: training loss 0.3822\n",
      "2025-06-01 14:49:32 [INFO]: epoch 15: training loss 0.3764\n",
      "2025-06-01 14:49:32 [INFO]: epoch 16: training loss 0.3792\n",
      "2025-06-01 14:49:32 [INFO]: epoch 17: training loss 0.2789\n",
      "2025-06-01 14:49:32 [INFO]: epoch 18: training loss 0.2959\n",
      "2025-06-01 14:49:32 [INFO]: epoch 19: training loss 0.2965\n",
      "2025-06-01 14:49:32 [INFO]: epoch 20: training loss 0.3116\n",
      "2025-06-01 14:49:32 [INFO]: epoch 21: training loss 0.3735\n",
      "2025-06-01 14:49:32 [INFO]: epoch 22: training loss 0.3116\n",
      "2025-06-01 14:49:32 [INFO]: epoch 23: training loss 0.2830\n",
      "2025-06-01 14:49:32 [INFO]: epoch 24: training loss 0.2891\n",
      "2025-06-01 14:49:32 [INFO]: epoch 25: training loss 0.2735\n",
      "2025-06-01 14:49:32 [INFO]: epoch 26: training loss 0.2624\n",
      "2025-06-01 14:49:32 [INFO]: epoch 27: training loss 0.2386\n",
      "2025-06-01 14:49:32 [INFO]: epoch 28: training loss 0.2594\n",
      "2025-06-01 14:49:32 [INFO]: epoch 29: training loss 0.2233\n",
      "2025-06-01 14:49:32 [INFO]: epoch 30: training loss 0.2440\n",
      "2025-06-01 14:49:32 [INFO]: epoch 31: training loss 0.2548\n",
      "2025-06-01 14:49:33 [INFO]: epoch 32: training loss 0.2305\n",
      "2025-06-01 14:49:33 [INFO]: epoch 33: training loss 0.2369\n",
      "2025-06-01 14:49:33 [INFO]: epoch 34: training loss 0.2252\n",
      "2025-06-01 14:49:33 [INFO]: epoch 35: training loss 0.2193\n",
      "2025-06-01 14:49:33 [INFO]: epoch 36: training loss 0.2104\n",
      "2025-06-01 14:49:33 [INFO]: epoch 37: training loss 0.2208\n",
      "2025-06-01 14:49:33 [INFO]: epoch 38: training loss 0.2021\n",
      "2025-06-01 14:49:33 [INFO]: epoch 39: training loss 0.2151\n",
      "2025-06-01 14:49:33 [INFO]: epoch 40: training loss 0.2084\n",
      "2025-06-01 14:49:33 [INFO]: epoch 41: training loss 0.1919\n",
      "2025-06-01 14:49:33 [INFO]: epoch 42: training loss 0.2043\n",
      "2025-06-01 14:49:33 [INFO]: epoch 43: training loss 0.1933\n",
      "2025-06-01 14:49:33 [INFO]: epoch 44: training loss 0.2010\n",
      "2025-06-01 14:49:33 [INFO]: epoch 45: training loss 0.1731\n",
      "2025-06-01 14:49:33 [INFO]: epoch 46: training loss 0.1695\n",
      "2025-06-01 14:49:33 [INFO]: epoch 47: training loss 0.1657\n",
      "2025-06-01 14:49:33 [INFO]: epoch 48: training loss 0.1708\n",
      "2025-06-01 14:49:33 [INFO]: epoch 49: training loss 0.1665\n",
      "2025-06-01 14:49:33 [INFO]: epoch 50: training loss 0.1767\n",
      "2025-06-01 14:49:33 [INFO]: epoch 51: training loss 0.1695\n",
      "2025-06-01 14:49:33 [INFO]: epoch 52: training loss 0.1550\n",
      "2025-06-01 14:49:33 [INFO]: epoch 53: training loss 0.1539\n",
      "2025-06-01 14:49:33 [INFO]: epoch 54: training loss 0.1545\n",
      "2025-06-01 14:49:33 [INFO]: epoch 55: training loss 0.1478\n",
      "2025-06-01 14:49:33 [INFO]: epoch 56: training loss 0.1548\n",
      "2025-06-01 14:49:33 [INFO]: epoch 57: training loss 0.1445\n",
      "2025-06-01 14:49:33 [INFO]: epoch 58: training loss 0.1498\n",
      "2025-06-01 14:49:33 [INFO]: epoch 59: training loss 0.1519\n",
      "2025-06-01 14:49:33 [INFO]: epoch 60: training loss 0.1517\n",
      "2025-06-01 14:49:33 [INFO]: epoch 61: training loss 0.1409\n",
      "2025-06-01 14:49:33 [INFO]: epoch 62: training loss 0.1329\n",
      "2025-06-01 14:49:33 [INFO]: epoch 63: training loss 0.1363\n",
      "2025-06-01 14:49:33 [INFO]: epoch 64: training loss 0.1254\n",
      "2025-06-01 14:49:33 [INFO]: epoch 65: training loss 0.1322\n",
      "2025-06-01 14:49:33 [INFO]: epoch 66: training loss 0.1474\n",
      "2025-06-01 14:49:33 [INFO]: epoch 67: training loss 0.1335\n",
      "2025-06-01 14:49:33 [INFO]: epoch 68: training loss 0.1147\n",
      "2025-06-01 14:49:33 [INFO]: epoch 69: training loss 0.1028\n",
      "2025-06-01 14:49:33 [INFO]: epoch 70: training loss 0.1058\n",
      "2025-06-01 14:49:33 [INFO]: epoch 71: training loss 0.1197\n",
      "2025-06-01 14:49:33 [INFO]: epoch 72: training loss 0.1435\n",
      "2025-06-01 14:49:33 [INFO]: epoch 73: training loss 0.1278\n",
      "2025-06-01 14:49:33 [INFO]: epoch 74: training loss 0.1127\n",
      "2025-06-01 14:49:33 [INFO]: epoch 75: training loss 0.1030\n",
      "2025-06-01 14:49:33 [INFO]: epoch 76: training loss 0.1057\n",
      "2025-06-01 14:49:33 [INFO]: epoch 77: training loss 0.1330\n",
      "2025-06-01 14:49:33 [INFO]: epoch 78: training loss 0.1238\n",
      "2025-06-01 14:49:33 [INFO]: epoch 79: training loss 0.1247\n",
      "2025-06-01 14:49:33 [INFO]: epoch 80: training loss 0.0859\n",
      "2025-06-01 14:49:33 [INFO]: epoch 81: training loss 0.0911\n",
      "2025-06-01 14:49:33 [INFO]: epoch 82: training loss 0.1108\n",
      "2025-06-01 14:49:33 [INFO]: epoch 83: training loss 0.1127\n",
      "2025-06-01 14:49:33 [INFO]: epoch 84: training loss 0.0983\n",
      "2025-06-01 14:49:33 [INFO]: epoch 85: training loss 0.0940\n",
      "2025-06-01 14:49:33 [INFO]: epoch 86: training loss 0.0871\n",
      "2025-06-01 14:49:33 [INFO]: epoch 87: training loss 0.0988\n",
      "2025-06-01 14:49:33 [INFO]: epoch 88: training loss 0.1167\n",
      "2025-06-01 14:49:33 [INFO]: epoch 89: training loss 0.1139\n",
      "2025-06-01 14:49:33 [INFO]: epoch 90: training loss 0.1006\n",
      "2025-06-01 14:49:33 [INFO]: epoch 91: training loss 0.0880\n",
      "2025-06-01 14:49:33 [INFO]: epoch 92: training loss 0.1123\n",
      "2025-06-01 14:49:33 [INFO]: epoch 93: training loss 0.1005\n",
      "2025-06-01 14:49:33 [INFO]: epoch 94: training loss 0.1015\n",
      "2025-06-01 14:49:33 [INFO]: epoch 95: training loss 0.1060\n",
      "2025-06-01 14:49:33 [INFO]: epoch 96: training loss 0.0944\n",
      "2025-06-01 14:49:33 [INFO]: epoch 97: training loss 0.0815\n",
      "2025-06-01 14:49:33 [INFO]: epoch 98: training loss 0.0865\n",
      "2025-06-01 14:49:33 [INFO]: epoch 99: training loss 0.0999\n",
      "2025-06-01 14:49:33 [INFO]: epoch 100: training loss 0.1260\n",
      "2025-06-01 14:49:33 [INFO]: epoch 101: training loss 0.1078\n",
      "2025-06-01 14:49:33 [INFO]: epoch 102: training loss 0.1083\n",
      "2025-06-01 14:49:33 [INFO]: epoch 103: training loss 0.1061\n",
      "2025-06-01 14:49:33 [INFO]: epoch 104: training loss 0.1092\n",
      "2025-06-01 14:49:33 [INFO]: epoch 105: training loss 0.1089\n",
      "2025-06-01 14:49:33 [INFO]: epoch 106: training loss 0.1035\n",
      "2025-06-01 14:49:34 [INFO]: epoch 107: training loss 0.1051\n",
      "2025-06-01 14:49:34 [INFO]: epoch 108: training loss 0.0958\n",
      "2025-06-01 14:49:34 [INFO]: epoch 109: training loss 0.0924\n",
      "2025-06-01 14:49:34 [INFO]: epoch 110: training loss 0.0736\n",
      "2025-06-01 14:49:34 [INFO]: epoch 111: training loss 0.0837\n",
      "2025-06-01 14:49:34 [INFO]: epoch 112: training loss 0.0995\n",
      "2025-06-01 14:49:34 [INFO]: epoch 113: training loss 0.0825\n",
      "2025-06-01 14:49:34 [INFO]: epoch 114: training loss 0.0794\n",
      "2025-06-01 14:49:34 [INFO]: epoch 115: training loss 0.0789\n",
      "2025-06-01 14:49:34 [INFO]: epoch 116: training loss 0.0946\n",
      "2025-06-01 14:49:34 [INFO]: epoch 117: training loss 0.0809\n",
      "2025-06-01 14:49:34 [INFO]: epoch 118: training loss 0.0978\n",
      "2025-06-01 14:49:34 [INFO]: epoch 119: training loss 0.0773\n",
      "2025-06-01 14:49:34 [INFO]: epoch 120: training loss 0.0704\n",
      "2025-06-01 14:49:34 [INFO]: epoch 121: training loss 0.0731\n",
      "2025-06-01 14:49:34 [INFO]: epoch 122: training loss 0.0756\n",
      "2025-06-01 14:49:34 [INFO]: epoch 123: training loss 0.0951\n",
      "2025-06-01 14:49:34 [INFO]: epoch 124: training loss 0.0792\n",
      "2025-06-01 14:49:34 [INFO]: epoch 125: training loss 0.0763\n",
      "2025-06-01 14:49:34 [INFO]: epoch 126: training loss 0.0807\n",
      "2025-06-01 14:49:34 [INFO]: epoch 127: training loss 0.0914\n",
      "2025-06-01 14:49:34 [INFO]: epoch 128: training loss 0.0925\n",
      "2025-06-01 14:49:34 [INFO]: epoch 129: training loss 0.1013\n",
      "2025-06-01 14:49:34 [INFO]: epoch 130: training loss 0.0837\n",
      "2025-06-01 14:49:34 [INFO]: epoch 131: training loss 0.0955\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-01 14:49:34 [INFO]: epoch 132: training loss 0.0840\n",
      "2025-06-01 14:49:34 [INFO]: epoch 133: training loss 0.0803\n",
      "2025-06-01 14:49:34 [INFO]: epoch 134: training loss 0.0789\n",
      "2025-06-01 14:49:34 [INFO]: epoch 135: training loss 0.0767\n",
      "2025-06-01 14:49:34 [INFO]: epoch 136: training loss 0.0791\n",
      "2025-06-01 14:49:34 [INFO]: epoch 137: training loss 0.0717\n",
      "2025-06-01 14:49:34 [INFO]: epoch 138: training loss 0.0870\n",
      "2025-06-01 14:49:34 [INFO]: epoch 139: training loss 0.0862\n",
      "2025-06-01 14:49:34 [INFO]: epoch 140: training loss 0.0711\n",
      "2025-06-01 14:49:34 [INFO]: epoch 141: training loss 0.0804\n",
      "2025-06-01 14:49:34 [INFO]: epoch 142: training loss 0.0879\n",
      "2025-06-01 14:49:34 [INFO]: epoch 143: training loss 0.0747\n",
      "2025-06-01 14:49:34 [INFO]: epoch 144: training loss 0.0731\n",
      "2025-06-01 14:49:34 [INFO]: epoch 145: training loss 0.0860\n",
      "2025-06-01 14:49:34 [INFO]: epoch 146: training loss 0.0769\n",
      "2025-06-01 14:49:34 [INFO]: epoch 147: training loss 0.0792\n",
      "2025-06-01 14:49:34 [INFO]: epoch 148: training loss 0.0800\n",
      "2025-06-01 14:49:34 [INFO]: epoch 149: training loss 0.0751\n",
      "2025-06-01 14:49:34 [INFO]: epoch 150: training loss 0.0927\n",
      "2025-06-01 14:49:34 [INFO]: epoch 151: training loss 0.1060\n",
      "2025-06-01 14:49:34 [INFO]: epoch 152: training loss 0.0766\n",
      "2025-06-01 14:49:34 [INFO]: epoch 153: training loss 0.0783\n",
      "2025-06-01 14:49:34 [INFO]: epoch 154: training loss 0.0759\n",
      "2025-06-01 14:49:34 [INFO]: epoch 155: training loss 0.0782\n",
      "2025-06-01 14:49:34 [INFO]: epoch 156: training loss 0.0849\n",
      "2025-06-01 14:49:34 [INFO]: epoch 157: training loss 0.0630\n",
      "2025-06-01 14:49:34 [INFO]: epoch 158: training loss 0.0799\n",
      "2025-06-01 14:49:34 [INFO]: epoch 159: training loss 0.0907\n",
      "2025-06-01 14:49:34 [INFO]: epoch 160: training loss 0.0748\n",
      "2025-06-01 14:49:34 [INFO]: epoch 161: training loss 0.0691\n",
      "2025-06-01 14:49:34 [INFO]: epoch 162: training loss 0.0725\n",
      "2025-06-01 14:49:34 [INFO]: epoch 163: training loss 0.0688\n",
      "2025-06-01 14:49:34 [INFO]: epoch 164: training loss 0.0738\n",
      "2025-06-01 14:49:34 [INFO]: epoch 165: training loss 0.0834\n",
      "2025-06-01 14:49:34 [INFO]: epoch 166: training loss 0.0688\n",
      "2025-06-01 14:49:34 [INFO]: epoch 167: training loss 0.0731\n",
      "2025-06-01 14:49:34 [INFO]: epoch 168: training loss 0.0805\n",
      "2025-06-01 14:49:34 [INFO]: epoch 169: training loss 0.0858\n",
      "2025-06-01 14:49:34 [INFO]: epoch 170: training loss 0.0624\n",
      "2025-06-01 14:49:34 [INFO]: epoch 171: training loss 0.0723\n",
      "2025-06-01 14:49:34 [INFO]: epoch 172: training loss 0.0799\n",
      "2025-06-01 14:49:34 [INFO]: epoch 173: training loss 0.0687\n",
      "2025-06-01 14:49:34 [INFO]: epoch 174: training loss 0.0779\n",
      "2025-06-01 14:49:34 [INFO]: epoch 175: training loss 0.0718\n",
      "2025-06-01 14:49:34 [INFO]: epoch 176: training loss 0.0669\n",
      "2025-06-01 14:49:34 [INFO]: epoch 177: training loss 0.0675\n",
      "2025-06-01 14:49:34 [INFO]: epoch 178: training loss 0.0714\n",
      "2025-06-01 14:49:34 [INFO]: epoch 179: training loss 0.0657\n",
      "2025-06-01 14:49:34 [INFO]: epoch 180: training loss 0.0652\n",
      "2025-06-01 14:49:34 [INFO]: epoch 181: training loss 0.0698\n",
      "2025-06-01 14:49:34 [INFO]: epoch 182: training loss 0.0713\n",
      "2025-06-01 14:49:34 [INFO]: epoch 183: training loss 0.0588\n",
      "2025-06-01 14:49:34 [INFO]: epoch 184: training loss 0.0596\n",
      "2025-06-01 14:49:35 [INFO]: epoch 185: training loss 0.0581\n",
      "2025-06-01 14:49:35 [INFO]: epoch 186: training loss 0.0626\n",
      "2025-06-01 14:49:35 [INFO]: epoch 187: training loss 0.0631\n",
      "2025-06-01 14:49:35 [INFO]: epoch 188: training loss 0.0672\n",
      "2025-06-01 14:49:35 [INFO]: epoch 189: training loss 0.0699\n",
      "2025-06-01 14:49:35 [INFO]: epoch 190: training loss 0.0633\n",
      "2025-06-01 14:49:35 [INFO]: epoch 191: training loss 0.0554\n",
      "2025-06-01 14:49:35 [INFO]: epoch 192: training loss 0.0570\n",
      "2025-06-01 14:49:35 [INFO]: epoch 193: training loss 0.0632\n",
      "2025-06-01 14:49:35 [INFO]: epoch 194: training loss 0.0589\n",
      "2025-06-01 14:49:35 [INFO]: epoch 195: training loss 0.0547\n",
      "2025-06-01 14:49:35 [INFO]: epoch 196: training loss 0.0601\n",
      "2025-06-01 14:49:35 [INFO]: epoch 197: training loss 0.0648\n",
      "2025-06-01 14:49:35 [INFO]: epoch 198: training loss 0.0616\n",
      "2025-06-01 14:49:35 [INFO]: epoch 199: training loss 0.0566\n",
      "2025-06-01 14:49:35 [INFO]: epoch 200: training loss 0.0669\n",
      "2025-06-01 14:49:35 [INFO]: epoch 201: training loss 0.0682\n",
      "2025-06-01 14:49:35 [INFO]: epoch 202: training loss 0.0590\n",
      "2025-06-01 14:49:35 [INFO]: epoch 203: training loss 0.0522\n",
      "2025-06-01 14:49:35 [INFO]: epoch 204: training loss 0.0688\n",
      "2025-06-01 14:49:35 [INFO]: epoch 205: training loss 0.0656\n",
      "2025-06-01 14:49:35 [INFO]: epoch 206: training loss 0.0516\n",
      "2025-06-01 14:49:35 [INFO]: epoch 207: training loss 0.0554\n",
      "2025-06-01 14:49:35 [INFO]: epoch 208: training loss 0.0557\n",
      "2025-06-01 14:49:35 [INFO]: epoch 209: training loss 0.0488\n",
      "2025-06-01 14:49:35 [INFO]: epoch 210: training loss 0.0452\n",
      "2025-06-01 14:49:35 [INFO]: epoch 211: training loss 0.0551\n",
      "2025-06-01 14:49:35 [INFO]: epoch 212: training loss 0.0536\n",
      "2025-06-01 14:49:35 [INFO]: epoch 213: training loss 0.0522\n",
      "2025-06-01 14:49:35 [INFO]: epoch 214: training loss 0.0511\n",
      "2025-06-01 14:49:35 [INFO]: epoch 215: training loss 0.0524\n",
      "2025-06-01 14:49:35 [INFO]: epoch 216: training loss 0.0513\n",
      "2025-06-01 14:49:35 [INFO]: epoch 217: training loss 0.0519\n",
      "2025-06-01 14:49:35 [INFO]: epoch 218: training loss 0.0430\n",
      "2025-06-01 14:49:35 [INFO]: epoch 219: training loss 0.0401\n",
      "2025-06-01 14:49:35 [INFO]: epoch 220: training loss 0.0511\n",
      "2025-06-01 14:49:35 [INFO]: epoch 221: training loss 0.0618\n",
      "2025-06-01 14:49:35 [INFO]: epoch 222: training loss 0.0516\n",
      "2025-06-01 14:49:35 [INFO]: epoch 223: training loss 0.0548\n",
      "2025-06-01 14:49:35 [INFO]: epoch 224: training loss 0.0546\n",
      "2025-06-01 14:49:35 [INFO]: epoch 225: training loss 0.0565\n",
      "2025-06-01 14:49:35 [INFO]: epoch 226: training loss 0.0716\n",
      "2025-06-01 14:49:35 [INFO]: epoch 227: training loss 0.0571\n",
      "2025-06-01 14:49:35 [INFO]: epoch 228: training loss 0.0526\n",
      "2025-06-01 14:49:35 [INFO]: epoch 229: training loss 0.0651\n",
      "2025-06-01 14:49:35 [INFO]: epoch 230: training loss 0.0589\n",
      "2025-06-01 14:49:35 [INFO]: epoch 231: training loss 0.0544\n",
      "2025-06-01 14:49:35 [INFO]: epoch 232: training loss 0.0560\n",
      "2025-06-01 14:49:35 [INFO]: epoch 233: training loss 0.0650\n",
      "2025-06-01 14:49:35 [INFO]: epoch 234: training loss 0.0590\n",
      "2025-06-01 14:49:35 [INFO]: epoch 235: training loss 0.0533\n",
      "2025-06-01 14:49:35 [INFO]: epoch 236: training loss 0.0684\n",
      "2025-06-01 14:49:35 [INFO]: epoch 237: training loss 0.0636\n",
      "2025-06-01 14:49:35 [INFO]: epoch 238: training loss 0.0638\n",
      "2025-06-01 14:49:35 [INFO]: epoch 239: training loss 0.0527\n",
      "2025-06-01 14:49:35 [INFO]: epoch 240: training loss 0.0643\n",
      "2025-06-01 14:49:35 [INFO]: epoch 241: training loss 0.0638\n",
      "2025-06-01 14:49:35 [INFO]: epoch 242: training loss 0.0713\n",
      "2025-06-01 14:49:35 [INFO]: epoch 243: training loss 0.0537\n",
      "2025-06-01 14:49:35 [INFO]: epoch 244: training loss 0.0591\n",
      "2025-06-01 14:49:35 [INFO]: epoch 245: training loss 0.0604\n",
      "2025-06-01 14:49:35 [INFO]: epoch 246: training loss 0.0597\n",
      "2025-06-01 14:49:35 [INFO]: epoch 247: training loss 0.0559\n",
      "2025-06-01 14:49:35 [INFO]: epoch 248: training loss 0.0534\n",
      "2025-06-01 14:49:35 [INFO]: epoch 249: training loss 0.0623\n",
      "2025-06-01 14:49:35 [INFO]: epoch 250: training loss 0.0674\n",
      "2025-06-01 14:49:35 [INFO]: epoch 251: training loss 0.0562\n",
      "2025-06-01 14:49:35 [INFO]: epoch 252: training loss 0.0514\n",
      "2025-06-01 14:49:35 [INFO]: epoch 253: training loss 0.0538\n",
      "2025-06-01 14:49:35 [INFO]: epoch 254: training loss 0.0493\n",
      "2025-06-01 14:49:35 [INFO]: epoch 255: training loss 0.0528\n",
      "2025-06-01 14:49:35 [INFO]: epoch 256: training loss 0.0493\n",
      "2025-06-01 14:49:35 [INFO]: epoch 257: training loss 0.0510\n",
      "2025-06-01 14:49:35 [INFO]: epoch 258: training loss 0.0519\n",
      "2025-06-01 14:49:35 [INFO]: epoch 259: training loss 0.0557\n",
      "2025-06-01 14:49:36 [INFO]: epoch 260: training loss 0.0387\n",
      "2025-06-01 14:49:36 [INFO]: epoch 261: training loss 0.0546\n",
      "2025-06-01 14:49:36 [INFO]: epoch 262: training loss 0.0443\n",
      "2025-06-01 14:49:36 [INFO]: epoch 263: training loss 0.0418\n",
      "2025-06-01 14:49:36 [INFO]: epoch 264: training loss 0.0460\n",
      "2025-06-01 14:49:36 [INFO]: epoch 265: training loss 0.0416\n",
      "2025-06-01 14:49:36 [INFO]: epoch 266: training loss 0.0463\n",
      "2025-06-01 14:49:36 [INFO]: epoch 267: training loss 0.0487\n",
      "2025-06-01 14:49:36 [INFO]: epoch 268: training loss 0.0453\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-01 14:49:36 [INFO]: epoch 269: training loss 0.0386\n",
      "2025-06-01 14:49:36 [INFO]: epoch 270: training loss 0.0435\n",
      "2025-06-01 14:49:36 [INFO]: epoch 271: training loss 0.0463\n",
      "2025-06-01 14:49:36 [INFO]: epoch 272: training loss 0.0467\n",
      "2025-06-01 14:49:36 [INFO]: epoch 273: training loss 0.0481\n",
      "2025-06-01 14:49:36 [INFO]: epoch 274: training loss 0.0434\n",
      "2025-06-01 14:49:36 [INFO]: epoch 275: training loss 0.0581\n",
      "2025-06-01 14:49:36 [INFO]: epoch 276: training loss 0.0544\n",
      "2025-06-01 14:49:36 [INFO]: epoch 277: training loss 0.0443\n",
      "2025-06-01 14:49:36 [INFO]: epoch 278: training loss 0.0542\n",
      "2025-06-01 14:49:36 [INFO]: epoch 279: training loss 0.0593\n",
      "2025-06-01 14:49:36 [INFO]: epoch 280: training loss 0.0481\n",
      "2025-06-01 14:49:36 [INFO]: epoch 281: training loss 0.0406\n",
      "2025-06-01 14:49:36 [INFO]: epoch 282: training loss 0.0503\n",
      "2025-06-01 14:49:36 [INFO]: epoch 283: training loss 0.0473\n",
      "2025-06-01 14:49:36 [INFO]: epoch 284: training loss 0.0419\n",
      "2025-06-01 14:49:36 [INFO]: epoch 285: training loss 0.0494\n",
      "2025-06-01 14:49:36 [INFO]: epoch 286: training loss 0.0480\n",
      "2025-06-01 14:49:36 [INFO]: epoch 287: training loss 0.0444\n",
      "2025-06-01 14:49:36 [INFO]: epoch 288: training loss 0.0480\n",
      "2025-06-01 14:49:36 [INFO]: epoch 289: training loss 0.0432\n",
      "2025-06-01 14:49:36 [INFO]: epoch 290: training loss 0.0370\n",
      "2025-06-01 14:49:36 [INFO]: epoch 291: training loss 0.0432\n",
      "2025-06-01 14:49:36 [INFO]: epoch 292: training loss 0.0400\n",
      "2025-06-01 14:49:36 [INFO]: epoch 293: training loss 0.0322\n",
      "2025-06-01 14:49:36 [INFO]: epoch 294: training loss 0.0376\n",
      "2025-06-01 14:49:36 [INFO]: epoch 295: training loss 0.0394\n",
      "2025-06-01 14:49:36 [INFO]: epoch 296: training loss 0.0416\n",
      "2025-06-01 14:49:36 [INFO]: epoch 297: training loss 0.0394\n",
      "2025-06-01 14:49:36 [INFO]: epoch 298: training loss 0.0437\n",
      "2025-06-01 14:49:36 [INFO]: epoch 299: training loss 0.0432\n",
      "2025-06-01 14:49:36 [INFO]: epoch 300: training loss 0.0392\n",
      "2025-06-01 14:49:36 [INFO]: epoch 301: training loss 0.0421\n",
      "2025-06-01 14:49:36 [INFO]: epoch 302: training loss 0.0384\n",
      "2025-06-01 14:49:36 [INFO]: epoch 303: training loss 0.0367\n",
      "2025-06-01 14:49:36 [INFO]: epoch 304: training loss 0.0370\n",
      "2025-06-01 14:49:36 [INFO]: epoch 305: training loss 0.0446\n",
      "2025-06-01 14:49:36 [INFO]: epoch 306: training loss 0.0354\n",
      "2025-06-01 14:49:36 [INFO]: epoch 307: training loss 0.0377\n",
      "2025-06-01 14:49:36 [INFO]: epoch 308: training loss 0.0389\n",
      "2025-06-01 14:49:36 [INFO]: epoch 309: training loss 0.0504\n",
      "2025-06-01 14:49:36 [INFO]: epoch 310: training loss 0.0464\n",
      "2025-06-01 14:49:36 [INFO]: epoch 311: training loss 0.0380\n",
      "2025-06-01 14:49:36 [INFO]: epoch 312: training loss 0.0445\n",
      "2025-06-01 14:49:36 [INFO]: epoch 313: training loss 0.0617\n",
      "2025-06-01 14:49:36 [INFO]: epoch 314: training loss 0.0436\n",
      "2025-06-01 14:49:36 [INFO]: epoch 315: training loss 0.0392\n",
      "2025-06-01 14:49:36 [INFO]: epoch 316: training loss 0.0480\n",
      "2025-06-01 14:49:36 [INFO]: epoch 317: training loss 0.0638\n",
      "2025-06-01 14:49:36 [INFO]: epoch 318: training loss 0.0445\n",
      "2025-06-01 14:49:36 [INFO]: epoch 319: training loss 0.0361\n",
      "2025-06-01 14:49:36 [INFO]: epoch 320: training loss 0.0455\n",
      "2025-06-01 14:49:36 [INFO]: epoch 321: training loss 0.0515\n",
      "2025-06-01 14:49:36 [INFO]: epoch 322: training loss 0.0381\n",
      "2025-06-01 14:49:36 [INFO]: epoch 323: training loss 0.0403\n",
      "2025-06-01 14:49:36 [INFO]: epoch 324: training loss 0.0389\n",
      "2025-06-01 14:49:36 [INFO]: epoch 325: training loss 0.0412\n",
      "2025-06-01 14:49:36 [INFO]: epoch 326: training loss 0.0432\n",
      "2025-06-01 14:49:36 [INFO]: epoch 327: training loss 0.0446\n",
      "2025-06-01 14:49:36 [INFO]: epoch 328: training loss 0.0432\n",
      "2025-06-01 14:49:36 [INFO]: epoch 329: training loss 0.0401\n",
      "2025-06-01 14:49:36 [INFO]: epoch 330: training loss 0.0481\n",
      "2025-06-01 14:49:36 [INFO]: epoch 331: training loss 0.0495\n",
      "2025-06-01 14:49:36 [INFO]: epoch 332: training loss 0.0387\n",
      "2025-06-01 14:49:36 [INFO]: epoch 333: training loss 0.0502\n",
      "2025-06-01 14:49:36 [INFO]: epoch 334: training loss 0.0407\n",
      "2025-06-01 14:49:36 [INFO]: epoch 335: training loss 0.0359\n",
      "2025-06-01 14:49:37 [INFO]: epoch 336: training loss 0.0457\n",
      "2025-06-01 14:49:37 [INFO]: epoch 337: training loss 0.0426\n",
      "2025-06-01 14:49:37 [INFO]: epoch 338: training loss 0.0446\n",
      "2025-06-01 14:49:37 [INFO]: epoch 339: training loss 0.0364\n",
      "2025-06-01 14:49:37 [INFO]: epoch 340: training loss 0.0431\n",
      "2025-06-01 14:49:37 [INFO]: epoch 341: training loss 0.0360\n",
      "2025-06-01 14:49:37 [INFO]: epoch 342: training loss 0.0368\n",
      "2025-06-01 14:49:37 [INFO]: epoch 343: training loss 0.0406\n",
      "2025-06-01 14:49:37 [INFO]: epoch 344: training loss 0.0343\n",
      "2025-06-01 14:49:37 [INFO]: epoch 345: training loss 0.0371\n",
      "2025-06-01 14:49:37 [INFO]: epoch 346: training loss 0.0380\n",
      "2025-06-01 14:49:37 [INFO]: epoch 347: training loss 0.0329\n",
      "2025-06-01 14:49:37 [INFO]: epoch 348: training loss 0.0311\n",
      "2025-06-01 14:49:37 [INFO]: epoch 349: training loss 0.0353\n",
      "2025-06-01 14:49:37 [INFO]: epoch 350: training loss 0.0394\n",
      "2025-06-01 14:49:37 [INFO]: epoch 351: training loss 0.0342\n",
      "2025-06-01 14:49:37 [INFO]: epoch 352: training loss 0.0321\n",
      "2025-06-01 14:49:37 [INFO]: epoch 353: training loss 0.0318\n",
      "2025-06-01 14:49:37 [INFO]: epoch 354: training loss 0.0384\n",
      "2025-06-01 14:49:37 [INFO]: epoch 355: training loss 0.0385\n",
      "2025-06-01 14:49:37 [INFO]: epoch 356: training loss 0.0313\n",
      "2025-06-01 14:49:37 [INFO]: epoch 357: training loss 0.0365\n",
      "2025-06-01 14:49:37 [INFO]: epoch 358: training loss 0.0381\n",
      "2025-06-01 14:49:37 [INFO]: epoch 359: training loss 0.0321\n",
      "2025-06-01 14:49:37 [INFO]: epoch 360: training loss 0.0303\n",
      "2025-06-01 14:49:37 [INFO]: epoch 361: training loss 0.0431\n",
      "2025-06-01 14:49:37 [INFO]: epoch 362: training loss 0.0422\n",
      "2025-06-01 14:49:37 [INFO]: epoch 363: training loss 0.0326\n",
      "2025-06-01 14:49:37 [INFO]: epoch 364: training loss 0.0402\n",
      "2025-06-01 14:49:37 [INFO]: epoch 365: training loss 0.0490\n",
      "2025-06-01 14:49:37 [INFO]: epoch 366: training loss 0.0357\n",
      "2025-06-01 14:49:37 [INFO]: epoch 367: training loss 0.0379\n",
      "2025-06-01 14:49:37 [INFO]: epoch 368: training loss 0.0459\n",
      "2025-06-01 14:49:37 [INFO]: epoch 369: training loss 0.0460\n",
      "2025-06-01 14:49:37 [INFO]: epoch 370: training loss 0.0364\n",
      "2025-06-01 14:49:37 [INFO]: epoch 371: training loss 0.0531\n",
      "2025-06-01 14:49:37 [INFO]: epoch 372: training loss 0.0410\n",
      "2025-06-01 14:49:37 [INFO]: epoch 373: training loss 0.0334\n",
      "2025-06-01 14:49:37 [INFO]: epoch 374: training loss 0.0423\n",
      "2025-06-01 14:49:37 [INFO]: epoch 375: training loss 0.0367\n",
      "2025-06-01 14:49:37 [INFO]: epoch 376: training loss 0.0319\n",
      "2025-06-01 14:49:37 [INFO]: epoch 377: training loss 0.0367\n",
      "2025-06-01 14:49:37 [INFO]: epoch 378: training loss 0.0322\n",
      "2025-06-01 14:49:37 [INFO]: epoch 379: training loss 0.0420\n",
      "2025-06-01 14:49:37 [INFO]: epoch 380: training loss 0.0438\n",
      "2025-06-01 14:49:37 [INFO]: epoch 381: training loss 0.0356\n",
      "2025-06-01 14:49:37 [INFO]: epoch 382: training loss 0.0429\n",
      "2025-06-01 14:49:37 [INFO]: epoch 383: training loss 0.0382\n",
      "2025-06-01 14:49:37 [INFO]: epoch 384: training loss 0.0353\n",
      "2025-06-01 14:49:37 [INFO]: epoch 385: training loss 0.0383\n",
      "2025-06-01 14:49:37 [INFO]: epoch 386: training loss 0.0378\n",
      "2025-06-01 14:49:37 [INFO]: epoch 387: training loss 0.0409\n",
      "2025-06-01 14:49:37 [INFO]: epoch 388: training loss 0.0339\n",
      "2025-06-01 14:49:37 [INFO]: epoch 389: training loss 0.0343\n",
      "2025-06-01 14:49:37 [INFO]: epoch 390: training loss 0.0392\n",
      "2025-06-01 14:49:37 [INFO]: epoch 391: training loss 0.0340\n",
      "2025-06-01 14:49:37 [INFO]: epoch 392: training loss 0.0411\n",
      "2025-06-01 14:49:37 [INFO]: epoch 393: training loss 0.0380\n",
      "2025-06-01 14:49:37 [INFO]: epoch 394: training loss 0.0350\n",
      "2025-06-01 14:49:37 [INFO]: epoch 395: training loss 0.0404\n",
      "2025-06-01 14:49:37 [INFO]: epoch 396: training loss 0.0383\n",
      "2025-06-01 14:49:37 [INFO]: epoch 397: training loss 0.0413\n",
      "2025-06-01 14:49:37 [INFO]: epoch 398: training loss 0.0425\n",
      "2025-06-01 14:49:37 [INFO]: epoch 399: training loss 0.0417\n",
      "2025-06-01 14:49:37 [INFO]: Finished training.\n",
      "2025-06-01 14:49:37 [WARNING]: 🚨DeprecationWarning: The method impute is deprecated. Please use `predict` instead.\n",
      " 17%|██████████████                                                                      | 1/6 [00:05<00:27,  5.47s/it]2025-06-01 14:49:37 [INFO]: No given device, using default device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-01 14:49:37 [WARNING]: saving_path not given. Model files and tensorboard file will not be saved.\n",
      "2025-06-01 14:49:37 [INFO]: Model initialized successfully with the number of trainable parameters: 597,780\n",
      "2025-06-01 14:49:37 [INFO]: epoch 0: training loss 1.1712\n",
      "2025-06-01 14:49:37 [INFO]: epoch 1: training loss 0.6002\n",
      "2025-06-01 14:49:37 [INFO]: epoch 2: training loss 0.5720\n",
      "2025-06-01 14:49:37 [INFO]: epoch 3: training loss 0.5631\n",
      "2025-06-01 14:49:37 [INFO]: epoch 4: training loss 0.5904\n",
      "2025-06-01 14:49:38 [INFO]: epoch 5: training loss 0.5683\n",
      "2025-06-01 14:49:38 [INFO]: epoch 6: training loss 0.5111\n",
      "2025-06-01 14:49:38 [INFO]: epoch 7: training loss 0.4287\n",
      "2025-06-01 14:49:38 [INFO]: epoch 8: training loss 0.4858\n",
      "2025-06-01 14:49:38 [INFO]: epoch 9: training loss 0.4511\n",
      "2025-06-01 14:49:38 [INFO]: epoch 10: training loss 0.4886\n",
      "2025-06-01 14:49:38 [INFO]: epoch 11: training loss 0.4549\n",
      "2025-06-01 14:49:38 [INFO]: epoch 12: training loss 0.5273\n",
      "2025-06-01 14:49:38 [INFO]: epoch 13: training loss 0.4443\n",
      "2025-06-01 14:49:38 [INFO]: epoch 14: training loss 0.4097\n",
      "2025-06-01 14:49:38 [INFO]: epoch 15: training loss 0.4103\n",
      "2025-06-01 14:49:38 [INFO]: epoch 16: training loss 0.3898\n",
      "2025-06-01 14:49:38 [INFO]: epoch 17: training loss 0.4019\n",
      "2025-06-01 14:49:38 [INFO]: epoch 18: training loss 0.4069\n",
      "2025-06-01 14:49:38 [INFO]: epoch 19: training loss 0.4290\n",
      "2025-06-01 14:49:38 [INFO]: epoch 20: training loss 0.4104\n",
      "2025-06-01 14:49:38 [INFO]: epoch 21: training loss 0.3941\n",
      "2025-06-01 14:49:38 [INFO]: epoch 22: training loss 0.3861\n",
      "2025-06-01 14:49:38 [INFO]: epoch 23: training loss 0.3623\n",
      "2025-06-01 14:49:38 [INFO]: epoch 24: training loss 0.4030\n",
      "2025-06-01 14:49:38 [INFO]: epoch 25: training loss 0.3847\n",
      "2025-06-01 14:49:38 [INFO]: epoch 26: training loss 0.3745\n",
      "2025-06-01 14:49:38 [INFO]: epoch 27: training loss 0.3745\n",
      "2025-06-01 14:49:38 [INFO]: epoch 28: training loss 0.3886\n",
      "2025-06-01 14:49:38 [INFO]: epoch 29: training loss 0.4029\n",
      "2025-06-01 14:49:38 [INFO]: epoch 30: training loss 0.4163\n",
      "2025-06-01 14:49:38 [INFO]: epoch 31: training loss 0.3887\n",
      "2025-06-01 14:49:38 [INFO]: epoch 32: training loss 0.3960\n",
      "2025-06-01 14:49:38 [INFO]: epoch 33: training loss 0.3827\n",
      "2025-06-01 14:49:38 [INFO]: epoch 34: training loss 0.3861\n",
      "2025-06-01 14:49:38 [INFO]: epoch 35: training loss 0.3896\n",
      "2025-06-01 14:49:38 [INFO]: epoch 36: training loss 0.3928\n",
      "2025-06-01 14:49:38 [INFO]: epoch 37: training loss 0.3689\n",
      "2025-06-01 14:49:38 [INFO]: epoch 38: training loss 0.3883\n",
      "2025-06-01 14:49:38 [INFO]: epoch 39: training loss 0.3791\n",
      "2025-06-01 14:49:38 [INFO]: epoch 40: training loss 0.3817\n",
      "2025-06-01 14:49:38 [INFO]: epoch 41: training loss 0.3683\n",
      "2025-06-01 14:49:38 [INFO]: epoch 42: training loss 0.3812\n",
      "2025-06-01 14:49:38 [INFO]: epoch 43: training loss 0.3630\n",
      "2025-06-01 14:49:38 [INFO]: epoch 44: training loss 0.3705\n",
      "2025-06-01 14:49:38 [INFO]: epoch 45: training loss 0.3373\n",
      "2025-06-01 14:49:38 [INFO]: epoch 46: training loss 0.3543\n",
      "2025-06-01 14:49:38 [INFO]: epoch 47: training loss 0.3430\n",
      "2025-06-01 14:49:38 [INFO]: epoch 48: training loss 0.3541\n",
      "2025-06-01 14:49:38 [INFO]: epoch 49: training loss 0.3302\n",
      "2025-06-01 14:49:38 [INFO]: epoch 50: training loss 0.3595\n",
      "2025-06-01 14:49:38 [INFO]: epoch 51: training loss 0.3565\n",
      "2025-06-01 14:49:38 [INFO]: epoch 52: training loss 0.3609\n",
      "2025-06-01 14:49:38 [INFO]: epoch 53: training loss 0.3481\n",
      "2025-06-01 14:49:38 [INFO]: epoch 54: training loss 0.3197\n",
      "2025-06-01 14:49:38 [INFO]: epoch 55: training loss 0.3348\n",
      "2025-06-01 14:49:38 [INFO]: epoch 56: training loss 0.3582\n",
      "2025-06-01 14:49:38 [INFO]: epoch 57: training loss 0.3649\n",
      "2025-06-01 14:49:38 [INFO]: epoch 58: training loss 0.3543\n",
      "2025-06-01 14:49:38 [INFO]: epoch 59: training loss 0.3293\n",
      "2025-06-01 14:49:38 [INFO]: epoch 60: training loss 0.3110\n",
      "2025-06-01 14:49:38 [INFO]: epoch 61: training loss 0.3201\n",
      "2025-06-01 14:49:38 [INFO]: epoch 62: training loss 0.3566\n",
      "2025-06-01 14:49:38 [INFO]: epoch 63: training loss 0.3247\n",
      "2025-06-01 14:49:38 [INFO]: epoch 64: training loss 0.3330\n",
      "2025-06-01 14:49:38 [INFO]: epoch 65: training loss 0.3341\n",
      "2025-06-01 14:49:38 [INFO]: epoch 66: training loss 0.3274\n",
      "2025-06-01 14:49:38 [INFO]: epoch 67: training loss 0.3053\n",
      "2025-06-01 14:49:38 [INFO]: epoch 68: training loss 0.3083\n",
      "2025-06-01 14:49:38 [INFO]: epoch 69: training loss 0.2962\n",
      "2025-06-01 14:49:38 [INFO]: epoch 70: training loss 0.3048\n",
      "2025-06-01 14:49:38 [INFO]: epoch 71: training loss 0.3278\n",
      "2025-06-01 14:49:38 [INFO]: epoch 72: training loss 0.3331\n",
      "2025-06-01 14:49:38 [INFO]: epoch 73: training loss 0.2937\n",
      "2025-06-01 14:49:38 [INFO]: epoch 74: training loss 0.3128\n",
      "2025-06-01 14:49:38 [INFO]: epoch 75: training loss 0.3055\n",
      "2025-06-01 14:49:38 [INFO]: epoch 76: training loss 0.3068\n",
      "2025-06-01 14:49:38 [INFO]: epoch 77: training loss 0.3008\n",
      "2025-06-01 14:49:38 [INFO]: epoch 78: training loss 0.3039\n",
      "2025-06-01 14:49:38 [INFO]: epoch 79: training loss 0.3047\n",
      "2025-06-01 14:49:39 [INFO]: epoch 80: training loss 0.3093\n",
      "2025-06-01 14:49:39 [INFO]: epoch 81: training loss 0.3134\n",
      "2025-06-01 14:49:39 [INFO]: epoch 82: training loss 0.2980\n",
      "2025-06-01 14:49:39 [INFO]: epoch 83: training loss 0.2868\n",
      "2025-06-01 14:49:39 [INFO]: epoch 84: training loss 0.3038\n",
      "2025-06-01 14:49:39 [INFO]: epoch 85: training loss 0.3058\n",
      "2025-06-01 14:49:39 [INFO]: epoch 86: training loss 0.3006\n",
      "2025-06-01 14:49:39 [INFO]: epoch 87: training loss 0.2780\n",
      "2025-06-01 14:49:39 [INFO]: epoch 88: training loss 0.3044\n",
      "2025-06-01 14:49:39 [INFO]: epoch 89: training loss 0.3024\n",
      "2025-06-01 14:49:39 [INFO]: epoch 90: training loss 0.2882\n",
      "2025-06-01 14:49:39 [INFO]: epoch 91: training loss 0.2876\n",
      "2025-06-01 14:49:39 [INFO]: epoch 92: training loss 0.2988\n",
      "2025-06-01 14:49:39 [INFO]: epoch 93: training loss 0.2874\n",
      "2025-06-01 14:49:39 [INFO]: epoch 94: training loss 0.2891\n",
      "2025-06-01 14:49:39 [INFO]: epoch 95: training loss 0.2843\n",
      "2025-06-01 14:49:39 [INFO]: epoch 96: training loss 0.2809\n",
      "2025-06-01 14:49:39 [INFO]: epoch 97: training loss 0.2962\n",
      "2025-06-01 14:49:39 [INFO]: epoch 98: training loss 0.2804\n",
      "2025-06-01 14:49:39 [INFO]: epoch 99: training loss 0.2782\n",
      "2025-06-01 14:49:39 [INFO]: epoch 100: training loss 0.3042\n",
      "2025-06-01 14:49:39 [INFO]: epoch 101: training loss 0.3155\n",
      "2025-06-01 14:49:39 [INFO]: epoch 102: training loss 0.3000\n",
      "2025-06-01 14:49:39 [INFO]: epoch 103: training loss 0.2697\n",
      "2025-06-01 14:49:39 [INFO]: epoch 104: training loss 0.2664\n",
      "2025-06-01 14:49:39 [INFO]: epoch 105: training loss 0.3013\n",
      "2025-06-01 14:49:39 [INFO]: epoch 106: training loss 0.2967\n",
      "2025-06-01 14:49:39 [INFO]: epoch 107: training loss 0.3230\n",
      "2025-06-01 14:49:39 [INFO]: epoch 108: training loss 0.2791\n",
      "2025-06-01 14:49:39 [INFO]: epoch 109: training loss 0.2796\n",
      "2025-06-01 14:49:39 [INFO]: epoch 110: training loss 0.2715\n",
      "2025-06-01 14:49:39 [INFO]: epoch 111: training loss 0.2634\n",
      "2025-06-01 14:49:39 [INFO]: epoch 112: training loss 0.2760\n",
      "2025-06-01 14:49:39 [INFO]: epoch 113: training loss 0.2812\n",
      "2025-06-01 14:49:39 [INFO]: epoch 114: training loss 0.2643\n",
      "2025-06-01 14:49:39 [INFO]: epoch 115: training loss 0.2575\n",
      "2025-06-01 14:49:39 [INFO]: epoch 116: training loss 0.2677\n",
      "2025-06-01 14:49:39 [INFO]: epoch 117: training loss 0.2873\n",
      "2025-06-01 14:49:39 [INFO]: epoch 118: training loss 0.2841\n",
      "2025-06-01 14:49:39 [INFO]: epoch 119: training loss 0.2733\n",
      "2025-06-01 14:49:39 [INFO]: epoch 120: training loss 0.2755\n",
      "2025-06-01 14:49:39 [INFO]: epoch 121: training loss 0.2695\n",
      "2025-06-01 14:49:39 [INFO]: epoch 122: training loss 0.2737\n",
      "2025-06-01 14:49:39 [INFO]: epoch 123: training loss 0.2730\n",
      "2025-06-01 14:49:39 [INFO]: epoch 124: training loss 0.2872\n",
      "2025-06-01 14:49:39 [INFO]: epoch 125: training loss 0.2812\n",
      "2025-06-01 14:49:39 [INFO]: epoch 126: training loss 0.2681\n",
      "2025-06-01 14:49:39 [INFO]: epoch 127: training loss 0.2587\n",
      "2025-06-01 14:49:39 [INFO]: epoch 128: training loss 0.2659\n",
      "2025-06-01 14:49:39 [INFO]: epoch 129: training loss 0.2854\n",
      "2025-06-01 14:49:39 [INFO]: epoch 130: training loss 0.2544\n",
      "2025-06-01 14:49:39 [INFO]: epoch 131: training loss 0.2482\n",
      "2025-06-01 14:49:39 [INFO]: epoch 132: training loss 0.2585\n",
      "2025-06-01 14:49:39 [INFO]: epoch 133: training loss 0.2463\n",
      "2025-06-01 14:49:39 [INFO]: epoch 134: training loss 0.2831\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-01 14:49:39 [INFO]: epoch 135: training loss 0.2842\n",
      "2025-06-01 14:49:39 [INFO]: epoch 136: training loss 0.2644\n",
      "2025-06-01 14:49:39 [INFO]: epoch 137: training loss 0.2420\n",
      "2025-06-01 14:49:39 [INFO]: epoch 138: training loss 0.2538\n",
      "2025-06-01 14:49:39 [INFO]: epoch 139: training loss 0.2699\n",
      "2025-06-01 14:49:39 [INFO]: epoch 140: training loss 0.2569\n",
      "2025-06-01 14:49:39 [INFO]: epoch 141: training loss 0.2586\n",
      "2025-06-01 14:49:39 [INFO]: epoch 142: training loss 0.2611\n",
      "2025-06-01 14:49:39 [INFO]: epoch 143: training loss 0.2509\n",
      "2025-06-01 14:49:39 [INFO]: epoch 144: training loss 0.2471\n",
      "2025-06-01 14:49:39 [INFO]: epoch 145: training loss 0.2528\n",
      "2025-06-01 14:49:39 [INFO]: epoch 146: training loss 0.2582\n",
      "2025-06-01 14:49:39 [INFO]: epoch 147: training loss 0.2792\n",
      "2025-06-01 14:49:39 [INFO]: epoch 148: training loss 0.2373\n",
      "2025-06-01 14:49:39 [INFO]: epoch 149: training loss 0.2435\n",
      "2025-06-01 14:49:39 [INFO]: epoch 150: training loss 0.2603\n",
      "2025-06-01 14:49:39 [INFO]: epoch 151: training loss 0.2390\n",
      "2025-06-01 14:49:39 [INFO]: epoch 152: training loss 0.2527\n",
      "2025-06-01 14:49:39 [INFO]: epoch 153: training loss 0.2434\n",
      "2025-06-01 14:49:39 [INFO]: epoch 154: training loss 0.2445\n",
      "2025-06-01 14:49:39 [INFO]: epoch 155: training loss 0.2564\n",
      "2025-06-01 14:49:40 [INFO]: epoch 156: training loss 0.2576\n",
      "2025-06-01 14:49:40 [INFO]: epoch 157: training loss 0.2555\n",
      "2025-06-01 14:49:40 [INFO]: epoch 158: training loss 0.2372\n",
      "2025-06-01 14:49:40 [INFO]: epoch 159: training loss 0.2570\n",
      "2025-06-01 14:49:40 [INFO]: epoch 160: training loss 0.2437\n",
      "2025-06-01 14:49:40 [INFO]: epoch 161: training loss 0.2309\n",
      "2025-06-01 14:49:40 [INFO]: epoch 162: training loss 0.2564\n",
      "2025-06-01 14:49:40 [INFO]: epoch 163: training loss 0.2504\n",
      "2025-06-01 14:49:40 [INFO]: epoch 164: training loss 0.2505\n",
      "2025-06-01 14:49:40 [INFO]: epoch 165: training loss 0.2470\n",
      "2025-06-01 14:49:40 [INFO]: epoch 166: training loss 0.2500\n",
      "2025-06-01 14:49:40 [INFO]: epoch 167: training loss 0.2433\n",
      "2025-06-01 14:49:40 [INFO]: epoch 168: training loss 0.2382\n",
      "2025-06-01 14:49:40 [INFO]: epoch 169: training loss 0.2354\n",
      "2025-06-01 14:49:40 [INFO]: epoch 170: training loss 0.2326\n",
      "2025-06-01 14:49:40 [INFO]: epoch 171: training loss 0.2508\n",
      "2025-06-01 14:49:40 [INFO]: epoch 172: training loss 0.2176\n",
      "2025-06-01 14:49:40 [INFO]: epoch 173: training loss 0.2204\n",
      "2025-06-01 14:49:40 [INFO]: epoch 174: training loss 0.2289\n",
      "2025-06-01 14:49:40 [INFO]: epoch 175: training loss 0.2345\n",
      "2025-06-01 14:49:40 [INFO]: epoch 176: training loss 0.2393\n",
      "2025-06-01 14:49:40 [INFO]: epoch 177: training loss 0.2313\n",
      "2025-06-01 14:49:40 [INFO]: epoch 178: training loss 0.2167\n",
      "2025-06-01 14:49:40 [INFO]: epoch 179: training loss 0.2282\n",
      "2025-06-01 14:49:40 [INFO]: epoch 180: training loss 0.2211\n",
      "2025-06-01 14:49:40 [INFO]: epoch 181: training loss 0.2196\n",
      "2025-06-01 14:49:40 [INFO]: epoch 182: training loss 0.2210\n",
      "2025-06-01 14:49:40 [INFO]: epoch 183: training loss 0.2328\n",
      "2025-06-01 14:49:40 [INFO]: epoch 184: training loss 0.2342\n",
      "2025-06-01 14:49:40 [INFO]: epoch 185: training loss 0.2273\n",
      "2025-06-01 14:49:40 [INFO]: epoch 186: training loss 0.2255\n",
      "2025-06-01 14:49:40 [INFO]: epoch 187: training loss 0.2394\n",
      "2025-06-01 14:49:40 [INFO]: epoch 188: training loss 0.2341\n",
      "2025-06-01 14:49:40 [INFO]: epoch 189: training loss 0.2374\n",
      "2025-06-01 14:49:40 [INFO]: epoch 190: training loss 0.2219\n",
      "2025-06-01 14:49:40 [INFO]: epoch 191: training loss 0.2338\n",
      "2025-06-01 14:49:40 [INFO]: epoch 192: training loss 0.2371\n",
      "2025-06-01 14:49:40 [INFO]: epoch 193: training loss 0.2221\n",
      "2025-06-01 14:49:40 [INFO]: epoch 194: training loss 0.2204\n",
      "2025-06-01 14:49:40 [INFO]: epoch 195: training loss 0.2221\n",
      "2025-06-01 14:49:40 [INFO]: epoch 196: training loss 0.2390\n",
      "2025-06-01 14:49:40 [INFO]: epoch 197: training loss 0.2222\n",
      "2025-06-01 14:49:40 [INFO]: epoch 198: training loss 0.2255\n",
      "2025-06-01 14:49:40 [INFO]: epoch 199: training loss 0.2301\n",
      "2025-06-01 14:49:40 [INFO]: epoch 200: training loss 0.2383\n",
      "2025-06-01 14:49:40 [INFO]: epoch 201: training loss 0.2397\n",
      "2025-06-01 14:49:40 [INFO]: epoch 202: training loss 0.2257\n",
      "2025-06-01 14:49:40 [INFO]: epoch 203: training loss 0.2229\n",
      "2025-06-01 14:49:40 [INFO]: epoch 204: training loss 0.2039\n",
      "2025-06-01 14:49:40 [INFO]: epoch 205: training loss 0.2303\n",
      "2025-06-01 14:49:40 [INFO]: epoch 206: training loss 0.2182\n",
      "2025-06-01 14:49:40 [INFO]: epoch 207: training loss 0.2427\n",
      "2025-06-01 14:49:40 [INFO]: epoch 208: training loss 0.2329\n",
      "2025-06-01 14:49:40 [INFO]: epoch 209: training loss 0.2296\n",
      "2025-06-01 14:49:40 [INFO]: epoch 210: training loss 0.2299\n",
      "2025-06-01 14:49:40 [INFO]: epoch 211: training loss 0.2151\n",
      "2025-06-01 14:49:40 [INFO]: epoch 212: training loss 0.2229\n",
      "2025-06-01 14:49:40 [INFO]: epoch 213: training loss 0.2118\n",
      "2025-06-01 14:49:40 [INFO]: epoch 214: training loss 0.2327\n",
      "2025-06-01 14:49:40 [INFO]: epoch 215: training loss 0.2293\n",
      "2025-06-01 14:49:40 [INFO]: epoch 216: training loss 0.2163\n",
      "2025-06-01 14:49:40 [INFO]: epoch 217: training loss 0.2139\n",
      "2025-06-01 14:49:40 [INFO]: epoch 218: training loss 0.2040\n",
      "2025-06-01 14:49:40 [INFO]: epoch 219: training loss 0.2105\n",
      "2025-06-01 14:49:40 [INFO]: epoch 220: training loss 0.2205\n",
      "2025-06-01 14:49:40 [INFO]: epoch 221: training loss 0.2224\n",
      "2025-06-01 14:49:40 [INFO]: epoch 222: training loss 0.2279\n",
      "2025-06-01 14:49:40 [INFO]: epoch 223: training loss 0.2073\n",
      "2025-06-01 14:49:40 [INFO]: epoch 224: training loss 0.2011\n",
      "2025-06-01 14:49:40 [INFO]: epoch 225: training loss 0.2143\n",
      "2025-06-01 14:49:40 [INFO]: epoch 226: training loss 0.2213\n",
      "2025-06-01 14:49:40 [INFO]: epoch 227: training loss 0.1970\n",
      "2025-06-01 14:49:40 [INFO]: epoch 228: training loss 0.2076\n",
      "2025-06-01 14:49:40 [INFO]: epoch 229: training loss 0.2160\n",
      "2025-06-01 14:49:40 [INFO]: epoch 230: training loss 0.2230\n",
      "2025-06-01 14:49:41 [INFO]: epoch 231: training loss 0.2153\n",
      "2025-06-01 14:49:41 [INFO]: epoch 232: training loss 0.2014\n",
      "2025-06-01 14:49:41 [INFO]: epoch 233: training loss 0.2106\n",
      "2025-06-01 14:49:41 [INFO]: epoch 234: training loss 0.2139\n",
      "2025-06-01 14:49:41 [INFO]: epoch 235: training loss 0.2160\n",
      "2025-06-01 14:49:41 [INFO]: epoch 236: training loss 0.2077\n",
      "2025-06-01 14:49:41 [INFO]: epoch 237: training loss 0.2082\n",
      "2025-06-01 14:49:41 [INFO]: epoch 238: training loss 0.1973\n",
      "2025-06-01 14:49:41 [INFO]: epoch 239: training loss 0.2019\n",
      "2025-06-01 14:49:41 [INFO]: epoch 240: training loss 0.2115\n",
      "2025-06-01 14:49:41 [INFO]: epoch 241: training loss 0.2065\n",
      "2025-06-01 14:49:41 [INFO]: epoch 242: training loss 0.2082\n",
      "2025-06-01 14:49:41 [INFO]: epoch 243: training loss 0.1907\n",
      "2025-06-01 14:49:41 [INFO]: epoch 244: training loss 0.2038\n",
      "2025-06-01 14:49:41 [INFO]: epoch 245: training loss 0.1883\n",
      "2025-06-01 14:49:41 [INFO]: epoch 246: training loss 0.1862\n",
      "2025-06-01 14:49:41 [INFO]: epoch 247: training loss 0.1982\n",
      "2025-06-01 14:49:41 [INFO]: epoch 248: training loss 0.2001\n",
      "2025-06-01 14:49:41 [INFO]: epoch 249: training loss 0.2047\n",
      "2025-06-01 14:49:41 [INFO]: epoch 250: training loss 0.1965\n",
      "2025-06-01 14:49:41 [INFO]: epoch 251: training loss 0.1965\n",
      "2025-06-01 14:49:41 [INFO]: epoch 252: training loss 0.2020\n",
      "2025-06-01 14:49:41 [INFO]: epoch 253: training loss 0.2010\n",
      "2025-06-01 14:49:41 [INFO]: epoch 254: training loss 0.1984\n",
      "2025-06-01 14:49:41 [INFO]: epoch 255: training loss 0.1989\n",
      "2025-06-01 14:49:41 [INFO]: epoch 256: training loss 0.1937\n",
      "2025-06-01 14:49:41 [INFO]: epoch 257: training loss 0.1888\n",
      "2025-06-01 14:49:41 [INFO]: epoch 258: training loss 0.1934\n",
      "2025-06-01 14:49:41 [INFO]: epoch 259: training loss 0.1840\n",
      "2025-06-01 14:49:41 [INFO]: epoch 260: training loss 0.2031\n",
      "2025-06-01 14:49:41 [INFO]: epoch 261: training loss 0.1919\n",
      "2025-06-01 14:49:41 [INFO]: epoch 262: training loss 0.1913\n",
      "2025-06-01 14:49:41 [INFO]: epoch 263: training loss 0.1776\n",
      "2025-06-01 14:49:41 [INFO]: epoch 264: training loss 0.1802\n",
      "2025-06-01 14:49:41 [INFO]: epoch 265: training loss 0.1911\n",
      "2025-06-01 14:49:41 [INFO]: epoch 266: training loss 0.1867\n",
      "2025-06-01 14:49:41 [INFO]: epoch 267: training loss 0.1876\n",
      "2025-06-01 14:49:41 [INFO]: epoch 268: training loss 0.1959\n",
      "2025-06-01 14:49:41 [INFO]: epoch 269: training loss 0.1799\n",
      "2025-06-01 14:49:41 [INFO]: epoch 270: training loss 0.1930\n",
      "2025-06-01 14:49:41 [INFO]: epoch 271: training loss 0.1998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-01 14:49:41 [INFO]: epoch 272: training loss 0.1976\n",
      "2025-06-01 14:49:41 [INFO]: epoch 273: training loss 0.1893\n",
      "2025-06-01 14:49:41 [INFO]: epoch 274: training loss 0.1837\n",
      "2025-06-01 14:49:41 [INFO]: epoch 275: training loss 0.1761\n",
      "2025-06-01 14:49:41 [INFO]: epoch 276: training loss 0.1811\n",
      "2025-06-01 14:49:41 [INFO]: epoch 277: training loss 0.1910\n",
      "2025-06-01 14:49:41 [INFO]: epoch 278: training loss 0.1871\n",
      "2025-06-01 14:49:41 [INFO]: epoch 279: training loss 0.1752\n",
      "2025-06-01 14:49:41 [INFO]: epoch 280: training loss 0.1682\n",
      "2025-06-01 14:49:41 [INFO]: epoch 281: training loss 0.1935\n",
      "2025-06-01 14:49:41 [INFO]: epoch 282: training loss 0.1946\n",
      "2025-06-01 14:49:41 [INFO]: epoch 283: training loss 0.1830\n",
      "2025-06-01 14:49:41 [INFO]: epoch 284: training loss 0.1886\n",
      "2025-06-01 14:49:41 [INFO]: epoch 285: training loss 0.1880\n",
      "2025-06-01 14:49:41 [INFO]: epoch 286: training loss 0.1735\n",
      "2025-06-01 14:49:41 [INFO]: epoch 287: training loss 0.1787\n",
      "2025-06-01 14:49:41 [INFO]: epoch 288: training loss 0.1877\n",
      "2025-06-01 14:49:41 [INFO]: epoch 289: training loss 0.1755\n",
      "2025-06-01 14:49:41 [INFO]: epoch 290: training loss 0.1619\n",
      "2025-06-01 14:49:41 [INFO]: epoch 291: training loss 0.1871\n",
      "2025-06-01 14:49:41 [INFO]: epoch 292: training loss 0.1817\n",
      "2025-06-01 14:49:41 [INFO]: epoch 293: training loss 0.1922\n",
      "2025-06-01 14:49:41 [INFO]: epoch 294: training loss 0.1732\n",
      "2025-06-01 14:49:41 [INFO]: epoch 295: training loss 0.1756\n",
      "2025-06-01 14:49:41 [INFO]: epoch 296: training loss 0.1794\n",
      "2025-06-01 14:49:41 [INFO]: epoch 297: training loss 0.1858\n",
      "2025-06-01 14:49:41 [INFO]: epoch 298: training loss 0.1747\n",
      "2025-06-01 14:49:41 [INFO]: epoch 299: training loss 0.1766\n",
      "2025-06-01 14:49:41 [INFO]: epoch 300: training loss 0.1888\n",
      "2025-06-01 14:49:41 [INFO]: epoch 301: training loss 0.1910\n",
      "2025-06-01 14:49:41 [INFO]: epoch 302: training loss 0.2006\n",
      "2025-06-01 14:49:41 [INFO]: epoch 303: training loss 0.1798\n",
      "2025-06-01 14:49:41 [INFO]: epoch 304: training loss 0.1949\n",
      "2025-06-01 14:49:41 [INFO]: epoch 305: training loss 0.1898\n",
      "2025-06-01 14:49:41 [INFO]: epoch 306: training loss 0.1887\n",
      "2025-06-01 14:49:41 [INFO]: epoch 307: training loss 0.1928\n",
      "2025-06-01 14:49:42 [INFO]: epoch 308: training loss 0.1864\n",
      "2025-06-01 14:49:42 [INFO]: epoch 309: training loss 0.1841\n",
      "2025-06-01 14:49:42 [INFO]: epoch 310: training loss 0.1852\n",
      "2025-06-01 14:49:42 [INFO]: epoch 311: training loss 0.1792\n",
      "2025-06-01 14:49:42 [INFO]: epoch 312: training loss 0.1838\n",
      "2025-06-01 14:49:42 [INFO]: epoch 313: training loss 0.1694\n",
      "2025-06-01 14:49:42 [INFO]: epoch 314: training loss 0.1868\n",
      "2025-06-01 14:49:42 [INFO]: epoch 315: training loss 0.1784\n",
      "2025-06-01 14:49:42 [INFO]: epoch 316: training loss 0.1821\n",
      "2025-06-01 14:49:42 [INFO]: epoch 317: training loss 0.1633\n",
      "2025-06-01 14:49:42 [INFO]: epoch 318: training loss 0.1701\n",
      "2025-06-01 14:49:42 [INFO]: epoch 319: training loss 0.1715\n",
      "2025-06-01 14:49:42 [INFO]: epoch 320: training loss 0.1672\n",
      "2025-06-01 14:49:42 [INFO]: epoch 321: training loss 0.1817\n",
      "2025-06-01 14:49:42 [INFO]: epoch 322: training loss 0.1699\n",
      "2025-06-01 14:49:42 [INFO]: epoch 323: training loss 0.1608\n",
      "2025-06-01 14:49:42 [INFO]: epoch 324: training loss 0.1683\n",
      "2025-06-01 14:49:42 [INFO]: epoch 325: training loss 0.1649\n",
      "2025-06-01 14:49:42 [INFO]: epoch 326: training loss 0.1697\n",
      "2025-06-01 14:49:42 [INFO]: epoch 327: training loss 0.1585\n",
      "2025-06-01 14:49:42 [INFO]: epoch 328: training loss 0.1596\n",
      "2025-06-01 14:49:42 [INFO]: epoch 329: training loss 0.1604\n",
      "2025-06-01 14:49:42 [INFO]: epoch 330: training loss 0.1526\n",
      "2025-06-01 14:49:42 [INFO]: epoch 331: training loss 0.1590\n",
      "2025-06-01 14:49:42 [INFO]: epoch 332: training loss 0.1659\n",
      "2025-06-01 14:49:42 [INFO]: epoch 333: training loss 0.1734\n",
      "2025-06-01 14:49:42 [INFO]: epoch 334: training loss 0.1612\n",
      "2025-06-01 14:49:42 [INFO]: epoch 335: training loss 0.1573\n",
      "2025-06-01 14:49:42 [INFO]: epoch 336: training loss 0.1667\n",
      "2025-06-01 14:49:42 [INFO]: epoch 337: training loss 0.1629\n",
      "2025-06-01 14:49:42 [INFO]: epoch 338: training loss 0.1692\n",
      "2025-06-01 14:49:42 [INFO]: epoch 339: training loss 0.1557\n",
      "2025-06-01 14:49:42 [INFO]: epoch 340: training loss 0.1523\n",
      "2025-06-01 14:49:42 [INFO]: epoch 341: training loss 0.1540\n",
      "2025-06-01 14:49:42 [INFO]: epoch 342: training loss 0.1650\n",
      "2025-06-01 14:49:42 [INFO]: epoch 343: training loss 0.1723\n",
      "2025-06-01 14:49:42 [INFO]: epoch 344: training loss 0.1576\n",
      "2025-06-01 14:49:42 [INFO]: epoch 345: training loss 0.1563\n",
      "2025-06-01 14:49:42 [INFO]: epoch 346: training loss 0.1530\n",
      "2025-06-01 14:49:42 [INFO]: epoch 347: training loss 0.1514\n",
      "2025-06-01 14:49:42 [INFO]: epoch 348: training loss 0.1559\n",
      "2025-06-01 14:49:42 [INFO]: epoch 349: training loss 0.1528\n",
      "2025-06-01 14:49:42 [INFO]: epoch 350: training loss 0.1500\n",
      "2025-06-01 14:49:42 [INFO]: epoch 351: training loss 0.1474\n",
      "2025-06-01 14:49:42 [INFO]: epoch 352: training loss 0.1639\n",
      "2025-06-01 14:49:42 [INFO]: epoch 353: training loss 0.1597\n",
      "2025-06-01 14:49:42 [INFO]: epoch 354: training loss 0.1401\n",
      "2025-06-01 14:49:42 [INFO]: epoch 355: training loss 0.1415\n",
      "2025-06-01 14:49:42 [INFO]: epoch 356: training loss 0.1631\n",
      "2025-06-01 14:49:42 [INFO]: epoch 357: training loss 0.1529\n",
      "2025-06-01 14:49:42 [INFO]: epoch 358: training loss 0.1399\n",
      "2025-06-01 14:49:42 [INFO]: epoch 359: training loss 0.1417\n",
      "2025-06-01 14:49:42 [INFO]: epoch 360: training loss 0.1523\n",
      "2025-06-01 14:49:42 [INFO]: epoch 361: training loss 0.1451\n",
      "2025-06-01 14:49:42 [INFO]: epoch 362: training loss 0.1385\n",
      "2025-06-01 14:49:42 [INFO]: epoch 363: training loss 0.1355\n",
      "2025-06-01 14:49:42 [INFO]: epoch 364: training loss 0.1452\n",
      "2025-06-01 14:49:42 [INFO]: epoch 365: training loss 0.1399\n",
      "2025-06-01 14:49:42 [INFO]: epoch 366: training loss 0.1610\n",
      "2025-06-01 14:49:42 [INFO]: epoch 367: training loss 0.1488\n",
      "2025-06-01 14:49:42 [INFO]: epoch 368: training loss 0.1492\n",
      "2025-06-01 14:49:42 [INFO]: epoch 369: training loss 0.1459\n",
      "2025-06-01 14:49:42 [INFO]: epoch 370: training loss 0.1482\n",
      "2025-06-01 14:49:42 [INFO]: epoch 371: training loss 0.1394\n",
      "2025-06-01 14:49:42 [INFO]: epoch 372: training loss 0.1425\n",
      "2025-06-01 14:49:42 [INFO]: epoch 373: training loss 0.1505\n",
      "2025-06-01 14:49:42 [INFO]: epoch 374: training loss 0.1364\n",
      "2025-06-01 14:49:42 [INFO]: epoch 375: training loss 0.1412\n",
      "2025-06-01 14:49:42 [INFO]: epoch 376: training loss 0.1461\n",
      "2025-06-01 14:49:42 [INFO]: epoch 377: training loss 0.1324\n",
      "2025-06-01 14:49:42 [INFO]: epoch 378: training loss 0.1424\n",
      "2025-06-01 14:49:42 [INFO]: epoch 379: training loss 0.1504\n",
      "2025-06-01 14:49:42 [INFO]: epoch 380: training loss 0.1378\n",
      "2025-06-01 14:49:42 [INFO]: epoch 381: training loss 0.1305\n",
      "2025-06-01 14:49:42 [INFO]: epoch 382: training loss 0.1482\n",
      "2025-06-01 14:49:42 [INFO]: epoch 383: training loss 0.1515\n",
      "2025-06-01 14:49:42 [INFO]: epoch 384: training loss 0.1423\n",
      "2025-06-01 14:49:43 [INFO]: epoch 385: training loss 0.1424\n",
      "2025-06-01 14:49:43 [INFO]: epoch 386: training loss 0.1365\n",
      "2025-06-01 14:49:43 [INFO]: epoch 387: training loss 0.1424\n",
      "2025-06-01 14:49:43 [INFO]: epoch 388: training loss 0.1463\n",
      "2025-06-01 14:49:43 [INFO]: epoch 389: training loss 0.1346\n",
      "2025-06-01 14:49:43 [INFO]: epoch 390: training loss 0.1304\n",
      "2025-06-01 14:49:43 [INFO]: epoch 391: training loss 0.1359\n",
      "2025-06-01 14:49:43 [INFO]: epoch 392: training loss 0.1279\n",
      "2025-06-01 14:49:43 [INFO]: epoch 393: training loss 0.1403\n",
      "2025-06-01 14:49:43 [INFO]: epoch 394: training loss 0.1344\n",
      "2025-06-01 14:49:43 [INFO]: epoch 395: training loss 0.1242\n",
      "2025-06-01 14:49:43 [INFO]: epoch 396: training loss 0.1286\n",
      "2025-06-01 14:49:43 [INFO]: epoch 397: training loss 0.1484\n",
      "2025-06-01 14:49:43 [INFO]: epoch 398: training loss 0.1426\n",
      "2025-06-01 14:49:43 [INFO]: epoch 399: training loss 0.1305\n",
      "2025-06-01 14:49:43 [INFO]: Finished training.\n",
      "2025-06-01 14:49:43 [WARNING]: 🚨DeprecationWarning: The method impute is deprecated. Please use `predict` instead.\n",
      " 33%|████████████████████████████                                                        | 2/6 [00:10<00:21,  5.40s/it]2025-06-01 14:49:43 [INFO]: No given device, using default device: cuda\n",
      "2025-06-01 14:49:43 [WARNING]: saving_path not given. Model files and tensorboard file will not be saved.\n",
      "2025-06-01 14:49:43 [INFO]: Model initialized successfully with the number of trainable parameters: 597,780\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-01 14:49:43 [INFO]: epoch 0: training loss 1.4149\n",
      "2025-06-01 14:49:43 [INFO]: epoch 1: training loss 0.6279\n",
      "2025-06-01 14:49:43 [INFO]: epoch 2: training loss 0.6225\n",
      "2025-06-01 14:49:43 [INFO]: epoch 3: training loss 0.6635\n",
      "2025-06-01 14:49:43 [INFO]: epoch 4: training loss 0.6133\n",
      "2025-06-01 14:49:43 [INFO]: epoch 5: training loss 0.5342\n",
      "2025-06-01 14:49:43 [INFO]: epoch 6: training loss 0.5041\n",
      "2025-06-01 14:49:43 [INFO]: epoch 7: training loss 0.4927\n",
      "2025-06-01 14:49:43 [INFO]: epoch 8: training loss 0.4906\n",
      "2025-06-01 14:49:43 [INFO]: epoch 9: training loss 0.5772\n",
      "2025-06-01 14:49:43 [INFO]: epoch 10: training loss 0.5258\n",
      "2025-06-01 14:49:43 [INFO]: epoch 11: training loss 0.4421\n",
      "2025-06-01 14:49:43 [INFO]: epoch 12: training loss 0.4308\n",
      "2025-06-01 14:49:43 [INFO]: epoch 13: training loss 0.4547\n",
      "2025-06-01 14:49:43 [INFO]: epoch 14: training loss 0.4960\n",
      "2025-06-01 14:49:43 [INFO]: epoch 15: training loss 0.4411\n",
      "2025-06-01 14:49:43 [INFO]: epoch 16: training loss 0.3650\n",
      "2025-06-01 14:49:43 [INFO]: epoch 17: training loss 0.4378\n",
      "2025-06-01 14:49:43 [INFO]: epoch 18: training loss 0.4263\n",
      "2025-06-01 14:49:43 [INFO]: epoch 19: training loss 0.4205\n",
      "2025-06-01 14:49:43 [INFO]: epoch 20: training loss 0.4019\n",
      "2025-06-01 14:49:43 [INFO]: epoch 21: training loss 0.3676\n",
      "2025-06-01 14:49:43 [INFO]: epoch 22: training loss 0.4189\n",
      "2025-06-01 14:49:43 [INFO]: epoch 23: training loss 0.3659\n",
      "2025-06-01 14:49:43 [INFO]: epoch 24: training loss 0.4068\n",
      "2025-06-01 14:49:43 [INFO]: epoch 25: training loss 0.4328\n",
      "2025-06-01 14:49:43 [INFO]: epoch 26: training loss 0.4030\n",
      "2025-06-01 14:49:43 [INFO]: epoch 27: training loss 0.3776\n",
      "2025-06-01 14:49:43 [INFO]: epoch 28: training loss 0.3705\n",
      "2025-06-01 14:49:43 [INFO]: epoch 29: training loss 0.3745\n",
      "2025-06-01 14:49:43 [INFO]: epoch 30: training loss 0.4047\n",
      "2025-06-01 14:49:43 [INFO]: epoch 31: training loss 0.3721\n",
      "2025-06-01 14:49:43 [INFO]: epoch 32: training loss 0.3337\n",
      "2025-06-01 14:49:43 [INFO]: epoch 33: training loss 0.3550\n",
      "2025-06-01 14:49:43 [INFO]: epoch 34: training loss 0.3840\n",
      "2025-06-01 14:49:43 [INFO]: epoch 35: training loss 0.3950\n",
      "2025-06-01 14:49:43 [INFO]: epoch 36: training loss 0.3770\n",
      "2025-06-01 14:49:43 [INFO]: epoch 37: training loss 0.3335\n",
      "2025-06-01 14:49:43 [INFO]: epoch 38: training loss 0.3299\n",
      "2025-06-01 14:49:43 [INFO]: epoch 39: training loss 0.3387\n",
      "2025-06-01 14:49:43 [INFO]: epoch 40: training loss 0.3625\n",
      "2025-06-01 14:49:43 [INFO]: epoch 41: training loss 0.3569\n",
      "2025-06-01 14:49:43 [INFO]: epoch 42: training loss 0.3233\n",
      "2025-06-01 14:49:43 [INFO]: epoch 43: training loss 0.3259\n",
      "2025-06-01 14:49:43 [INFO]: epoch 44: training loss 0.3480\n",
      "2025-06-01 14:49:43 [INFO]: epoch 45: training loss 0.3952\n",
      "2025-06-01 14:49:43 [INFO]: epoch 46: training loss 0.3601\n",
      "2025-06-01 14:49:43 [INFO]: epoch 47: training loss 0.3140\n",
      "2025-06-01 14:49:43 [INFO]: epoch 48: training loss 0.3187\n",
      "2025-06-01 14:49:43 [INFO]: epoch 49: training loss 0.3402\n",
      "2025-06-01 14:49:43 [INFO]: epoch 50: training loss 0.3386\n",
      "2025-06-01 14:49:43 [INFO]: epoch 51: training loss 0.3538\n",
      "2025-06-01 14:49:43 [INFO]: epoch 52: training loss 0.3275\n",
      "2025-06-01 14:49:43 [INFO]: epoch 53: training loss 0.2761\n",
      "2025-06-01 14:49:44 [INFO]: epoch 54: training loss 0.2917\n",
      "2025-06-01 14:49:44 [INFO]: epoch 55: training loss 0.3300\n",
      "2025-06-01 14:49:44 [INFO]: epoch 56: training loss 0.2925\n",
      "2025-06-01 14:49:44 [INFO]: epoch 57: training loss 0.3061\n",
      "2025-06-01 14:49:44 [INFO]: epoch 58: training loss 0.2992\n",
      "2025-06-01 14:49:44 [INFO]: epoch 59: training loss 0.3037\n",
      "2025-06-01 14:49:44 [INFO]: epoch 60: training loss 0.2853\n",
      "2025-06-01 14:49:44 [INFO]: epoch 61: training loss 0.2934\n",
      "2025-06-01 14:49:44 [INFO]: epoch 62: training loss 0.2826\n",
      "2025-06-01 14:49:44 [INFO]: epoch 63: training loss 0.2528\n",
      "2025-06-01 14:49:44 [INFO]: epoch 64: training loss 0.2674\n",
      "2025-06-01 14:49:44 [INFO]: epoch 65: training loss 0.2746\n",
      "2025-06-01 14:49:44 [INFO]: epoch 66: training loss 0.2728\n",
      "2025-06-01 14:49:44 [INFO]: epoch 67: training loss 0.2728\n",
      "2025-06-01 14:49:44 [INFO]: epoch 68: training loss 0.2821\n",
      "2025-06-01 14:49:44 [INFO]: epoch 69: training loss 0.2857\n",
      "2025-06-01 14:49:44 [INFO]: epoch 70: training loss 0.2857\n",
      "2025-06-01 14:49:44 [INFO]: epoch 71: training loss 0.2324\n",
      "2025-06-01 14:49:44 [INFO]: epoch 72: training loss 0.2590\n",
      "2025-06-01 14:49:44 [INFO]: epoch 73: training loss 0.3042\n",
      "2025-06-01 14:49:44 [INFO]: epoch 74: training loss 0.2690\n",
      "2025-06-01 14:49:44 [INFO]: epoch 75: training loss 0.2519\n",
      "2025-06-01 14:49:44 [INFO]: epoch 76: training loss 0.2589\n",
      "2025-06-01 14:49:44 [INFO]: epoch 77: training loss 0.2591\n",
      "2025-06-01 14:49:44 [INFO]: epoch 78: training loss 0.2776\n",
      "2025-06-01 14:49:44 [INFO]: epoch 79: training loss 0.2415\n",
      "2025-06-01 14:49:44 [INFO]: epoch 80: training loss 0.2750\n",
      "2025-06-01 14:49:44 [INFO]: epoch 81: training loss 0.2588\n",
      "2025-06-01 14:49:44 [INFO]: epoch 82: training loss 0.2416\n",
      "2025-06-01 14:49:44 [INFO]: epoch 83: training loss 0.2429\n",
      "2025-06-01 14:49:44 [INFO]: epoch 84: training loss 0.2907\n",
      "2025-06-01 14:49:44 [INFO]: epoch 85: training loss 0.2429\n",
      "2025-06-01 14:49:44 [INFO]: epoch 86: training loss 0.2441\n",
      "2025-06-01 14:49:44 [INFO]: epoch 87: training loss 0.2560\n",
      "2025-06-01 14:49:44 [INFO]: epoch 88: training loss 0.2285\n",
      "2025-06-01 14:49:44 [INFO]: epoch 89: training loss 0.2485\n",
      "2025-06-01 14:49:44 [INFO]: epoch 90: training loss 0.2411\n",
      "2025-06-01 14:49:44 [INFO]: epoch 91: training loss 0.2266\n",
      "2025-06-01 14:49:44 [INFO]: epoch 92: training loss 0.2363\n",
      "2025-06-01 14:49:44 [INFO]: epoch 93: training loss 0.2365\n",
      "2025-06-01 14:49:44 [INFO]: epoch 94: training loss 0.2273\n",
      "2025-06-01 14:49:44 [INFO]: epoch 95: training loss 0.2228\n",
      "2025-06-01 14:49:44 [INFO]: epoch 96: training loss 0.2229\n",
      "2025-06-01 14:49:44 [INFO]: epoch 97: training loss 0.2280\n",
      "2025-06-01 14:49:44 [INFO]: epoch 98: training loss 0.2234\n",
      "2025-06-01 14:49:44 [INFO]: epoch 99: training loss 0.2349\n",
      "2025-06-01 14:49:44 [INFO]: epoch 100: training loss 0.2209\n",
      "2025-06-01 14:49:44 [INFO]: epoch 101: training loss 0.2125\n",
      "2025-06-01 14:49:44 [INFO]: epoch 102: training loss 0.2210\n",
      "2025-06-01 14:49:44 [INFO]: epoch 103: training loss 0.2219\n",
      "2025-06-01 14:49:44 [INFO]: epoch 104: training loss 0.2172\n",
      "2025-06-01 14:49:44 [INFO]: epoch 105: training loss 0.2316\n",
      "2025-06-01 14:49:44 [INFO]: epoch 106: training loss 0.1933\n",
      "2025-06-01 14:49:44 [INFO]: epoch 107: training loss 0.2325\n",
      "2025-06-01 14:49:44 [INFO]: epoch 108: training loss 0.2294\n",
      "2025-06-01 14:49:44 [INFO]: epoch 109: training loss 0.2251\n",
      "2025-06-01 14:49:44 [INFO]: epoch 110: training loss 0.1897\n",
      "2025-06-01 14:49:44 [INFO]: epoch 111: training loss 0.2017\n",
      "2025-06-01 14:49:44 [INFO]: epoch 112: training loss 0.2038\n",
      "2025-06-01 14:49:44 [INFO]: epoch 113: training loss 0.2044\n",
      "2025-06-01 14:49:44 [INFO]: epoch 114: training loss 0.2069\n",
      "2025-06-01 14:49:44 [INFO]: epoch 115: training loss 0.2004\n",
      "2025-06-01 14:49:44 [INFO]: epoch 116: training loss 0.1840\n",
      "2025-06-01 14:49:44 [INFO]: epoch 117: training loss 0.1848\n",
      "2025-06-01 14:49:44 [INFO]: epoch 118: training loss 0.1935\n",
      "2025-06-01 14:49:44 [INFO]: epoch 119: training loss 0.2042\n",
      "2025-06-01 14:49:44 [INFO]: epoch 120: training loss 0.2142\n",
      "2025-06-01 14:49:44 [INFO]: epoch 121: training loss 0.2234\n",
      "2025-06-01 14:49:44 [INFO]: epoch 122: training loss 0.2114\n",
      "2025-06-01 14:49:44 [INFO]: epoch 123: training loss 0.1955\n",
      "2025-06-01 14:49:44 [INFO]: epoch 124: training loss 0.1838\n",
      "2025-06-01 14:49:44 [INFO]: epoch 125: training loss 0.2149\n",
      "2025-06-01 14:49:44 [INFO]: epoch 126: training loss 0.1850\n",
      "2025-06-01 14:49:44 [INFO]: epoch 127: training loss 0.1702\n",
      "2025-06-01 14:49:44 [INFO]: epoch 128: training loss 0.2171\n",
      "2025-06-01 14:49:44 [INFO]: epoch 129: training loss 0.2262\n",
      "2025-06-01 14:49:44 [INFO]: epoch 130: training loss 0.2022\n",
      "2025-06-01 14:49:44 [INFO]: epoch 131: training loss 0.2261\n",
      "2025-06-01 14:49:45 [INFO]: epoch 132: training loss 0.1851\n",
      "2025-06-01 14:49:45 [INFO]: epoch 133: training loss 0.1921\n",
      "2025-06-01 14:49:45 [INFO]: epoch 134: training loss 0.1860\n",
      "2025-06-01 14:49:45 [INFO]: epoch 135: training loss 0.1871\n",
      "2025-06-01 14:49:45 [INFO]: epoch 136: training loss 0.1918\n",
      "2025-06-01 14:49:45 [INFO]: epoch 137: training loss 0.1850\n",
      "2025-06-01 14:49:45 [INFO]: epoch 138: training loss 0.2066\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-01 14:49:45 [INFO]: epoch 139: training loss 0.1809\n",
      "2025-06-01 14:49:45 [INFO]: epoch 140: training loss 0.1982\n",
      "2025-06-01 14:49:45 [INFO]: epoch 141: training loss 0.1760\n",
      "2025-06-01 14:49:45 [INFO]: epoch 142: training loss 0.1612\n",
      "2025-06-01 14:49:45 [INFO]: epoch 143: training loss 0.2073\n",
      "2025-06-01 14:49:45 [INFO]: epoch 144: training loss 0.1882\n",
      "2025-06-01 14:49:45 [INFO]: epoch 145: training loss 0.1670\n",
      "2025-06-01 14:49:45 [INFO]: epoch 146: training loss 0.1731\n",
      "2025-06-01 14:49:45 [INFO]: epoch 147: training loss 0.1886\n",
      "2025-06-01 14:49:45 [INFO]: epoch 148: training loss 0.1960\n",
      "2025-06-01 14:49:45 [INFO]: epoch 149: training loss 0.1713\n",
      "2025-06-01 14:49:45 [INFO]: epoch 150: training loss 0.1982\n",
      "2025-06-01 14:49:45 [INFO]: epoch 151: training loss 0.1705\n",
      "2025-06-01 14:49:45 [INFO]: epoch 152: training loss 0.1662\n",
      "2025-06-01 14:49:45 [INFO]: epoch 153: training loss 0.1650\n",
      "2025-06-01 14:49:45 [INFO]: epoch 154: training loss 0.1968\n",
      "2025-06-01 14:49:45 [INFO]: epoch 155: training loss 0.1800\n",
      "2025-06-01 14:49:45 [INFO]: epoch 156: training loss 0.1708\n",
      "2025-06-01 14:49:45 [INFO]: epoch 157: training loss 0.1604\n",
      "2025-06-01 14:49:45 [INFO]: epoch 158: training loss 0.1785\n",
      "2025-06-01 14:49:45 [INFO]: epoch 159: training loss 0.1782\n",
      "2025-06-01 14:49:45 [INFO]: epoch 160: training loss 0.1706\n",
      "2025-06-01 14:49:45 [INFO]: epoch 161: training loss 0.1741\n",
      "2025-06-01 14:49:45 [INFO]: epoch 162: training loss 0.1644\n",
      "2025-06-01 14:49:45 [INFO]: epoch 163: training loss 0.1711\n",
      "2025-06-01 14:49:45 [INFO]: epoch 164: training loss 0.1760\n",
      "2025-06-01 14:49:45 [INFO]: epoch 165: training loss 0.1567\n",
      "2025-06-01 14:49:45 [INFO]: epoch 166: training loss 0.1557\n",
      "2025-06-01 14:49:45 [INFO]: epoch 167: training loss 0.1687\n",
      "2025-06-01 14:49:45 [INFO]: epoch 168: training loss 0.1510\n",
      "2025-06-01 14:49:45 [INFO]: epoch 169: training loss 0.1666\n",
      "2025-06-01 14:49:45 [INFO]: epoch 170: training loss 0.1588\n",
      "2025-06-01 14:49:45 [INFO]: epoch 171: training loss 0.1549\n",
      "2025-06-01 14:49:45 [INFO]: epoch 172: training loss 0.1703\n",
      "2025-06-01 14:49:45 [INFO]: epoch 173: training loss 0.1468\n",
      "2025-06-01 14:49:45 [INFO]: epoch 174: training loss 0.1661\n",
      "2025-06-01 14:49:45 [INFO]: epoch 175: training loss 0.1810\n",
      "2025-06-01 14:49:45 [INFO]: epoch 176: training loss 0.1678\n",
      "2025-06-01 14:49:45 [INFO]: epoch 177: training loss 0.1624\n",
      "2025-06-01 14:49:45 [INFO]: epoch 178: training loss 0.1527\n",
      "2025-06-01 14:49:45 [INFO]: epoch 179: training loss 0.1612\n",
      "2025-06-01 14:49:45 [INFO]: epoch 180: training loss 0.1739\n",
      "2025-06-01 14:49:45 [INFO]: epoch 181: training loss 0.1845\n",
      "2025-06-01 14:49:45 [INFO]: epoch 182: training loss 0.1654\n",
      "2025-06-01 14:49:45 [INFO]: epoch 183: training loss 0.1523\n",
      "2025-06-01 14:49:45 [INFO]: epoch 184: training loss 0.1576\n",
      "2025-06-01 14:49:45 [INFO]: epoch 185: training loss 0.1767\n",
      "2025-06-01 14:49:45 [INFO]: epoch 186: training loss 0.1543\n",
      "2025-06-01 14:49:45 [INFO]: epoch 187: training loss 0.1457\n",
      "2025-06-01 14:49:45 [INFO]: epoch 188: training loss 0.1540\n",
      "2025-06-01 14:49:45 [INFO]: epoch 189: training loss 0.1518\n",
      "2025-06-01 14:49:45 [INFO]: epoch 190: training loss 0.1562\n",
      "2025-06-01 14:49:45 [INFO]: epoch 191: training loss 0.1762\n",
      "2025-06-01 14:49:45 [INFO]: epoch 192: training loss 0.1762\n",
      "2025-06-01 14:49:45 [INFO]: epoch 193: training loss 0.1671\n",
      "2025-06-01 14:49:45 [INFO]: epoch 194: training loss 0.1717\n",
      "2025-06-01 14:49:45 [INFO]: epoch 195: training loss 0.1502\n",
      "2025-06-01 14:49:45 [INFO]: epoch 196: training loss 0.1474\n",
      "2025-06-01 14:49:45 [INFO]: epoch 197: training loss 0.1722\n",
      "2025-06-01 14:49:45 [INFO]: epoch 198: training loss 0.1426\n",
      "2025-06-01 14:49:45 [INFO]: epoch 199: training loss 0.1469\n",
      "2025-06-01 14:49:45 [INFO]: epoch 200: training loss 0.1593\n",
      "2025-06-01 14:49:45 [INFO]: epoch 201: training loss 0.1368\n",
      "2025-06-01 14:49:45 [INFO]: epoch 202: training loss 0.1277\n",
      "2025-06-01 14:49:45 [INFO]: epoch 203: training loss 0.1398\n",
      "2025-06-01 14:49:45 [INFO]: epoch 204: training loss 0.1326\n",
      "2025-06-01 14:49:45 [INFO]: epoch 205: training loss 0.1607\n",
      "2025-06-01 14:49:45 [INFO]: epoch 206: training loss 0.1574\n",
      "2025-06-01 14:49:45 [INFO]: epoch 207: training loss 0.1473\n",
      "2025-06-01 14:49:45 [INFO]: epoch 208: training loss 0.1450\n",
      "2025-06-01 14:49:46 [INFO]: epoch 209: training loss 0.1576\n",
      "2025-06-01 14:49:46 [INFO]: epoch 210: training loss 0.1493\n",
      "2025-06-01 14:49:46 [INFO]: epoch 211: training loss 0.1510\n",
      "2025-06-01 14:49:46 [INFO]: epoch 212: training loss 0.1336\n",
      "2025-06-01 14:49:46 [INFO]: epoch 213: training loss 0.1453\n",
      "2025-06-01 14:49:46 [INFO]: epoch 214: training loss 0.1566\n",
      "2025-06-01 14:49:46 [INFO]: epoch 215: training loss 0.1374\n",
      "2025-06-01 14:49:46 [INFO]: epoch 216: training loss 0.1436\n",
      "2025-06-01 14:49:46 [INFO]: epoch 217: training loss 0.1433\n",
      "2025-06-01 14:49:46 [INFO]: epoch 218: training loss 0.1301\n",
      "2025-06-01 14:49:46 [INFO]: epoch 219: training loss 0.1529\n",
      "2025-06-01 14:49:46 [INFO]: epoch 220: training loss 0.1575\n",
      "2025-06-01 14:49:46 [INFO]: epoch 221: training loss 0.1306\n",
      "2025-06-01 14:49:46 [INFO]: epoch 222: training loss 0.1350\n",
      "2025-06-01 14:49:46 [INFO]: epoch 223: training loss 0.1383\n",
      "2025-06-01 14:49:46 [INFO]: epoch 224: training loss 0.1478\n",
      "2025-06-01 14:49:46 [INFO]: epoch 225: training loss 0.1360\n",
      "2025-06-01 14:49:46 [INFO]: epoch 226: training loss 0.1338\n",
      "2025-06-01 14:49:46 [INFO]: epoch 227: training loss 0.1300\n",
      "2025-06-01 14:49:46 [INFO]: epoch 228: training loss 0.1295\n",
      "2025-06-01 14:49:46 [INFO]: epoch 229: training loss 0.1357\n",
      "2025-06-01 14:49:46 [INFO]: epoch 230: training loss 0.1163\n",
      "2025-06-01 14:49:46 [INFO]: epoch 231: training loss 0.1309\n",
      "2025-06-01 14:49:46 [INFO]: epoch 232: training loss 0.1458\n",
      "2025-06-01 14:49:46 [INFO]: epoch 233: training loss 0.1280\n",
      "2025-06-01 14:49:46 [INFO]: epoch 234: training loss 0.1166\n",
      "2025-06-01 14:49:46 [INFO]: epoch 235: training loss 0.1398\n",
      "2025-06-01 14:49:46 [INFO]: epoch 236: training loss 0.1351\n",
      "2025-06-01 14:49:46 [INFO]: epoch 237: training loss 0.1249\n",
      "2025-06-01 14:49:46 [INFO]: epoch 238: training loss 0.1353\n",
      "2025-06-01 14:49:46 [INFO]: epoch 239: training loss 0.1216\n",
      "2025-06-01 14:49:46 [INFO]: epoch 240: training loss 0.1321\n",
      "2025-06-01 14:49:46 [INFO]: epoch 241: training loss 0.1079\n",
      "2025-06-01 14:49:46 [INFO]: epoch 242: training loss 0.1269\n",
      "2025-06-01 14:49:46 [INFO]: epoch 243: training loss 0.1165\n",
      "2025-06-01 14:49:46 [INFO]: epoch 244: training loss 0.1286\n",
      "2025-06-01 14:49:46 [INFO]: epoch 245: training loss 0.1429\n",
      "2025-06-01 14:49:46 [INFO]: epoch 246: training loss 0.1284\n",
      "2025-06-01 14:49:46 [INFO]: epoch 247: training loss 0.1155\n",
      "2025-06-01 14:49:46 [INFO]: epoch 248: training loss 0.1182\n",
      "2025-06-01 14:49:46 [INFO]: epoch 249: training loss 0.1167\n",
      "2025-06-01 14:49:46 [INFO]: epoch 250: training loss 0.1143\n",
      "2025-06-01 14:49:46 [INFO]: epoch 251: training loss 0.1271\n",
      "2025-06-01 14:49:46 [INFO]: epoch 252: training loss 0.1091\n",
      "2025-06-01 14:49:46 [INFO]: epoch 253: training loss 0.1144\n",
      "2025-06-01 14:49:46 [INFO]: epoch 254: training loss 0.1219\n",
      "2025-06-01 14:49:46 [INFO]: epoch 255: training loss 0.1116\n",
      "2025-06-01 14:49:46 [INFO]: epoch 256: training loss 0.1052\n",
      "2025-06-01 14:49:46 [INFO]: epoch 257: training loss 0.1258\n",
      "2025-06-01 14:49:46 [INFO]: epoch 258: training loss 0.1281\n",
      "2025-06-01 14:49:46 [INFO]: epoch 259: training loss 0.1177\n",
      "2025-06-01 14:49:46 [INFO]: epoch 260: training loss 0.1169\n",
      "2025-06-01 14:49:46 [INFO]: epoch 261: training loss 0.1149\n",
      "2025-06-01 14:49:46 [INFO]: epoch 262: training loss 0.1050\n",
      "2025-06-01 14:49:46 [INFO]: epoch 263: training loss 0.1161\n",
      "2025-06-01 14:49:46 [INFO]: epoch 264: training loss 0.1185\n",
      "2025-06-01 14:49:46 [INFO]: epoch 265: training loss 0.1145\n",
      "2025-06-01 14:49:46 [INFO]: epoch 266: training loss 0.1174\n",
      "2025-06-01 14:49:46 [INFO]: epoch 267: training loss 0.1237\n",
      "2025-06-01 14:49:46 [INFO]: epoch 268: training loss 0.1159\n",
      "2025-06-01 14:49:46 [INFO]: epoch 269: training loss 0.1162\n",
      "2025-06-01 14:49:46 [INFO]: epoch 270: training loss 0.1117\n",
      "2025-06-01 14:49:46 [INFO]: epoch 271: training loss 0.1118\n",
      "2025-06-01 14:49:46 [INFO]: epoch 272: training loss 0.1201\n",
      "2025-06-01 14:49:46 [INFO]: epoch 273: training loss 0.1023\n",
      "2025-06-01 14:49:46 [INFO]: epoch 274: training loss 0.1108\n",
      "2025-06-01 14:49:46 [INFO]: epoch 275: training loss 0.1094\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-01 14:49:46 [INFO]: epoch 276: training loss 0.1118\n",
      "2025-06-01 14:49:46 [INFO]: epoch 277: training loss 0.1188\n",
      "2025-06-01 14:49:46 [INFO]: epoch 278: training loss 0.1207\n",
      "2025-06-01 14:49:46 [INFO]: epoch 279: training loss 0.1036\n",
      "2025-06-01 14:49:46 [INFO]: epoch 280: training loss 0.1155\n",
      "2025-06-01 14:49:46 [INFO]: epoch 281: training loss 0.1166\n",
      "2025-06-01 14:49:46 [INFO]: epoch 282: training loss 0.1046\n",
      "2025-06-01 14:49:46 [INFO]: epoch 283: training loss 0.1055\n",
      "2025-06-01 14:49:46 [INFO]: epoch 284: training loss 0.1032\n",
      "2025-06-01 14:49:47 [INFO]: epoch 285: training loss 0.1069\n",
      "2025-06-01 14:49:47 [INFO]: epoch 286: training loss 0.1109\n",
      "2025-06-01 14:49:47 [INFO]: epoch 287: training loss 0.0949\n",
      "2025-06-01 14:49:47 [INFO]: epoch 288: training loss 0.1029\n",
      "2025-06-01 14:49:47 [INFO]: epoch 289: training loss 0.1011\n",
      "2025-06-01 14:49:47 [INFO]: epoch 290: training loss 0.1080\n",
      "2025-06-01 14:49:47 [INFO]: epoch 291: training loss 0.0876\n",
      "2025-06-01 14:49:47 [INFO]: epoch 292: training loss 0.1021\n",
      "2025-06-01 14:49:47 [INFO]: epoch 293: training loss 0.1000\n",
      "2025-06-01 14:49:47 [INFO]: epoch 294: training loss 0.1055\n",
      "2025-06-01 14:49:47 [INFO]: epoch 295: training loss 0.1001\n",
      "2025-06-01 14:49:47 [INFO]: epoch 296: training loss 0.1041\n",
      "2025-06-01 14:49:47 [INFO]: epoch 297: training loss 0.0990\n",
      "2025-06-01 14:49:47 [INFO]: epoch 298: training loss 0.1041\n",
      "2025-06-01 14:49:47 [INFO]: epoch 299: training loss 0.1084\n",
      "2025-06-01 14:49:47 [INFO]: epoch 300: training loss 0.0954\n",
      "2025-06-01 14:49:47 [INFO]: epoch 301: training loss 0.0993\n",
      "2025-06-01 14:49:47 [INFO]: epoch 302: training loss 0.1068\n",
      "2025-06-01 14:49:47 [INFO]: epoch 303: training loss 0.1126\n",
      "2025-06-01 14:49:47 [INFO]: epoch 304: training loss 0.1026\n",
      "2025-06-01 14:49:47 [INFO]: epoch 305: training loss 0.0892\n",
      "2025-06-01 14:49:47 [INFO]: epoch 306: training loss 0.1031\n",
      "2025-06-01 14:49:47 [INFO]: epoch 307: training loss 0.0966\n",
      "2025-06-01 14:49:47 [INFO]: epoch 308: training loss 0.1155\n",
      "2025-06-01 14:49:47 [INFO]: epoch 309: training loss 0.0988\n",
      "2025-06-01 14:49:47 [INFO]: epoch 310: training loss 0.1003\n",
      "2025-06-01 14:49:47 [INFO]: epoch 311: training loss 0.0797\n",
      "2025-06-01 14:49:47 [INFO]: epoch 312: training loss 0.1103\n",
      "2025-06-01 14:49:47 [INFO]: epoch 313: training loss 0.1122\n",
      "2025-06-01 14:49:47 [INFO]: epoch 314: training loss 0.1044\n",
      "2025-06-01 14:49:47 [INFO]: epoch 315: training loss 0.1092\n",
      "2025-06-01 14:49:47 [INFO]: epoch 316: training loss 0.1011\n",
      "2025-06-01 14:49:47 [INFO]: epoch 317: training loss 0.1020\n",
      "2025-06-01 14:49:47 [INFO]: epoch 318: training loss 0.0963\n",
      "2025-06-01 14:49:47 [INFO]: epoch 319: training loss 0.0914\n",
      "2025-06-01 14:49:47 [INFO]: epoch 320: training loss 0.1024\n",
      "2025-06-01 14:49:47 [INFO]: epoch 321: training loss 0.0877\n",
      "2025-06-01 14:49:47 [INFO]: epoch 322: training loss 0.0996\n",
      "2025-06-01 14:49:47 [INFO]: epoch 323: training loss 0.1015\n",
      "2025-06-01 14:49:47 [INFO]: epoch 324: training loss 0.0848\n",
      "2025-06-01 14:49:47 [INFO]: epoch 325: training loss 0.0988\n",
      "2025-06-01 14:49:47 [INFO]: epoch 326: training loss 0.0887\n",
      "2025-06-01 14:49:47 [INFO]: epoch 327: training loss 0.0957\n",
      "2025-06-01 14:49:47 [INFO]: epoch 328: training loss 0.0950\n",
      "2025-06-01 14:49:47 [INFO]: epoch 329: training loss 0.0909\n",
      "2025-06-01 14:49:47 [INFO]: epoch 330: training loss 0.0910\n",
      "2025-06-01 14:49:47 [INFO]: epoch 331: training loss 0.0960\n",
      "2025-06-01 14:49:47 [INFO]: epoch 332: training loss 0.1081\n",
      "2025-06-01 14:49:47 [INFO]: epoch 333: training loss 0.1007\n",
      "2025-06-01 14:49:47 [INFO]: epoch 334: training loss 0.1031\n",
      "2025-06-01 14:49:47 [INFO]: epoch 335: training loss 0.0989\n",
      "2025-06-01 14:49:47 [INFO]: epoch 336: training loss 0.0861\n",
      "2025-06-01 14:49:47 [INFO]: epoch 337: training loss 0.0890\n",
      "2025-06-01 14:49:47 [INFO]: epoch 338: training loss 0.0970\n",
      "2025-06-01 14:49:47 [INFO]: epoch 339: training loss 0.0870\n",
      "2025-06-01 14:49:47 [INFO]: epoch 340: training loss 0.0774\n",
      "2025-06-01 14:49:47 [INFO]: epoch 341: training loss 0.0845\n",
      "2025-06-01 14:49:47 [INFO]: epoch 342: training loss 0.0936\n",
      "2025-06-01 14:49:47 [INFO]: epoch 343: training loss 0.0828\n",
      "2025-06-01 14:49:47 [INFO]: epoch 344: training loss 0.0879\n",
      "2025-06-01 14:49:47 [INFO]: epoch 345: training loss 0.0925\n",
      "2025-06-01 14:49:47 [INFO]: epoch 346: training loss 0.0874\n",
      "2025-06-01 14:49:47 [INFO]: epoch 347: training loss 0.0976\n",
      "2025-06-01 14:49:47 [INFO]: epoch 348: training loss 0.0896\n",
      "2025-06-01 14:49:47 [INFO]: epoch 349: training loss 0.0859\n",
      "2025-06-01 14:49:47 [INFO]: epoch 350: training loss 0.0763\n",
      "2025-06-01 14:49:47 [INFO]: epoch 351: training loss 0.1030\n",
      "2025-06-01 14:49:47 [INFO]: epoch 352: training loss 0.0836\n",
      "2025-06-01 14:49:47 [INFO]: epoch 353: training loss 0.0818\n",
      "2025-06-01 14:49:47 [INFO]: epoch 354: training loss 0.0895\n",
      "2025-06-01 14:49:47 [INFO]: epoch 355: training loss 0.0910\n",
      "2025-06-01 14:49:47 [INFO]: epoch 356: training loss 0.0807\n",
      "2025-06-01 14:49:47 [INFO]: epoch 357: training loss 0.0807\n",
      "2025-06-01 14:49:47 [INFO]: epoch 358: training loss 0.0839\n",
      "2025-06-01 14:49:47 [INFO]: epoch 359: training loss 0.0828\n",
      "2025-06-01 14:49:47 [INFO]: epoch 360: training loss 0.0833\n",
      "2025-06-01 14:49:48 [INFO]: epoch 361: training loss 0.0678\n",
      "2025-06-01 14:49:48 [INFO]: epoch 362: training loss 0.0833\n",
      "2025-06-01 14:49:48 [INFO]: epoch 363: training loss 0.0994\n",
      "2025-06-01 14:49:48 [INFO]: epoch 364: training loss 0.0900\n",
      "2025-06-01 14:49:48 [INFO]: epoch 365: training loss 0.0700\n",
      "2025-06-01 14:49:48 [INFO]: epoch 366: training loss 0.0791\n",
      "2025-06-01 14:49:48 [INFO]: epoch 367: training loss 0.0782\n",
      "2025-06-01 14:49:48 [INFO]: epoch 368: training loss 0.0774\n",
      "2025-06-01 14:49:48 [INFO]: epoch 369: training loss 0.0809\n",
      "2025-06-01 14:49:48 [INFO]: epoch 370: training loss 0.0836\n",
      "2025-06-01 14:49:48 [INFO]: epoch 371: training loss 0.0784\n",
      "2025-06-01 14:49:48 [INFO]: epoch 372: training loss 0.0714\n",
      "2025-06-01 14:49:48 [INFO]: epoch 373: training loss 0.0773\n",
      "2025-06-01 14:49:48 [INFO]: epoch 374: training loss 0.0827\n",
      "2025-06-01 14:49:48 [INFO]: epoch 375: training loss 0.0741\n",
      "2025-06-01 14:49:48 [INFO]: epoch 376: training loss 0.0807\n",
      "2025-06-01 14:49:48 [INFO]: epoch 377: training loss 0.0891\n",
      "2025-06-01 14:49:48 [INFO]: epoch 378: training loss 0.0789\n",
      "2025-06-01 14:49:48 [INFO]: epoch 379: training loss 0.0760\n",
      "2025-06-01 14:49:48 [INFO]: epoch 380: training loss 0.0823\n",
      "2025-06-01 14:49:48 [INFO]: epoch 381: training loss 0.0743\n",
      "2025-06-01 14:49:48 [INFO]: epoch 382: training loss 0.0894\n",
      "2025-06-01 14:49:48 [INFO]: epoch 383: training loss 0.0789\n",
      "2025-06-01 14:49:48 [INFO]: epoch 384: training loss 0.0690\n",
      "2025-06-01 14:49:48 [INFO]: epoch 385: training loss 0.0852\n",
      "2025-06-01 14:49:48 [INFO]: epoch 386: training loss 0.0834\n",
      "2025-06-01 14:49:48 [INFO]: epoch 387: training loss 0.0661\n",
      "2025-06-01 14:49:48 [INFO]: epoch 388: training loss 0.0772\n",
      "2025-06-01 14:49:48 [INFO]: epoch 389: training loss 0.0729\n",
      "2025-06-01 14:49:48 [INFO]: epoch 390: training loss 0.0720\n",
      "2025-06-01 14:49:48 [INFO]: epoch 391: training loss 0.0732\n",
      "2025-06-01 14:49:48 [INFO]: epoch 392: training loss 0.0758\n",
      "2025-06-01 14:49:48 [INFO]: epoch 393: training loss 0.0767\n",
      "2025-06-01 14:49:48 [INFO]: epoch 394: training loss 0.0782\n",
      "2025-06-01 14:49:48 [INFO]: epoch 395: training loss 0.0679\n",
      "2025-06-01 14:49:48 [INFO]: epoch 396: training loss 0.0823\n",
      "2025-06-01 14:49:48 [INFO]: epoch 397: training loss 0.0712\n",
      "2025-06-01 14:49:48 [INFO]: epoch 398: training loss 0.0681\n",
      "2025-06-01 14:49:48 [INFO]: epoch 399: training loss 0.0782\n",
      "2025-06-01 14:49:48 [INFO]: Finished training.\n",
      "2025-06-01 14:49:48 [WARNING]: 🚨DeprecationWarning: The method impute is deprecated. Please use `predict` instead.\n",
      " 50%|██████████████████████████████████████████                                          | 3/6 [00:16<00:16,  5.36s/it]2025-06-01 14:49:48 [INFO]: No given device, using default device: cuda\n",
      "2025-06-01 14:49:48 [WARNING]: saving_path not given. Model files and tensorboard file will not be saved.\n",
      "2025-06-01 14:49:48 [INFO]: Model initialized successfully with the number of trainable parameters: 597,780\n",
      "2025-06-01 14:49:48 [INFO]: epoch 0: training loss 1.2273\n",
      "2025-06-01 14:49:48 [INFO]: epoch 1: training loss 0.8738\n",
      "2025-06-01 14:49:48 [INFO]: epoch 2: training loss 0.6541\n",
      "2025-06-01 14:49:48 [INFO]: epoch 3: training loss 0.6057\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-01 14:49:48 [INFO]: epoch 4: training loss 0.6427\n",
      "2025-06-01 14:49:48 [INFO]: epoch 5: training loss 0.7407\n",
      "2025-06-01 14:49:48 [INFO]: epoch 6: training loss 0.5085\n",
      "2025-06-01 14:49:48 [INFO]: epoch 7: training loss 0.4612\n",
      "2025-06-01 14:49:48 [INFO]: epoch 8: training loss 0.4488\n",
      "2025-06-01 14:49:48 [INFO]: epoch 9: training loss 0.4530\n",
      "2025-06-01 14:49:48 [INFO]: epoch 10: training loss 0.4718\n",
      "2025-06-01 14:49:48 [INFO]: epoch 11: training loss 0.4571\n",
      "2025-06-01 14:49:48 [INFO]: epoch 12: training loss 0.4265\n",
      "2025-06-01 14:49:48 [INFO]: epoch 13: training loss 0.4232\n",
      "2025-06-01 14:49:48 [INFO]: epoch 14: training loss 0.4112\n",
      "2025-06-01 14:49:48 [INFO]: epoch 15: training loss 0.4807\n",
      "2025-06-01 14:49:48 [INFO]: epoch 16: training loss 0.4178\n",
      "2025-06-01 14:49:48 [INFO]: epoch 17: training loss 0.4254\n",
      "2025-06-01 14:49:48 [INFO]: epoch 18: training loss 0.3606\n",
      "2025-06-01 14:49:48 [INFO]: epoch 19: training loss 0.3600\n",
      "2025-06-01 14:49:48 [INFO]: epoch 20: training loss 0.3547\n",
      "2025-06-01 14:49:48 [INFO]: epoch 21: training loss 0.3780\n",
      "2025-06-01 14:49:48 [INFO]: epoch 22: training loss 0.3791\n",
      "2025-06-01 14:49:48 [INFO]: epoch 23: training loss 0.3401\n",
      "2025-06-01 14:49:48 [INFO]: epoch 24: training loss 0.3377\n",
      "2025-06-01 14:49:48 [INFO]: epoch 25: training loss 0.3228\n",
      "2025-06-01 14:49:48 [INFO]: epoch 26: training loss 0.3316\n",
      "2025-06-01 14:49:48 [INFO]: epoch 27: training loss 0.3001\n",
      "2025-06-01 14:49:48 [INFO]: epoch 28: training loss 0.3205\n",
      "2025-06-01 14:49:49 [INFO]: epoch 29: training loss 0.2984\n",
      "2025-06-01 14:49:49 [INFO]: epoch 30: training loss 0.3174\n",
      "2025-06-01 14:49:49 [INFO]: epoch 31: training loss 0.3163\n",
      "2025-06-01 14:49:49 [INFO]: epoch 32: training loss 0.3095\n",
      "2025-06-01 14:49:49 [INFO]: epoch 33: training loss 0.2611\n",
      "2025-06-01 14:49:49 [INFO]: epoch 34: training loss 0.2801\n",
      "2025-06-01 14:49:49 [INFO]: epoch 35: training loss 0.2842\n",
      "2025-06-01 14:49:49 [INFO]: epoch 36: training loss 0.2779\n",
      "2025-06-01 14:49:49 [INFO]: epoch 37: training loss 0.2815\n",
      "2025-06-01 14:49:49 [INFO]: epoch 38: training loss 0.2195\n",
      "2025-06-01 14:49:49 [INFO]: epoch 39: training loss 0.2470\n",
      "2025-06-01 14:49:49 [INFO]: epoch 40: training loss 0.2584\n",
      "2025-06-01 14:49:49 [INFO]: epoch 41: training loss 0.2583\n",
      "2025-06-01 14:49:49 [INFO]: epoch 42: training loss 0.2685\n",
      "2025-06-01 14:49:49 [INFO]: epoch 43: training loss 0.2211\n",
      "2025-06-01 14:49:49 [INFO]: epoch 44: training loss 0.2416\n",
      "2025-06-01 14:49:49 [INFO]: epoch 45: training loss 0.2527\n",
      "2025-06-01 14:49:49 [INFO]: epoch 46: training loss 0.2426\n",
      "2025-06-01 14:49:49 [INFO]: epoch 47: training loss 0.2366\n",
      "2025-06-01 14:49:49 [INFO]: epoch 48: training loss 0.2202\n",
      "2025-06-01 14:49:49 [INFO]: epoch 49: training loss 0.2197\n",
      "2025-06-01 14:49:49 [INFO]: epoch 50: training loss 0.1809\n",
      "2025-06-01 14:49:49 [INFO]: epoch 51: training loss 0.1988\n",
      "2025-06-01 14:49:49 [INFO]: epoch 52: training loss 0.2060\n",
      "2025-06-01 14:49:49 [INFO]: epoch 53: training loss 0.1914\n",
      "2025-06-01 14:49:49 [INFO]: epoch 54: training loss 0.1729\n",
      "2025-06-01 14:49:49 [INFO]: epoch 55: training loss 0.1632\n",
      "2025-06-01 14:49:49 [INFO]: epoch 56: training loss 0.1690\n",
      "2025-06-01 14:49:49 [INFO]: epoch 57: training loss 0.1727\n",
      "2025-06-01 14:49:49 [INFO]: epoch 58: training loss 0.1707\n",
      "2025-06-01 14:49:49 [INFO]: epoch 59: training loss 0.1339\n",
      "2025-06-01 14:49:49 [INFO]: epoch 60: training loss 0.1693\n",
      "2025-06-01 14:49:49 [INFO]: epoch 61: training loss 0.1585\n",
      "2025-06-01 14:49:49 [INFO]: epoch 62: training loss 0.1516\n",
      "2025-06-01 14:49:49 [INFO]: epoch 63: training loss 0.1317\n",
      "2025-06-01 14:49:49 [INFO]: epoch 64: training loss 0.1380\n",
      "2025-06-01 14:49:49 [INFO]: epoch 65: training loss 0.1655\n",
      "2025-06-01 14:49:49 [INFO]: epoch 66: training loss 0.1441\n",
      "2025-06-01 14:49:49 [INFO]: epoch 67: training loss 0.1334\n",
      "2025-06-01 14:49:49 [INFO]: epoch 68: training loss 0.1411\n",
      "2025-06-01 14:49:49 [INFO]: epoch 69: training loss 0.1376\n",
      "2025-06-01 14:49:49 [INFO]: epoch 70: training loss 0.1282\n",
      "2025-06-01 14:49:49 [INFO]: epoch 71: training loss 0.1420\n",
      "2025-06-01 14:49:49 [INFO]: epoch 72: training loss 0.1463\n",
      "2025-06-01 14:49:49 [INFO]: epoch 73: training loss 0.1210\n",
      "2025-06-01 14:49:49 [INFO]: epoch 74: training loss 0.1365\n",
      "2025-06-01 14:49:49 [INFO]: epoch 75: training loss 0.1040\n",
      "2025-06-01 14:49:49 [INFO]: epoch 76: training loss 0.1385\n",
      "2025-06-01 14:49:49 [INFO]: epoch 77: training loss 0.1343\n",
      "2025-06-01 14:49:49 [INFO]: epoch 78: training loss 0.1428\n",
      "2025-06-01 14:49:49 [INFO]: epoch 79: training loss 0.1081\n",
      "2025-06-01 14:49:49 [INFO]: epoch 80: training loss 0.0954\n",
      "2025-06-01 14:49:49 [INFO]: epoch 81: training loss 0.1114\n",
      "2025-06-01 14:49:49 [INFO]: epoch 82: training loss 0.1262\n",
      "2025-06-01 14:49:49 [INFO]: epoch 83: training loss 0.1153\n",
      "2025-06-01 14:49:49 [INFO]: epoch 84: training loss 0.1159\n",
      "2025-06-01 14:49:49 [INFO]: epoch 85: training loss 0.1189\n",
      "2025-06-01 14:49:49 [INFO]: epoch 86: training loss 0.1092\n",
      "2025-06-01 14:49:49 [INFO]: epoch 87: training loss 0.1072\n",
      "2025-06-01 14:49:49 [INFO]: epoch 88: training loss 0.1051\n",
      "2025-06-01 14:49:49 [INFO]: epoch 89: training loss 0.1102\n",
      "2025-06-01 14:49:49 [INFO]: epoch 90: training loss 0.1151\n",
      "2025-06-01 14:49:49 [INFO]: epoch 91: training loss 0.0992\n",
      "2025-06-01 14:49:49 [INFO]: epoch 92: training loss 0.1114\n",
      "2025-06-01 14:49:49 [INFO]: epoch 93: training loss 0.1246\n",
      "2025-06-01 14:49:49 [INFO]: epoch 94: training loss 0.0963\n",
      "2025-06-01 14:49:49 [INFO]: epoch 95: training loss 0.1324\n",
      "2025-06-01 14:49:49 [INFO]: epoch 96: training loss 0.1129\n",
      "2025-06-01 14:49:49 [INFO]: epoch 97: training loss 0.1168\n",
      "2025-06-01 14:49:49 [INFO]: epoch 98: training loss 0.1247\n",
      "2025-06-01 14:49:49 [INFO]: epoch 99: training loss 0.1040\n",
      "2025-06-01 14:49:49 [INFO]: epoch 100: training loss 0.1113\n",
      "2025-06-01 14:49:49 [INFO]: epoch 101: training loss 0.1440\n",
      "2025-06-01 14:49:49 [INFO]: epoch 102: training loss 0.1351\n",
      "2025-06-01 14:49:49 [INFO]: epoch 103: training loss 0.1044\n",
      "2025-06-01 14:49:50 [INFO]: epoch 104: training loss 0.1070\n",
      "2025-06-01 14:49:50 [INFO]: epoch 105: training loss 0.1000\n",
      "2025-06-01 14:49:50 [INFO]: epoch 106: training loss 0.1179\n",
      "2025-06-01 14:49:50 [INFO]: epoch 107: training loss 0.1053\n",
      "2025-06-01 14:49:50 [INFO]: epoch 108: training loss 0.1043\n",
      "2025-06-01 14:49:50 [INFO]: epoch 109: training loss 0.0958\n",
      "2025-06-01 14:49:50 [INFO]: epoch 110: training loss 0.0986\n",
      "2025-06-01 14:49:50 [INFO]: epoch 111: training loss 0.0957\n",
      "2025-06-01 14:49:50 [INFO]: epoch 112: training loss 0.0929\n",
      "2025-06-01 14:49:50 [INFO]: epoch 113: training loss 0.0996\n",
      "2025-06-01 14:49:50 [INFO]: epoch 114: training loss 0.0974\n",
      "2025-06-01 14:49:50 [INFO]: epoch 115: training loss 0.1172\n",
      "2025-06-01 14:49:50 [INFO]: epoch 116: training loss 0.0891\n",
      "2025-06-01 14:49:50 [INFO]: epoch 117: training loss 0.0745\n",
      "2025-06-01 14:49:50 [INFO]: epoch 118: training loss 0.1046\n",
      "2025-06-01 14:49:50 [INFO]: epoch 119: training loss 0.1195\n",
      "2025-06-01 14:49:50 [INFO]: epoch 120: training loss 0.0859\n",
      "2025-06-01 14:49:50 [INFO]: epoch 121: training loss 0.1221\n",
      "2025-06-01 14:49:50 [INFO]: epoch 122: training loss 0.0986\n",
      "2025-06-01 14:49:50 [INFO]: epoch 123: training loss 0.0932\n",
      "2025-06-01 14:49:50 [INFO]: epoch 124: training loss 0.0946\n",
      "2025-06-01 14:49:50 [INFO]: epoch 125: training loss 0.0952\n",
      "2025-06-01 14:49:50 [INFO]: epoch 126: training loss 0.0957\n",
      "2025-06-01 14:49:50 [INFO]: epoch 127: training loss 0.0784\n",
      "2025-06-01 14:49:50 [INFO]: epoch 128: training loss 0.0913\n",
      "2025-06-01 14:49:50 [INFO]: epoch 129: training loss 0.0938\n",
      "2025-06-01 14:49:50 [INFO]: epoch 130: training loss 0.1004\n",
      "2025-06-01 14:49:50 [INFO]: epoch 131: training loss 0.0965\n",
      "2025-06-01 14:49:50 [INFO]: epoch 132: training loss 0.0950\n",
      "2025-06-01 14:49:50 [INFO]: epoch 133: training loss 0.0930\n",
      "2025-06-01 14:49:50 [INFO]: epoch 134: training loss 0.0943\n",
      "2025-06-01 14:49:50 [INFO]: epoch 135: training loss 0.0949\n",
      "2025-06-01 14:49:50 [INFO]: epoch 136: training loss 0.0812\n",
      "2025-06-01 14:49:50 [INFO]: epoch 137: training loss 0.0856\n",
      "2025-06-01 14:49:50 [INFO]: epoch 138: training loss 0.0884\n",
      "2025-06-01 14:49:50 [INFO]: epoch 139: training loss 0.0889\n",
      "2025-06-01 14:49:50 [INFO]: epoch 140: training loss 0.0917\n",
      "2025-06-01 14:49:50 [INFO]: epoch 141: training loss 0.0946\n",
      "2025-06-01 14:49:50 [INFO]: epoch 142: training loss 0.0887\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-01 14:49:50 [INFO]: epoch 143: training loss 0.0733\n",
      "2025-06-01 14:49:50 [INFO]: epoch 144: training loss 0.0838\n",
      "2025-06-01 14:49:50 [INFO]: epoch 145: training loss 0.0911\n",
      "2025-06-01 14:49:50 [INFO]: epoch 146: training loss 0.0992\n",
      "2025-06-01 14:49:50 [INFO]: epoch 147: training loss 0.0960\n",
      "2025-06-01 14:49:50 [INFO]: epoch 148: training loss 0.0851\n",
      "2025-06-01 14:49:50 [INFO]: epoch 149: training loss 0.0902\n",
      "2025-06-01 14:49:50 [INFO]: epoch 150: training loss 0.0963\n",
      "2025-06-01 14:49:50 [INFO]: epoch 151: training loss 0.1003\n",
      "2025-06-01 14:49:50 [INFO]: epoch 152: training loss 0.0968\n",
      "2025-06-01 14:49:50 [INFO]: epoch 153: training loss 0.0812\n",
      "2025-06-01 14:49:50 [INFO]: epoch 154: training loss 0.1063\n",
      "2025-06-01 14:49:50 [INFO]: epoch 155: training loss 0.1147\n",
      "2025-06-01 14:49:50 [INFO]: epoch 156: training loss 0.0832\n",
      "2025-06-01 14:49:50 [INFO]: epoch 157: training loss 0.0902\n",
      "2025-06-01 14:49:50 [INFO]: epoch 158: training loss 0.0864\n",
      "2025-06-01 14:49:50 [INFO]: epoch 159: training loss 0.0810\n",
      "2025-06-01 14:49:50 [INFO]: epoch 160: training loss 0.0959\n",
      "2025-06-01 14:49:50 [INFO]: epoch 161: training loss 0.0876\n",
      "2025-06-01 14:49:50 [INFO]: epoch 162: training loss 0.0865\n",
      "2025-06-01 14:49:50 [INFO]: epoch 163: training loss 0.0830\n",
      "2025-06-01 14:49:50 [INFO]: epoch 164: training loss 0.0697\n",
      "2025-06-01 14:49:50 [INFO]: epoch 165: training loss 0.0829\n",
      "2025-06-01 14:49:50 [INFO]: epoch 166: training loss 0.0834\n",
      "2025-06-01 14:49:50 [INFO]: epoch 167: training loss 0.0836\n",
      "2025-06-01 14:49:50 [INFO]: epoch 168: training loss 0.0767\n",
      "2025-06-01 14:49:50 [INFO]: epoch 169: training loss 0.0856\n",
      "2025-06-01 14:49:50 [INFO]: epoch 170: training loss 0.0887\n",
      "2025-06-01 14:49:50 [INFO]: epoch 171: training loss 0.0908\n",
      "2025-06-01 14:49:50 [INFO]: epoch 172: training loss 0.0761\n",
      "2025-06-01 14:49:50 [INFO]: epoch 173: training loss 0.0659\n",
      "2025-06-01 14:49:50 [INFO]: epoch 174: training loss 0.0974\n",
      "2025-06-01 14:49:50 [INFO]: epoch 175: training loss 0.0979\n",
      "2025-06-01 14:49:50 [INFO]: epoch 176: training loss 0.1061\n",
      "2025-06-01 14:49:50 [INFO]: epoch 177: training loss 0.0717\n",
      "2025-06-01 14:49:50 [INFO]: epoch 178: training loss 0.0742\n",
      "2025-06-01 14:49:50 [INFO]: epoch 179: training loss 0.0919\n",
      "2025-06-01 14:49:51 [INFO]: epoch 180: training loss 0.0803\n",
      "2025-06-01 14:49:51 [INFO]: epoch 181: training loss 0.0849\n",
      "2025-06-01 14:49:51 [INFO]: epoch 182: training loss 0.0887\n",
      "2025-06-01 14:49:51 [INFO]: epoch 183: training loss 0.0809\n",
      "2025-06-01 14:49:51 [INFO]: epoch 184: training loss 0.0739\n",
      "2025-06-01 14:49:51 [INFO]: epoch 185: training loss 0.0709\n",
      "2025-06-01 14:49:51 [INFO]: epoch 186: training loss 0.0893\n",
      "2025-06-01 14:49:51 [INFO]: epoch 187: training loss 0.0798\n",
      "2025-06-01 14:49:51 [INFO]: epoch 188: training loss 0.0857\n",
      "2025-06-01 14:49:51 [INFO]: epoch 189: training loss 0.0711\n",
      "2025-06-01 14:49:51 [INFO]: epoch 190: training loss 0.0971\n",
      "2025-06-01 14:49:51 [INFO]: epoch 191: training loss 0.0813\n",
      "2025-06-01 14:49:51 [INFO]: epoch 192: training loss 0.0785\n",
      "2025-06-01 14:49:51 [INFO]: epoch 193: training loss 0.0765\n",
      "2025-06-01 14:49:51 [INFO]: epoch 194: training loss 0.0704\n",
      "2025-06-01 14:49:51 [INFO]: epoch 195: training loss 0.0716\n",
      "2025-06-01 14:49:51 [INFO]: epoch 196: training loss 0.0643\n",
      "2025-06-01 14:49:51 [INFO]: epoch 197: training loss 0.0855\n",
      "2025-06-01 14:49:51 [INFO]: epoch 198: training loss 0.0674\n",
      "2025-06-01 14:49:51 [INFO]: epoch 199: training loss 0.0664\n",
      "2025-06-01 14:49:51 [INFO]: epoch 200: training loss 0.0753\n",
      "2025-06-01 14:49:51 [INFO]: epoch 201: training loss 0.0658\n",
      "2025-06-01 14:49:51 [INFO]: epoch 202: training loss 0.0753\n",
      "2025-06-01 14:49:51 [INFO]: epoch 203: training loss 0.0836\n",
      "2025-06-01 14:49:51 [INFO]: epoch 204: training loss 0.0702\n",
      "2025-06-01 14:49:51 [INFO]: epoch 205: training loss 0.0672\n",
      "2025-06-01 14:49:51 [INFO]: epoch 206: training loss 0.0928\n",
      "2025-06-01 14:49:51 [INFO]: epoch 207: training loss 0.0704\n",
      "2025-06-01 14:49:51 [INFO]: epoch 208: training loss 0.0847\n",
      "2025-06-01 14:49:51 [INFO]: epoch 209: training loss 0.0690\n",
      "2025-06-01 14:49:51 [INFO]: epoch 210: training loss 0.0666\n",
      "2025-06-01 14:49:51 [INFO]: epoch 211: training loss 0.0704\n",
      "2025-06-01 14:49:51 [INFO]: epoch 212: training loss 0.0835\n",
      "2025-06-01 14:49:51 [INFO]: epoch 213: training loss 0.0761\n",
      "2025-06-01 14:49:51 [INFO]: epoch 214: training loss 0.0561\n",
      "2025-06-01 14:49:51 [INFO]: epoch 215: training loss 0.0690\n",
      "2025-06-01 14:49:51 [INFO]: epoch 216: training loss 0.0773\n",
      "2025-06-01 14:49:51 [INFO]: epoch 217: training loss 0.0657\n",
      "2025-06-01 14:49:51 [INFO]: epoch 218: training loss 0.0678\n",
      "2025-06-01 14:49:51 [INFO]: epoch 219: training loss 0.0606\n",
      "2025-06-01 14:49:51 [INFO]: epoch 220: training loss 0.0699\n",
      "2025-06-01 14:49:51 [INFO]: epoch 221: training loss 0.0610\n",
      "2025-06-01 14:49:51 [INFO]: epoch 222: training loss 0.0709\n",
      "2025-06-01 14:49:51 [INFO]: epoch 223: training loss 0.0673\n",
      "2025-06-01 14:49:51 [INFO]: epoch 224: training loss 0.0676\n",
      "2025-06-01 14:49:51 [INFO]: epoch 225: training loss 0.0759\n",
      "2025-06-01 14:49:51 [INFO]: epoch 226: training loss 0.0702\n",
      "2025-06-01 14:49:51 [INFO]: epoch 227: training loss 0.0676\n",
      "2025-06-01 14:49:51 [INFO]: epoch 228: training loss 0.0586\n",
      "2025-06-01 14:49:51 [INFO]: epoch 229: training loss 0.0700\n",
      "2025-06-01 14:49:51 [INFO]: epoch 230: training loss 0.0630\n",
      "2025-06-01 14:49:51 [INFO]: epoch 231: training loss 0.0550\n",
      "2025-06-01 14:49:51 [INFO]: epoch 232: training loss 0.0648\n",
      "2025-06-01 14:49:51 [INFO]: epoch 233: training loss 0.0639\n",
      "2025-06-01 14:49:51 [INFO]: epoch 234: training loss 0.0606\n",
      "2025-06-01 14:49:51 [INFO]: epoch 235: training loss 0.0615\n",
      "2025-06-01 14:49:51 [INFO]: epoch 236: training loss 0.0649\n",
      "2025-06-01 14:49:51 [INFO]: epoch 237: training loss 0.0600\n",
      "2025-06-01 14:49:51 [INFO]: epoch 238: training loss 0.0699\n",
      "2025-06-01 14:49:51 [INFO]: epoch 239: training loss 0.0677\n",
      "2025-06-01 14:49:51 [INFO]: epoch 240: training loss 0.0553\n",
      "2025-06-01 14:49:51 [INFO]: epoch 241: training loss 0.0617\n",
      "2025-06-01 14:49:51 [INFO]: epoch 242: training loss 0.0522\n",
      "2025-06-01 14:49:51 [INFO]: epoch 243: training loss 0.0561\n",
      "2025-06-01 14:49:51 [INFO]: epoch 244: training loss 0.0519\n",
      "2025-06-01 14:49:51 [INFO]: epoch 245: training loss 0.0645\n",
      "2025-06-01 14:49:51 [INFO]: epoch 246: training loss 0.0591\n",
      "2025-06-01 14:49:51 [INFO]: epoch 247: training loss 0.0616\n",
      "2025-06-01 14:49:51 [INFO]: epoch 248: training loss 0.0519\n",
      "2025-06-01 14:49:51 [INFO]: epoch 249: training loss 0.0731\n",
      "2025-06-01 14:49:51 [INFO]: epoch 250: training loss 0.0570\n",
      "2025-06-01 14:49:51 [INFO]: epoch 251: training loss 0.0627\n",
      "2025-06-01 14:49:51 [INFO]: epoch 252: training loss 0.0584\n",
      "2025-06-01 14:49:51 [INFO]: epoch 253: training loss 0.0722\n",
      "2025-06-01 14:49:52 [INFO]: epoch 254: training loss 0.0647\n",
      "2025-06-01 14:49:52 [INFO]: epoch 255: training loss 0.0532\n",
      "2025-06-01 14:49:52 [INFO]: epoch 256: training loss 0.0595\n",
      "2025-06-01 14:49:52 [INFO]: epoch 257: training loss 0.0576\n",
      "2025-06-01 14:49:52 [INFO]: epoch 258: training loss 0.0564\n",
      "2025-06-01 14:49:52 [INFO]: epoch 259: training loss 0.0551\n",
      "2025-06-01 14:49:52 [INFO]: epoch 260: training loss 0.0541\n",
      "2025-06-01 14:49:52 [INFO]: epoch 261: training loss 0.0676\n",
      "2025-06-01 14:49:52 [INFO]: epoch 262: training loss 0.0632\n",
      "2025-06-01 14:49:52 [INFO]: epoch 263: training loss 0.0672\n",
      "2025-06-01 14:49:52 [INFO]: epoch 264: training loss 0.0617\n",
      "2025-06-01 14:49:52 [INFO]: epoch 265: training loss 0.0595\n",
      "2025-06-01 14:49:52 [INFO]: epoch 266: training loss 0.0532\n",
      "2025-06-01 14:49:52 [INFO]: epoch 267: training loss 0.0537\n",
      "2025-06-01 14:49:52 [INFO]: epoch 268: training loss 0.0575\n",
      "2025-06-01 14:49:52 [INFO]: epoch 269: training loss 0.0643\n",
      "2025-06-01 14:49:52 [INFO]: epoch 270: training loss 0.0653\n",
      "2025-06-01 14:49:52 [INFO]: epoch 271: training loss 0.0558\n",
      "2025-06-01 14:49:52 [INFO]: epoch 272: training loss 0.0811\n",
      "2025-06-01 14:49:52 [INFO]: epoch 273: training loss 0.0535\n",
      "2025-06-01 14:49:52 [INFO]: epoch 274: training loss 0.0505\n",
      "2025-06-01 14:49:52 [INFO]: epoch 275: training loss 0.0574\n",
      "2025-06-01 14:49:52 [INFO]: epoch 276: training loss 0.0632\n",
      "2025-06-01 14:49:52 [INFO]: epoch 277: training loss 0.0676\n",
      "2025-06-01 14:49:52 [INFO]: epoch 278: training loss 0.0472\n",
      "2025-06-01 14:49:52 [INFO]: epoch 279: training loss 0.0637\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-01 14:49:52 [INFO]: epoch 280: training loss 0.0560\n",
      "2025-06-01 14:49:52 [INFO]: epoch 281: training loss 0.0592\n",
      "2025-06-01 14:49:52 [INFO]: epoch 282: training loss 0.0556\n",
      "2025-06-01 14:49:52 [INFO]: epoch 283: training loss 0.0649\n",
      "2025-06-01 14:49:52 [INFO]: epoch 284: training loss 0.0570\n",
      "2025-06-01 14:49:52 [INFO]: epoch 285: training loss 0.0460\n",
      "2025-06-01 14:49:52 [INFO]: epoch 286: training loss 0.0525\n",
      "2025-06-01 14:49:52 [INFO]: epoch 287: training loss 0.0650\n",
      "2025-06-01 14:49:52 [INFO]: epoch 288: training loss 0.0540\n",
      "2025-06-01 14:49:52 [INFO]: epoch 289: training loss 0.0630\n",
      "2025-06-01 14:49:52 [INFO]: epoch 290: training loss 0.0499\n",
      "2025-06-01 14:49:52 [INFO]: epoch 291: training loss 0.0572\n",
      "2025-06-01 14:49:52 [INFO]: epoch 292: training loss 0.0626\n",
      "2025-06-01 14:49:52 [INFO]: epoch 293: training loss 0.0478\n",
      "2025-06-01 14:49:52 [INFO]: epoch 294: training loss 0.0577\n",
      "2025-06-01 14:49:52 [INFO]: epoch 295: training loss 0.0579\n",
      "2025-06-01 14:49:52 [INFO]: epoch 296: training loss 0.0524\n",
      "2025-06-01 14:49:52 [INFO]: epoch 297: training loss 0.0503\n",
      "2025-06-01 14:49:52 [INFO]: epoch 298: training loss 0.0515\n",
      "2025-06-01 14:49:52 [INFO]: epoch 299: training loss 0.0505\n",
      "2025-06-01 14:49:52 [INFO]: epoch 300: training loss 0.0492\n",
      "2025-06-01 14:49:52 [INFO]: epoch 301: training loss 0.0532\n",
      "2025-06-01 14:49:52 [INFO]: epoch 302: training loss 0.0612\n",
      "2025-06-01 14:49:52 [INFO]: epoch 303: training loss 0.0455\n",
      "2025-06-01 14:49:52 [INFO]: epoch 304: training loss 0.0505\n",
      "2025-06-01 14:49:52 [INFO]: epoch 305: training loss 0.0526\n",
      "2025-06-01 14:49:52 [INFO]: epoch 306: training loss 0.0437\n",
      "2025-06-01 14:49:52 [INFO]: epoch 307: training loss 0.0465\n",
      "2025-06-01 14:49:52 [INFO]: epoch 308: training loss 0.0497\n",
      "2025-06-01 14:49:52 [INFO]: epoch 309: training loss 0.0412\n",
      "2025-06-01 14:49:52 [INFO]: epoch 310: training loss 0.0578\n",
      "2025-06-01 14:49:52 [INFO]: epoch 311: training loss 0.0523\n",
      "2025-06-01 14:49:52 [INFO]: epoch 312: training loss 0.0481\n",
      "2025-06-01 14:49:52 [INFO]: epoch 313: training loss 0.0502\n",
      "2025-06-01 14:49:52 [INFO]: epoch 314: training loss 0.0456\n",
      "2025-06-01 14:49:52 [INFO]: epoch 315: training loss 0.0536\n",
      "2025-06-01 14:49:52 [INFO]: epoch 316: training loss 0.0527\n",
      "2025-06-01 14:49:52 [INFO]: epoch 317: training loss 0.0467\n",
      "2025-06-01 14:49:52 [INFO]: epoch 318: training loss 0.0572\n",
      "2025-06-01 14:49:52 [INFO]: epoch 319: training loss 0.0609\n",
      "2025-06-01 14:49:52 [INFO]: epoch 320: training loss 0.0482\n",
      "2025-06-01 14:49:52 [INFO]: epoch 321: training loss 0.0562\n",
      "2025-06-01 14:49:52 [INFO]: epoch 322: training loss 0.0547\n",
      "2025-06-01 14:49:52 [INFO]: epoch 323: training loss 0.0531\n",
      "2025-06-01 14:49:52 [INFO]: epoch 324: training loss 0.0737\n",
      "2025-06-01 14:49:52 [INFO]: epoch 325: training loss 0.0578\n",
      "2025-06-01 14:49:52 [INFO]: epoch 326: training loss 0.0526\n",
      "2025-06-01 14:49:52 [INFO]: epoch 327: training loss 0.0456\n",
      "2025-06-01 14:49:53 [INFO]: epoch 328: training loss 0.0595\n",
      "2025-06-01 14:49:53 [INFO]: epoch 329: training loss 0.0553\n",
      "2025-06-01 14:49:53 [INFO]: epoch 330: training loss 0.0446\n",
      "2025-06-01 14:49:53 [INFO]: epoch 331: training loss 0.0548\n",
      "2025-06-01 14:49:53 [INFO]: epoch 332: training loss 0.0495\n",
      "2025-06-01 14:49:53 [INFO]: epoch 333: training loss 0.0490\n",
      "2025-06-01 14:49:53 [INFO]: epoch 334: training loss 0.0538\n",
      "2025-06-01 14:49:53 [INFO]: epoch 335: training loss 0.0458\n",
      "2025-06-01 14:49:53 [INFO]: epoch 336: training loss 0.0402\n",
      "2025-06-01 14:49:53 [INFO]: epoch 337: training loss 0.0503\n",
      "2025-06-01 14:49:53 [INFO]: epoch 338: training loss 0.0417\n",
      "2025-06-01 14:49:53 [INFO]: epoch 339: training loss 0.0506\n",
      "2025-06-01 14:49:53 [INFO]: epoch 340: training loss 0.0636\n",
      "2025-06-01 14:49:53 [INFO]: epoch 341: training loss 0.0480\n",
      "2025-06-01 14:49:53 [INFO]: epoch 342: training loss 0.0615\n",
      "2025-06-01 14:49:53 [INFO]: epoch 343: training loss 0.0572\n",
      "2025-06-01 14:49:53 [INFO]: epoch 344: training loss 0.0518\n",
      "2025-06-01 14:49:53 [INFO]: epoch 345: training loss 0.0566\n",
      "2025-06-01 14:49:53 [INFO]: epoch 346: training loss 0.0485\n",
      "2025-06-01 14:49:53 [INFO]: epoch 347: training loss 0.0497\n",
      "2025-06-01 14:49:53 [INFO]: epoch 348: training loss 0.0681\n",
      "2025-06-01 14:49:53 [INFO]: epoch 349: training loss 0.0431\n",
      "2025-06-01 14:49:53 [INFO]: epoch 350: training loss 0.0552\n",
      "2025-06-01 14:49:53 [INFO]: epoch 351: training loss 0.0458\n",
      "2025-06-01 14:49:53 [INFO]: epoch 352: training loss 0.0576\n",
      "2025-06-01 14:49:53 [INFO]: epoch 353: training loss 0.0640\n",
      "2025-06-01 14:49:53 [INFO]: epoch 354: training loss 0.0458\n",
      "2025-06-01 14:49:53 [INFO]: epoch 355: training loss 0.0624\n",
      "2025-06-01 14:49:53 [INFO]: epoch 356: training loss 0.0474\n",
      "2025-06-01 14:49:53 [INFO]: epoch 357: training loss 0.0450\n",
      "2025-06-01 14:49:53 [INFO]: epoch 358: training loss 0.0502\n",
      "2025-06-01 14:49:53 [INFO]: epoch 359: training loss 0.0435\n",
      "2025-06-01 14:49:53 [INFO]: epoch 360: training loss 0.0402\n",
      "2025-06-01 14:49:53 [INFO]: epoch 361: training loss 0.0427\n",
      "2025-06-01 14:49:53 [INFO]: epoch 362: training loss 0.0402\n",
      "2025-06-01 14:49:53 [INFO]: epoch 363: training loss 0.0484\n",
      "2025-06-01 14:49:53 [INFO]: epoch 364: training loss 0.0389\n",
      "2025-06-01 14:49:53 [INFO]: epoch 365: training loss 0.0388\n",
      "2025-06-01 14:49:53 [INFO]: epoch 366: training loss 0.0317\n",
      "2025-06-01 14:49:53 [INFO]: epoch 367: training loss 0.0343\n",
      "2025-06-01 14:49:53 [INFO]: epoch 368: training loss 0.0433\n",
      "2025-06-01 14:49:53 [INFO]: epoch 369: training loss 0.0454\n",
      "2025-06-01 14:49:53 [INFO]: epoch 370: training loss 0.0383\n",
      "2025-06-01 14:49:53 [INFO]: epoch 371: training loss 0.0378\n",
      "2025-06-01 14:49:53 [INFO]: epoch 372: training loss 0.0418\n",
      "2025-06-01 14:49:53 [INFO]: epoch 373: training loss 0.0372\n",
      "2025-06-01 14:49:53 [INFO]: epoch 374: training loss 0.0372\n",
      "2025-06-01 14:49:53 [INFO]: epoch 375: training loss 0.0360\n",
      "2025-06-01 14:49:53 [INFO]: epoch 376: training loss 0.0391\n",
      "2025-06-01 14:49:53 [INFO]: epoch 377: training loss 0.0386\n",
      "2025-06-01 14:49:53 [INFO]: epoch 378: training loss 0.0438\n",
      "2025-06-01 14:49:53 [INFO]: epoch 379: training loss 0.0400\n",
      "2025-06-01 14:49:53 [INFO]: epoch 380: training loss 0.0357\n",
      "2025-06-01 14:49:53 [INFO]: epoch 381: training loss 0.0397\n",
      "2025-06-01 14:49:53 [INFO]: epoch 382: training loss 0.0406\n",
      "2025-06-01 14:49:53 [INFO]: epoch 383: training loss 0.0342\n",
      "2025-06-01 14:49:53 [INFO]: epoch 384: training loss 0.0441\n",
      "2025-06-01 14:49:53 [INFO]: epoch 385: training loss 0.0347\n",
      "2025-06-01 14:49:53 [INFO]: epoch 386: training loss 0.0445\n",
      "2025-06-01 14:49:53 [INFO]: epoch 387: training loss 0.0477\n",
      "2025-06-01 14:49:53 [INFO]: epoch 388: training loss 0.0400\n",
      "2025-06-01 14:49:53 [INFO]: epoch 389: training loss 0.0437\n",
      "2025-06-01 14:49:53 [INFO]: epoch 390: training loss 0.0498\n",
      "2025-06-01 14:49:53 [INFO]: epoch 391: training loss 0.0364\n",
      "2025-06-01 14:49:53 [INFO]: epoch 392: training loss 0.0473\n",
      "2025-06-01 14:49:53 [INFO]: epoch 393: training loss 0.0482\n",
      "2025-06-01 14:49:53 [INFO]: epoch 394: training loss 0.0377\n",
      "2025-06-01 14:49:53 [INFO]: epoch 395: training loss 0.0463\n",
      "2025-06-01 14:49:53 [INFO]: epoch 396: training loss 0.0550\n",
      "2025-06-01 14:49:53 [INFO]: epoch 397: training loss 0.0415\n",
      "2025-06-01 14:49:53 [INFO]: epoch 398: training loss 0.0439\n",
      "2025-06-01 14:49:53 [INFO]: epoch 399: training loss 0.0528\n",
      "2025-06-01 14:49:53 [INFO]: Finished training.\n",
      "2025-06-01 14:49:53 [WARNING]: 🚨DeprecationWarning: The method impute is deprecated. Please use `predict` instead.\n",
      " 67%|████████████████████████████████████████████████████████                            | 4/6 [00:21<00:10,  5.40s/it]2025-06-01 14:49:54 [INFO]: No given device, using default device: cuda\n",
      "2025-06-01 14:49:54 [WARNING]: saving_path not given. Model files and tensorboard file will not be saved.\n",
      "2025-06-01 14:49:54 [INFO]: Model initialized successfully with the number of trainable parameters: 597,780\n",
      "2025-06-01 14:49:54 [INFO]: epoch 0: training loss 1.3454\n",
      "2025-06-01 14:49:54 [INFO]: epoch 1: training loss 0.7932\n",
      "2025-06-01 14:49:54 [INFO]: epoch 2: training loss 0.7855\n",
      "2025-06-01 14:49:54 [INFO]: epoch 3: training loss 0.8091\n",
      "2025-06-01 14:49:54 [INFO]: epoch 4: training loss 0.7026\n",
      "2025-06-01 14:49:54 [INFO]: epoch 5: training loss 0.6337\n",
      "2025-06-01 14:49:54 [INFO]: epoch 6: training loss 0.5517\n",
      "2025-06-01 14:49:54 [INFO]: epoch 7: training loss 0.5942\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-01 14:49:54 [INFO]: epoch 8: training loss 0.5940\n",
      "2025-06-01 14:49:54 [INFO]: epoch 9: training loss 0.6558\n",
      "2025-06-01 14:49:54 [INFO]: epoch 10: training loss 0.5728\n",
      "2025-06-01 14:49:54 [INFO]: epoch 11: training loss 0.5878\n",
      "2025-06-01 14:49:54 [INFO]: epoch 12: training loss 0.5158\n",
      "2025-06-01 14:49:54 [INFO]: epoch 13: training loss 0.4931\n",
      "2025-06-01 14:49:54 [INFO]: epoch 14: training loss 0.4609\n",
      "2025-06-01 14:49:54 [INFO]: epoch 15: training loss 0.4624\n",
      "2025-06-01 14:49:54 [INFO]: epoch 16: training loss 0.4854\n",
      "2025-06-01 14:49:54 [INFO]: epoch 17: training loss 0.5118\n",
      "2025-06-01 14:49:54 [INFO]: epoch 18: training loss 0.4735\n",
      "2025-06-01 14:49:54 [INFO]: epoch 19: training loss 0.4691\n",
      "2025-06-01 14:49:54 [INFO]: epoch 20: training loss 0.4735\n",
      "2025-06-01 14:49:54 [INFO]: epoch 21: training loss 0.4434\n",
      "2025-06-01 14:49:54 [INFO]: epoch 22: training loss 0.4464\n",
      "2025-06-01 14:49:54 [INFO]: epoch 23: training loss 0.4354\n",
      "2025-06-01 14:49:54 [INFO]: epoch 24: training loss 0.4300\n",
      "2025-06-01 14:49:54 [INFO]: epoch 25: training loss 0.4332\n",
      "2025-06-01 14:49:54 [INFO]: epoch 26: training loss 0.3903\n",
      "2025-06-01 14:49:54 [INFO]: epoch 27: training loss 0.4423\n",
      "2025-06-01 14:49:54 [INFO]: epoch 28: training loss 0.4597\n",
      "2025-06-01 14:49:54 [INFO]: epoch 29: training loss 0.4420\n",
      "2025-06-01 14:49:54 [INFO]: epoch 30: training loss 0.4221\n",
      "2025-06-01 14:49:54 [INFO]: epoch 31: training loss 0.4284\n",
      "2025-06-01 14:49:54 [INFO]: epoch 32: training loss 0.4384\n",
      "2025-06-01 14:49:54 [INFO]: epoch 33: training loss 0.4323\n",
      "2025-06-01 14:49:54 [INFO]: epoch 34: training loss 0.4247\n",
      "2025-06-01 14:49:54 [INFO]: epoch 35: training loss 0.3973\n",
      "2025-06-01 14:49:54 [INFO]: epoch 36: training loss 0.4085\n",
      "2025-06-01 14:49:54 [INFO]: epoch 37: training loss 0.3936\n",
      "2025-06-01 14:49:54 [INFO]: epoch 38: training loss 0.4090\n",
      "2025-06-01 14:49:54 [INFO]: epoch 39: training loss 0.3821\n",
      "2025-06-01 14:49:54 [INFO]: epoch 40: training loss 0.4061\n",
      "2025-06-01 14:49:54 [INFO]: epoch 41: training loss 0.3923\n",
      "2025-06-01 14:49:54 [INFO]: epoch 42: training loss 0.3755\n",
      "2025-06-01 14:49:54 [INFO]: epoch 43: training loss 0.4002\n",
      "2025-06-01 14:49:54 [INFO]: epoch 44: training loss 0.4239\n",
      "2025-06-01 14:49:54 [INFO]: epoch 45: training loss 0.3972\n",
      "2025-06-01 14:49:54 [INFO]: epoch 46: training loss 0.3769\n",
      "2025-06-01 14:49:54 [INFO]: epoch 47: training loss 0.3937\n",
      "2025-06-01 14:49:54 [INFO]: epoch 48: training loss 0.3746\n",
      "2025-06-01 14:49:54 [INFO]: epoch 49: training loss 0.3807\n",
      "2025-06-01 14:49:54 [INFO]: epoch 50: training loss 0.4175\n",
      "2025-06-01 14:49:54 [INFO]: epoch 51: training loss 0.3955\n",
      "2025-06-01 14:49:54 [INFO]: epoch 52: training loss 0.3767\n",
      "2025-06-01 14:49:54 [INFO]: epoch 53: training loss 0.3662\n",
      "2025-06-01 14:49:54 [INFO]: epoch 54: training loss 0.3646\n",
      "2025-06-01 14:49:54 [INFO]: epoch 55: training loss 0.4033\n",
      "2025-06-01 14:49:54 [INFO]: epoch 56: training loss 0.3617\n",
      "2025-06-01 14:49:54 [INFO]: epoch 57: training loss 0.3869\n",
      "2025-06-01 14:49:54 [INFO]: epoch 58: training loss 0.3598\n",
      "2025-06-01 14:49:54 [INFO]: epoch 59: training loss 0.3759\n",
      "2025-06-01 14:49:54 [INFO]: epoch 60: training loss 0.3694\n",
      "2025-06-01 14:49:54 [INFO]: epoch 61: training loss 0.3512\n",
      "2025-06-01 14:49:54 [INFO]: epoch 62: training loss 0.3589\n",
      "2025-06-01 14:49:54 [INFO]: epoch 63: training loss 0.3625\n",
      "2025-06-01 14:49:54 [INFO]: epoch 64: training loss 0.3308\n",
      "2025-06-01 14:49:54 [INFO]: epoch 65: training loss 0.3321\n",
      "2025-06-01 14:49:54 [INFO]: epoch 66: training loss 0.3317\n",
      "2025-06-01 14:49:54 [INFO]: epoch 67: training loss 0.3434\n",
      "2025-06-01 14:49:54 [INFO]: epoch 68: training loss 0.3157\n",
      "2025-06-01 14:49:54 [INFO]: epoch 69: training loss 0.3277\n",
      "2025-06-01 14:49:54 [INFO]: epoch 70: training loss 0.3197\n",
      "2025-06-01 14:49:55 [INFO]: epoch 71: training loss 0.3372\n",
      "2025-06-01 14:49:55 [INFO]: epoch 72: training loss 0.2957\n",
      "2025-06-01 14:49:55 [INFO]: epoch 73: training loss 0.3016\n",
      "2025-06-01 14:49:55 [INFO]: epoch 74: training loss 0.3108\n",
      "2025-06-01 14:49:55 [INFO]: epoch 75: training loss 0.3173\n",
      "2025-06-01 14:49:55 [INFO]: epoch 76: training loss 0.3218\n",
      "2025-06-01 14:49:55 [INFO]: epoch 77: training loss 0.3071\n",
      "2025-06-01 14:49:55 [INFO]: epoch 78: training loss 0.3160\n",
      "2025-06-01 14:49:55 [INFO]: epoch 79: training loss 0.3016\n",
      "2025-06-01 14:49:55 [INFO]: epoch 80: training loss 0.2928\n",
      "2025-06-01 14:49:55 [INFO]: epoch 81: training loss 0.3068\n",
      "2025-06-01 14:49:55 [INFO]: epoch 82: training loss 0.2690\n",
      "2025-06-01 14:49:55 [INFO]: epoch 83: training loss 0.3259\n",
      "2025-06-01 14:49:55 [INFO]: epoch 84: training loss 0.3252\n",
      "2025-06-01 14:49:55 [INFO]: epoch 85: training loss 0.3127\n",
      "2025-06-01 14:49:55 [INFO]: epoch 86: training loss 0.3184\n",
      "2025-06-01 14:49:55 [INFO]: epoch 87: training loss 0.2951\n",
      "2025-06-01 14:49:55 [INFO]: epoch 88: training loss 0.2862\n",
      "2025-06-01 14:49:55 [INFO]: epoch 89: training loss 0.2936\n",
      "2025-06-01 14:49:55 [INFO]: epoch 90: training loss 0.3264\n",
      "2025-06-01 14:49:55 [INFO]: epoch 91: training loss 0.2821\n",
      "2025-06-01 14:49:55 [INFO]: epoch 92: training loss 0.2774\n",
      "2025-06-01 14:49:55 [INFO]: epoch 93: training loss 0.3016\n",
      "2025-06-01 14:49:55 [INFO]: epoch 94: training loss 0.3197\n",
      "2025-06-01 14:49:55 [INFO]: epoch 95: training loss 0.2882\n",
      "2025-06-01 14:49:55 [INFO]: epoch 96: training loss 0.2737\n",
      "2025-06-01 14:49:55 [INFO]: epoch 97: training loss 0.2863\n",
      "2025-06-01 14:49:55 [INFO]: epoch 98: training loss 0.2730\n",
      "2025-06-01 14:49:55 [INFO]: epoch 99: training loss 0.2882\n",
      "2025-06-01 14:49:55 [INFO]: epoch 100: training loss 0.2920\n",
      "2025-06-01 14:49:55 [INFO]: epoch 101: training loss 0.2922\n",
      "2025-06-01 14:49:55 [INFO]: epoch 102: training loss 0.2982\n",
      "2025-06-01 14:49:55 [INFO]: epoch 103: training loss 0.2878\n",
      "2025-06-01 14:49:55 [INFO]: epoch 104: training loss 0.2948\n",
      "2025-06-01 14:49:55 [INFO]: epoch 105: training loss 0.2818\n",
      "2025-06-01 14:49:55 [INFO]: epoch 106: training loss 0.3026\n",
      "2025-06-01 14:49:55 [INFO]: epoch 107: training loss 0.2922\n",
      "2025-06-01 14:49:55 [INFO]: epoch 108: training loss 0.2636\n",
      "2025-06-01 14:49:55 [INFO]: epoch 109: training loss 0.2817\n",
      "2025-06-01 14:49:55 [INFO]: epoch 110: training loss 0.2449\n",
      "2025-06-01 14:49:55 [INFO]: epoch 111: training loss 0.2815\n",
      "2025-06-01 14:49:55 [INFO]: epoch 112: training loss 0.2984\n",
      "2025-06-01 14:49:55 [INFO]: epoch 113: training loss 0.2761\n",
      "2025-06-01 14:49:55 [INFO]: epoch 114: training loss 0.2677\n",
      "2025-06-01 14:49:55 [INFO]: epoch 115: training loss 0.2823\n",
      "2025-06-01 14:49:55 [INFO]: epoch 116: training loss 0.2719\n",
      "2025-06-01 14:49:55 [INFO]: epoch 117: training loss 0.2618\n",
      "2025-06-01 14:49:55 [INFO]: epoch 118: training loss 0.2733\n",
      "2025-06-01 14:49:55 [INFO]: epoch 119: training loss 0.2718\n",
      "2025-06-01 14:49:55 [INFO]: epoch 120: training loss 0.2548\n",
      "2025-06-01 14:49:55 [INFO]: epoch 121: training loss 0.2303\n",
      "2025-06-01 14:49:55 [INFO]: epoch 122: training loss 0.2670\n",
      "2025-06-01 14:49:55 [INFO]: epoch 123: training loss 0.2592\n",
      "2025-06-01 14:49:55 [INFO]: epoch 124: training loss 0.2689\n",
      "2025-06-01 14:49:55 [INFO]: epoch 125: training loss 0.2389\n",
      "2025-06-01 14:49:55 [INFO]: epoch 126: training loss 0.2276\n",
      "2025-06-01 14:49:55 [INFO]: epoch 127: training loss 0.2680\n",
      "2025-06-01 14:49:55 [INFO]: epoch 128: training loss 0.2545\n",
      "2025-06-01 14:49:55 [INFO]: epoch 129: training loss 0.2627\n",
      "2025-06-01 14:49:55 [INFO]: epoch 130: training loss 0.2431\n",
      "2025-06-01 14:49:55 [INFO]: epoch 131: training loss 0.2592\n",
      "2025-06-01 14:49:55 [INFO]: epoch 132: training loss 0.2597\n",
      "2025-06-01 14:49:55 [INFO]: epoch 133: training loss 0.2366\n",
      "2025-06-01 14:49:55 [INFO]: epoch 134: training loss 0.2465\n",
      "2025-06-01 14:49:55 [INFO]: epoch 135: training loss 0.2536\n",
      "2025-06-01 14:49:55 [INFO]: epoch 136: training loss 0.2226\n",
      "2025-06-01 14:49:55 [INFO]: epoch 137: training loss 0.2197\n",
      "2025-06-01 14:49:55 [INFO]: epoch 138: training loss 0.2527\n",
      "2025-06-01 14:49:55 [INFO]: epoch 139: training loss 0.2300\n",
      "2025-06-01 14:49:55 [INFO]: epoch 140: training loss 0.2377\n",
      "2025-06-01 14:49:55 [INFO]: epoch 141: training loss 0.2505\n",
      "2025-06-01 14:49:55 [INFO]: epoch 142: training loss 0.2314\n",
      "2025-06-01 14:49:55 [INFO]: epoch 143: training loss 0.2162\n",
      "2025-06-01 14:49:55 [INFO]: epoch 144: training loss 0.2258\n",
      "2025-06-01 14:49:55 [INFO]: epoch 145: training loss 0.2109\n",
      "2025-06-01 14:49:56 [INFO]: epoch 146: training loss 0.2236\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-01 14:49:56 [INFO]: epoch 147: training loss 0.2362\n",
      "2025-06-01 14:49:56 [INFO]: epoch 148: training loss 0.2203\n",
      "2025-06-01 14:49:56 [INFO]: epoch 149: training loss 0.2366\n",
      "2025-06-01 14:49:56 [INFO]: epoch 150: training loss 0.2245\n",
      "2025-06-01 14:49:56 [INFO]: epoch 151: training loss 0.2216\n",
      "2025-06-01 14:49:56 [INFO]: epoch 152: training loss 0.2020\n",
      "2025-06-01 14:49:56 [INFO]: epoch 153: training loss 0.2405\n",
      "2025-06-01 14:49:56 [INFO]: epoch 154: training loss 0.2499\n",
      "2025-06-01 14:49:56 [INFO]: epoch 155: training loss 0.2217\n",
      "2025-06-01 14:49:56 [INFO]: epoch 156: training loss 0.2234\n",
      "2025-06-01 14:49:56 [INFO]: epoch 157: training loss 0.2414\n",
      "2025-06-01 14:49:56 [INFO]: epoch 158: training loss 0.2299\n",
      "2025-06-01 14:49:56 [INFO]: epoch 159: training loss 0.2271\n",
      "2025-06-01 14:49:56 [INFO]: epoch 160: training loss 0.2262\n",
      "2025-06-01 14:49:56 [INFO]: epoch 161: training loss 0.2168\n",
      "2025-06-01 14:49:56 [INFO]: epoch 162: training loss 0.1997\n",
      "2025-06-01 14:49:56 [INFO]: epoch 163: training loss 0.2064\n",
      "2025-06-01 14:49:56 [INFO]: epoch 164: training loss 0.2237\n",
      "2025-06-01 14:49:56 [INFO]: epoch 165: training loss 0.2356\n",
      "2025-06-01 14:49:56 [INFO]: epoch 166: training loss 0.2377\n",
      "2025-06-01 14:49:56 [INFO]: epoch 167: training loss 0.2269\n",
      "2025-06-01 14:49:56 [INFO]: epoch 168: training loss 0.2081\n",
      "2025-06-01 14:49:56 [INFO]: epoch 169: training loss 0.2300\n",
      "2025-06-01 14:49:56 [INFO]: epoch 170: training loss 0.2209\n",
      "2025-06-01 14:49:56 [INFO]: epoch 171: training loss 0.2088\n",
      "2025-06-01 14:49:56 [INFO]: epoch 172: training loss 0.2299\n",
      "2025-06-01 14:49:56 [INFO]: epoch 173: training loss 0.2346\n",
      "2025-06-01 14:49:56 [INFO]: epoch 174: training loss 0.2223\n",
      "2025-06-01 14:49:56 [INFO]: epoch 175: training loss 0.2046\n",
      "2025-06-01 14:49:56 [INFO]: epoch 176: training loss 0.2200\n",
      "2025-06-01 14:49:56 [INFO]: epoch 177: training loss 0.2360\n",
      "2025-06-01 14:49:56 [INFO]: epoch 178: training loss 0.1989\n",
      "2025-06-01 14:49:56 [INFO]: epoch 179: training loss 0.2136\n",
      "2025-06-01 14:49:56 [INFO]: epoch 180: training loss 0.2101\n",
      "2025-06-01 14:49:56 [INFO]: epoch 181: training loss 0.2120\n",
      "2025-06-01 14:49:56 [INFO]: epoch 182: training loss 0.2007\n",
      "2025-06-01 14:49:56 [INFO]: epoch 183: training loss 0.2081\n",
      "2025-06-01 14:49:56 [INFO]: epoch 184: training loss 0.2136\n",
      "2025-06-01 14:49:56 [INFO]: epoch 185: training loss 0.1961\n",
      "2025-06-01 14:49:56 [INFO]: epoch 186: training loss 0.2040\n",
      "2025-06-01 14:49:56 [INFO]: epoch 187: training loss 0.1910\n",
      "2025-06-01 14:49:56 [INFO]: epoch 188: training loss 0.1936\n",
      "2025-06-01 14:49:56 [INFO]: epoch 189: training loss 0.1848\n",
      "2025-06-01 14:49:56 [INFO]: epoch 190: training loss 0.1967\n",
      "2025-06-01 14:49:56 [INFO]: epoch 191: training loss 0.2046\n",
      "2025-06-01 14:49:56 [INFO]: epoch 192: training loss 0.2004\n",
      "2025-06-01 14:49:56 [INFO]: epoch 193: training loss 0.2038\n",
      "2025-06-01 14:49:56 [INFO]: epoch 194: training loss 0.1959\n",
      "2025-06-01 14:49:56 [INFO]: epoch 195: training loss 0.2055\n",
      "2025-06-01 14:49:56 [INFO]: epoch 196: training loss 0.1677\n",
      "2025-06-01 14:49:56 [INFO]: epoch 197: training loss 0.1903\n",
      "2025-06-01 14:49:56 [INFO]: epoch 198: training loss 0.1802\n",
      "2025-06-01 14:49:56 [INFO]: epoch 199: training loss 0.1701\n",
      "2025-06-01 14:49:56 [INFO]: epoch 200: training loss 0.1597\n",
      "2025-06-01 14:49:56 [INFO]: epoch 201: training loss 0.1888\n",
      "2025-06-01 14:49:56 [INFO]: epoch 202: training loss 0.1896\n",
      "2025-06-01 14:49:56 [INFO]: epoch 203: training loss 0.1956\n",
      "2025-06-01 14:49:56 [INFO]: epoch 204: training loss 0.1775\n",
      "2025-06-01 14:49:56 [INFO]: epoch 205: training loss 0.1714\n",
      "2025-06-01 14:49:56 [INFO]: epoch 206: training loss 0.1828\n",
      "2025-06-01 14:49:56 [INFO]: epoch 207: training loss 0.1785\n",
      "2025-06-01 14:49:56 [INFO]: epoch 208: training loss 0.1700\n",
      "2025-06-01 14:49:56 [INFO]: epoch 209: training loss 0.1728\n",
      "2025-06-01 14:49:56 [INFO]: epoch 210: training loss 0.1769\n",
      "2025-06-01 14:49:56 [INFO]: epoch 211: training loss 0.1597\n",
      "2025-06-01 14:49:56 [INFO]: epoch 212: training loss 0.1842\n",
      "2025-06-01 14:49:56 [INFO]: epoch 213: training loss 0.1677\n",
      "2025-06-01 14:49:56 [INFO]: epoch 214: training loss 0.1565\n",
      "2025-06-01 14:49:56 [INFO]: epoch 215: training loss 0.1856\n",
      "2025-06-01 14:49:56 [INFO]: epoch 216: training loss 0.1720\n",
      "2025-06-01 14:49:56 [INFO]: epoch 217: training loss 0.1927\n",
      "2025-06-01 14:49:56 [INFO]: epoch 218: training loss 0.1751\n",
      "2025-06-01 14:49:56 [INFO]: epoch 219: training loss 0.1538\n",
      "2025-06-01 14:49:56 [INFO]: epoch 220: training loss 0.1679\n",
      "2025-06-01 14:49:56 [INFO]: epoch 221: training loss 0.1789\n",
      "2025-06-01 14:49:56 [INFO]: epoch 222: training loss 0.1765\n",
      "2025-06-01 14:49:57 [INFO]: epoch 223: training loss 0.1811\n",
      "2025-06-01 14:49:57 [INFO]: epoch 224: training loss 0.1894\n",
      "2025-06-01 14:49:57 [INFO]: epoch 225: training loss 0.1526\n",
      "2025-06-01 14:49:57 [INFO]: epoch 226: training loss 0.1553\n",
      "2025-06-01 14:49:57 [INFO]: epoch 227: training loss 0.1993\n",
      "2025-06-01 14:49:57 [INFO]: epoch 228: training loss 0.1968\n",
      "2025-06-01 14:49:57 [INFO]: epoch 229: training loss 0.1744\n",
      "2025-06-01 14:49:57 [INFO]: epoch 230: training loss 0.1622\n",
      "2025-06-01 14:49:57 [INFO]: epoch 231: training loss 0.1890\n",
      "2025-06-01 14:49:57 [INFO]: epoch 232: training loss 0.1571\n",
      "2025-06-01 14:49:57 [INFO]: epoch 233: training loss 0.1628\n",
      "2025-06-01 14:49:57 [INFO]: epoch 234: training loss 0.1783\n",
      "2025-06-01 14:49:57 [INFO]: epoch 235: training loss 0.1858\n",
      "2025-06-01 14:49:57 [INFO]: epoch 236: training loss 0.1708\n",
      "2025-06-01 14:49:57 [INFO]: epoch 237: training loss 0.1569\n",
      "2025-06-01 14:49:57 [INFO]: epoch 238: training loss 0.1634\n",
      "2025-06-01 14:49:57 [INFO]: epoch 239: training loss 0.1794\n",
      "2025-06-01 14:49:57 [INFO]: epoch 240: training loss 0.1903\n",
      "2025-06-01 14:49:57 [INFO]: epoch 241: training loss 0.1957\n",
      "2025-06-01 14:49:57 [INFO]: epoch 242: training loss 0.1757\n",
      "2025-06-01 14:49:57 [INFO]: epoch 243: training loss 0.1639\n",
      "2025-06-01 14:49:57 [INFO]: epoch 244: training loss 0.1615\n",
      "2025-06-01 14:49:57 [INFO]: epoch 245: training loss 0.1435\n",
      "2025-06-01 14:49:57 [INFO]: epoch 246: training loss 0.1441\n",
      "2025-06-01 14:49:57 [INFO]: epoch 247: training loss 0.1632\n",
      "2025-06-01 14:49:57 [INFO]: epoch 248: training loss 0.1682\n",
      "2025-06-01 14:49:57 [INFO]: epoch 249: training loss 0.1419\n",
      "2025-06-01 14:49:57 [INFO]: epoch 250: training loss 0.1344\n",
      "2025-06-01 14:49:57 [INFO]: epoch 251: training loss 0.1467\n",
      "2025-06-01 14:49:57 [INFO]: epoch 252: training loss 0.1452\n",
      "2025-06-01 14:49:57 [INFO]: epoch 253: training loss 0.1513\n",
      "2025-06-01 14:49:57 [INFO]: epoch 254: training loss 0.1671\n",
      "2025-06-01 14:49:57 [INFO]: epoch 255: training loss 0.1572\n",
      "2025-06-01 14:49:57 [INFO]: epoch 256: training loss 0.1281\n",
      "2025-06-01 14:49:57 [INFO]: epoch 257: training loss 0.1609\n",
      "2025-06-01 14:49:57 [INFO]: epoch 258: training loss 0.1447\n",
      "2025-06-01 14:49:57 [INFO]: epoch 259: training loss 0.1340\n",
      "2025-06-01 14:49:57 [INFO]: epoch 260: training loss 0.1237\n",
      "2025-06-01 14:49:57 [INFO]: epoch 261: training loss 0.1407\n",
      "2025-06-01 14:49:57 [INFO]: epoch 262: training loss 0.1384\n",
      "2025-06-01 14:49:57 [INFO]: epoch 263: training loss 0.1669\n",
      "2025-06-01 14:49:57 [INFO]: epoch 264: training loss 0.1570\n",
      "2025-06-01 14:49:57 [INFO]: epoch 265: training loss 0.1380\n",
      "2025-06-01 14:49:57 [INFO]: epoch 266: training loss 0.1303\n",
      "2025-06-01 14:49:57 [INFO]: epoch 267: training loss 0.1251\n",
      "2025-06-01 14:49:57 [INFO]: epoch 268: training loss 0.1465\n",
      "2025-06-01 14:49:57 [INFO]: epoch 269: training loss 0.1551\n",
      "2025-06-01 14:49:57 [INFO]: epoch 270: training loss 0.1257\n",
      "2025-06-01 14:49:57 [INFO]: epoch 271: training loss 0.1430\n",
      "2025-06-01 14:49:57 [INFO]: epoch 272: training loss 0.1221\n",
      "2025-06-01 14:49:57 [INFO]: epoch 273: training loss 0.1306\n",
      "2025-06-01 14:49:57 [INFO]: epoch 274: training loss 0.1336\n",
      "2025-06-01 14:49:57 [INFO]: epoch 275: training loss 0.1383\n",
      "2025-06-01 14:49:57 [INFO]: epoch 276: training loss 0.1393\n",
      "2025-06-01 14:49:57 [INFO]: epoch 277: training loss 0.1341\n",
      "2025-06-01 14:49:57 [INFO]: epoch 278: training loss 0.1336\n",
      "2025-06-01 14:49:57 [INFO]: epoch 279: training loss 0.1224\n",
      "2025-06-01 14:49:57 [INFO]: epoch 280: training loss 0.1279\n",
      "2025-06-01 14:49:57 [INFO]: epoch 281: training loss 0.1349\n",
      "2025-06-01 14:49:57 [INFO]: epoch 282: training loss 0.1323\n",
      "2025-06-01 14:49:57 [INFO]: epoch 283: training loss 0.1234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-01 14:49:57 [INFO]: epoch 284: training loss 0.1330\n",
      "2025-06-01 14:49:57 [INFO]: epoch 285: training loss 0.1244\n",
      "2025-06-01 14:49:57 [INFO]: epoch 286: training loss 0.1282\n",
      "2025-06-01 14:49:57 [INFO]: epoch 287: training loss 0.1245\n",
      "2025-06-01 14:49:57 [INFO]: epoch 288: training loss 0.1343\n",
      "2025-06-01 14:49:57 [INFO]: epoch 289: training loss 0.1335\n",
      "2025-06-01 14:49:57 [INFO]: epoch 290: training loss 0.1166\n",
      "2025-06-01 14:49:57 [INFO]: epoch 291: training loss 0.1111\n",
      "2025-06-01 14:49:57 [INFO]: epoch 292: training loss 0.1332\n",
      "2025-06-01 14:49:57 [INFO]: epoch 293: training loss 0.1290\n",
      "2025-06-01 14:49:57 [INFO]: epoch 294: training loss 0.1264\n",
      "2025-06-01 14:49:57 [INFO]: epoch 295: training loss 0.1336\n",
      "2025-06-01 14:49:57 [INFO]: epoch 296: training loss 0.1183\n",
      "2025-06-01 14:49:57 [INFO]: epoch 297: training loss 0.1243\n",
      "2025-06-01 14:49:57 [INFO]: epoch 298: training loss 0.1127\n",
      "2025-06-01 14:49:58 [INFO]: epoch 299: training loss 0.1219\n",
      "2025-06-01 14:49:58 [INFO]: epoch 300: training loss 0.1274\n",
      "2025-06-01 14:49:58 [INFO]: epoch 301: training loss 0.1226\n",
      "2025-06-01 14:49:58 [INFO]: epoch 302: training loss 0.1198\n",
      "2025-06-01 14:49:58 [INFO]: epoch 303: training loss 0.1226\n",
      "2025-06-01 14:49:58 [INFO]: epoch 304: training loss 0.1193\n",
      "2025-06-01 14:49:58 [INFO]: epoch 305: training loss 0.1186\n",
      "2025-06-01 14:49:58 [INFO]: epoch 306: training loss 0.1127\n",
      "2025-06-01 14:49:58 [INFO]: epoch 307: training loss 0.1226\n",
      "2025-06-01 14:49:58 [INFO]: epoch 308: training loss 0.1065\n",
      "2025-06-01 14:49:58 [INFO]: epoch 309: training loss 0.1199\n",
      "2025-06-01 14:49:58 [INFO]: epoch 310: training loss 0.1185\n",
      "2025-06-01 14:49:58 [INFO]: epoch 311: training loss 0.1235\n",
      "2025-06-01 14:49:58 [INFO]: epoch 312: training loss 0.1291\n",
      "2025-06-01 14:49:58 [INFO]: epoch 313: training loss 0.1223\n",
      "2025-06-01 14:49:58 [INFO]: epoch 314: training loss 0.1138\n",
      "2025-06-01 14:49:58 [INFO]: epoch 315: training loss 0.1168\n",
      "2025-06-01 14:49:58 [INFO]: epoch 316: training loss 0.0992\n",
      "2025-06-01 14:49:58 [INFO]: epoch 317: training loss 0.1099\n",
      "2025-06-01 14:49:58 [INFO]: epoch 318: training loss 0.1240\n",
      "2025-06-01 14:49:58 [INFO]: epoch 319: training loss 0.1126\n",
      "2025-06-01 14:49:58 [INFO]: epoch 320: training loss 0.1195\n",
      "2025-06-01 14:49:58 [INFO]: epoch 321: training loss 0.1090\n",
      "2025-06-01 14:49:58 [INFO]: epoch 322: training loss 0.1161\n",
      "2025-06-01 14:49:58 [INFO]: epoch 323: training loss 0.1174\n",
      "2025-06-01 14:49:58 [INFO]: epoch 324: training loss 0.1080\n",
      "2025-06-01 14:49:58 [INFO]: epoch 325: training loss 0.1246\n",
      "2025-06-01 14:49:58 [INFO]: epoch 326: training loss 0.1191\n",
      "2025-06-01 14:49:58 [INFO]: epoch 327: training loss 0.1021\n",
      "2025-06-01 14:49:58 [INFO]: epoch 328: training loss 0.1143\n",
      "2025-06-01 14:49:58 [INFO]: epoch 329: training loss 0.1091\n",
      "2025-06-01 14:49:58 [INFO]: epoch 330: training loss 0.1150\n",
      "2025-06-01 14:49:58 [INFO]: epoch 331: training loss 0.1045\n",
      "2025-06-01 14:49:58 [INFO]: epoch 332: training loss 0.0981\n",
      "2025-06-01 14:49:58 [INFO]: epoch 333: training loss 0.1124\n",
      "2025-06-01 14:49:58 [INFO]: epoch 334: training loss 0.1163\n",
      "2025-06-01 14:49:58 [INFO]: epoch 335: training loss 0.1174\n",
      "2025-06-01 14:49:58 [INFO]: epoch 336: training loss 0.1084\n",
      "2025-06-01 14:49:58 [INFO]: epoch 337: training loss 0.1078\n",
      "2025-06-01 14:49:58 [INFO]: epoch 338: training loss 0.1010\n",
      "2025-06-01 14:49:58 [INFO]: epoch 339: training loss 0.0973\n",
      "2025-06-01 14:49:58 [INFO]: epoch 340: training loss 0.1163\n",
      "2025-06-01 14:49:58 [INFO]: epoch 341: training loss 0.1148\n",
      "2025-06-01 14:49:58 [INFO]: epoch 342: training loss 0.0941\n",
      "2025-06-01 14:49:58 [INFO]: epoch 343: training loss 0.1023\n",
      "2025-06-01 14:49:58 [INFO]: epoch 344: training loss 0.1038\n",
      "2025-06-01 14:49:58 [INFO]: epoch 345: training loss 0.0986\n",
      "2025-06-01 14:49:58 [INFO]: epoch 346: training loss 0.1045\n",
      "2025-06-01 14:49:58 [INFO]: epoch 347: training loss 0.0878\n",
      "2025-06-01 14:49:58 [INFO]: epoch 348: training loss 0.1022\n",
      "2025-06-01 14:49:58 [INFO]: epoch 349: training loss 0.1006\n",
      "2025-06-01 14:49:58 [INFO]: epoch 350: training loss 0.1026\n",
      "2025-06-01 14:49:58 [INFO]: epoch 351: training loss 0.1064\n",
      "2025-06-01 14:49:58 [INFO]: epoch 352: training loss 0.1022\n",
      "2025-06-01 14:49:58 [INFO]: epoch 353: training loss 0.0932\n",
      "2025-06-01 14:49:58 [INFO]: epoch 354: training loss 0.1034\n",
      "2025-06-01 14:49:58 [INFO]: epoch 355: training loss 0.0845\n",
      "2025-06-01 14:49:58 [INFO]: epoch 356: training loss 0.0969\n",
      "2025-06-01 14:49:58 [INFO]: epoch 357: training loss 0.0896\n",
      "2025-06-01 14:49:58 [INFO]: epoch 358: training loss 0.0939\n",
      "2025-06-01 14:49:58 [INFO]: epoch 359: training loss 0.0880\n",
      "2025-06-01 14:49:58 [INFO]: epoch 360: training loss 0.0921\n",
      "2025-06-01 14:49:58 [INFO]: epoch 361: training loss 0.0958\n",
      "2025-06-01 14:49:58 [INFO]: epoch 362: training loss 0.0937\n",
      "2025-06-01 14:49:58 [INFO]: epoch 363: training loss 0.1162\n",
      "2025-06-01 14:49:58 [INFO]: epoch 364: training loss 0.0964\n",
      "2025-06-01 14:49:58 [INFO]: epoch 365: training loss 0.0925\n",
      "2025-06-01 14:49:58 [INFO]: epoch 366: training loss 0.1001\n",
      "2025-06-01 14:49:58 [INFO]: epoch 367: training loss 0.1089\n",
      "2025-06-01 14:49:58 [INFO]: epoch 368: training loss 0.1020\n",
      "2025-06-01 14:49:58 [INFO]: epoch 369: training loss 0.1002\n",
      "2025-06-01 14:49:58 [INFO]: epoch 370: training loss 0.1014\n",
      "2025-06-01 14:49:58 [INFO]: epoch 371: training loss 0.0928\n",
      "2025-06-01 14:49:58 [INFO]: epoch 372: training loss 0.1065\n",
      "2025-06-01 14:49:58 [INFO]: epoch 373: training loss 0.1036\n",
      "2025-06-01 14:49:58 [INFO]: epoch 374: training loss 0.1056\n",
      "2025-06-01 14:49:58 [INFO]: epoch 375: training loss 0.0970\n",
      "2025-06-01 14:49:59 [INFO]: epoch 376: training loss 0.1011\n",
      "2025-06-01 14:49:59 [INFO]: epoch 377: training loss 0.0981\n",
      "2025-06-01 14:49:59 [INFO]: epoch 378: training loss 0.0958\n",
      "2025-06-01 14:49:59 [INFO]: epoch 379: training loss 0.0989\n",
      "2025-06-01 14:49:59 [INFO]: epoch 380: training loss 0.1170\n",
      "2025-06-01 14:49:59 [INFO]: epoch 381: training loss 0.0905\n",
      "2025-06-01 14:49:59 [INFO]: epoch 382: training loss 0.0792\n",
      "2025-06-01 14:49:59 [INFO]: epoch 383: training loss 0.0959\n",
      "2025-06-01 14:49:59 [INFO]: epoch 384: training loss 0.1051\n",
      "2025-06-01 14:49:59 [INFO]: epoch 385: training loss 0.1045\n",
      "2025-06-01 14:49:59 [INFO]: epoch 386: training loss 0.1091\n",
      "2025-06-01 14:49:59 [INFO]: epoch 387: training loss 0.0789\n",
      "2025-06-01 14:49:59 [INFO]: epoch 388: training loss 0.1060\n",
      "2025-06-01 14:49:59 [INFO]: epoch 389: training loss 0.0952\n",
      "2025-06-01 14:49:59 [INFO]: epoch 390: training loss 0.1104\n",
      "2025-06-01 14:49:59 [INFO]: epoch 391: training loss 0.0977\n",
      "2025-06-01 14:49:59 [INFO]: epoch 392: training loss 0.0995\n",
      "2025-06-01 14:49:59 [INFO]: epoch 393: training loss 0.0903\n",
      "2025-06-01 14:49:59 [INFO]: epoch 394: training loss 0.0865\n",
      "2025-06-01 14:49:59 [INFO]: epoch 395: training loss 0.0930\n",
      "2025-06-01 14:49:59 [INFO]: epoch 396: training loss 0.0992\n",
      "2025-06-01 14:49:59 [INFO]: epoch 397: training loss 0.0897\n",
      "2025-06-01 14:49:59 [INFO]: epoch 398: training loss 0.0812\n",
      "2025-06-01 14:49:59 [INFO]: epoch 399: training loss 0.0891\n",
      "2025-06-01 14:49:59 [INFO]: Finished training.\n",
      "2025-06-01 14:49:59 [WARNING]: 🚨DeprecationWarning: The method impute is deprecated. Please use `predict` instead.\n",
      " 83%|██████████████████████████████████████████████████████████████████████              | 5/6 [00:26<00:05,  5.38s/it]2025-06-01 14:49:59 [INFO]: No given device, using default device: cuda\n",
      "2025-06-01 14:49:59 [WARNING]: saving_path not given. Model files and tensorboard file will not be saved.\n",
      "2025-06-01 14:49:59 [INFO]: Model initialized successfully with the number of trainable parameters: 597,780\n",
      "2025-06-01 14:49:59 [INFO]: epoch 0: training loss 1.3390\n",
      "2025-06-01 14:49:59 [INFO]: epoch 1: training loss 0.7851\n",
      "2025-06-01 14:49:59 [INFO]: epoch 2: training loss 0.6589\n",
      "2025-06-01 14:49:59 [INFO]: epoch 3: training loss 0.5502\n",
      "2025-06-01 14:49:59 [INFO]: epoch 4: training loss 0.6786\n",
      "2025-06-01 14:49:59 [INFO]: epoch 5: training loss 0.5922\n",
      "2025-06-01 14:49:59 [INFO]: epoch 6: training loss 0.5569\n",
      "2025-06-01 14:49:59 [INFO]: epoch 7: training loss 0.5128\n",
      "2025-06-01 14:49:59 [INFO]: epoch 8: training loss 0.4895\n",
      "2025-06-01 14:49:59 [INFO]: epoch 9: training loss 0.4394\n",
      "2025-06-01 14:49:59 [INFO]: epoch 10: training loss 0.4686\n",
      "2025-06-01 14:49:59 [INFO]: epoch 11: training loss 0.4277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-01 14:49:59 [INFO]: epoch 12: training loss 0.4702\n",
      "2025-06-01 14:49:59 [INFO]: epoch 13: training loss 0.4653\n",
      "2025-06-01 14:49:59 [INFO]: epoch 14: training loss 0.4690\n",
      "2025-06-01 14:49:59 [INFO]: epoch 15: training loss 0.4021\n",
      "2025-06-01 14:49:59 [INFO]: epoch 16: training loss 0.3963\n",
      "2025-06-01 14:49:59 [INFO]: epoch 17: training loss 0.4139\n",
      "2025-06-01 14:49:59 [INFO]: epoch 18: training loss 0.4592\n",
      "2025-06-01 14:49:59 [INFO]: epoch 19: training loss 0.3498\n",
      "2025-06-01 14:49:59 [INFO]: epoch 20: training loss 0.3793\n",
      "2025-06-01 14:49:59 [INFO]: epoch 21: training loss 0.3828\n",
      "2025-06-01 14:49:59 [INFO]: epoch 22: training loss 0.3813\n",
      "2025-06-01 14:49:59 [INFO]: epoch 23: training loss 0.3810\n",
      "2025-06-01 14:49:59 [INFO]: epoch 24: training loss 0.3850\n",
      "2025-06-01 14:49:59 [INFO]: epoch 25: training loss 0.3871\n",
      "2025-06-01 14:49:59 [INFO]: epoch 26: training loss 0.3730\n",
      "2025-06-01 14:49:59 [INFO]: epoch 27: training loss 0.3688\n",
      "2025-06-01 14:49:59 [INFO]: epoch 28: training loss 0.3431\n",
      "2025-06-01 14:49:59 [INFO]: epoch 29: training loss 0.3549\n",
      "2025-06-01 14:49:59 [INFO]: epoch 30: training loss 0.3316\n",
      "2025-06-01 14:49:59 [INFO]: epoch 31: training loss 0.3406\n",
      "2025-06-01 14:49:59 [INFO]: epoch 32: training loss 0.3425\n",
      "2025-06-01 14:49:59 [INFO]: epoch 33: training loss 0.3521\n",
      "2025-06-01 14:49:59 [INFO]: epoch 34: training loss 0.3362\n",
      "2025-06-01 14:49:59 [INFO]: epoch 35: training loss 0.3428\n",
      "2025-06-01 14:49:59 [INFO]: epoch 36: training loss 0.3166\n",
      "2025-06-01 14:49:59 [INFO]: epoch 37: training loss 0.3449\n",
      "2025-06-01 14:49:59 [INFO]: epoch 38: training loss 0.3548\n",
      "2025-06-01 14:49:59 [INFO]: epoch 39: training loss 0.3170\n",
      "2025-06-01 14:49:59 [INFO]: epoch 40: training loss 0.3210\n",
      "2025-06-01 14:49:59 [INFO]: epoch 41: training loss 0.3265\n",
      "2025-06-01 14:49:59 [INFO]: epoch 42: training loss 0.3172\n",
      "2025-06-01 14:50:00 [INFO]: epoch 43: training loss 0.3183\n",
      "2025-06-01 14:50:00 [INFO]: epoch 44: training loss 0.3272\n",
      "2025-06-01 14:50:00 [INFO]: epoch 45: training loss 0.3183\n",
      "2025-06-01 14:50:00 [INFO]: epoch 46: training loss 0.3031\n",
      "2025-06-01 14:50:00 [INFO]: epoch 47: training loss 0.2860\n",
      "2025-06-01 14:50:00 [INFO]: epoch 48: training loss 0.3213\n",
      "2025-06-01 14:50:00 [INFO]: epoch 49: training loss 0.3240\n",
      "2025-06-01 14:50:00 [INFO]: epoch 50: training loss 0.3232\n",
      "2025-06-01 14:50:00 [INFO]: epoch 51: training loss 0.3215\n",
      "2025-06-01 14:50:00 [INFO]: epoch 52: training loss 0.3260\n",
      "2025-06-01 14:50:00 [INFO]: epoch 53: training loss 0.3216\n",
      "2025-06-01 14:50:00 [INFO]: epoch 54: training loss 0.3133\n",
      "2025-06-01 14:50:00 [INFO]: epoch 55: training loss 0.2928\n",
      "2025-06-01 14:50:00 [INFO]: epoch 56: training loss 0.2965\n",
      "2025-06-01 14:50:00 [INFO]: epoch 57: training loss 0.3040\n",
      "2025-06-01 14:50:00 [INFO]: epoch 58: training loss 0.2738\n",
      "2025-06-01 14:50:00 [INFO]: epoch 59: training loss 0.3047\n",
      "2025-06-01 14:50:00 [INFO]: epoch 60: training loss 0.2893\n",
      "2025-06-01 14:50:00 [INFO]: epoch 61: training loss 0.3070\n",
      "2025-06-01 14:50:00 [INFO]: epoch 62: training loss 0.2865\n",
      "2025-06-01 14:50:00 [INFO]: epoch 63: training loss 0.3286\n",
      "2025-06-01 14:50:00 [INFO]: epoch 64: training loss 0.2910\n",
      "2025-06-01 14:50:00 [INFO]: epoch 65: training loss 0.2887\n",
      "2025-06-01 14:50:00 [INFO]: epoch 66: training loss 0.2666\n",
      "2025-06-01 14:50:00 [INFO]: epoch 67: training loss 0.2991\n",
      "2025-06-01 14:50:00 [INFO]: epoch 68: training loss 0.2953\n",
      "2025-06-01 14:50:00 [INFO]: epoch 69: training loss 0.2833\n",
      "2025-06-01 14:50:00 [INFO]: epoch 70: training loss 0.2973\n",
      "2025-06-01 14:50:00 [INFO]: epoch 71: training loss 0.3149\n",
      "2025-06-01 14:50:00 [INFO]: epoch 72: training loss 0.2920\n",
      "2025-06-01 14:50:00 [INFO]: epoch 73: training loss 0.2897\n",
      "2025-06-01 14:50:00 [INFO]: epoch 74: training loss 0.2929\n",
      "2025-06-01 14:50:00 [INFO]: epoch 75: training loss 0.2646\n",
      "2025-06-01 14:50:00 [INFO]: epoch 76: training loss 0.2584\n",
      "2025-06-01 14:50:00 [INFO]: epoch 77: training loss 0.2955\n",
      "2025-06-01 14:50:00 [INFO]: epoch 78: training loss 0.2707\n",
      "2025-06-01 14:50:00 [INFO]: epoch 79: training loss 0.2506\n",
      "2025-06-01 14:50:00 [INFO]: epoch 80: training loss 0.2747\n",
      "2025-06-01 14:50:00 [INFO]: epoch 81: training loss 0.2826\n",
      "2025-06-01 14:50:00 [INFO]: epoch 82: training loss 0.2604\n",
      "2025-06-01 14:50:00 [INFO]: epoch 83: training loss 0.2860\n",
      "2025-06-01 14:50:00 [INFO]: epoch 84: training loss 0.2780\n",
      "2025-06-01 14:50:00 [INFO]: epoch 85: training loss 0.2468\n",
      "2025-06-01 14:50:00 [INFO]: epoch 86: training loss 0.2863\n",
      "2025-06-01 14:50:00 [INFO]: epoch 87: training loss 0.2888\n",
      "2025-06-01 14:50:00 [INFO]: epoch 88: training loss 0.2693\n",
      "2025-06-01 14:50:00 [INFO]: epoch 89: training loss 0.2456\n",
      "2025-06-01 14:50:00 [INFO]: epoch 90: training loss 0.2640\n",
      "2025-06-01 14:50:00 [INFO]: epoch 91: training loss 0.2611\n",
      "2025-06-01 14:50:00 [INFO]: epoch 92: training loss 0.2526\n",
      "2025-06-01 14:50:00 [INFO]: epoch 93: training loss 0.2590\n",
      "2025-06-01 14:50:00 [INFO]: epoch 94: training loss 0.2587\n",
      "2025-06-01 14:50:00 [INFO]: epoch 95: training loss 0.2559\n",
      "2025-06-01 14:50:00 [INFO]: epoch 96: training loss 0.2660\n",
      "2025-06-01 14:50:00 [INFO]: epoch 97: training loss 0.2349\n",
      "2025-06-01 14:50:00 [INFO]: epoch 98: training loss 0.2248\n",
      "2025-06-01 14:50:00 [INFO]: epoch 99: training loss 0.2633\n",
      "2025-06-01 14:50:00 [INFO]: epoch 100: training loss 0.2579\n",
      "2025-06-01 14:50:00 [INFO]: epoch 101: training loss 0.2140\n",
      "2025-06-01 14:50:00 [INFO]: epoch 102: training loss 0.2310\n",
      "2025-06-01 14:50:00 [INFO]: epoch 103: training loss 0.2462\n",
      "2025-06-01 14:50:00 [INFO]: epoch 104: training loss 0.2460\n",
      "2025-06-01 14:50:00 [INFO]: epoch 105: training loss 0.2342\n",
      "2025-06-01 14:50:00 [INFO]: epoch 106: training loss 0.2190\n",
      "2025-06-01 14:50:00 [INFO]: epoch 107: training loss 0.2185\n",
      "2025-06-01 14:50:00 [INFO]: epoch 108: training loss 0.2431\n",
      "2025-06-01 14:50:00 [INFO]: epoch 109: training loss 0.2480\n",
      "2025-06-01 14:50:00 [INFO]: epoch 110: training loss 0.2401\n",
      "2025-06-01 14:50:00 [INFO]: epoch 111: training loss 0.2218\n",
      "2025-06-01 14:50:00 [INFO]: epoch 112: training loss 0.2472\n",
      "2025-06-01 14:50:00 [INFO]: epoch 113: training loss 0.2307\n",
      "2025-06-01 14:50:00 [INFO]: epoch 114: training loss 0.2125\n",
      "2025-06-01 14:50:00 [INFO]: epoch 115: training loss 0.2463\n",
      "2025-06-01 14:50:00 [INFO]: epoch 116: training loss 0.2470\n",
      "2025-06-01 14:50:00 [INFO]: epoch 117: training loss 0.2408\n",
      "2025-06-01 14:50:01 [INFO]: epoch 118: training loss 0.2364\n",
      "2025-06-01 14:50:01 [INFO]: epoch 119: training loss 0.2161\n",
      "2025-06-01 14:50:01 [INFO]: epoch 120: training loss 0.2256\n",
      "2025-06-01 14:50:01 [INFO]: epoch 121: training loss 0.2539\n",
      "2025-06-01 14:50:01 [INFO]: epoch 122: training loss 0.2636\n",
      "2025-06-01 14:50:01 [INFO]: epoch 123: training loss 0.2449\n",
      "2025-06-01 14:50:01 [INFO]: epoch 124: training loss 0.2528\n",
      "2025-06-01 14:50:01 [INFO]: epoch 125: training loss 0.2263\n",
      "2025-06-01 14:50:01 [INFO]: epoch 126: training loss 0.2392\n",
      "2025-06-01 14:50:01 [INFO]: epoch 127: training loss 0.2420\n",
      "2025-06-01 14:50:01 [INFO]: epoch 128: training loss 0.2351\n",
      "2025-06-01 14:50:01 [INFO]: epoch 129: training loss 0.2197\n",
      "2025-06-01 14:50:01 [INFO]: epoch 130: training loss 0.2301\n",
      "2025-06-01 14:50:01 [INFO]: epoch 131: training loss 0.2179\n",
      "2025-06-01 14:50:01 [INFO]: epoch 132: training loss 0.2083\n",
      "2025-06-01 14:50:01 [INFO]: epoch 133: training loss 0.2157\n",
      "2025-06-01 14:50:01 [INFO]: epoch 134: training loss 0.2040\n",
      "2025-06-01 14:50:01 [INFO]: epoch 135: training loss 0.1949\n",
      "2025-06-01 14:50:01 [INFO]: epoch 136: training loss 0.2305\n",
      "2025-06-01 14:50:01 [INFO]: epoch 137: training loss 0.2064\n",
      "2025-06-01 14:50:01 [INFO]: epoch 138: training loss 0.2137\n",
      "2025-06-01 14:50:01 [INFO]: epoch 139: training loss 0.1990\n",
      "2025-06-01 14:50:01 [INFO]: epoch 140: training loss 0.2002\n",
      "2025-06-01 14:50:01 [INFO]: epoch 141: training loss 0.2076\n",
      "2025-06-01 14:50:01 [INFO]: epoch 142: training loss 0.2272\n",
      "2025-06-01 14:50:01 [INFO]: epoch 143: training loss 0.2200\n",
      "2025-06-01 14:50:01 [INFO]: epoch 144: training loss 0.2169\n",
      "2025-06-01 14:50:01 [INFO]: epoch 145: training loss 0.2010\n",
      "2025-06-01 14:50:01 [INFO]: epoch 146: training loss 0.2186\n",
      "2025-06-01 14:50:01 [INFO]: epoch 147: training loss 0.1960\n",
      "2025-06-01 14:50:01 [INFO]: epoch 148: training loss 0.1934\n",
      "2025-06-01 14:50:01 [INFO]: epoch 149: training loss 0.1975\n",
      "2025-06-01 14:50:01 [INFO]: epoch 150: training loss 0.1999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-01 14:50:01 [INFO]: epoch 151: training loss 0.2092\n",
      "2025-06-01 14:50:01 [INFO]: epoch 152: training loss 0.2354\n",
      "2025-06-01 14:50:01 [INFO]: epoch 153: training loss 0.1958\n",
      "2025-06-01 14:50:01 [INFO]: epoch 154: training loss 0.2093\n",
      "2025-06-01 14:50:01 [INFO]: epoch 155: training loss 0.1851\n",
      "2025-06-01 14:50:01 [INFO]: epoch 156: training loss 0.1945\n",
      "2025-06-01 14:50:01 [INFO]: epoch 157: training loss 0.2111\n",
      "2025-06-01 14:50:01 [INFO]: epoch 158: training loss 0.2155\n",
      "2025-06-01 14:50:01 [INFO]: epoch 159: training loss 0.1944\n",
      "2025-06-01 14:50:01 [INFO]: epoch 160: training loss 0.1993\n",
      "2025-06-01 14:50:01 [INFO]: epoch 161: training loss 0.1904\n",
      "2025-06-01 14:50:01 [INFO]: epoch 162: training loss 0.1978\n",
      "2025-06-01 14:50:01 [INFO]: epoch 163: training loss 0.1968\n",
      "2025-06-01 14:50:01 [INFO]: epoch 164: training loss 0.2093\n",
      "2025-06-01 14:50:01 [INFO]: epoch 165: training loss 0.2001\n",
      "2025-06-01 14:50:01 [INFO]: epoch 166: training loss 0.2015\n",
      "2025-06-01 14:50:01 [INFO]: epoch 167: training loss 0.1735\n",
      "2025-06-01 14:50:01 [INFO]: epoch 168: training loss 0.1857\n",
      "2025-06-01 14:50:01 [INFO]: epoch 169: training loss 0.1998\n",
      "2025-06-01 14:50:01 [INFO]: epoch 170: training loss 0.1986\n",
      "2025-06-01 14:50:01 [INFO]: epoch 171: training loss 0.1790\n",
      "2025-06-01 14:50:01 [INFO]: epoch 172: training loss 0.2071\n",
      "2025-06-01 14:50:01 [INFO]: epoch 173: training loss 0.1769\n",
      "2025-06-01 14:50:01 [INFO]: epoch 174: training loss 0.1672\n",
      "2025-06-01 14:50:01 [INFO]: epoch 175: training loss 0.1929\n",
      "2025-06-01 14:50:01 [INFO]: epoch 176: training loss 0.1896\n",
      "2025-06-01 14:50:01 [INFO]: epoch 177: training loss 0.1707\n",
      "2025-06-01 14:50:01 [INFO]: epoch 178: training loss 0.1803\n",
      "2025-06-01 14:50:01 [INFO]: epoch 179: training loss 0.2043\n",
      "2025-06-01 14:50:01 [INFO]: epoch 180: training loss 0.1776\n",
      "2025-06-01 14:50:01 [INFO]: epoch 181: training loss 0.1725\n",
      "2025-06-01 14:50:01 [INFO]: epoch 182: training loss 0.1871\n",
      "2025-06-01 14:50:01 [INFO]: epoch 183: training loss 0.1745\n",
      "2025-06-01 14:50:01 [INFO]: epoch 184: training loss 0.1753\n",
      "2025-06-01 14:50:01 [INFO]: epoch 185: training loss 0.1825\n",
      "2025-06-01 14:50:01 [INFO]: epoch 186: training loss 0.1819\n",
      "2025-06-01 14:50:01 [INFO]: epoch 187: training loss 0.1752\n",
      "2025-06-01 14:50:01 [INFO]: epoch 188: training loss 0.2015\n",
      "2025-06-01 14:50:01 [INFO]: epoch 189: training loss 0.2008\n",
      "2025-06-01 14:50:01 [INFO]: epoch 190: training loss 0.1742\n",
      "2025-06-01 14:50:01 [INFO]: epoch 191: training loss 0.1831\n",
      "2025-06-01 14:50:01 [INFO]: epoch 192: training loss 0.1770\n",
      "2025-06-01 14:50:02 [INFO]: epoch 193: training loss 0.1764\n",
      "2025-06-01 14:50:02 [INFO]: epoch 194: training loss 0.1689\n",
      "2025-06-01 14:50:02 [INFO]: epoch 195: training loss 0.1736\n",
      "2025-06-01 14:50:02 [INFO]: epoch 196: training loss 0.1752\n",
      "2025-06-01 14:50:02 [INFO]: epoch 197: training loss 0.1805\n",
      "2025-06-01 14:50:02 [INFO]: epoch 198: training loss 0.1749\n",
      "2025-06-01 14:50:02 [INFO]: epoch 199: training loss 0.2011\n",
      "2025-06-01 14:50:02 [INFO]: epoch 200: training loss 0.1724\n",
      "2025-06-01 14:50:02 [INFO]: epoch 201: training loss 0.1860\n",
      "2025-06-01 14:50:02 [INFO]: epoch 202: training loss 0.1716\n",
      "2025-06-01 14:50:02 [INFO]: epoch 203: training loss 0.1761\n",
      "2025-06-01 14:50:02 [INFO]: epoch 204: training loss 0.1778\n",
      "2025-06-01 14:50:02 [INFO]: epoch 205: training loss 0.1817\n",
      "2025-06-01 14:50:02 [INFO]: epoch 206: training loss 0.1605\n",
      "2025-06-01 14:50:02 [INFO]: epoch 207: training loss 0.1625\n",
      "2025-06-01 14:50:02 [INFO]: epoch 208: training loss 0.1763\n",
      "2025-06-01 14:50:02 [INFO]: epoch 209: training loss 0.1618\n",
      "2025-06-01 14:50:02 [INFO]: epoch 210: training loss 0.1656\n",
      "2025-06-01 14:50:02 [INFO]: epoch 211: training loss 0.1799\n",
      "2025-06-01 14:50:02 [INFO]: epoch 212: training loss 0.1560\n",
      "2025-06-01 14:50:02 [INFO]: epoch 213: training loss 0.1583\n",
      "2025-06-01 14:50:02 [INFO]: epoch 214: training loss 0.1691\n",
      "2025-06-01 14:50:02 [INFO]: epoch 215: training loss 0.1685\n",
      "2025-06-01 14:50:02 [INFO]: epoch 216: training loss 0.1755\n",
      "2025-06-01 14:50:02 [INFO]: epoch 217: training loss 0.1674\n",
      "2025-06-01 14:50:02 [INFO]: epoch 218: training loss 0.1619\n",
      "2025-06-01 14:50:02 [INFO]: epoch 219: training loss 0.1569\n",
      "2025-06-01 14:50:02 [INFO]: epoch 220: training loss 0.1677\n",
      "2025-06-01 14:50:02 [INFO]: epoch 221: training loss 0.1633\n",
      "2025-06-01 14:50:02 [INFO]: epoch 222: training loss 0.1540\n",
      "2025-06-01 14:50:02 [INFO]: epoch 223: training loss 0.1754\n",
      "2025-06-01 14:50:02 [INFO]: epoch 224: training loss 0.1512\n",
      "2025-06-01 14:50:02 [INFO]: epoch 225: training loss 0.1582\n",
      "2025-06-01 14:50:02 [INFO]: epoch 226: training loss 0.1496\n",
      "2025-06-01 14:50:02 [INFO]: epoch 227: training loss 0.1597\n",
      "2025-06-01 14:50:02 [INFO]: epoch 228: training loss 0.1662\n",
      "2025-06-01 14:50:02 [INFO]: epoch 229: training loss 0.1707\n",
      "2025-06-01 14:50:02 [INFO]: epoch 230: training loss 0.1660\n",
      "2025-06-01 14:50:02 [INFO]: epoch 231: training loss 0.1601\n",
      "2025-06-01 14:50:02 [INFO]: epoch 232: training loss 0.1433\n",
      "2025-06-01 14:50:02 [INFO]: epoch 233: training loss 0.1585\n",
      "2025-06-01 14:50:02 [INFO]: epoch 234: training loss 0.1549\n",
      "2025-06-01 14:50:02 [INFO]: epoch 235: training loss 0.1447\n",
      "2025-06-01 14:50:02 [INFO]: epoch 236: training loss 0.1510\n",
      "2025-06-01 14:50:02 [INFO]: epoch 237: training loss 0.1617\n",
      "2025-06-01 14:50:02 [INFO]: epoch 238: training loss 0.1546\n",
      "2025-06-01 14:50:02 [INFO]: epoch 239: training loss 0.1645\n",
      "2025-06-01 14:50:02 [INFO]: epoch 240: training loss 0.1592\n",
      "2025-06-01 14:50:02 [INFO]: epoch 241: training loss 0.1454\n",
      "2025-06-01 14:50:02 [INFO]: epoch 242: training loss 0.1533\n",
      "2025-06-01 14:50:02 [INFO]: epoch 243: training loss 0.1484\n",
      "2025-06-01 14:50:02 [INFO]: epoch 244: training loss 0.1395\n",
      "2025-06-01 14:50:02 [INFO]: epoch 245: training loss 0.1709\n",
      "2025-06-01 14:50:02 [INFO]: epoch 246: training loss 0.1513\n",
      "2025-06-01 14:50:02 [INFO]: epoch 247: training loss 0.1455\n",
      "2025-06-01 14:50:02 [INFO]: epoch 248: training loss 0.1536\n",
      "2025-06-01 14:50:02 [INFO]: epoch 249: training loss 0.1457\n",
      "2025-06-01 14:50:02 [INFO]: epoch 250: training loss 0.1372\n",
      "2025-06-01 14:50:02 [INFO]: epoch 251: training loss 0.1313\n",
      "2025-06-01 14:50:02 [INFO]: epoch 252: training loss 0.1392\n",
      "2025-06-01 14:50:02 [INFO]: epoch 253: training loss 0.1451\n",
      "2025-06-01 14:50:02 [INFO]: epoch 254: training loss 0.1514\n",
      "2025-06-01 14:50:02 [INFO]: epoch 255: training loss 0.1374\n",
      "2025-06-01 14:50:02 [INFO]: epoch 256: training loss 0.1426\n",
      "2025-06-01 14:50:02 [INFO]: epoch 257: training loss 0.1456\n",
      "2025-06-01 14:50:02 [INFO]: epoch 258: training loss 0.1596\n",
      "2025-06-01 14:50:02 [INFO]: epoch 259: training loss 0.1468\n",
      "2025-06-01 14:50:02 [INFO]: epoch 260: training loss 0.1352\n",
      "2025-06-01 14:50:02 [INFO]: epoch 261: training loss 0.1378\n",
      "2025-06-01 14:50:02 [INFO]: epoch 262: training loss 0.1341\n",
      "2025-06-01 14:50:02 [INFO]: epoch 263: training loss 0.1227\n",
      "2025-06-01 14:50:02 [INFO]: epoch 264: training loss 0.1388\n",
      "2025-06-01 14:50:02 [INFO]: epoch 265: training loss 0.1430\n",
      "2025-06-01 14:50:02 [INFO]: epoch 266: training loss 0.1298\n",
      "2025-06-01 14:50:02 [INFO]: epoch 267: training loss 0.1466\n",
      "2025-06-01 14:50:02 [INFO]: epoch 268: training loss 0.1384\n",
      "2025-06-01 14:50:03 [INFO]: epoch 269: training loss 0.1437\n",
      "2025-06-01 14:50:03 [INFO]: epoch 270: training loss 0.1497\n",
      "2025-06-01 14:50:03 [INFO]: epoch 271: training loss 0.1265\n",
      "2025-06-01 14:50:03 [INFO]: epoch 272: training loss 0.1295\n",
      "2025-06-01 14:50:03 [INFO]: epoch 273: training loss 0.1565\n",
      "2025-06-01 14:50:03 [INFO]: epoch 274: training loss 0.1463\n",
      "2025-06-01 14:50:03 [INFO]: epoch 275: training loss 0.1267\n",
      "2025-06-01 14:50:03 [INFO]: epoch 276: training loss 0.1389\n",
      "2025-06-01 14:50:03 [INFO]: epoch 277: training loss 0.1361\n",
      "2025-06-01 14:50:03 [INFO]: epoch 278: training loss 0.1311\n",
      "2025-06-01 14:50:03 [INFO]: epoch 279: training loss 0.1394\n",
      "2025-06-01 14:50:03 [INFO]: epoch 280: training loss 0.1336\n",
      "2025-06-01 14:50:03 [INFO]: epoch 281: training loss 0.1277\n",
      "2025-06-01 14:50:03 [INFO]: epoch 282: training loss 0.1231\n",
      "2025-06-01 14:50:03 [INFO]: epoch 283: training loss 0.1247\n",
      "2025-06-01 14:50:03 [INFO]: epoch 284: training loss 0.1326\n",
      "2025-06-01 14:50:03 [INFO]: epoch 285: training loss 0.1268\n",
      "2025-06-01 14:50:03 [INFO]: epoch 286: training loss 0.1407\n",
      "2025-06-01 14:50:03 [INFO]: epoch 287: training loss 0.1470\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-01 14:50:03 [INFO]: epoch 288: training loss 0.1372\n",
      "2025-06-01 14:50:03 [INFO]: epoch 289: training loss 0.1382\n",
      "2025-06-01 14:50:03 [INFO]: epoch 290: training loss 0.1351\n",
      "2025-06-01 14:50:03 [INFO]: epoch 291: training loss 0.1218\n",
      "2025-06-01 14:50:03 [INFO]: epoch 292: training loss 0.1281\n",
      "2025-06-01 14:50:03 [INFO]: epoch 293: training loss 0.1386\n",
      "2025-06-01 14:50:03 [INFO]: epoch 294: training loss 0.1246\n",
      "2025-06-01 14:50:03 [INFO]: epoch 295: training loss 0.1322\n",
      "2025-06-01 14:50:03 [INFO]: epoch 296: training loss 0.1342\n",
      "2025-06-01 14:50:03 [INFO]: epoch 297: training loss 0.1336\n",
      "2025-06-01 14:50:03 [INFO]: epoch 298: training loss 0.1300\n",
      "2025-06-01 14:50:03 [INFO]: epoch 299: training loss 0.1207\n",
      "2025-06-01 14:50:03 [INFO]: epoch 300: training loss 0.1207\n",
      "2025-06-01 14:50:03 [INFO]: epoch 301: training loss 0.1332\n",
      "2025-06-01 14:50:03 [INFO]: epoch 302: training loss 0.1411\n",
      "2025-06-01 14:50:03 [INFO]: epoch 303: training loss 0.1259\n",
      "2025-06-01 14:50:03 [INFO]: epoch 304: training loss 0.1211\n",
      "2025-06-01 14:50:03 [INFO]: epoch 305: training loss 0.1201\n",
      "2025-06-01 14:50:03 [INFO]: epoch 306: training loss 0.1141\n",
      "2025-06-01 14:50:03 [INFO]: epoch 307: training loss 0.1275\n",
      "2025-06-01 14:50:03 [INFO]: epoch 308: training loss 0.1252\n",
      "2025-06-01 14:50:03 [INFO]: epoch 309: training loss 0.1224\n",
      "2025-06-01 14:50:03 [INFO]: epoch 310: training loss 0.1183\n",
      "2025-06-01 14:50:03 [INFO]: epoch 311: training loss 0.1215\n",
      "2025-06-01 14:50:03 [INFO]: epoch 312: training loss 0.1153\n",
      "2025-06-01 14:50:03 [INFO]: epoch 313: training loss 0.1168\n",
      "2025-06-01 14:50:03 [INFO]: epoch 314: training loss 0.1103\n",
      "2025-06-01 14:50:03 [INFO]: epoch 315: training loss 0.1150\n",
      "2025-06-01 14:50:03 [INFO]: epoch 316: training loss 0.1122\n",
      "2025-06-01 14:50:03 [INFO]: epoch 317: training loss 0.1140\n",
      "2025-06-01 14:50:03 [INFO]: epoch 318: training loss 0.1098\n",
      "2025-06-01 14:50:03 [INFO]: epoch 319: training loss 0.1109\n",
      "2025-06-01 14:50:03 [INFO]: epoch 320: training loss 0.1072\n",
      "2025-06-01 14:50:03 [INFO]: epoch 321: training loss 0.1028\n",
      "2025-06-01 14:50:03 [INFO]: epoch 322: training loss 0.1054\n",
      "2025-06-01 14:50:03 [INFO]: epoch 323: training loss 0.1196\n",
      "2025-06-01 14:50:03 [INFO]: epoch 324: training loss 0.1103\n",
      "2025-06-01 14:50:03 [INFO]: epoch 325: training loss 0.1109\n",
      "2025-06-01 14:50:03 [INFO]: epoch 326: training loss 0.0999\n",
      "2025-06-01 14:50:03 [INFO]: epoch 327: training loss 0.1055\n",
      "2025-06-01 14:50:03 [INFO]: epoch 328: training loss 0.0969\n",
      "2025-06-01 14:50:03 [INFO]: epoch 329: training loss 0.1034\n",
      "2025-06-01 14:50:03 [INFO]: epoch 330: training loss 0.1048\n",
      "2025-06-01 14:50:03 [INFO]: epoch 331: training loss 0.1080\n",
      "2025-06-01 14:50:03 [INFO]: epoch 332: training loss 0.1129\n",
      "2025-06-01 14:50:03 [INFO]: epoch 333: training loss 0.1171\n",
      "2025-06-01 14:50:03 [INFO]: epoch 334: training loss 0.1075\n",
      "2025-06-01 14:50:03 [INFO]: epoch 335: training loss 0.1048\n",
      "2025-06-01 14:50:03 [INFO]: epoch 336: training loss 0.1099\n",
      "2025-06-01 14:50:03 [INFO]: epoch 337: training loss 0.1226\n",
      "2025-06-01 14:50:03 [INFO]: epoch 338: training loss 0.1171\n",
      "2025-06-01 14:50:03 [INFO]: epoch 339: training loss 0.1107\n",
      "2025-06-01 14:50:03 [INFO]: epoch 340: training loss 0.1090\n",
      "2025-06-01 14:50:03 [INFO]: epoch 341: training loss 0.1076\n",
      "2025-06-01 14:50:03 [INFO]: epoch 342: training loss 0.1193\n",
      "2025-06-01 14:50:03 [INFO]: epoch 343: training loss 0.1107\n",
      "2025-06-01 14:50:03 [INFO]: epoch 344: training loss 0.1023\n",
      "2025-06-01 14:50:04 [INFO]: epoch 345: training loss 0.1149\n",
      "2025-06-01 14:50:04 [INFO]: epoch 346: training loss 0.1020\n",
      "2025-06-01 14:50:04 [INFO]: epoch 347: training loss 0.1098\n",
      "2025-06-01 14:50:04 [INFO]: epoch 348: training loss 0.1064\n",
      "2025-06-01 14:50:04 [INFO]: epoch 349: training loss 0.1046\n",
      "2025-06-01 14:50:04 [INFO]: epoch 350: training loss 0.1062\n",
      "2025-06-01 14:50:04 [INFO]: epoch 351: training loss 0.0920\n",
      "2025-06-01 14:50:04 [INFO]: epoch 352: training loss 0.1086\n",
      "2025-06-01 14:50:04 [INFO]: epoch 353: training loss 0.0986\n",
      "2025-06-01 14:50:04 [INFO]: epoch 354: training loss 0.1063\n",
      "2025-06-01 14:50:04 [INFO]: epoch 355: training loss 0.0968\n",
      "2025-06-01 14:50:04 [INFO]: epoch 356: training loss 0.1037\n",
      "2025-06-01 14:50:04 [INFO]: epoch 357: training loss 0.1078\n",
      "2025-06-01 14:50:04 [INFO]: epoch 358: training loss 0.0928\n",
      "2025-06-01 14:50:04 [INFO]: epoch 359: training loss 0.0943\n",
      "2025-06-01 14:50:04 [INFO]: epoch 360: training loss 0.0982\n",
      "2025-06-01 14:50:04 [INFO]: epoch 361: training loss 0.1000\n",
      "2025-06-01 14:50:04 [INFO]: epoch 362: training loss 0.1106\n",
      "2025-06-01 14:50:04 [INFO]: epoch 363: training loss 0.1105\n",
      "2025-06-01 14:50:04 [INFO]: epoch 364: training loss 0.0928\n",
      "2025-06-01 14:50:04 [INFO]: epoch 365: training loss 0.1210\n",
      "2025-06-01 14:50:04 [INFO]: epoch 366: training loss 0.1156\n",
      "2025-06-01 14:50:04 [INFO]: epoch 367: training loss 0.0953\n",
      "2025-06-01 14:50:04 [INFO]: epoch 368: training loss 0.1004\n",
      "2025-06-01 14:50:04 [INFO]: epoch 369: training loss 0.1230\n",
      "2025-06-01 14:50:04 [INFO]: epoch 370: training loss 0.1054\n",
      "2025-06-01 14:50:04 [INFO]: epoch 371: training loss 0.0879\n",
      "2025-06-01 14:50:04 [INFO]: epoch 372: training loss 0.1004\n",
      "2025-06-01 14:50:04 [INFO]: epoch 373: training loss 0.1092\n",
      "2025-06-01 14:50:04 [INFO]: epoch 374: training loss 0.1000\n",
      "2025-06-01 14:50:04 [INFO]: epoch 375: training loss 0.0908\n",
      "2025-06-01 14:50:04 [INFO]: epoch 376: training loss 0.1046\n",
      "2025-06-01 14:50:04 [INFO]: epoch 377: training loss 0.0951\n",
      "2025-06-01 14:50:04 [INFO]: epoch 378: training loss 0.0850\n",
      "2025-06-01 14:50:04 [INFO]: epoch 379: training loss 0.0909\n",
      "2025-06-01 14:50:04 [INFO]: epoch 380: training loss 0.0896\n",
      "2025-06-01 14:50:04 [INFO]: epoch 381: training loss 0.0923\n",
      "2025-06-01 14:50:04 [INFO]: epoch 382: training loss 0.0964\n",
      "2025-06-01 14:50:04 [INFO]: epoch 383: training loss 0.0937\n",
      "2025-06-01 14:50:04 [INFO]: epoch 384: training loss 0.0930\n",
      "2025-06-01 14:50:04 [INFO]: epoch 385: training loss 0.0829\n",
      "2025-06-01 14:50:04 [INFO]: epoch 386: training loss 0.0933\n",
      "2025-06-01 14:50:04 [INFO]: epoch 387: training loss 0.0802\n",
      "2025-06-01 14:50:04 [INFO]: epoch 388: training loss 0.0898\n",
      "2025-06-01 14:50:04 [INFO]: epoch 389: training loss 0.0818\n",
      "2025-06-01 14:50:04 [INFO]: epoch 390: training loss 0.0786\n",
      "2025-06-01 14:50:04 [INFO]: epoch 391: training loss 0.0868\n",
      "2025-06-01 14:50:04 [INFO]: epoch 392: training loss 0.0887\n",
      "2025-06-01 14:50:04 [INFO]: epoch 393: training loss 0.0875\n",
      "2025-06-01 14:50:04 [INFO]: epoch 394: training loss 0.0785\n",
      "2025-06-01 14:50:04 [INFO]: epoch 395: training loss 0.0790\n",
      "2025-06-01 14:50:04 [INFO]: epoch 396: training loss 0.0857\n",
      "2025-06-01 14:50:04 [INFO]: epoch 397: training loss 0.0820\n",
      "2025-06-01 14:50:04 [INFO]: epoch 398: training loss 0.0771\n",
      "2025-06-01 14:50:04 [INFO]: epoch 399: training loss 0.0861\n",
      "2025-06-01 14:50:04 [INFO]: Finished training.\n",
      "2025-06-01 14:50:04 [WARNING]: 🚨DeprecationWarning: The method impute is deprecated. Please use `predict` instead.\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 6/6 [00:32<00:00,  5.39s/it]\n",
      "  0%|                                                                                            | 0/6 [00:00<?, ?it/s]2025-06-01 14:50:04 [INFO]: No given device, using default device: cuda\n",
      "2025-06-01 14:50:04 [WARNING]: saving_path not given. Model files and tensorboard file will not be saved.\n",
      "2025-06-01 14:50:04 [INFO]: Model initialized successfully with the number of trainable parameters: 597,780\n",
      "2025-06-01 14:50:04 [INFO]: epoch 0: training loss 1.2684\n",
      "2025-06-01 14:50:04 [INFO]: epoch 1: training loss 0.6277\n",
      "2025-06-01 14:50:04 [INFO]: epoch 2: training loss 0.5997\n",
      "2025-06-01 14:50:04 [INFO]: epoch 3: training loss 0.7179\n",
      "2025-06-01 14:50:04 [INFO]: epoch 4: training loss 0.6452\n",
      "2025-06-01 14:50:04 [INFO]: epoch 5: training loss 0.5776\n",
      "2025-06-01 14:50:04 [INFO]: epoch 6: training loss 0.4520\n",
      "2025-06-01 14:50:04 [INFO]: epoch 7: training loss 0.4840\n",
      "2025-06-01 14:50:04 [INFO]: epoch 8: training loss 0.5198\n",
      "2025-06-01 14:50:04 [INFO]: epoch 9: training loss 0.5108\n",
      "2025-06-01 14:50:04 [INFO]: epoch 10: training loss 0.5266\n",
      "2025-06-01 14:50:04 [INFO]: epoch 11: training loss 0.4952\n",
      "2025-06-01 14:50:04 [INFO]: epoch 12: training loss 0.5198\n",
      "2025-06-01 14:50:05 [INFO]: epoch 13: training loss 0.4888\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-01 14:50:05 [INFO]: epoch 14: training loss 0.4599\n",
      "2025-06-01 14:50:05 [INFO]: epoch 15: training loss 0.4401\n",
      "2025-06-01 14:50:05 [INFO]: epoch 16: training loss 0.4229\n",
      "2025-06-01 14:50:05 [INFO]: epoch 17: training loss 0.4189\n",
      "2025-06-01 14:50:05 [INFO]: epoch 18: training loss 0.4380\n",
      "2025-06-01 14:50:05 [INFO]: epoch 19: training loss 0.4333\n",
      "2025-06-01 14:50:05 [INFO]: epoch 20: training loss 0.4337\n",
      "2025-06-01 14:50:05 [INFO]: epoch 21: training loss 0.4044\n",
      "2025-06-01 14:50:05 [INFO]: epoch 22: training loss 0.4352\n",
      "2025-06-01 14:50:05 [INFO]: epoch 23: training loss 0.4309\n",
      "2025-06-01 14:50:05 [INFO]: epoch 24: training loss 0.4269\n",
      "2025-06-01 14:50:05 [INFO]: epoch 25: training loss 0.4230\n",
      "2025-06-01 14:50:05 [INFO]: epoch 26: training loss 0.4296\n",
      "2025-06-01 14:50:05 [INFO]: epoch 27: training loss 0.4537\n",
      "2025-06-01 14:50:05 [INFO]: epoch 28: training loss 0.4208\n",
      "2025-06-01 14:50:05 [INFO]: epoch 29: training loss 0.4279\n",
      "2025-06-01 14:50:05 [INFO]: epoch 30: training loss 0.4404\n",
      "2025-06-01 14:50:05 [INFO]: epoch 31: training loss 0.4118\n",
      "2025-06-01 14:50:05 [INFO]: epoch 32: training loss 0.4008\n",
      "2025-06-01 14:50:05 [INFO]: epoch 33: training loss 0.4213\n",
      "2025-06-01 14:50:05 [INFO]: epoch 34: training loss 0.4337\n",
      "2025-06-01 14:50:05 [INFO]: epoch 35: training loss 0.4326\n",
      "2025-06-01 14:50:05 [INFO]: epoch 36: training loss 0.4270\n",
      "2025-06-01 14:50:05 [INFO]: epoch 37: training loss 0.4135\n",
      "2025-06-01 14:50:05 [INFO]: epoch 38: training loss 0.4102\n",
      "2025-06-01 14:50:05 [INFO]: epoch 39: training loss 0.4148\n",
      "2025-06-01 14:50:05 [INFO]: epoch 40: training loss 0.4327\n",
      "2025-06-01 14:50:05 [INFO]: epoch 41: training loss 0.3732\n",
      "2025-06-01 14:50:05 [INFO]: epoch 42: training loss 0.3955\n",
      "2025-06-01 14:50:05 [INFO]: epoch 43: training loss 0.4152\n",
      "2025-06-01 14:50:05 [INFO]: epoch 44: training loss 0.4074\n",
      "2025-06-01 14:50:05 [INFO]: epoch 45: training loss 0.4191\n",
      "2025-06-01 14:50:05 [INFO]: epoch 46: training loss 0.3684\n",
      "2025-06-01 14:50:05 [INFO]: epoch 47: training loss 0.3908\n",
      "2025-06-01 14:50:05 [INFO]: epoch 48: training loss 0.4030\n",
      "2025-06-01 14:50:05 [INFO]: epoch 49: training loss 0.3577\n",
      "2025-06-01 14:50:05 [INFO]: epoch 50: training loss 0.3910\n",
      "2025-06-01 14:50:05 [INFO]: epoch 51: training loss 0.3601\n",
      "2025-06-01 14:50:05 [INFO]: epoch 52: training loss 0.3899\n",
      "2025-06-01 14:50:05 [INFO]: epoch 53: training loss 0.3788\n",
      "2025-06-01 14:50:05 [INFO]: epoch 54: training loss 0.3668\n",
      "2025-06-01 14:50:05 [INFO]: epoch 55: training loss 0.3877\n",
      "2025-06-01 14:50:05 [INFO]: epoch 56: training loss 0.3678\n",
      "2025-06-01 14:50:05 [INFO]: epoch 57: training loss 0.3701\n",
      "2025-06-01 14:50:05 [INFO]: epoch 58: training loss 0.3424\n",
      "2025-06-01 14:50:05 [INFO]: epoch 59: training loss 0.3468\n",
      "2025-06-01 14:50:05 [INFO]: epoch 60: training loss 0.3703\n",
      "2025-06-01 14:50:05 [INFO]: epoch 61: training loss 0.3951\n",
      "2025-06-01 14:50:05 [INFO]: epoch 62: training loss 0.3292\n",
      "2025-06-01 14:50:05 [INFO]: epoch 63: training loss 0.3506\n",
      "2025-06-01 14:50:05 [INFO]: epoch 64: training loss 0.3899\n",
      "2025-06-01 14:50:05 [INFO]: epoch 65: training loss 0.3819\n",
      "2025-06-01 14:50:05 [INFO]: epoch 66: training loss 0.3760\n",
      "2025-06-01 14:50:05 [INFO]: epoch 67: training loss 0.3497\n",
      "2025-06-01 14:50:05 [INFO]: epoch 68: training loss 0.3365\n",
      "2025-06-01 14:50:05 [INFO]: epoch 69: training loss 0.3534\n",
      "2025-06-01 14:50:05 [INFO]: epoch 70: training loss 0.3876\n",
      "2025-06-01 14:50:05 [INFO]: epoch 71: training loss 0.3755\n",
      "2025-06-01 14:50:05 [INFO]: epoch 72: training loss 0.3348\n",
      "2025-06-01 14:50:05 [INFO]: epoch 73: training loss 0.3372\n",
      "2025-06-01 14:50:05 [INFO]: epoch 74: training loss 0.3631\n",
      "2025-06-01 14:50:05 [INFO]: epoch 75: training loss 0.3450\n",
      "2025-06-01 14:50:05 [INFO]: epoch 76: training loss 0.3520\n",
      "2025-06-01 14:50:05 [INFO]: epoch 77: training loss 0.3523\n",
      "2025-06-01 14:50:05 [INFO]: epoch 78: training loss 0.3537\n",
      "2025-06-01 14:50:05 [INFO]: epoch 79: training loss 0.3320\n",
      "2025-06-01 14:50:05 [INFO]: epoch 80: training loss 0.3332\n",
      "2025-06-01 14:50:05 [INFO]: epoch 81: training loss 0.3667\n",
      "2025-06-01 14:50:05 [INFO]: epoch 82: training loss 0.3487\n",
      "2025-06-01 14:50:05 [INFO]: epoch 83: training loss 0.3438\n",
      "2025-06-01 14:50:05 [INFO]: epoch 84: training loss 0.3419\n",
      "2025-06-01 14:50:05 [INFO]: epoch 85: training loss 0.3631\n",
      "2025-06-01 14:50:05 [INFO]: epoch 86: training loss 0.3713\n",
      "2025-06-01 14:50:05 [INFO]: epoch 87: training loss 0.3664\n",
      "2025-06-01 14:50:05 [INFO]: epoch 88: training loss 0.3436\n",
      "2025-06-01 14:50:06 [INFO]: epoch 89: training loss 0.3207\n",
      "2025-06-01 14:50:06 [INFO]: epoch 90: training loss 0.3349\n",
      "2025-06-01 14:50:06 [INFO]: epoch 91: training loss 0.3032\n",
      "2025-06-01 14:50:06 [INFO]: epoch 92: training loss 0.3316\n",
      "2025-06-01 14:50:06 [INFO]: epoch 93: training loss 0.3314\n",
      "2025-06-01 14:50:06 [INFO]: epoch 94: training loss 0.3263\n",
      "2025-06-01 14:50:06 [INFO]: epoch 95: training loss 0.3550\n",
      "2025-06-01 14:50:06 [INFO]: epoch 96: training loss 0.3310\n",
      "2025-06-01 14:50:06 [INFO]: epoch 97: training loss 0.3389\n",
      "2025-06-01 14:50:06 [INFO]: epoch 98: training loss 0.3292\n",
      "2025-06-01 14:50:06 [INFO]: epoch 99: training loss 0.3045\n",
      "2025-06-01 14:50:06 [INFO]: epoch 100: training loss 0.3080\n",
      "2025-06-01 14:50:06 [INFO]: epoch 101: training loss 0.3376\n",
      "2025-06-01 14:50:06 [INFO]: epoch 102: training loss 0.3483\n",
      "2025-06-01 14:50:06 [INFO]: epoch 103: training loss 0.3012\n",
      "2025-06-01 14:50:06 [INFO]: epoch 104: training loss 0.3113\n",
      "2025-06-01 14:50:06 [INFO]: epoch 105: training loss 0.3042\n",
      "2025-06-01 14:50:06 [INFO]: epoch 106: training loss 0.3176\n",
      "2025-06-01 14:50:06 [INFO]: epoch 107: training loss 0.3214\n",
      "2025-06-01 14:50:06 [INFO]: epoch 108: training loss 0.3164\n",
      "2025-06-01 14:50:06 [INFO]: epoch 109: training loss 0.3093\n",
      "2025-06-01 14:50:06 [INFO]: epoch 110: training loss 0.3206\n",
      "2025-06-01 14:50:06 [INFO]: epoch 111: training loss 0.2991\n",
      "2025-06-01 14:50:06 [INFO]: epoch 112: training loss 0.3147\n",
      "2025-06-01 14:50:06 [INFO]: epoch 113: training loss 0.3198\n",
      "2025-06-01 14:50:06 [INFO]: epoch 114: training loss 0.3266\n",
      "2025-06-01 14:50:06 [INFO]: epoch 115: training loss 0.3118\n",
      "2025-06-01 14:50:06 [INFO]: epoch 116: training loss 0.3059\n",
      "2025-06-01 14:50:06 [INFO]: epoch 117: training loss 0.3222\n",
      "2025-06-01 14:50:06 [INFO]: epoch 118: training loss 0.3180\n",
      "2025-06-01 14:50:06 [INFO]: epoch 119: training loss 0.3169\n",
      "2025-06-01 14:50:06 [INFO]: epoch 120: training loss 0.3076\n",
      "2025-06-01 14:50:06 [INFO]: epoch 121: training loss 0.3047\n",
      "2025-06-01 14:50:06 [INFO]: epoch 122: training loss 0.3127\n",
      "2025-06-01 14:50:06 [INFO]: epoch 123: training loss 0.3048\n",
      "2025-06-01 14:50:06 [INFO]: epoch 124: training loss 0.2972\n",
      "2025-06-01 14:50:06 [INFO]: epoch 125: training loss 0.3107\n",
      "2025-06-01 14:50:06 [INFO]: epoch 126: training loss 0.3167\n",
      "2025-06-01 14:50:06 [INFO]: epoch 127: training loss 0.2859\n",
      "2025-06-01 14:50:06 [INFO]: epoch 128: training loss 0.2951\n",
      "2025-06-01 14:50:06 [INFO]: epoch 129: training loss 0.3007\n",
      "2025-06-01 14:50:06 [INFO]: epoch 130: training loss 0.3190\n",
      "2025-06-01 14:50:06 [INFO]: epoch 131: training loss 0.3125\n",
      "2025-06-01 14:50:06 [INFO]: epoch 132: training loss 0.3045\n",
      "2025-06-01 14:50:06 [INFO]: epoch 133: training loss 0.3003\n",
      "2025-06-01 14:50:06 [INFO]: epoch 134: training loss 0.2807\n",
      "2025-06-01 14:50:06 [INFO]: epoch 135: training loss 0.2915\n",
      "2025-06-01 14:50:06 [INFO]: epoch 136: training loss 0.3147\n",
      "2025-06-01 14:50:06 [INFO]: epoch 137: training loss 0.3211\n",
      "2025-06-01 14:50:06 [INFO]: epoch 138: training loss 0.3093\n",
      "2025-06-01 14:50:06 [INFO]: epoch 139: training loss 0.2919\n",
      "2025-06-01 14:50:06 [INFO]: epoch 140: training loss 0.2791\n",
      "2025-06-01 14:50:06 [INFO]: epoch 141: training loss 0.2888\n",
      "2025-06-01 14:50:06 [INFO]: epoch 142: training loss 0.2736\n",
      "2025-06-01 14:50:06 [INFO]: epoch 143: training loss 0.2893\n",
      "2025-06-01 14:50:06 [INFO]: epoch 144: training loss 0.2658\n",
      "2025-06-01 14:50:06 [INFO]: epoch 145: training loss 0.2849\n",
      "2025-06-01 14:50:06 [INFO]: epoch 146: training loss 0.3156\n",
      "2025-06-01 14:50:06 [INFO]: epoch 147: training loss 0.3060\n",
      "2025-06-01 14:50:06 [INFO]: epoch 148: training loss 0.2737\n",
      "2025-06-01 14:50:06 [INFO]: epoch 149: training loss 0.2556\n",
      "2025-06-01 14:50:06 [INFO]: epoch 150: training loss 0.2865\n",
      "2025-06-01 14:50:06 [INFO]: epoch 151: training loss 0.2824\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-01 14:50:06 [INFO]: epoch 152: training loss 0.2756\n",
      "2025-06-01 14:50:06 [INFO]: epoch 153: training loss 0.2676\n",
      "2025-06-01 14:50:06 [INFO]: epoch 154: training loss 0.2620\n",
      "2025-06-01 14:50:06 [INFO]: epoch 155: training loss 0.2797\n",
      "2025-06-01 14:50:06 [INFO]: epoch 156: training loss 0.2862\n",
      "2025-06-01 14:50:06 [INFO]: epoch 157: training loss 0.2742\n",
      "2025-06-01 14:50:06 [INFO]: epoch 158: training loss 0.2806\n",
      "2025-06-01 14:50:06 [INFO]: epoch 159: training loss 0.2700\n",
      "2025-06-01 14:50:06 [INFO]: epoch 160: training loss 0.2723\n",
      "2025-06-01 14:50:06 [INFO]: epoch 161: training loss 0.2653\n",
      "2025-06-01 14:50:06 [INFO]: epoch 162: training loss 0.2784\n",
      "2025-06-01 14:50:06 [INFO]: epoch 163: training loss 0.3036\n",
      "2025-06-01 14:50:06 [INFO]: epoch 164: training loss 0.2702\n",
      "2025-06-01 14:50:07 [INFO]: epoch 165: training loss 0.2654\n",
      "2025-06-01 14:50:07 [INFO]: epoch 166: training loss 0.2584\n",
      "2025-06-01 14:50:07 [INFO]: epoch 167: training loss 0.2903\n",
      "2025-06-01 14:50:07 [INFO]: epoch 168: training loss 0.2689\n",
      "2025-06-01 14:50:07 [INFO]: epoch 169: training loss 0.2692\n",
      "2025-06-01 14:50:07 [INFO]: epoch 170: training loss 0.2586\n",
      "2025-06-01 14:50:07 [INFO]: epoch 171: training loss 0.2418\n",
      "2025-06-01 14:50:07 [INFO]: epoch 172: training loss 0.2599\n",
      "2025-06-01 14:50:07 [INFO]: epoch 173: training loss 0.2699\n",
      "2025-06-01 14:50:07 [INFO]: epoch 174: training loss 0.2695\n",
      "2025-06-01 14:50:07 [INFO]: epoch 175: training loss 0.2390\n",
      "2025-06-01 14:50:07 [INFO]: epoch 176: training loss 0.2589\n",
      "2025-06-01 14:50:07 [INFO]: epoch 177: training loss 0.2676\n",
      "2025-06-01 14:50:07 [INFO]: epoch 178: training loss 0.2606\n",
      "2025-06-01 14:50:07 [INFO]: epoch 179: training loss 0.2521\n",
      "2025-06-01 14:50:07 [INFO]: epoch 180: training loss 0.2638\n",
      "2025-06-01 14:50:07 [INFO]: epoch 181: training loss 0.2563\n",
      "2025-06-01 14:50:07 [INFO]: epoch 182: training loss 0.2474\n",
      "2025-06-01 14:50:07 [INFO]: epoch 183: training loss 0.2438\n",
      "2025-06-01 14:50:07 [INFO]: epoch 184: training loss 0.2572\n",
      "2025-06-01 14:50:07 [INFO]: epoch 185: training loss 0.2380\n",
      "2025-06-01 14:50:07 [INFO]: epoch 186: training loss 0.2606\n",
      "2025-06-01 14:50:07 [INFO]: epoch 187: training loss 0.2495\n",
      "2025-06-01 14:50:07 [INFO]: epoch 188: training loss 0.2572\n",
      "2025-06-01 14:50:07 [INFO]: epoch 189: training loss 0.2590\n",
      "2025-06-01 14:50:07 [INFO]: epoch 190: training loss 0.2615\n",
      "2025-06-01 14:50:07 [INFO]: epoch 191: training loss 0.2552\n",
      "2025-06-01 14:50:07 [INFO]: epoch 192: training loss 0.2565\n",
      "2025-06-01 14:50:07 [INFO]: epoch 193: training loss 0.2662\n",
      "2025-06-01 14:50:07 [INFO]: epoch 194: training loss 0.2682\n",
      "2025-06-01 14:50:07 [INFO]: epoch 195: training loss 0.2656\n",
      "2025-06-01 14:50:07 [INFO]: epoch 196: training loss 0.2698\n",
      "2025-06-01 14:50:07 [INFO]: epoch 197: training loss 0.2465\n",
      "2025-06-01 14:50:07 [INFO]: epoch 198: training loss 0.2431\n",
      "2025-06-01 14:50:07 [INFO]: epoch 199: training loss 0.2452\n",
      "2025-06-01 14:50:07 [INFO]: epoch 200: training loss 0.2529\n",
      "2025-06-01 14:50:07 [INFO]: epoch 201: training loss 0.2505\n",
      "2025-06-01 14:50:07 [INFO]: epoch 202: training loss 0.2568\n",
      "2025-06-01 14:50:07 [INFO]: epoch 203: training loss 0.2357\n",
      "2025-06-01 14:50:07 [INFO]: epoch 204: training loss 0.2519\n",
      "2025-06-01 14:50:07 [INFO]: epoch 205: training loss 0.2537\n",
      "2025-06-01 14:50:07 [INFO]: epoch 206: training loss 0.2430\n",
      "2025-06-01 14:50:07 [INFO]: epoch 207: training loss 0.2347\n",
      "2025-06-01 14:50:07 [INFO]: epoch 208: training loss 0.2290\n",
      "2025-06-01 14:50:07 [INFO]: epoch 209: training loss 0.2553\n",
      "2025-06-01 14:50:07 [INFO]: epoch 210: training loss 0.2796\n",
      "2025-06-01 14:50:07 [INFO]: epoch 211: training loss 0.2470\n",
      "2025-06-01 14:50:07 [INFO]: epoch 212: training loss 0.2440\n",
      "2025-06-01 14:50:07 [INFO]: epoch 213: training loss 0.2329\n",
      "2025-06-01 14:50:07 [INFO]: epoch 214: training loss 0.2666\n",
      "2025-06-01 14:50:07 [INFO]: epoch 215: training loss 0.2619\n",
      "2025-06-01 14:50:07 [INFO]: epoch 216: training loss 0.2336\n",
      "2025-06-01 14:50:07 [INFO]: epoch 217: training loss 0.2388\n",
      "2025-06-01 14:50:07 [INFO]: epoch 218: training loss 0.2287\n",
      "2025-06-01 14:50:07 [INFO]: epoch 219: training loss 0.2511\n",
      "2025-06-01 14:50:07 [INFO]: epoch 220: training loss 0.2560\n",
      "2025-06-01 14:50:07 [INFO]: epoch 221: training loss 0.2368\n",
      "2025-06-01 14:50:07 [INFO]: epoch 222: training loss 0.2329\n",
      "2025-06-01 14:50:07 [INFO]: epoch 223: training loss 0.2235\n",
      "2025-06-01 14:50:07 [INFO]: epoch 224: training loss 0.2421\n",
      "2025-06-01 14:50:07 [INFO]: epoch 225: training loss 0.2259\n",
      "2025-06-01 14:50:07 [INFO]: epoch 226: training loss 0.2420\n",
      "2025-06-01 14:50:07 [INFO]: epoch 227: training loss 0.2300\n",
      "2025-06-01 14:50:07 [INFO]: epoch 228: training loss 0.2250\n",
      "2025-06-01 14:50:07 [INFO]: epoch 229: training loss 0.2365\n",
      "2025-06-01 14:50:07 [INFO]: epoch 230: training loss 0.2357\n",
      "2025-06-01 14:50:07 [INFO]: epoch 231: training loss 0.2349\n",
      "2025-06-01 14:50:07 [INFO]: epoch 232: training loss 0.2262\n",
      "2025-06-01 14:50:07 [INFO]: epoch 233: training loss 0.2282\n",
      "2025-06-01 14:50:07 [INFO]: epoch 234: training loss 0.2263\n",
      "2025-06-01 14:50:07 [INFO]: epoch 235: training loss 0.2369\n",
      "2025-06-01 14:50:07 [INFO]: epoch 236: training loss 0.2244\n",
      "2025-06-01 14:50:07 [INFO]: epoch 237: training loss 0.2283\n",
      "2025-06-01 14:50:07 [INFO]: epoch 238: training loss 0.2428\n",
      "2025-06-01 14:50:07 [INFO]: epoch 239: training loss 0.2339\n",
      "2025-06-01 14:50:07 [INFO]: epoch 240: training loss 0.2270\n",
      "2025-06-01 14:50:08 [INFO]: epoch 241: training loss 0.2420\n",
      "2025-06-01 14:50:08 [INFO]: epoch 242: training loss 0.2258\n",
      "2025-06-01 14:50:08 [INFO]: epoch 243: training loss 0.2301\n",
      "2025-06-01 14:50:08 [INFO]: epoch 244: training loss 0.2373\n",
      "2025-06-01 14:50:08 [INFO]: epoch 245: training loss 0.2236\n",
      "2025-06-01 14:50:08 [INFO]: epoch 246: training loss 0.2252\n",
      "2025-06-01 14:50:08 [INFO]: epoch 247: training loss 0.2315\n",
      "2025-06-01 14:50:08 [INFO]: epoch 248: training loss 0.2248\n",
      "2025-06-01 14:50:08 [INFO]: epoch 249: training loss 0.2130\n",
      "2025-06-01 14:50:08 [INFO]: epoch 250: training loss 0.2209\n",
      "2025-06-01 14:50:08 [INFO]: epoch 251: training loss 0.2249\n",
      "2025-06-01 14:50:08 [INFO]: epoch 252: training loss 0.2406\n",
      "2025-06-01 14:50:08 [INFO]: epoch 253: training loss 0.2288\n",
      "2025-06-01 14:50:08 [INFO]: epoch 254: training loss 0.2176\n",
      "2025-06-01 14:50:08 [INFO]: epoch 255: training loss 0.2148\n",
      "2025-06-01 14:50:08 [INFO]: epoch 256: training loss 0.2162\n",
      "2025-06-01 14:50:08 [INFO]: epoch 257: training loss 0.2175\n",
      "2025-06-01 14:50:08 [INFO]: epoch 258: training loss 0.2172\n",
      "2025-06-01 14:50:08 [INFO]: epoch 259: training loss 0.2078\n",
      "2025-06-01 14:50:08 [INFO]: epoch 260: training loss 0.2138\n",
      "2025-06-01 14:50:08 [INFO]: epoch 261: training loss 0.2313\n",
      "2025-06-01 14:50:08 [INFO]: epoch 262: training loss 0.2116\n",
      "2025-06-01 14:50:08 [INFO]: epoch 263: training loss 0.2065\n",
      "2025-06-01 14:50:08 [INFO]: epoch 264: training loss 0.2096\n",
      "2025-06-01 14:50:08 [INFO]: epoch 265: training loss 0.2195\n",
      "2025-06-01 14:50:08 [INFO]: epoch 266: training loss 0.2216\n",
      "2025-06-01 14:50:08 [INFO]: epoch 267: training loss 0.2316\n",
      "2025-06-01 14:50:08 [INFO]: epoch 268: training loss 0.2148\n",
      "2025-06-01 14:50:08 [INFO]: epoch 269: training loss 0.2176\n",
      "2025-06-01 14:50:08 [INFO]: epoch 270: training loss 0.2041\n",
      "2025-06-01 14:50:08 [INFO]: epoch 271: training loss 0.2025\n",
      "2025-06-01 14:50:08 [INFO]: epoch 272: training loss 0.2002\n",
      "2025-06-01 14:50:08 [INFO]: epoch 273: training loss 0.2168\n",
      "2025-06-01 14:50:08 [INFO]: epoch 274: training loss 0.2200\n",
      "2025-06-01 14:50:08 [INFO]: epoch 275: training loss 0.2233\n",
      "2025-06-01 14:50:08 [INFO]: epoch 276: training loss 0.2154\n",
      "2025-06-01 14:50:08 [INFO]: epoch 277: training loss 0.2084\n",
      "2025-06-01 14:50:08 [INFO]: epoch 278: training loss 0.2056\n",
      "2025-06-01 14:50:08 [INFO]: epoch 279: training loss 0.2030\n",
      "2025-06-01 14:50:08 [INFO]: epoch 280: training loss 0.2140\n",
      "2025-06-01 14:50:08 [INFO]: epoch 281: training loss 0.2157\n",
      "2025-06-01 14:50:08 [INFO]: epoch 282: training loss 0.2065\n",
      "2025-06-01 14:50:08 [INFO]: epoch 283: training loss 0.2152\n",
      "2025-06-01 14:50:08 [INFO]: epoch 284: training loss 0.2078\n",
      "2025-06-01 14:50:08 [INFO]: epoch 285: training loss 0.2069\n",
      "2025-06-01 14:50:08 [INFO]: epoch 286: training loss 0.2003\n",
      "2025-06-01 14:50:08 [INFO]: epoch 287: training loss 0.2131\n",
      "2025-06-01 14:50:08 [INFO]: epoch 288: training loss 0.1987\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-01 14:50:08 [INFO]: epoch 289: training loss 0.1973\n",
      "2025-06-01 14:50:08 [INFO]: epoch 290: training loss 0.1945\n",
      "2025-06-01 14:50:08 [INFO]: epoch 291: training loss 0.2019\n",
      "2025-06-01 14:50:08 [INFO]: epoch 292: training loss 0.2176\n",
      "2025-06-01 14:50:08 [INFO]: epoch 293: training loss 0.2046\n",
      "2025-06-01 14:50:08 [INFO]: epoch 294: training loss 0.2127\n",
      "2025-06-01 14:50:08 [INFO]: epoch 295: training loss 0.2092\n",
      "2025-06-01 14:50:08 [INFO]: epoch 296: training loss 0.2014\n",
      "2025-06-01 14:50:08 [INFO]: epoch 297: training loss 0.1906\n",
      "2025-06-01 14:50:08 [INFO]: epoch 298: training loss 0.1960\n",
      "2025-06-01 14:50:08 [INFO]: epoch 299: training loss 0.2109\n",
      "2025-06-01 14:50:08 [INFO]: epoch 300: training loss 0.2107\n",
      "2025-06-01 14:50:08 [INFO]: epoch 301: training loss 0.1933\n",
      "2025-06-01 14:50:08 [INFO]: epoch 302: training loss 0.1970\n",
      "2025-06-01 14:50:08 [INFO]: epoch 303: training loss 0.2131\n",
      "2025-06-01 14:50:08 [INFO]: epoch 304: training loss 0.2161\n",
      "2025-06-01 14:50:08 [INFO]: epoch 305: training loss 0.2126\n",
      "2025-06-01 14:50:08 [INFO]: epoch 306: training loss 0.1985\n",
      "2025-06-01 14:50:08 [INFO]: epoch 307: training loss 0.2095\n",
      "2025-06-01 14:50:08 [INFO]: epoch 308: training loss 0.2187\n",
      "2025-06-01 14:50:08 [INFO]: epoch 309: training loss 0.2246\n",
      "2025-06-01 14:50:08 [INFO]: epoch 310: training loss 0.2029\n",
      "2025-06-01 14:50:08 [INFO]: epoch 311: training loss 0.2056\n",
      "2025-06-01 14:50:08 [INFO]: epoch 312: training loss 0.1970\n",
      "2025-06-01 14:50:08 [INFO]: epoch 313: training loss 0.2064\n",
      "2025-06-01 14:50:08 [INFO]: epoch 314: training loss 0.2035\n",
      "2025-06-01 14:50:09 [INFO]: epoch 315: training loss 0.1941\n",
      "2025-06-01 14:50:09 [INFO]: epoch 316: training loss 0.1987\n",
      "2025-06-01 14:50:09 [INFO]: epoch 317: training loss 0.2000\n",
      "2025-06-01 14:50:09 [INFO]: epoch 318: training loss 0.1996\n",
      "2025-06-01 14:50:09 [INFO]: epoch 319: training loss 0.2047\n",
      "2025-06-01 14:50:09 [INFO]: epoch 320: training loss 0.2022\n",
      "2025-06-01 14:50:09 [INFO]: epoch 321: training loss 0.1875\n",
      "2025-06-01 14:50:09 [INFO]: epoch 322: training loss 0.1983\n",
      "2025-06-01 14:50:09 [INFO]: epoch 323: training loss 0.1912\n",
      "2025-06-01 14:50:09 [INFO]: epoch 324: training loss 0.2096\n",
      "2025-06-01 14:50:09 [INFO]: epoch 325: training loss 0.1872\n",
      "2025-06-01 14:50:09 [INFO]: epoch 326: training loss 0.1863\n",
      "2025-06-01 14:50:09 [INFO]: epoch 327: training loss 0.2069\n",
      "2025-06-01 14:50:09 [INFO]: epoch 328: training loss 0.1939\n",
      "2025-06-01 14:50:09 [INFO]: epoch 329: training loss 0.1955\n",
      "2025-06-01 14:50:09 [INFO]: epoch 330: training loss 0.1921\n",
      "2025-06-01 14:50:09 [INFO]: epoch 331: training loss 0.1920\n",
      "2025-06-01 14:50:09 [INFO]: epoch 332: training loss 0.1922\n",
      "2025-06-01 14:50:09 [INFO]: epoch 333: training loss 0.1916\n",
      "2025-06-01 14:50:09 [INFO]: epoch 334: training loss 0.1898\n",
      "2025-06-01 14:50:09 [INFO]: epoch 335: training loss 0.1845\n",
      "2025-06-01 14:50:09 [INFO]: epoch 336: training loss 0.1748\n",
      "2025-06-01 14:50:09 [INFO]: epoch 337: training loss 0.1908\n",
      "2025-06-01 14:50:09 [INFO]: epoch 338: training loss 0.1812\n",
      "2025-06-01 14:50:09 [INFO]: epoch 339: training loss 0.1932\n",
      "2025-06-01 14:50:09 [INFO]: epoch 340: training loss 0.1774\n",
      "2025-06-01 14:50:09 [INFO]: epoch 341: training loss 0.1742\n",
      "2025-06-01 14:50:09 [INFO]: epoch 342: training loss 0.1989\n",
      "2025-06-01 14:50:09 [INFO]: epoch 343: training loss 0.1943\n",
      "2025-06-01 14:50:09 [INFO]: epoch 344: training loss 0.1956\n",
      "2025-06-01 14:50:09 [INFO]: epoch 345: training loss 0.1818\n",
      "2025-06-01 14:50:09 [INFO]: epoch 346: training loss 0.1813\n",
      "2025-06-01 14:50:09 [INFO]: epoch 347: training loss 0.1823\n",
      "2025-06-01 14:50:09 [INFO]: epoch 348: training loss 0.1826\n",
      "2025-06-01 14:50:09 [INFO]: epoch 349: training loss 0.1777\n",
      "2025-06-01 14:50:09 [INFO]: epoch 350: training loss 0.2028\n",
      "2025-06-01 14:50:09 [INFO]: epoch 351: training loss 0.1885\n",
      "2025-06-01 14:50:09 [INFO]: epoch 352: training loss 0.1809\n",
      "2025-06-01 14:50:09 [INFO]: epoch 353: training loss 0.1818\n",
      "2025-06-01 14:50:09 [INFO]: epoch 354: training loss 0.1947\n",
      "2025-06-01 14:50:09 [INFO]: epoch 355: training loss 0.1841\n",
      "2025-06-01 14:50:09 [INFO]: epoch 356: training loss 0.1863\n",
      "2025-06-01 14:50:09 [INFO]: epoch 357: training loss 0.1697\n",
      "2025-06-01 14:50:09 [INFO]: epoch 358: training loss 0.1803\n",
      "2025-06-01 14:50:09 [INFO]: epoch 359: training loss 0.1747\n",
      "2025-06-01 14:50:09 [INFO]: epoch 360: training loss 0.1774\n",
      "2025-06-01 14:50:09 [INFO]: epoch 361: training loss 0.1729\n",
      "2025-06-01 14:50:09 [INFO]: epoch 362: training loss 0.1654\n",
      "2025-06-01 14:50:09 [INFO]: epoch 363: training loss 0.1697\n",
      "2025-06-01 14:50:09 [INFO]: epoch 364: training loss 0.1731\n",
      "2025-06-01 14:50:09 [INFO]: epoch 365: training loss 0.1757\n",
      "2025-06-01 14:50:09 [INFO]: epoch 366: training loss 0.1670\n",
      "2025-06-01 14:50:09 [INFO]: epoch 367: training loss 0.1594\n",
      "2025-06-01 14:50:09 [INFO]: epoch 368: training loss 0.1654\n",
      "2025-06-01 14:50:09 [INFO]: epoch 369: training loss 0.1690\n",
      "2025-06-01 14:50:09 [INFO]: epoch 370: training loss 0.1633\n",
      "2025-06-01 14:50:09 [INFO]: epoch 371: training loss 0.1710\n",
      "2025-06-01 14:50:09 [INFO]: epoch 372: training loss 0.1677\n",
      "2025-06-01 14:50:09 [INFO]: epoch 373: training loss 0.1654\n",
      "2025-06-01 14:50:09 [INFO]: epoch 374: training loss 0.1662\n",
      "2025-06-01 14:50:09 [INFO]: epoch 375: training loss 0.1682\n",
      "2025-06-01 14:50:09 [INFO]: epoch 376: training loss 0.1535\n",
      "2025-06-01 14:50:09 [INFO]: epoch 377: training loss 0.1611\n",
      "2025-06-01 14:50:09 [INFO]: epoch 378: training loss 0.1581\n",
      "2025-06-01 14:50:09 [INFO]: epoch 379: training loss 0.1630\n",
      "2025-06-01 14:50:09 [INFO]: epoch 380: training loss 0.1596\n",
      "2025-06-01 14:50:09 [INFO]: epoch 381: training loss 0.1622\n",
      "2025-06-01 14:50:09 [INFO]: epoch 382: training loss 0.1566\n",
      "2025-06-01 14:50:09 [INFO]: epoch 383: training loss 0.1555\n",
      "2025-06-01 14:50:09 [INFO]: epoch 384: training loss 0.1661\n",
      "2025-06-01 14:50:09 [INFO]: epoch 385: training loss 0.1568\n",
      "2025-06-01 14:50:09 [INFO]: epoch 386: training loss 0.1589\n",
      "2025-06-01 14:50:09 [INFO]: epoch 387: training loss 0.1557\n",
      "2025-06-01 14:50:09 [INFO]: epoch 388: training loss 0.1615\n",
      "2025-06-01 14:50:09 [INFO]: epoch 389: training loss 0.1542\n",
      "2025-06-01 14:50:09 [INFO]: epoch 390: training loss 0.1603\n",
      "2025-06-01 14:50:09 [INFO]: epoch 391: training loss 0.1497\n",
      "2025-06-01 14:50:10 [INFO]: epoch 392: training loss 0.1593\n",
      "2025-06-01 14:50:10 [INFO]: epoch 393: training loss 0.1569\n",
      "2025-06-01 14:50:10 [INFO]: epoch 394: training loss 0.1541\n",
      "2025-06-01 14:50:10 [INFO]: epoch 395: training loss 0.1583\n",
      "2025-06-01 14:50:10 [INFO]: epoch 396: training loss 0.1496\n",
      "2025-06-01 14:50:10 [INFO]: epoch 397: training loss 0.1504\n",
      "2025-06-01 14:50:10 [INFO]: epoch 398: training loss 0.1513\n",
      "2025-06-01 14:50:10 [INFO]: epoch 399: training loss 0.1510\n",
      "2025-06-01 14:50:10 [INFO]: epoch 400: training loss 0.1517\n",
      "2025-06-01 14:50:10 [INFO]: epoch 401: training loss 0.1564\n",
      "2025-06-01 14:50:10 [INFO]: epoch 402: training loss 0.1419\n",
      "2025-06-01 14:50:10 [INFO]: epoch 403: training loss 0.1531\n",
      "2025-06-01 14:50:10 [INFO]: epoch 404: training loss 0.1572\n",
      "2025-06-01 14:50:10 [INFO]: epoch 405: training loss 0.1493\n",
      "2025-06-01 14:50:10 [INFO]: epoch 406: training loss 0.1484\n",
      "2025-06-01 14:50:10 [INFO]: epoch 407: training loss 0.1418\n",
      "2025-06-01 14:50:10 [INFO]: epoch 408: training loss 0.1357\n",
      "2025-06-01 14:50:10 [INFO]: epoch 409: training loss 0.1426\n",
      "2025-06-01 14:50:10 [INFO]: epoch 410: training loss 0.1534\n",
      "2025-06-01 14:50:10 [INFO]: epoch 411: training loss 0.1357\n",
      "2025-06-01 14:50:10 [INFO]: epoch 412: training loss 0.1577\n",
      "2025-06-01 14:50:10 [INFO]: epoch 413: training loss 0.1570\n",
      "2025-06-01 14:50:10 [INFO]: epoch 414: training loss 0.1553\n",
      "2025-06-01 14:50:10 [INFO]: epoch 415: training loss 0.1443\n",
      "2025-06-01 14:50:10 [INFO]: epoch 416: training loss 0.1511\n",
      "2025-06-01 14:50:10 [INFO]: epoch 417: training loss 0.1581\n",
      "2025-06-01 14:50:10 [INFO]: epoch 418: training loss 0.1489\n",
      "2025-06-01 14:50:10 [INFO]: epoch 419: training loss 0.1560\n",
      "2025-06-01 14:50:10 [INFO]: epoch 420: training loss 0.1419\n",
      "2025-06-01 14:50:10 [INFO]: epoch 421: training loss 0.1488\n",
      "2025-06-01 14:50:10 [INFO]: epoch 422: training loss 0.1405\n",
      "2025-06-01 14:50:10 [INFO]: epoch 423: training loss 0.1596\n",
      "2025-06-01 14:50:10 [INFO]: epoch 424: training loss 0.1470\n",
      "2025-06-01 14:50:10 [INFO]: epoch 425: training loss 0.1395\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-01 14:50:10 [INFO]: epoch 426: training loss 0.1529\n",
      "2025-06-01 14:50:10 [INFO]: epoch 427: training loss 0.1437\n",
      "2025-06-01 14:50:10 [INFO]: epoch 428: training loss 0.1487\n",
      "2025-06-01 14:50:10 [INFO]: epoch 429: training loss 0.1496\n",
      "2025-06-01 14:50:10 [INFO]: epoch 430: training loss 0.1496\n",
      "2025-06-01 14:50:10 [INFO]: epoch 431: training loss 0.1431\n",
      "2025-06-01 14:50:10 [INFO]: epoch 432: training loss 0.1498\n",
      "2025-06-01 14:50:10 [INFO]: epoch 433: training loss 0.1615\n",
      "2025-06-01 14:50:10 [INFO]: epoch 434: training loss 0.1615\n",
      "2025-06-01 14:50:10 [INFO]: epoch 435: training loss 0.1423\n",
      "2025-06-01 14:50:10 [INFO]: epoch 436: training loss 0.1477\n",
      "2025-06-01 14:50:10 [INFO]: epoch 437: training loss 0.1543\n",
      "2025-06-01 14:50:10 [INFO]: epoch 438: training loss 0.1607\n",
      "2025-06-01 14:50:10 [INFO]: epoch 439: training loss 0.1530\n",
      "2025-06-01 14:50:10 [INFO]: epoch 440: training loss 0.1428\n",
      "2025-06-01 14:50:10 [INFO]: epoch 441: training loss 0.1363\n",
      "2025-06-01 14:50:10 [INFO]: epoch 442: training loss 0.1357\n",
      "2025-06-01 14:50:10 [INFO]: epoch 443: training loss 0.1473\n",
      "2025-06-01 14:50:10 [INFO]: epoch 444: training loss 0.1390\n",
      "2025-06-01 14:50:10 [INFO]: epoch 445: training loss 0.1382\n",
      "2025-06-01 14:50:10 [INFO]: epoch 446: training loss 0.1426\n",
      "2025-06-01 14:50:10 [INFO]: epoch 447: training loss 0.1443\n",
      "2025-06-01 14:50:10 [INFO]: epoch 448: training loss 0.1441\n",
      "2025-06-01 14:50:10 [INFO]: epoch 449: training loss 0.1431\n",
      "2025-06-01 14:50:10 [INFO]: epoch 450: training loss 0.1380\n",
      "2025-06-01 14:50:10 [INFO]: epoch 451: training loss 0.1393\n",
      "2025-06-01 14:50:10 [INFO]: epoch 452: training loss 0.1453\n",
      "2025-06-01 14:50:10 [INFO]: epoch 453: training loss 0.1350\n",
      "2025-06-01 14:50:10 [INFO]: epoch 454: training loss 0.1336\n",
      "2025-06-01 14:50:10 [INFO]: epoch 455: training loss 0.1483\n",
      "2025-06-01 14:50:10 [INFO]: epoch 456: training loss 0.1457\n",
      "2025-06-01 14:50:10 [INFO]: epoch 457: training loss 0.1374\n",
      "2025-06-01 14:50:10 [INFO]: epoch 458: training loss 0.1327\n",
      "2025-06-01 14:50:10 [INFO]: epoch 459: training loss 0.1405\n",
      "2025-06-01 14:50:10 [INFO]: epoch 460: training loss 0.1566\n",
      "2025-06-01 14:50:10 [INFO]: epoch 461: training loss 0.1578\n",
      "2025-06-01 14:50:10 [INFO]: epoch 462: training loss 0.1487\n",
      "2025-06-01 14:50:10 [INFO]: epoch 463: training loss 0.1365\n",
      "2025-06-01 14:50:10 [INFO]: epoch 464: training loss 0.1384\n",
      "2025-06-01 14:50:10 [INFO]: epoch 465: training loss 0.1368\n",
      "2025-06-01 14:50:10 [INFO]: epoch 466: training loss 0.1422\n",
      "2025-06-01 14:50:10 [INFO]: epoch 467: training loss 0.1386\n",
      "2025-06-01 14:50:10 [INFO]: epoch 468: training loss 0.1329\n",
      "2025-06-01 14:50:11 [INFO]: epoch 469: training loss 0.1304\n",
      "2025-06-01 14:50:11 [INFO]: epoch 470: training loss 0.1416\n",
      "2025-06-01 14:50:11 [INFO]: epoch 471: training loss 0.1378\n",
      "2025-06-01 14:50:11 [INFO]: epoch 472: training loss 0.1457\n",
      "2025-06-01 14:50:11 [INFO]: epoch 473: training loss 0.1382\n",
      "2025-06-01 14:50:11 [INFO]: epoch 474: training loss 0.1340\n",
      "2025-06-01 14:50:11 [INFO]: epoch 475: training loss 0.1377\n",
      "2025-06-01 14:50:11 [INFO]: epoch 476: training loss 0.1404\n",
      "2025-06-01 14:50:11 [INFO]: epoch 477: training loss 0.1397\n",
      "2025-06-01 14:50:11 [INFO]: epoch 478: training loss 0.1263\n",
      "2025-06-01 14:50:11 [INFO]: epoch 479: training loss 0.1376\n",
      "2025-06-01 14:50:11 [INFO]: epoch 480: training loss 0.1379\n",
      "2025-06-01 14:50:11 [INFO]: epoch 481: training loss 0.1416\n",
      "2025-06-01 14:50:11 [INFO]: epoch 482: training loss 0.1375\n",
      "2025-06-01 14:50:11 [INFO]: epoch 483: training loss 0.1241\n",
      "2025-06-01 14:50:11 [INFO]: epoch 484: training loss 0.1405\n",
      "2025-06-01 14:50:11 [INFO]: epoch 485: training loss 0.1378\n",
      "2025-06-01 14:50:11 [INFO]: epoch 486: training loss 0.1288\n",
      "2025-06-01 14:50:11 [INFO]: epoch 487: training loss 0.1279\n",
      "2025-06-01 14:50:11 [INFO]: epoch 488: training loss 0.1284\n",
      "2025-06-01 14:50:11 [INFO]: epoch 489: training loss 0.1358\n",
      "2025-06-01 14:50:11 [INFO]: epoch 490: training loss 0.1285\n",
      "2025-06-01 14:50:11 [INFO]: epoch 491: training loss 0.1303\n",
      "2025-06-01 14:50:11 [INFO]: epoch 492: training loss 0.1363\n",
      "2025-06-01 14:50:11 [INFO]: epoch 493: training loss 0.1362\n",
      "2025-06-01 14:50:11 [INFO]: epoch 494: training loss 0.1257\n",
      "2025-06-01 14:50:11 [INFO]: epoch 495: training loss 0.1260\n",
      "2025-06-01 14:50:11 [INFO]: epoch 496: training loss 0.1238\n",
      "2025-06-01 14:50:11 [INFO]: epoch 497: training loss 0.1275\n",
      "2025-06-01 14:50:11 [INFO]: epoch 498: training loss 0.1244\n",
      "2025-06-01 14:50:11 [INFO]: epoch 499: training loss 0.1436\n",
      "2025-06-01 14:50:11 [INFO]: epoch 500: training loss 0.1373\n",
      "2025-06-01 14:50:11 [INFO]: epoch 501: training loss 0.1281\n",
      "2025-06-01 14:50:11 [INFO]: epoch 502: training loss 0.1218\n",
      "2025-06-01 14:50:11 [INFO]: epoch 503: training loss 0.1229\n",
      "2025-06-01 14:50:11 [INFO]: epoch 504: training loss 0.1239\n",
      "2025-06-01 14:50:11 [INFO]: epoch 505: training loss 0.1283\n",
      "2025-06-01 14:50:11 [INFO]: epoch 506: training loss 0.1291\n",
      "2025-06-01 14:50:11 [INFO]: epoch 507: training loss 0.1223\n",
      "2025-06-01 14:50:11 [INFO]: epoch 508: training loss 0.1206\n",
      "2025-06-01 14:50:11 [INFO]: epoch 509: training loss 0.1244\n",
      "2025-06-01 14:50:11 [INFO]: epoch 510: training loss 0.1358\n",
      "2025-06-01 14:50:11 [INFO]: epoch 511: training loss 0.1436\n",
      "2025-06-01 14:50:11 [INFO]: epoch 512: training loss 0.1303\n",
      "2025-06-01 14:50:11 [INFO]: epoch 513: training loss 0.1324\n",
      "2025-06-01 14:50:11 [INFO]: epoch 514: training loss 0.1308\n",
      "2025-06-01 14:50:11 [INFO]: epoch 515: training loss 0.1247\n",
      "2025-06-01 14:50:11 [INFO]: epoch 516: training loss 0.1247\n",
      "2025-06-01 14:50:11 [INFO]: epoch 517: training loss 0.1250\n",
      "2025-06-01 14:50:11 [INFO]: epoch 518: training loss 0.1275\n",
      "2025-06-01 14:50:11 [INFO]: epoch 519: training loss 0.1241\n",
      "2025-06-01 14:50:11 [INFO]: epoch 520: training loss 0.1217\n",
      "2025-06-01 14:50:11 [INFO]: epoch 521: training loss 0.1228\n",
      "2025-06-01 14:50:11 [INFO]: epoch 522: training loss 0.1257\n",
      "2025-06-01 14:50:11 [INFO]: epoch 523: training loss 0.1264\n",
      "2025-06-01 14:50:11 [INFO]: epoch 524: training loss 0.1216\n",
      "2025-06-01 14:50:11 [INFO]: epoch 525: training loss 0.1245\n",
      "2025-06-01 14:50:11 [INFO]: epoch 526: training loss 0.1153\n",
      "2025-06-01 14:50:11 [INFO]: epoch 527: training loss 0.1182\n",
      "2025-06-01 14:50:11 [INFO]: epoch 528: training loss 0.1199\n",
      "2025-06-01 14:50:11 [INFO]: epoch 529: training loss 0.1233\n",
      "2025-06-01 14:50:11 [INFO]: epoch 530: training loss 0.1197\n",
      "2025-06-01 14:50:11 [INFO]: epoch 531: training loss 0.1177\n",
      "2025-06-01 14:50:11 [INFO]: epoch 532: training loss 0.1133\n",
      "2025-06-01 14:50:11 [INFO]: epoch 533: training loss 0.1237\n",
      "2025-06-01 14:50:11 [INFO]: epoch 534: training loss 0.1197\n",
      "2025-06-01 14:50:11 [INFO]: epoch 535: training loss 0.1113\n",
      "2025-06-01 14:50:11 [INFO]: epoch 536: training loss 0.1255\n",
      "2025-06-01 14:50:11 [INFO]: epoch 537: training loss 0.1142\n",
      "2025-06-01 14:50:11 [INFO]: epoch 538: training loss 0.1233\n",
      "2025-06-01 14:50:11 [INFO]: epoch 539: training loss 0.1246\n",
      "2025-06-01 14:50:11 [INFO]: epoch 540: training loss 0.1188\n",
      "2025-06-01 14:50:11 [INFO]: epoch 541: training loss 0.1242\n",
      "2025-06-01 14:50:11 [INFO]: epoch 542: training loss 0.1209\n",
      "2025-06-01 14:50:11 [INFO]: epoch 543: training loss 0.1211\n",
      "2025-06-01 14:50:11 [INFO]: epoch 544: training loss 0.1191\n",
      "2025-06-01 14:50:12 [INFO]: epoch 545: training loss 0.1174\n",
      "2025-06-01 14:50:12 [INFO]: epoch 546: training loss 0.1205\n",
      "2025-06-01 14:50:12 [INFO]: epoch 547: training loss 0.1106\n",
      "2025-06-01 14:50:12 [INFO]: epoch 548: training loss 0.1203\n",
      "2025-06-01 14:50:12 [INFO]: epoch 549: training loss 0.1187\n",
      "2025-06-01 14:50:12 [INFO]: epoch 550: training loss 0.1256\n",
      "2025-06-01 14:50:12 [INFO]: epoch 551: training loss 0.1136\n",
      "2025-06-01 14:50:12 [INFO]: epoch 552: training loss 0.1132\n",
      "2025-06-01 14:50:12 [INFO]: epoch 553: training loss 0.1082\n",
      "2025-06-01 14:50:12 [INFO]: epoch 554: training loss 0.1193\n",
      "2025-06-01 14:50:12 [INFO]: epoch 555: training loss 0.1144\n",
      "2025-06-01 14:50:12 [INFO]: epoch 556: training loss 0.1150\n",
      "2025-06-01 14:50:12 [INFO]: epoch 557: training loss 0.1098\n",
      "2025-06-01 14:50:12 [INFO]: epoch 558: training loss 0.1234\n",
      "2025-06-01 14:50:12 [INFO]: epoch 559: training loss 0.1144\n",
      "2025-06-01 14:50:12 [INFO]: epoch 560: training loss 0.1117\n",
      "2025-06-01 14:50:12 [INFO]: epoch 561: training loss 0.1158\n",
      "2025-06-01 14:50:12 [INFO]: epoch 562: training loss 0.1097\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-01 14:50:12 [INFO]: epoch 563: training loss 0.1198\n",
      "2025-06-01 14:50:12 [INFO]: epoch 564: training loss 0.1165\n",
      "2025-06-01 14:50:12 [INFO]: epoch 565: training loss 0.1085\n",
      "2025-06-01 14:50:12 [INFO]: epoch 566: training loss 0.1142\n",
      "2025-06-01 14:50:12 [INFO]: epoch 567: training loss 0.1225\n",
      "2025-06-01 14:50:12 [INFO]: epoch 568: training loss 0.1112\n",
      "2025-06-01 14:50:12 [INFO]: epoch 569: training loss 0.1059\n",
      "2025-06-01 14:50:12 [INFO]: epoch 570: training loss 0.1065\n",
      "2025-06-01 14:50:12 [INFO]: epoch 571: training loss 0.1166\n",
      "2025-06-01 14:50:12 [INFO]: epoch 572: training loss 0.1117\n",
      "2025-06-01 14:50:12 [INFO]: epoch 573: training loss 0.1109\n",
      "2025-06-01 14:50:12 [INFO]: epoch 574: training loss 0.1029\n",
      "2025-06-01 14:50:12 [INFO]: epoch 575: training loss 0.0981\n",
      "2025-06-01 14:50:12 [INFO]: epoch 576: training loss 0.1077\n",
      "2025-06-01 14:50:12 [INFO]: epoch 577: training loss 0.1073\n",
      "2025-06-01 14:50:12 [INFO]: epoch 578: training loss 0.0977\n",
      "2025-06-01 14:50:12 [INFO]: epoch 579: training loss 0.1081\n",
      "2025-06-01 14:50:12 [INFO]: epoch 580: training loss 0.1048\n",
      "2025-06-01 14:50:12 [INFO]: epoch 581: training loss 0.1095\n",
      "2025-06-01 14:50:12 [INFO]: epoch 582: training loss 0.1015\n",
      "2025-06-01 14:50:12 [INFO]: epoch 583: training loss 0.1083\n",
      "2025-06-01 14:50:12 [INFO]: epoch 584: training loss 0.1140\n",
      "2025-06-01 14:50:12 [INFO]: epoch 585: training loss 0.1036\n",
      "2025-06-01 14:50:12 [INFO]: epoch 586: training loss 0.1073\n",
      "2025-06-01 14:50:12 [INFO]: epoch 587: training loss 0.1003\n",
      "2025-06-01 14:50:12 [INFO]: epoch 588: training loss 0.0990\n",
      "2025-06-01 14:50:12 [INFO]: epoch 589: training loss 0.1106\n",
      "2025-06-01 14:50:12 [INFO]: epoch 590: training loss 0.1093\n",
      "2025-06-01 14:50:12 [INFO]: epoch 591: training loss 0.1060\n",
      "2025-06-01 14:50:12 [INFO]: epoch 592: training loss 0.0980\n",
      "2025-06-01 14:50:12 [INFO]: epoch 593: training loss 0.1034\n",
      "2025-06-01 14:50:12 [INFO]: epoch 594: training loss 0.0981\n",
      "2025-06-01 14:50:12 [INFO]: epoch 595: training loss 0.0992\n",
      "2025-06-01 14:50:12 [INFO]: epoch 596: training loss 0.1037\n",
      "2025-06-01 14:50:12 [INFO]: epoch 597: training loss 0.0997\n",
      "2025-06-01 14:50:12 [INFO]: epoch 598: training loss 0.1069\n",
      "2025-06-01 14:50:12 [INFO]: epoch 599: training loss 0.1091\n",
      "2025-06-01 14:50:12 [INFO]: epoch 600: training loss 0.1054\n",
      "2025-06-01 14:50:12 [INFO]: epoch 601: training loss 0.1008\n",
      "2025-06-01 14:50:12 [INFO]: epoch 602: training loss 0.0960\n",
      "2025-06-01 14:50:12 [INFO]: epoch 603: training loss 0.1070\n",
      "2025-06-01 14:50:12 [INFO]: epoch 604: training loss 0.1021\n",
      "2025-06-01 14:50:12 [INFO]: epoch 605: training loss 0.1045\n",
      "2025-06-01 14:50:12 [INFO]: epoch 606: training loss 0.0983\n",
      "2025-06-01 14:50:12 [INFO]: epoch 607: training loss 0.1047\n",
      "2025-06-01 14:50:12 [INFO]: epoch 608: training loss 0.0966\n",
      "2025-06-01 14:50:12 [INFO]: epoch 609: training loss 0.1035\n",
      "2025-06-01 14:50:12 [INFO]: epoch 610: training loss 0.1032\n",
      "2025-06-01 14:50:12 [INFO]: epoch 611: training loss 0.1038\n",
      "2025-06-01 14:50:12 [INFO]: epoch 612: training loss 0.0963\n",
      "2025-06-01 14:50:12 [INFO]: epoch 613: training loss 0.0916\n",
      "2025-06-01 14:50:12 [INFO]: epoch 614: training loss 0.0997\n",
      "2025-06-01 14:50:12 [INFO]: epoch 615: training loss 0.1001\n",
      "2025-06-01 14:50:12 [INFO]: epoch 616: training loss 0.0989\n",
      "2025-06-01 14:50:12 [INFO]: epoch 617: training loss 0.1008\n",
      "2025-06-01 14:50:12 [INFO]: epoch 618: training loss 0.1063\n",
      "2025-06-01 14:50:12 [INFO]: epoch 619: training loss 0.1039\n",
      "2025-06-01 14:50:12 [INFO]: epoch 620: training loss 0.1006\n",
      "2025-06-01 14:50:13 [INFO]: epoch 621: training loss 0.1008\n",
      "2025-06-01 14:50:13 [INFO]: epoch 622: training loss 0.1026\n",
      "2025-06-01 14:50:13 [INFO]: epoch 623: training loss 0.0950\n",
      "2025-06-01 14:50:13 [INFO]: epoch 624: training loss 0.0961\n",
      "2025-06-01 14:50:13 [INFO]: epoch 625: training loss 0.0973\n",
      "2025-06-01 14:50:13 [INFO]: epoch 626: training loss 0.0981\n",
      "2025-06-01 14:50:13 [INFO]: epoch 627: training loss 0.0938\n",
      "2025-06-01 14:50:13 [INFO]: epoch 628: training loss 0.0978\n",
      "2025-06-01 14:50:13 [INFO]: epoch 629: training loss 0.0956\n",
      "2025-06-01 14:50:13 [INFO]: epoch 630: training loss 0.0944\n",
      "2025-06-01 14:50:13 [INFO]: epoch 631: training loss 0.0942\n",
      "2025-06-01 14:50:13 [INFO]: epoch 632: training loss 0.0907\n",
      "2025-06-01 14:50:13 [INFO]: epoch 633: training loss 0.0934\n",
      "2025-06-01 14:50:13 [INFO]: epoch 634: training loss 0.0904\n",
      "2025-06-01 14:50:13 [INFO]: epoch 635: training loss 0.0971\n",
      "2025-06-01 14:50:13 [INFO]: epoch 636: training loss 0.0901\n",
      "2025-06-01 14:50:13 [INFO]: epoch 637: training loss 0.0999\n",
      "2025-06-01 14:50:13 [INFO]: epoch 638: training loss 0.0959\n",
      "2025-06-01 14:50:13 [INFO]: epoch 639: training loss 0.0912\n",
      "2025-06-01 14:50:13 [INFO]: epoch 640: training loss 0.0989\n",
      "2025-06-01 14:50:13 [INFO]: epoch 641: training loss 0.0929\n",
      "2025-06-01 14:50:13 [INFO]: epoch 642: training loss 0.0935\n",
      "2025-06-01 14:50:13 [INFO]: epoch 643: training loss 0.0929\n",
      "2025-06-01 14:50:13 [INFO]: epoch 644: training loss 0.0856\n",
      "2025-06-01 14:50:13 [INFO]: epoch 645: training loss 0.0846\n",
      "2025-06-01 14:50:13 [INFO]: epoch 646: training loss 0.0983\n",
      "2025-06-01 14:50:13 [INFO]: epoch 647: training loss 0.0977\n",
      "2025-06-01 14:50:13 [INFO]: epoch 648: training loss 0.0890\n",
      "2025-06-01 14:50:13 [INFO]: epoch 649: training loss 0.0910\n",
      "2025-06-01 14:50:13 [INFO]: epoch 650: training loss 0.0897\n",
      "2025-06-01 14:50:13 [INFO]: epoch 651: training loss 0.0917\n",
      "2025-06-01 14:50:13 [INFO]: epoch 652: training loss 0.0933\n",
      "2025-06-01 14:50:13 [INFO]: epoch 653: training loss 0.0883\n",
      "2025-06-01 14:50:13 [INFO]: epoch 654: training loss 0.0876\n",
      "2025-06-01 14:50:13 [INFO]: epoch 655: training loss 0.0988\n",
      "2025-06-01 14:50:13 [INFO]: epoch 656: training loss 0.0912\n",
      "2025-06-01 14:50:13 [INFO]: epoch 657: training loss 0.0919\n",
      "2025-06-01 14:50:13 [INFO]: epoch 658: training loss 0.0947\n",
      "2025-06-01 14:50:13 [INFO]: epoch 659: training loss 0.0896\n",
      "2025-06-01 14:50:13 [INFO]: epoch 660: training loss 0.0898\n",
      "2025-06-01 14:50:13 [INFO]: epoch 661: training loss 0.0912\n",
      "2025-06-01 14:50:13 [INFO]: epoch 662: training loss 0.0900\n",
      "2025-06-01 14:50:13 [INFO]: epoch 663: training loss 0.0938\n",
      "2025-06-01 14:50:13 [INFO]: epoch 664: training loss 0.0908\n",
      "2025-06-01 14:50:13 [INFO]: epoch 665: training loss 0.0910\n",
      "2025-06-01 14:50:13 [INFO]: epoch 666: training loss 0.0829\n",
      "2025-06-01 14:50:13 [INFO]: epoch 667: training loss 0.0866\n",
      "2025-06-01 14:50:13 [INFO]: epoch 668: training loss 0.0919\n",
      "2025-06-01 14:50:13 [INFO]: epoch 669: training loss 0.1000\n",
      "2025-06-01 14:50:13 [INFO]: epoch 670: training loss 0.0936\n",
      "2025-06-01 14:50:13 [INFO]: epoch 671: training loss 0.0848\n",
      "2025-06-01 14:50:13 [INFO]: epoch 672: training loss 0.0941\n",
      "2025-06-01 14:50:13 [INFO]: epoch 673: training loss 0.0945\n",
      "2025-06-01 14:50:13 [INFO]: epoch 674: training loss 0.0913\n",
      "2025-06-01 14:50:13 [INFO]: epoch 675: training loss 0.0846\n",
      "2025-06-01 14:50:13 [INFO]: epoch 676: training loss 0.0902\n",
      "2025-06-01 14:50:13 [INFO]: epoch 677: training loss 0.0935\n",
      "2025-06-01 14:50:13 [INFO]: epoch 678: training loss 0.0924\n",
      "2025-06-01 14:50:13 [INFO]: epoch 679: training loss 0.0927\n",
      "2025-06-01 14:50:13 [INFO]: epoch 680: training loss 0.0878\n",
      "2025-06-01 14:50:13 [INFO]: epoch 681: training loss 0.0896\n",
      "2025-06-01 14:50:13 [INFO]: epoch 682: training loss 0.0835\n",
      "2025-06-01 14:50:13 [INFO]: epoch 683: training loss 0.0830\n",
      "2025-06-01 14:50:13 [INFO]: epoch 684: training loss 0.0872\n",
      "2025-06-01 14:50:13 [INFO]: epoch 685: training loss 0.0946\n",
      "2025-06-01 14:50:13 [INFO]: epoch 686: training loss 0.0916\n",
      "2025-06-01 14:50:13 [INFO]: epoch 687: training loss 0.0898\n",
      "2025-06-01 14:50:13 [INFO]: epoch 688: training loss 0.0886\n",
      "2025-06-01 14:50:13 [INFO]: epoch 689: training loss 0.0871\n",
      "2025-06-01 14:50:13 [INFO]: epoch 690: training loss 0.0843\n",
      "2025-06-01 14:50:13 [INFO]: epoch 691: training loss 0.0810\n",
      "2025-06-01 14:50:13 [INFO]: epoch 692: training loss 0.0778\n",
      "2025-06-01 14:50:13 [INFO]: epoch 693: training loss 0.0917\n",
      "2025-06-01 14:50:13 [INFO]: epoch 694: training loss 0.0826\n",
      "2025-06-01 14:50:13 [INFO]: epoch 695: training loss 0.0875\n",
      "2025-06-01 14:50:13 [INFO]: epoch 696: training loss 0.0895\n",
      "2025-06-01 14:50:13 [INFO]: epoch 697: training loss 0.0723\n",
      "2025-06-01 14:50:14 [INFO]: epoch 698: training loss 0.0805\n",
      "2025-06-01 14:50:14 [INFO]: epoch 699: training loss 0.0885\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-01 14:50:14 [INFO]: epoch 700: training loss 0.0823\n",
      "2025-06-01 14:50:14 [INFO]: epoch 701: training loss 0.0923\n",
      "2025-06-01 14:50:14 [INFO]: epoch 702: training loss 0.0864\n",
      "2025-06-01 14:50:14 [INFO]: epoch 703: training loss 0.0862\n",
      "2025-06-01 14:50:14 [INFO]: epoch 704: training loss 0.0897\n",
      "2025-06-01 14:50:14 [INFO]: epoch 705: training loss 0.0810\n",
      "2025-06-01 14:50:14 [INFO]: epoch 706: training loss 0.0857\n",
      "2025-06-01 14:50:14 [INFO]: epoch 707: training loss 0.0753\n",
      "2025-06-01 14:50:14 [INFO]: epoch 708: training loss 0.0828\n",
      "2025-06-01 14:50:14 [INFO]: epoch 709: training loss 0.0810\n",
      "2025-06-01 14:50:14 [INFO]: epoch 710: training loss 0.0760\n",
      "2025-06-01 14:50:14 [INFO]: epoch 711: training loss 0.0775\n",
      "2025-06-01 14:50:14 [INFO]: epoch 712: training loss 0.0874\n",
      "2025-06-01 14:50:14 [INFO]: epoch 713: training loss 0.0775\n",
      "2025-06-01 14:50:14 [INFO]: epoch 714: training loss 0.0827\n",
      "2025-06-01 14:50:14 [INFO]: epoch 715: training loss 0.0851\n",
      "2025-06-01 14:50:14 [INFO]: epoch 716: training loss 0.0848\n",
      "2025-06-01 14:50:14 [INFO]: epoch 717: training loss 0.0806\n",
      "2025-06-01 14:50:14 [INFO]: epoch 718: training loss 0.0781\n",
      "2025-06-01 14:50:14 [INFO]: epoch 719: training loss 0.0838\n",
      "2025-06-01 14:50:14 [INFO]: epoch 720: training loss 0.0833\n",
      "2025-06-01 14:50:14 [INFO]: epoch 721: training loss 0.0904\n",
      "2025-06-01 14:50:14 [INFO]: epoch 722: training loss 0.0793\n",
      "2025-06-01 14:50:14 [INFO]: epoch 723: training loss 0.0827\n",
      "2025-06-01 14:50:14 [INFO]: epoch 724: training loss 0.0787\n",
      "2025-06-01 14:50:14 [INFO]: epoch 725: training loss 0.0889\n",
      "2025-06-01 14:50:14 [INFO]: epoch 726: training loss 0.0811\n",
      "2025-06-01 14:50:14 [INFO]: epoch 727: training loss 0.0773\n",
      "2025-06-01 14:50:14 [INFO]: epoch 728: training loss 0.0756\n",
      "2025-06-01 14:50:14 [INFO]: epoch 729: training loss 0.0789\n",
      "2025-06-01 14:50:14 [INFO]: epoch 730: training loss 0.0821\n",
      "2025-06-01 14:50:14 [INFO]: epoch 731: training loss 0.0723\n",
      "2025-06-01 14:50:14 [INFO]: epoch 732: training loss 0.0722\n",
      "2025-06-01 14:50:14 [INFO]: epoch 733: training loss 0.0732\n",
      "2025-06-01 14:50:14 [INFO]: epoch 734: training loss 0.0723\n",
      "2025-06-01 14:50:14 [INFO]: epoch 735: training loss 0.0797\n",
      "2025-06-01 14:50:14 [INFO]: epoch 736: training loss 0.0786\n",
      "2025-06-01 14:50:14 [INFO]: epoch 737: training loss 0.0763\n",
      "2025-06-01 14:50:14 [INFO]: epoch 738: training loss 0.0807\n",
      "2025-06-01 14:50:14 [INFO]: epoch 739: training loss 0.0819\n",
      "2025-06-01 14:50:14 [INFO]: epoch 740: training loss 0.0763\n",
      "2025-06-01 14:50:14 [INFO]: epoch 741: training loss 0.0712\n",
      "2025-06-01 14:50:14 [INFO]: epoch 742: training loss 0.0813\n",
      "2025-06-01 14:50:14 [INFO]: epoch 743: training loss 0.0739\n",
      "2025-06-01 14:50:14 [INFO]: epoch 744: training loss 0.0776\n",
      "2025-06-01 14:50:14 [INFO]: epoch 745: training loss 0.0675\n",
      "2025-06-01 14:50:14 [INFO]: epoch 746: training loss 0.0773\n",
      "2025-06-01 14:50:14 [INFO]: epoch 747: training loss 0.0740\n",
      "2025-06-01 14:50:14 [INFO]: epoch 748: training loss 0.0773\n",
      "2025-06-01 14:50:14 [INFO]: epoch 749: training loss 0.0718\n",
      "2025-06-01 14:50:14 [INFO]: epoch 750: training loss 0.0754\n",
      "2025-06-01 14:50:14 [INFO]: epoch 751: training loss 0.0750\n",
      "2025-06-01 14:50:14 [INFO]: epoch 752: training loss 0.0726\n",
      "2025-06-01 14:50:14 [INFO]: epoch 753: training loss 0.0670\n",
      "2025-06-01 14:50:14 [INFO]: epoch 754: training loss 0.0729\n",
      "2025-06-01 14:50:14 [INFO]: epoch 755: training loss 0.0728\n",
      "2025-06-01 14:50:14 [INFO]: epoch 756: training loss 0.0696\n",
      "2025-06-01 14:50:14 [INFO]: epoch 757: training loss 0.0714\n",
      "2025-06-01 14:50:14 [INFO]: epoch 758: training loss 0.0788\n",
      "2025-06-01 14:50:14 [INFO]: epoch 759: training loss 0.0671\n",
      "2025-06-01 14:50:14 [INFO]: epoch 760: training loss 0.0749\n",
      "2025-06-01 14:50:14 [INFO]: epoch 761: training loss 0.0712\n",
      "2025-06-01 14:50:14 [INFO]: epoch 762: training loss 0.0853\n",
      "2025-06-01 14:50:14 [INFO]: epoch 763: training loss 0.0782\n",
      "2025-06-01 14:50:14 [INFO]: epoch 764: training loss 0.0687\n",
      "2025-06-01 14:50:14 [INFO]: epoch 765: training loss 0.0711\n",
      "2025-06-01 14:50:14 [INFO]: epoch 766: training loss 0.0713\n",
      "2025-06-01 14:50:14 [INFO]: epoch 767: training loss 0.0737\n",
      "2025-06-01 14:50:14 [INFO]: epoch 768: training loss 0.0682\n",
      "2025-06-01 14:50:14 [INFO]: epoch 769: training loss 0.0674\n",
      "2025-06-01 14:50:14 [INFO]: epoch 770: training loss 0.0681\n",
      "2025-06-01 14:50:14 [INFO]: epoch 771: training loss 0.0658\n",
      "2025-06-01 14:50:14 [INFO]: epoch 772: training loss 0.0715\n",
      "2025-06-01 14:50:14 [INFO]: epoch 773: training loss 0.0713\n",
      "2025-06-01 14:50:14 [INFO]: epoch 774: training loss 0.0696\n",
      "2025-06-01 14:50:15 [INFO]: epoch 775: training loss 0.0671\n",
      "2025-06-01 14:50:15 [INFO]: epoch 776: training loss 0.0665\n",
      "2025-06-01 14:50:15 [INFO]: epoch 777: training loss 0.0719\n",
      "2025-06-01 14:50:15 [INFO]: epoch 778: training loss 0.0692\n",
      "2025-06-01 14:50:15 [INFO]: epoch 779: training loss 0.0654\n",
      "2025-06-01 14:50:15 [INFO]: epoch 780: training loss 0.0701\n",
      "2025-06-01 14:50:15 [INFO]: epoch 781: training loss 0.0671\n",
      "2025-06-01 14:50:15 [INFO]: epoch 782: training loss 0.0711\n",
      "2025-06-01 14:50:15 [INFO]: epoch 783: training loss 0.0723\n",
      "2025-06-01 14:50:15 [INFO]: epoch 784: training loss 0.0699\n",
      "2025-06-01 14:50:15 [INFO]: epoch 785: training loss 0.0709\n",
      "2025-06-01 14:50:15 [INFO]: epoch 786: training loss 0.0687\n",
      "2025-06-01 14:50:15 [INFO]: epoch 787: training loss 0.0734\n",
      "2025-06-01 14:50:15 [INFO]: epoch 788: training loss 0.0729\n",
      "2025-06-01 14:50:15 [INFO]: epoch 789: training loss 0.0712\n",
      "2025-06-01 14:50:15 [INFO]: epoch 790: training loss 0.0679\n",
      "2025-06-01 14:50:15 [INFO]: epoch 791: training loss 0.0725\n",
      "2025-06-01 14:50:15 [INFO]: epoch 792: training loss 0.0761\n",
      "2025-06-01 14:50:15 [INFO]: epoch 793: training loss 0.0662\n",
      "2025-06-01 14:50:15 [INFO]: epoch 794: training loss 0.0650\n",
      "2025-06-01 14:50:15 [INFO]: epoch 795: training loss 0.0699\n",
      "2025-06-01 14:50:15 [INFO]: epoch 796: training loss 0.0744\n",
      "2025-06-01 14:50:15 [INFO]: epoch 797: training loss 0.0682\n",
      "2025-06-01 14:50:15 [INFO]: epoch 798: training loss 0.0633\n",
      "2025-06-01 14:50:15 [INFO]: epoch 799: training loss 0.0592\n",
      "2025-06-01 14:50:15 [INFO]: Finished training.\n",
      "2025-06-01 14:50:15 [WARNING]: 🚨DeprecationWarning: The method impute is deprecated. Please use `predict` instead.\n",
      " 17%|██████████████                                                                      | 1/6 [00:10<00:52, 10.59s/it]2025-06-01 14:50:15 [INFO]: No given device, using default device: cuda\n",
      "2025-06-01 14:50:15 [WARNING]: saving_path not given. Model files and tensorboard file will not be saved.\n",
      "2025-06-01 14:50:15 [INFO]: Model initialized successfully with the number of trainable parameters: 597,780\n",
      "2025-06-01 14:50:15 [INFO]: epoch 0: training loss 1.4105\n",
      "2025-06-01 14:50:15 [INFO]: epoch 1: training loss 0.7160\n",
      "2025-06-01 14:50:15 [INFO]: epoch 2: training loss 0.5924\n",
      "2025-06-01 14:50:15 [INFO]: epoch 3: training loss 0.6243\n",
      "2025-06-01 14:50:15 [INFO]: epoch 4: training loss 0.5621\n",
      "2025-06-01 14:50:15 [INFO]: epoch 5: training loss 0.5525\n",
      "2025-06-01 14:50:15 [INFO]: epoch 6: training loss 0.4349\n",
      "2025-06-01 14:50:15 [INFO]: epoch 7: training loss 0.3694\n",
      "2025-06-01 14:50:15 [INFO]: epoch 8: training loss 0.3888\n",
      "2025-06-01 14:50:15 [INFO]: epoch 9: training loss 0.3948\n",
      "2025-06-01 14:50:15 [INFO]: epoch 10: training loss 0.4246\n",
      "2025-06-01 14:50:15 [INFO]: epoch 11: training loss 0.3546\n",
      "2025-06-01 14:50:15 [INFO]: epoch 12: training loss 0.3115\n",
      "2025-06-01 14:50:15 [INFO]: epoch 13: training loss 0.2623\n",
      "2025-06-01 14:50:15 [INFO]: epoch 14: training loss 0.2683\n",
      "2025-06-01 14:50:15 [INFO]: epoch 15: training loss 0.2980\n",
      "2025-06-01 14:50:15 [INFO]: epoch 16: training loss 0.2475\n",
      "2025-06-01 14:50:15 [INFO]: epoch 17: training loss 0.2048\n",
      "2025-06-01 14:50:15 [INFO]: epoch 18: training loss 0.2095\n",
      "2025-06-01 14:50:15 [INFO]: epoch 19: training loss 0.2084\n",
      "2025-06-01 14:50:15 [INFO]: epoch 20: training loss 0.1857\n",
      "2025-06-01 14:50:15 [INFO]: epoch 21: training loss 0.1829\n",
      "2025-06-01 14:50:15 [INFO]: epoch 22: training loss 0.2037\n",
      "2025-06-01 14:50:15 [INFO]: epoch 23: training loss 0.2109\n",
      "2025-06-01 14:50:15 [INFO]: epoch 24: training loss 0.1817\n",
      "2025-06-01 14:50:15 [INFO]: epoch 25: training loss 0.1782\n",
      "2025-06-01 14:50:15 [INFO]: epoch 26: training loss 0.1481\n",
      "2025-06-01 14:50:15 [INFO]: epoch 27: training loss 0.1785\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-01 14:50:15 [INFO]: epoch 28: training loss 0.1717\n",
      "2025-06-01 14:50:15 [INFO]: epoch 29: training loss 0.1541\n",
      "2025-06-01 14:50:15 [INFO]: epoch 30: training loss 0.1331\n",
      "2025-06-01 14:50:15 [INFO]: epoch 31: training loss 0.1632\n",
      "2025-06-01 14:50:15 [INFO]: epoch 32: training loss 0.1399\n",
      "2025-06-01 14:50:15 [INFO]: epoch 33: training loss 0.1445\n",
      "2025-06-01 14:50:15 [INFO]: epoch 34: training loss 0.1463\n",
      "2025-06-01 14:50:15 [INFO]: epoch 35: training loss 0.1487\n",
      "2025-06-01 14:50:15 [INFO]: epoch 36: training loss 0.1460\n",
      "2025-06-01 14:50:15 [INFO]: epoch 37: training loss 0.1233\n",
      "2025-06-01 14:50:15 [INFO]: epoch 38: training loss 0.1520\n",
      "2025-06-01 14:50:15 [INFO]: epoch 39: training loss 0.1514\n",
      "2025-06-01 14:50:15 [INFO]: epoch 40: training loss 0.1304\n",
      "2025-06-01 14:50:15 [INFO]: epoch 41: training loss 0.1224\n",
      "2025-06-01 14:50:15 [INFO]: epoch 42: training loss 0.1537\n",
      "2025-06-01 14:50:16 [INFO]: epoch 43: training loss 0.1247\n",
      "2025-06-01 14:50:16 [INFO]: epoch 44: training loss 0.1082\n",
      "2025-06-01 14:50:16 [INFO]: epoch 45: training loss 0.1258\n",
      "2025-06-01 14:50:16 [INFO]: epoch 46: training loss 0.1177\n",
      "2025-06-01 14:50:16 [INFO]: epoch 47: training loss 0.1180\n",
      "2025-06-01 14:50:16 [INFO]: epoch 48: training loss 0.1213\n",
      "2025-06-01 14:50:16 [INFO]: epoch 49: training loss 0.1075\n",
      "2025-06-01 14:50:16 [INFO]: epoch 50: training loss 0.1204\n",
      "2025-06-01 14:50:16 [INFO]: epoch 51: training loss 0.1237\n",
      "2025-06-01 14:50:16 [INFO]: epoch 52: training loss 0.1226\n",
      "2025-06-01 14:50:16 [INFO]: epoch 53: training loss 0.1112\n",
      "2025-06-01 14:50:16 [INFO]: epoch 54: training loss 0.1147\n",
      "2025-06-01 14:50:16 [INFO]: epoch 55: training loss 0.1092\n",
      "2025-06-01 14:50:16 [INFO]: epoch 56: training loss 0.1276\n",
      "2025-06-01 14:50:16 [INFO]: epoch 57: training loss 0.1117\n",
      "2025-06-01 14:50:16 [INFO]: epoch 58: training loss 0.1019\n",
      "2025-06-01 14:50:16 [INFO]: epoch 59: training loss 0.1083\n",
      "2025-06-01 14:50:16 [INFO]: epoch 60: training loss 0.1202\n",
      "2025-06-01 14:50:16 [INFO]: epoch 61: training loss 0.1174\n",
      "2025-06-01 14:50:16 [INFO]: epoch 62: training loss 0.1005\n",
      "2025-06-01 14:50:16 [INFO]: epoch 63: training loss 0.1177\n",
      "2025-06-01 14:50:16 [INFO]: epoch 64: training loss 0.1096\n",
      "2025-06-01 14:50:16 [INFO]: epoch 65: training loss 0.1073\n",
      "2025-06-01 14:50:16 [INFO]: epoch 66: training loss 0.1122\n",
      "2025-06-01 14:50:16 [INFO]: epoch 67: training loss 0.1101\n",
      "2025-06-01 14:50:16 [INFO]: epoch 68: training loss 0.0994\n",
      "2025-06-01 14:50:16 [INFO]: epoch 69: training loss 0.1035\n",
      "2025-06-01 14:50:16 [INFO]: epoch 70: training loss 0.0978\n",
      "2025-06-01 14:50:16 [INFO]: epoch 71: training loss 0.1141\n",
      "2025-06-01 14:50:16 [INFO]: epoch 72: training loss 0.1029\n",
      "2025-06-01 14:50:16 [INFO]: epoch 73: training loss 0.0884\n",
      "2025-06-01 14:50:16 [INFO]: epoch 74: training loss 0.1025\n",
      "2025-06-01 14:50:16 [INFO]: epoch 75: training loss 0.1055\n",
      "2025-06-01 14:50:16 [INFO]: epoch 76: training loss 0.1011\n",
      "2025-06-01 14:50:16 [INFO]: epoch 77: training loss 0.1138\n",
      "2025-06-01 14:50:16 [INFO]: epoch 78: training loss 0.0975\n",
      "2025-06-01 14:50:16 [INFO]: epoch 79: training loss 0.0999\n",
      "2025-06-01 14:50:16 [INFO]: epoch 80: training loss 0.0832\n",
      "2025-06-01 14:50:16 [INFO]: epoch 81: training loss 0.0841\n",
      "2025-06-01 14:50:16 [INFO]: epoch 82: training loss 0.0975\n",
      "2025-06-01 14:50:16 [INFO]: epoch 83: training loss 0.0909\n",
      "2025-06-01 14:50:16 [INFO]: epoch 84: training loss 0.0997\n",
      "2025-06-01 14:50:16 [INFO]: epoch 85: training loss 0.1073\n",
      "2025-06-01 14:50:16 [INFO]: epoch 86: training loss 0.1015\n",
      "2025-06-01 14:50:16 [INFO]: epoch 87: training loss 0.1023\n",
      "2025-06-01 14:50:16 [INFO]: epoch 88: training loss 0.0892\n",
      "2025-06-01 14:50:16 [INFO]: epoch 89: training loss 0.0982\n",
      "2025-06-01 14:50:16 [INFO]: epoch 90: training loss 0.0823\n",
      "2025-06-01 14:50:16 [INFO]: epoch 91: training loss 0.1081\n",
      "2025-06-01 14:50:16 [INFO]: epoch 92: training loss 0.0942\n",
      "2025-06-01 14:50:16 [INFO]: epoch 93: training loss 0.0885\n",
      "2025-06-01 14:50:16 [INFO]: epoch 94: training loss 0.0919\n",
      "2025-06-01 14:50:16 [INFO]: epoch 95: training loss 0.0916\n",
      "2025-06-01 14:50:16 [INFO]: epoch 96: training loss 0.1066\n",
      "2025-06-01 14:50:16 [INFO]: epoch 97: training loss 0.1004\n",
      "2025-06-01 14:50:16 [INFO]: epoch 98: training loss 0.0847\n",
      "2025-06-01 14:50:16 [INFO]: epoch 99: training loss 0.0965\n",
      "2025-06-01 14:50:16 [INFO]: epoch 100: training loss 0.0820\n",
      "2025-06-01 14:50:16 [INFO]: epoch 101: training loss 0.0802\n",
      "2025-06-01 14:50:16 [INFO]: epoch 102: training loss 0.0811\n",
      "2025-06-01 14:50:16 [INFO]: epoch 103: training loss 0.0862\n",
      "2025-06-01 14:50:16 [INFO]: epoch 104: training loss 0.0871\n",
      "2025-06-01 14:50:16 [INFO]: epoch 105: training loss 0.0794\n",
      "2025-06-01 14:50:16 [INFO]: epoch 106: training loss 0.1062\n",
      "2025-06-01 14:50:16 [INFO]: epoch 107: training loss 0.0880\n",
      "2025-06-01 14:50:16 [INFO]: epoch 108: training loss 0.0916\n",
      "2025-06-01 14:50:16 [INFO]: epoch 109: training loss 0.1039\n",
      "2025-06-01 14:50:16 [INFO]: epoch 110: training loss 0.0908\n",
      "2025-06-01 14:50:16 [INFO]: epoch 111: training loss 0.0841\n",
      "2025-06-01 14:50:16 [INFO]: epoch 112: training loss 0.1110\n",
      "2025-06-01 14:50:16 [INFO]: epoch 113: training loss 0.1166\n",
      "2025-06-01 14:50:16 [INFO]: epoch 114: training loss 0.1074\n",
      "2025-06-01 14:50:16 [INFO]: epoch 115: training loss 0.1020\n",
      "2025-06-01 14:50:16 [INFO]: epoch 116: training loss 0.0870\n",
      "2025-06-01 14:50:16 [INFO]: epoch 117: training loss 0.0857\n",
      "2025-06-01 14:50:16 [INFO]: epoch 118: training loss 0.0814\n",
      "2025-06-01 14:50:16 [INFO]: epoch 119: training loss 0.0859\n",
      "2025-06-01 14:50:17 [INFO]: epoch 120: training loss 0.0958\n",
      "2025-06-01 14:50:17 [INFO]: epoch 121: training loss 0.0927\n",
      "2025-06-01 14:50:17 [INFO]: epoch 122: training loss 0.0905\n",
      "2025-06-01 14:50:17 [INFO]: epoch 123: training loss 0.0833\n",
      "2025-06-01 14:50:17 [INFO]: epoch 124: training loss 0.0938\n",
      "2025-06-01 14:50:17 [INFO]: epoch 125: training loss 0.0919\n",
      "2025-06-01 14:50:17 [INFO]: epoch 126: training loss 0.0810\n",
      "2025-06-01 14:50:17 [INFO]: epoch 127: training loss 0.0833\n",
      "2025-06-01 14:50:17 [INFO]: epoch 128: training loss 0.0798\n",
      "2025-06-01 14:50:17 [INFO]: epoch 129: training loss 0.0811\n",
      "2025-06-01 14:50:17 [INFO]: epoch 130: training loss 0.0848\n",
      "2025-06-01 14:50:17 [INFO]: epoch 131: training loss 0.0865\n",
      "2025-06-01 14:50:17 [INFO]: epoch 132: training loss 0.0723\n",
      "2025-06-01 14:50:17 [INFO]: epoch 133: training loss 0.0797\n",
      "2025-06-01 14:50:17 [INFO]: epoch 134: training loss 0.0943\n",
      "2025-06-01 14:50:17 [INFO]: epoch 135: training loss 0.1001\n",
      "2025-06-01 14:50:17 [INFO]: epoch 136: training loss 0.0923\n",
      "2025-06-01 14:50:17 [INFO]: epoch 137: training loss 0.0782\n",
      "2025-06-01 14:50:17 [INFO]: epoch 138: training loss 0.0814\n",
      "2025-06-01 14:50:17 [INFO]: epoch 139: training loss 0.0979\n",
      "2025-06-01 14:50:17 [INFO]: epoch 140: training loss 0.0888\n",
      "2025-06-01 14:50:17 [INFO]: epoch 141: training loss 0.0751\n",
      "2025-06-01 14:50:17 [INFO]: epoch 142: training loss 0.0797\n",
      "2025-06-01 14:50:17 [INFO]: epoch 143: training loss 0.0745\n",
      "2025-06-01 14:50:17 [INFO]: epoch 144: training loss 0.0823\n",
      "2025-06-01 14:50:17 [INFO]: epoch 145: training loss 0.0740\n",
      "2025-06-01 14:50:17 [INFO]: epoch 146: training loss 0.0651\n",
      "2025-06-01 14:50:17 [INFO]: epoch 147: training loss 0.0903\n",
      "2025-06-01 14:50:17 [INFO]: epoch 148: training loss 0.0880\n",
      "2025-06-01 14:50:17 [INFO]: epoch 149: training loss 0.0879\n",
      "2025-06-01 14:50:17 [INFO]: epoch 150: training loss 0.0893\n",
      "2025-06-01 14:50:17 [INFO]: epoch 151: training loss 0.0955\n",
      "2025-06-01 14:50:17 [INFO]: epoch 152: training loss 0.0861\n",
      "2025-06-01 14:50:17 [INFO]: epoch 153: training loss 0.0936\n",
      "2025-06-01 14:50:17 [INFO]: epoch 154: training loss 0.0814\n",
      "2025-06-01 14:50:17 [INFO]: epoch 155: training loss 0.0748\n",
      "2025-06-01 14:50:17 [INFO]: epoch 156: training loss 0.0860\n",
      "2025-06-01 14:50:17 [INFO]: epoch 157: training loss 0.0899\n",
      "2025-06-01 14:50:17 [INFO]: epoch 158: training loss 0.0764\n",
      "2025-06-01 14:50:17 [INFO]: epoch 159: training loss 0.0741\n",
      "2025-06-01 14:50:17 [INFO]: epoch 160: training loss 0.0753\n",
      "2025-06-01 14:50:17 [INFO]: epoch 161: training loss 0.0837\n",
      "2025-06-01 14:50:17 [INFO]: epoch 162: training loss 0.0666\n",
      "2025-06-01 14:50:17 [INFO]: epoch 163: training loss 0.0813\n",
      "2025-06-01 14:50:17 [INFO]: epoch 164: training loss 0.0920\n",
      "2025-06-01 14:50:17 [INFO]: epoch 165: training loss 0.0778\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-01 14:50:17 [INFO]: epoch 166: training loss 0.0724\n",
      "2025-06-01 14:50:17 [INFO]: epoch 167: training loss 0.0976\n",
      "2025-06-01 14:50:17 [INFO]: epoch 168: training loss 0.0826\n",
      "2025-06-01 14:50:17 [INFO]: epoch 169: training loss 0.0767\n",
      "2025-06-01 14:50:17 [INFO]: epoch 170: training loss 0.0686\n",
      "2025-06-01 14:50:17 [INFO]: epoch 171: training loss 0.0856\n",
      "2025-06-01 14:50:17 [INFO]: epoch 172: training loss 0.0623\n",
      "2025-06-01 14:50:17 [INFO]: epoch 173: training loss 0.0724\n",
      "2025-06-01 14:50:17 [INFO]: epoch 174: training loss 0.0654\n",
      "2025-06-01 14:50:17 [INFO]: epoch 175: training loss 0.0674\n",
      "2025-06-01 14:50:17 [INFO]: epoch 176: training loss 0.0574\n",
      "2025-06-01 14:50:17 [INFO]: epoch 177: training loss 0.0565\n",
      "2025-06-01 14:50:17 [INFO]: epoch 178: training loss 0.0902\n",
      "2025-06-01 14:50:17 [INFO]: epoch 179: training loss 0.0727\n",
      "2025-06-01 14:50:17 [INFO]: epoch 180: training loss 0.0659\n",
      "2025-06-01 14:50:17 [INFO]: epoch 181: training loss 0.0588\n",
      "2025-06-01 14:50:17 [INFO]: epoch 182: training loss 0.0737\n",
      "2025-06-01 14:50:17 [INFO]: epoch 183: training loss 0.0587\n",
      "2025-06-01 14:50:17 [INFO]: epoch 184: training loss 0.0611\n",
      "2025-06-01 14:50:17 [INFO]: epoch 185: training loss 0.0636\n",
      "2025-06-01 14:50:17 [INFO]: epoch 186: training loss 0.0760\n",
      "2025-06-01 14:50:17 [INFO]: epoch 187: training loss 0.0825\n",
      "2025-06-01 14:50:17 [INFO]: epoch 188: training loss 0.0695\n",
      "2025-06-01 14:50:17 [INFO]: epoch 189: training loss 0.0729\n",
      "2025-06-01 14:50:17 [INFO]: epoch 190: training loss 0.0670\n",
      "2025-06-01 14:50:17 [INFO]: epoch 191: training loss 0.0711\n",
      "2025-06-01 14:50:17 [INFO]: epoch 192: training loss 0.0647\n",
      "2025-06-01 14:50:17 [INFO]: epoch 193: training loss 0.0713\n",
      "2025-06-01 14:50:17 [INFO]: epoch 194: training loss 0.0734\n",
      "2025-06-01 14:50:18 [INFO]: epoch 195: training loss 0.0659\n",
      "2025-06-01 14:50:18 [INFO]: epoch 196: training loss 0.0603\n",
      "2025-06-01 14:50:18 [INFO]: epoch 197: training loss 0.0622\n",
      "2025-06-01 14:50:18 [INFO]: epoch 198: training loss 0.0814\n",
      "2025-06-01 14:50:18 [INFO]: epoch 199: training loss 0.0574\n",
      "2025-06-01 14:50:18 [INFO]: epoch 200: training loss 0.0764\n",
      "2025-06-01 14:50:18 [INFO]: epoch 201: training loss 0.0584\n",
      "2025-06-01 14:50:18 [INFO]: epoch 202: training loss 0.0529\n",
      "2025-06-01 14:50:18 [INFO]: epoch 203: training loss 0.0597\n",
      "2025-06-01 14:50:18 [INFO]: epoch 204: training loss 0.0559\n",
      "2025-06-01 14:50:18 [INFO]: epoch 205: training loss 0.0528\n",
      "2025-06-01 14:50:18 [INFO]: epoch 206: training loss 0.0556\n",
      "2025-06-01 14:50:18 [INFO]: epoch 207: training loss 0.0608\n",
      "2025-06-01 14:50:18 [INFO]: epoch 208: training loss 0.0596\n",
      "2025-06-01 14:50:18 [INFO]: epoch 209: training loss 0.0651\n",
      "2025-06-01 14:50:18 [INFO]: epoch 210: training loss 0.0623\n",
      "2025-06-01 14:50:18 [INFO]: epoch 211: training loss 0.0533\n",
      "2025-06-01 14:50:18 [INFO]: epoch 212: training loss 0.0524\n",
      "2025-06-01 14:50:18 [INFO]: epoch 213: training loss 0.0564\n",
      "2025-06-01 14:50:18 [INFO]: epoch 214: training loss 0.0616\n",
      "2025-06-01 14:50:18 [INFO]: epoch 215: training loss 0.0722\n",
      "2025-06-01 14:50:18 [INFO]: epoch 216: training loss 0.0553\n",
      "2025-06-01 14:50:18 [INFO]: epoch 217: training loss 0.0584\n",
      "2025-06-01 14:50:18 [INFO]: epoch 218: training loss 0.0600\n",
      "2025-06-01 14:50:18 [INFO]: epoch 219: training loss 0.0602\n",
      "2025-06-01 14:50:18 [INFO]: epoch 220: training loss 0.0628\n",
      "2025-06-01 14:50:18 [INFO]: epoch 221: training loss 0.0541\n",
      "2025-06-01 14:50:18 [INFO]: epoch 222: training loss 0.0544\n",
      "2025-06-01 14:50:18 [INFO]: epoch 223: training loss 0.0570\n",
      "2025-06-01 14:50:18 [INFO]: epoch 224: training loss 0.0675\n",
      "2025-06-01 14:50:18 [INFO]: epoch 225: training loss 0.0583\n",
      "2025-06-01 14:50:18 [INFO]: epoch 226: training loss 0.0538\n",
      "2025-06-01 14:50:18 [INFO]: epoch 227: training loss 0.0614\n",
      "2025-06-01 14:50:18 [INFO]: epoch 228: training loss 0.0572\n",
      "2025-06-01 14:50:18 [INFO]: epoch 229: training loss 0.0624\n",
      "2025-06-01 14:50:18 [INFO]: epoch 230: training loss 0.0698\n",
      "2025-06-01 14:50:18 [INFO]: epoch 231: training loss 0.0598\n",
      "2025-06-01 14:50:18 [INFO]: epoch 232: training loss 0.0614\n",
      "2025-06-01 14:50:18 [INFO]: epoch 233: training loss 0.0608\n",
      "2025-06-01 14:50:18 [INFO]: epoch 234: training loss 0.0591\n",
      "2025-06-01 14:50:18 [INFO]: epoch 235: training loss 0.0555\n",
      "2025-06-01 14:50:18 [INFO]: epoch 236: training loss 0.0558\n",
      "2025-06-01 14:50:18 [INFO]: epoch 237: training loss 0.0483\n",
      "2025-06-01 14:50:18 [INFO]: epoch 238: training loss 0.0578\n",
      "2025-06-01 14:50:18 [INFO]: epoch 239: training loss 0.0596\n",
      "2025-06-01 14:50:18 [INFO]: epoch 240: training loss 0.0507\n",
      "2025-06-01 14:50:18 [INFO]: epoch 241: training loss 0.0530\n",
      "2025-06-01 14:50:18 [INFO]: epoch 242: training loss 0.0557\n",
      "2025-06-01 14:50:18 [INFO]: epoch 243: training loss 0.0540\n",
      "2025-06-01 14:50:18 [INFO]: epoch 244: training loss 0.0541\n",
      "2025-06-01 14:50:18 [INFO]: epoch 245: training loss 0.0549\n",
      "2025-06-01 14:50:18 [INFO]: epoch 246: training loss 0.0520\n",
      "2025-06-01 14:50:18 [INFO]: epoch 247: training loss 0.0578\n",
      "2025-06-01 14:50:18 [INFO]: epoch 248: training loss 0.0551\n",
      "2025-06-01 14:50:18 [INFO]: epoch 249: training loss 0.0505\n",
      "2025-06-01 14:50:18 [INFO]: epoch 250: training loss 0.0586\n",
      "2025-06-01 14:50:18 [INFO]: epoch 251: training loss 0.0515\n",
      "2025-06-01 14:50:18 [INFO]: epoch 252: training loss 0.0510\n",
      "2025-06-01 14:50:18 [INFO]: epoch 253: training loss 0.0488\n",
      "2025-06-01 14:50:18 [INFO]: epoch 254: training loss 0.0482\n",
      "2025-06-01 14:50:18 [INFO]: epoch 255: training loss 0.0545\n",
      "2025-06-01 14:50:18 [INFO]: epoch 256: training loss 0.0484\n",
      "2025-06-01 14:50:18 [INFO]: epoch 257: training loss 0.0493\n",
      "2025-06-01 14:50:18 [INFO]: epoch 258: training loss 0.0443\n",
      "2025-06-01 14:50:18 [INFO]: epoch 259: training loss 0.0467\n",
      "2025-06-01 14:50:18 [INFO]: epoch 260: training loss 0.0592\n",
      "2025-06-01 14:50:18 [INFO]: epoch 261: training loss 0.0636\n",
      "2025-06-01 14:50:18 [INFO]: epoch 262: training loss 0.0526\n",
      "2025-06-01 14:50:18 [INFO]: epoch 263: training loss 0.0492\n",
      "2025-06-01 14:50:18 [INFO]: epoch 264: training loss 0.0501\n",
      "2025-06-01 14:50:18 [INFO]: epoch 265: training loss 0.0618\n",
      "2025-06-01 14:50:18 [INFO]: epoch 266: training loss 0.0556\n",
      "2025-06-01 14:50:18 [INFO]: epoch 267: training loss 0.0568\n",
      "2025-06-01 14:50:18 [INFO]: epoch 268: training loss 0.0610\n",
      "2025-06-01 14:50:18 [INFO]: epoch 269: training loss 0.0444\n",
      "2025-06-01 14:50:18 [INFO]: epoch 270: training loss 0.0627\n",
      "2025-06-01 14:50:18 [INFO]: epoch 271: training loss 0.0478\n",
      "2025-06-01 14:50:18 [INFO]: epoch 272: training loss 0.0444\n",
      "2025-06-01 14:50:19 [INFO]: epoch 273: training loss 0.0667\n",
      "2025-06-01 14:50:19 [INFO]: epoch 274: training loss 0.0477\n",
      "2025-06-01 14:50:19 [INFO]: epoch 275: training loss 0.0549\n",
      "2025-06-01 14:50:19 [INFO]: epoch 276: training loss 0.0646\n",
      "2025-06-01 14:50:19 [INFO]: epoch 277: training loss 0.0532\n",
      "2025-06-01 14:50:19 [INFO]: epoch 278: training loss 0.0505\n",
      "2025-06-01 14:50:19 [INFO]: epoch 279: training loss 0.0555\n",
      "2025-06-01 14:50:19 [INFO]: epoch 280: training loss 0.0577\n",
      "2025-06-01 14:50:19 [INFO]: epoch 281: training loss 0.0544\n",
      "2025-06-01 14:50:19 [INFO]: epoch 282: training loss 0.0548\n",
      "2025-06-01 14:50:19 [INFO]: epoch 283: training loss 0.0524\n",
      "2025-06-01 14:50:19 [INFO]: epoch 284: training loss 0.0581\n",
      "2025-06-01 14:50:19 [INFO]: epoch 285: training loss 0.0487\n",
      "2025-06-01 14:50:19 [INFO]: epoch 286: training loss 0.0639\n",
      "2025-06-01 14:50:19 [INFO]: epoch 287: training loss 0.0715\n",
      "2025-06-01 14:50:19 [INFO]: epoch 288: training loss 0.0507\n",
      "2025-06-01 14:50:19 [INFO]: epoch 289: training loss 0.0512\n",
      "2025-06-01 14:50:19 [INFO]: epoch 290: training loss 0.0794\n",
      "2025-06-01 14:50:19 [INFO]: epoch 291: training loss 0.0642\n",
      "2025-06-01 14:50:19 [INFO]: epoch 292: training loss 0.0436\n",
      "2025-06-01 14:50:19 [INFO]: epoch 293: training loss 0.0613\n",
      "2025-06-01 14:50:19 [INFO]: epoch 294: training loss 0.0553\n",
      "2025-06-01 14:50:19 [INFO]: epoch 295: training loss 0.0443\n",
      "2025-06-01 14:50:19 [INFO]: epoch 296: training loss 0.0392\n",
      "2025-06-01 14:50:19 [INFO]: epoch 297: training loss 0.0481\n",
      "2025-06-01 14:50:19 [INFO]: epoch 298: training loss 0.0513\n",
      "2025-06-01 14:50:19 [INFO]: epoch 299: training loss 0.0474\n",
      "2025-06-01 14:50:19 [INFO]: epoch 300: training loss 0.0381\n",
      "2025-06-01 14:50:19 [INFO]: epoch 301: training loss 0.0483\n",
      "2025-06-01 14:50:19 [INFO]: epoch 302: training loss 0.0491\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-01 14:50:19 [INFO]: epoch 303: training loss 0.0438\n",
      "2025-06-01 14:50:19 [INFO]: epoch 304: training loss 0.0465\n",
      "2025-06-01 14:50:19 [INFO]: epoch 305: training loss 0.0392\n",
      "2025-06-01 14:50:19 [INFO]: epoch 306: training loss 0.0492\n",
      "2025-06-01 14:50:19 [INFO]: epoch 307: training loss 0.0461\n",
      "2025-06-01 14:50:19 [INFO]: epoch 308: training loss 0.0394\n",
      "2025-06-01 14:50:19 [INFO]: epoch 309: training loss 0.0465\n",
      "2025-06-01 14:50:19 [INFO]: epoch 310: training loss 0.0550\n",
      "2025-06-01 14:50:19 [INFO]: epoch 311: training loss 0.0484\n",
      "2025-06-01 14:50:19 [INFO]: epoch 312: training loss 0.0388\n",
      "2025-06-01 14:50:19 [INFO]: epoch 313: training loss 0.0537\n",
      "2025-06-01 14:50:19 [INFO]: epoch 314: training loss 0.0451\n",
      "2025-06-01 14:50:19 [INFO]: epoch 315: training loss 0.0421\n",
      "2025-06-01 14:50:19 [INFO]: epoch 316: training loss 0.0410\n",
      "2025-06-01 14:50:19 [INFO]: epoch 317: training loss 0.0407\n",
      "2025-06-01 14:50:19 [INFO]: epoch 318: training loss 0.0428\n",
      "2025-06-01 14:50:19 [INFO]: epoch 319: training loss 0.0446\n",
      "2025-06-01 14:50:19 [INFO]: epoch 320: training loss 0.0515\n",
      "2025-06-01 14:50:19 [INFO]: epoch 321: training loss 0.0554\n",
      "2025-06-01 14:50:19 [INFO]: epoch 322: training loss 0.0450\n",
      "2025-06-01 14:50:19 [INFO]: epoch 323: training loss 0.0380\n",
      "2025-06-01 14:50:19 [INFO]: epoch 324: training loss 0.0498\n",
      "2025-06-01 14:50:19 [INFO]: epoch 325: training loss 0.0412\n",
      "2025-06-01 14:50:19 [INFO]: epoch 326: training loss 0.0432\n",
      "2025-06-01 14:50:19 [INFO]: epoch 327: training loss 0.0443\n",
      "2025-06-01 14:50:19 [INFO]: epoch 328: training loss 0.0461\n",
      "2025-06-01 14:50:19 [INFO]: epoch 329: training loss 0.0443\n",
      "2025-06-01 14:50:19 [INFO]: epoch 330: training loss 0.0423\n",
      "2025-06-01 14:50:19 [INFO]: epoch 331: training loss 0.0457\n",
      "2025-06-01 14:50:19 [INFO]: epoch 332: training loss 0.0446\n",
      "2025-06-01 14:50:19 [INFO]: epoch 333: training loss 0.0472\n",
      "2025-06-01 14:50:19 [INFO]: epoch 334: training loss 0.0474\n",
      "2025-06-01 14:50:19 [INFO]: epoch 335: training loss 0.0474\n",
      "2025-06-01 14:50:19 [INFO]: epoch 336: training loss 0.0485\n",
      "2025-06-01 14:50:19 [INFO]: epoch 337: training loss 0.0483\n",
      "2025-06-01 14:50:19 [INFO]: epoch 338: training loss 0.0385\n",
      "2025-06-01 14:50:19 [INFO]: epoch 339: training loss 0.0472\n",
      "2025-06-01 14:50:19 [INFO]: epoch 340: training loss 0.0462\n",
      "2025-06-01 14:50:19 [INFO]: epoch 341: training loss 0.0362\n",
      "2025-06-01 14:50:19 [INFO]: epoch 342: training loss 0.0481\n",
      "2025-06-01 14:50:19 [INFO]: epoch 343: training loss 0.0606\n",
      "2025-06-01 14:50:19 [INFO]: epoch 344: training loss 0.0465\n",
      "2025-06-01 14:50:19 [INFO]: epoch 345: training loss 0.0311\n",
      "2025-06-01 14:50:19 [INFO]: epoch 346: training loss 0.0491\n",
      "2025-06-01 14:50:19 [INFO]: epoch 347: training loss 0.0486\n",
      "2025-06-01 14:50:19 [INFO]: epoch 348: training loss 0.0400\n",
      "2025-06-01 14:50:20 [INFO]: epoch 349: training loss 0.0487\n",
      "2025-06-01 14:50:20 [INFO]: epoch 350: training loss 0.0484\n",
      "2025-06-01 14:50:20 [INFO]: epoch 351: training loss 0.0421\n",
      "2025-06-01 14:50:20 [INFO]: epoch 352: training loss 0.0481\n",
      "2025-06-01 14:50:20 [INFO]: epoch 353: training loss 0.0640\n",
      "2025-06-01 14:50:20 [INFO]: epoch 354: training loss 0.0510\n",
      "2025-06-01 14:50:20 [INFO]: epoch 355: training loss 0.0510\n",
      "2025-06-01 14:50:20 [INFO]: epoch 356: training loss 0.0547\n",
      "2025-06-01 14:50:20 [INFO]: epoch 357: training loss 0.0545\n",
      "2025-06-01 14:50:20 [INFO]: epoch 358: training loss 0.0404\n",
      "2025-06-01 14:50:20 [INFO]: epoch 359: training loss 0.0503\n",
      "2025-06-01 14:50:20 [INFO]: epoch 360: training loss 0.0531\n",
      "2025-06-01 14:50:20 [INFO]: epoch 361: training loss 0.0355\n",
      "2025-06-01 14:50:20 [INFO]: epoch 362: training loss 0.0461\n",
      "2025-06-01 14:50:20 [INFO]: epoch 363: training loss 0.0452\n",
      "2025-06-01 14:50:20 [INFO]: epoch 364: training loss 0.0445\n",
      "2025-06-01 14:50:20 [INFO]: epoch 365: training loss 0.0429\n",
      "2025-06-01 14:50:20 [INFO]: epoch 366: training loss 0.0465\n",
      "2025-06-01 14:50:20 [INFO]: epoch 367: training loss 0.0389\n",
      "2025-06-01 14:50:20 [INFO]: epoch 368: training loss 0.0371\n",
      "2025-06-01 14:50:20 [INFO]: epoch 369: training loss 0.0465\n",
      "2025-06-01 14:50:20 [INFO]: epoch 370: training loss 0.0458\n",
      "2025-06-01 14:50:20 [INFO]: epoch 371: training loss 0.0434\n",
      "2025-06-01 14:50:20 [INFO]: epoch 372: training loss 0.0397\n",
      "2025-06-01 14:50:20 [INFO]: epoch 373: training loss 0.0489\n",
      "2025-06-01 14:50:20 [INFO]: epoch 374: training loss 0.0424\n",
      "2025-06-01 14:50:20 [INFO]: epoch 375: training loss 0.0415\n",
      "2025-06-01 14:50:20 [INFO]: epoch 376: training loss 0.0379\n",
      "2025-06-01 14:50:20 [INFO]: epoch 377: training loss 0.0456\n",
      "2025-06-01 14:50:20 [INFO]: epoch 378: training loss 0.0392\n",
      "2025-06-01 14:50:20 [INFO]: epoch 379: training loss 0.0409\n",
      "2025-06-01 14:50:20 [INFO]: epoch 380: training loss 0.0449\n",
      "2025-06-01 14:50:20 [INFO]: epoch 381: training loss 0.0442\n",
      "2025-06-01 14:50:20 [INFO]: epoch 382: training loss 0.0382\n",
      "2025-06-01 14:50:20 [INFO]: epoch 383: training loss 0.0371\n",
      "2025-06-01 14:50:20 [INFO]: epoch 384: training loss 0.0474\n",
      "2025-06-01 14:50:20 [INFO]: epoch 385: training loss 0.0483\n",
      "2025-06-01 14:50:20 [INFO]: epoch 386: training loss 0.0539\n",
      "2025-06-01 14:50:20 [INFO]: epoch 387: training loss 0.0483\n",
      "2025-06-01 14:50:20 [INFO]: epoch 388: training loss 0.0390\n",
      "2025-06-01 14:50:20 [INFO]: epoch 389: training loss 0.0509\n",
      "2025-06-01 14:50:20 [INFO]: epoch 390: training loss 0.0421\n",
      "2025-06-01 14:50:20 [INFO]: epoch 391: training loss 0.0417\n",
      "2025-06-01 14:50:20 [INFO]: epoch 392: training loss 0.0373\n",
      "2025-06-01 14:50:20 [INFO]: epoch 393: training loss 0.0376\n",
      "2025-06-01 14:50:20 [INFO]: epoch 394: training loss 0.0387\n",
      "2025-06-01 14:50:20 [INFO]: epoch 395: training loss 0.0435\n",
      "2025-06-01 14:50:20 [INFO]: epoch 396: training loss 0.0341\n",
      "2025-06-01 14:50:20 [INFO]: epoch 397: training loss 0.0334\n",
      "2025-06-01 14:50:20 [INFO]: epoch 398: training loss 0.0427\n",
      "2025-06-01 14:50:20 [INFO]: epoch 399: training loss 0.0378\n",
      "2025-06-01 14:50:20 [INFO]: epoch 400: training loss 0.0331\n",
      "2025-06-01 14:50:20 [INFO]: epoch 401: training loss 0.0420\n",
      "2025-06-01 14:50:20 [INFO]: epoch 402: training loss 0.0348\n",
      "2025-06-01 14:50:20 [INFO]: epoch 403: training loss 0.0345\n",
      "2025-06-01 14:50:20 [INFO]: epoch 404: training loss 0.0322\n",
      "2025-06-01 14:50:20 [INFO]: epoch 405: training loss 0.0391\n",
      "2025-06-01 14:50:20 [INFO]: epoch 406: training loss 0.0352\n",
      "2025-06-01 14:50:20 [INFO]: epoch 407: training loss 0.0401\n",
      "2025-06-01 14:50:20 [INFO]: epoch 408: training loss 0.0381\n",
      "2025-06-01 14:50:20 [INFO]: epoch 409: training loss 0.0401\n",
      "2025-06-01 14:50:20 [INFO]: epoch 410: training loss 0.0343\n",
      "2025-06-01 14:50:20 [INFO]: epoch 411: training loss 0.0354\n",
      "2025-06-01 14:50:20 [INFO]: epoch 412: training loss 0.0375\n",
      "2025-06-01 14:50:20 [INFO]: epoch 413: training loss 0.0421\n",
      "2025-06-01 14:50:20 [INFO]: epoch 414: training loss 0.0341\n",
      "2025-06-01 14:50:20 [INFO]: epoch 415: training loss 0.0338\n",
      "2025-06-01 14:50:20 [INFO]: epoch 416: training loss 0.0421\n",
      "2025-06-01 14:50:20 [INFO]: epoch 417: training loss 0.0288\n",
      "2025-06-01 14:50:20 [INFO]: epoch 418: training loss 0.0425\n",
      "2025-06-01 14:50:20 [INFO]: epoch 419: training loss 0.0460\n",
      "2025-06-01 14:50:20 [INFO]: epoch 420: training loss 0.0342\n",
      "2025-06-01 14:50:20 [INFO]: epoch 421: training loss 0.0337\n",
      "2025-06-01 14:50:20 [INFO]: epoch 422: training loss 0.0431\n",
      "2025-06-01 14:50:20 [INFO]: epoch 423: training loss 0.0353\n",
      "2025-06-01 14:50:20 [INFO]: epoch 424: training loss 0.0392\n",
      "2025-06-01 14:50:21 [INFO]: epoch 425: training loss 0.0442\n",
      "2025-06-01 14:50:21 [INFO]: epoch 426: training loss 0.0405\n",
      "2025-06-01 14:50:21 [INFO]: epoch 427: training loss 0.0374\n",
      "2025-06-01 14:50:21 [INFO]: epoch 428: training loss 0.0459\n",
      "2025-06-01 14:50:21 [INFO]: epoch 429: training loss 0.0520\n",
      "2025-06-01 14:50:21 [INFO]: epoch 430: training loss 0.0393\n",
      "2025-06-01 14:50:21 [INFO]: epoch 431: training loss 0.0469\n",
      "2025-06-01 14:50:21 [INFO]: epoch 432: training loss 0.0515\n",
      "2025-06-01 14:50:21 [INFO]: epoch 433: training loss 0.0576\n",
      "2025-06-01 14:50:21 [INFO]: epoch 434: training loss 0.0569\n",
      "2025-06-01 14:50:21 [INFO]: epoch 435: training loss 0.0535\n",
      "2025-06-01 14:50:21 [INFO]: epoch 436: training loss 0.0463\n",
      "2025-06-01 14:50:21 [INFO]: epoch 437: training loss 0.0434\n",
      "2025-06-01 14:50:21 [INFO]: epoch 438: training loss 0.0325\n",
      "2025-06-01 14:50:21 [INFO]: epoch 439: training loss 0.0337\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-01 14:50:21 [INFO]: epoch 440: training loss 0.0393\n",
      "2025-06-01 14:50:21 [INFO]: epoch 441: training loss 0.0336\n",
      "2025-06-01 14:50:21 [INFO]: epoch 442: training loss 0.0378\n",
      "2025-06-01 14:50:21 [INFO]: epoch 443: training loss 0.0424\n",
      "2025-06-01 14:50:21 [INFO]: epoch 444: training loss 0.0405\n",
      "2025-06-01 14:50:21 [INFO]: epoch 445: training loss 0.0310\n",
      "2025-06-01 14:50:21 [INFO]: epoch 446: training loss 0.0354\n",
      "2025-06-01 14:50:21 [INFO]: epoch 447: training loss 0.0323\n",
      "2025-06-01 14:50:21 [INFO]: epoch 448: training loss 0.0339\n",
      "2025-06-01 14:50:21 [INFO]: epoch 449: training loss 0.0355\n",
      "2025-06-01 14:50:21 [INFO]: epoch 450: training loss 0.0378\n",
      "2025-06-01 14:50:21 [INFO]: epoch 451: training loss 0.0371\n",
      "2025-06-01 14:50:21 [INFO]: epoch 452: training loss 0.0335\n",
      "2025-06-01 14:50:21 [INFO]: epoch 453: training loss 0.0409\n",
      "2025-06-01 14:50:21 [INFO]: epoch 454: training loss 0.0429\n",
      "2025-06-01 14:50:21 [INFO]: epoch 455: training loss 0.0343\n",
      "2025-06-01 14:50:21 [INFO]: epoch 456: training loss 0.0369\n",
      "2025-06-01 14:50:21 [INFO]: epoch 457: training loss 0.0401\n",
      "2025-06-01 14:50:21 [INFO]: epoch 458: training loss 0.0327\n",
      "2025-06-01 14:50:21 [INFO]: epoch 459: training loss 0.0428\n",
      "2025-06-01 14:50:21 [INFO]: epoch 460: training loss 0.0385\n",
      "2025-06-01 14:50:21 [INFO]: epoch 461: training loss 0.0336\n",
      "2025-06-01 14:50:21 [INFO]: epoch 462: training loss 0.0461\n",
      "2025-06-01 14:50:21 [INFO]: epoch 463: training loss 0.0290\n",
      "2025-06-01 14:50:21 [INFO]: epoch 464: training loss 0.0303\n",
      "2025-06-01 14:50:21 [INFO]: epoch 465: training loss 0.0373\n",
      "2025-06-01 14:50:21 [INFO]: epoch 466: training loss 0.0327\n",
      "2025-06-01 14:50:21 [INFO]: epoch 467: training loss 0.0287\n",
      "2025-06-01 14:50:21 [INFO]: epoch 468: training loss 0.0421\n",
      "2025-06-01 14:50:21 [INFO]: epoch 469: training loss 0.0361\n",
      "2025-06-01 14:50:21 [INFO]: epoch 470: training loss 0.0322\n",
      "2025-06-01 14:50:21 [INFO]: epoch 471: training loss 0.0446\n",
      "2025-06-01 14:50:21 [INFO]: epoch 472: training loss 0.0319\n",
      "2025-06-01 14:50:21 [INFO]: epoch 473: training loss 0.0358\n",
      "2025-06-01 14:50:21 [INFO]: epoch 474: training loss 0.0417\n",
      "2025-06-01 14:50:21 [INFO]: epoch 475: training loss 0.0305\n",
      "2025-06-01 14:50:21 [INFO]: epoch 476: training loss 0.0304\n",
      "2025-06-01 14:50:21 [INFO]: epoch 477: training loss 0.0399\n",
      "2025-06-01 14:50:21 [INFO]: epoch 478: training loss 0.0318\n",
      "2025-06-01 14:50:21 [INFO]: epoch 479: training loss 0.0347\n",
      "2025-06-01 14:50:21 [INFO]: epoch 480: training loss 0.0390\n",
      "2025-06-01 14:50:21 [INFO]: epoch 481: training loss 0.0313\n",
      "2025-06-01 14:50:21 [INFO]: epoch 482: training loss 0.0314\n",
      "2025-06-01 14:50:21 [INFO]: epoch 483: training loss 0.0394\n",
      "2025-06-01 14:50:21 [INFO]: epoch 484: training loss 0.0320\n",
      "2025-06-01 14:50:21 [INFO]: epoch 485: training loss 0.0284\n",
      "2025-06-01 14:50:21 [INFO]: epoch 486: training loss 0.0281\n",
      "2025-06-01 14:50:21 [INFO]: epoch 487: training loss 0.0287\n",
      "2025-06-01 14:50:21 [INFO]: epoch 488: training loss 0.0306\n",
      "2025-06-01 14:50:21 [INFO]: epoch 489: training loss 0.0306\n",
      "2025-06-01 14:50:21 [INFO]: epoch 490: training loss 0.0281\n",
      "2025-06-01 14:50:21 [INFO]: epoch 491: training loss 0.0360\n",
      "2025-06-01 14:50:21 [INFO]: epoch 492: training loss 0.0307\n",
      "2025-06-01 14:50:21 [INFO]: epoch 493: training loss 0.0386\n",
      "2025-06-01 14:50:21 [INFO]: epoch 494: training loss 0.0431\n",
      "2025-06-01 14:50:21 [INFO]: epoch 495: training loss 0.0339\n",
      "2025-06-01 14:50:21 [INFO]: epoch 496: training loss 0.0340\n",
      "2025-06-01 14:50:21 [INFO]: epoch 497: training loss 0.0427\n",
      "2025-06-01 14:50:21 [INFO]: epoch 498: training loss 0.0485\n",
      "2025-06-01 14:50:21 [INFO]: epoch 499: training loss 0.0351\n",
      "2025-06-01 14:50:21 [INFO]: epoch 500: training loss 0.0384\n",
      "2025-06-01 14:50:22 [INFO]: epoch 501: training loss 0.0351\n",
      "2025-06-01 14:50:22 [INFO]: epoch 502: training loss 0.0270\n",
      "2025-06-01 14:50:22 [INFO]: epoch 503: training loss 0.0353\n",
      "2025-06-01 14:50:22 [INFO]: epoch 504: training loss 0.0428\n",
      "2025-06-01 14:50:22 [INFO]: epoch 505: training loss 0.0309\n",
      "2025-06-01 14:50:22 [INFO]: epoch 506: training loss 0.0359\n",
      "2025-06-01 14:50:22 [INFO]: epoch 507: training loss 0.0331\n",
      "2025-06-01 14:50:22 [INFO]: epoch 508: training loss 0.0360\n",
      "2025-06-01 14:50:22 [INFO]: epoch 509: training loss 0.0339\n",
      "2025-06-01 14:50:22 [INFO]: epoch 510: training loss 0.0350\n",
      "2025-06-01 14:50:22 [INFO]: epoch 511: training loss 0.0236\n",
      "2025-06-01 14:50:22 [INFO]: epoch 512: training loss 0.0351\n",
      "2025-06-01 14:50:22 [INFO]: epoch 513: training loss 0.0387\n",
      "2025-06-01 14:50:22 [INFO]: epoch 514: training loss 0.0294\n",
      "2025-06-01 14:50:22 [INFO]: epoch 515: training loss 0.0299\n",
      "2025-06-01 14:50:22 [INFO]: epoch 516: training loss 0.0339\n",
      "2025-06-01 14:50:22 [INFO]: epoch 517: training loss 0.0411\n",
      "2025-06-01 14:50:22 [INFO]: epoch 518: training loss 0.0309\n",
      "2025-06-01 14:50:22 [INFO]: epoch 519: training loss 0.0356\n",
      "2025-06-01 14:50:22 [INFO]: epoch 520: training loss 0.0284\n",
      "2025-06-01 14:50:22 [INFO]: epoch 521: training loss 0.0302\n",
      "2025-06-01 14:50:22 [INFO]: epoch 522: training loss 0.0314\n",
      "2025-06-01 14:50:22 [INFO]: epoch 523: training loss 0.0279\n",
      "2025-06-01 14:50:22 [INFO]: epoch 524: training loss 0.0355\n",
      "2025-06-01 14:50:22 [INFO]: epoch 525: training loss 0.0282\n",
      "2025-06-01 14:50:22 [INFO]: epoch 526: training loss 0.0319\n",
      "2025-06-01 14:50:22 [INFO]: epoch 527: training loss 0.0292\n",
      "2025-06-01 14:50:22 [INFO]: epoch 528: training loss 0.0304\n",
      "2025-06-01 14:50:22 [INFO]: epoch 529: training loss 0.0267\n",
      "2025-06-01 14:50:22 [INFO]: epoch 530: training loss 0.0323\n",
      "2025-06-01 14:50:22 [INFO]: epoch 531: training loss 0.0288\n",
      "2025-06-01 14:50:22 [INFO]: epoch 532: training loss 0.0272\n",
      "2025-06-01 14:50:22 [INFO]: epoch 533: training loss 0.0372\n",
      "2025-06-01 14:50:22 [INFO]: epoch 534: training loss 0.0361\n",
      "2025-06-01 14:50:22 [INFO]: epoch 535: training loss 0.0323\n",
      "2025-06-01 14:50:22 [INFO]: epoch 536: training loss 0.0346\n",
      "2025-06-01 14:50:22 [INFO]: epoch 537: training loss 0.0363\n",
      "2025-06-01 14:50:22 [INFO]: epoch 538: training loss 0.0390\n",
      "2025-06-01 14:50:22 [INFO]: epoch 539: training loss 0.0286\n",
      "2025-06-01 14:50:22 [INFO]: epoch 540: training loss 0.0332\n",
      "2025-06-01 14:50:22 [INFO]: epoch 541: training loss 0.0333\n",
      "2025-06-01 14:50:22 [INFO]: epoch 542: training loss 0.0272\n",
      "2025-06-01 14:50:22 [INFO]: epoch 543: training loss 0.0306\n",
      "2025-06-01 14:50:22 [INFO]: epoch 544: training loss 0.0388\n",
      "2025-06-01 14:50:22 [INFO]: epoch 545: training loss 0.0295\n",
      "2025-06-01 14:50:22 [INFO]: epoch 546: training loss 0.0354\n",
      "2025-06-01 14:50:22 [INFO]: epoch 547: training loss 0.0403\n",
      "2025-06-01 14:50:22 [INFO]: epoch 548: training loss 0.0294\n",
      "2025-06-01 14:50:22 [INFO]: epoch 549: training loss 0.0384\n",
      "2025-06-01 14:50:22 [INFO]: epoch 550: training loss 0.0316\n",
      "2025-06-01 14:50:22 [INFO]: epoch 551: training loss 0.0322\n",
      "2025-06-01 14:50:22 [INFO]: epoch 552: training loss 0.0459\n",
      "2025-06-01 14:50:22 [INFO]: epoch 553: training loss 0.0387\n",
      "2025-06-01 14:50:22 [INFO]: epoch 554: training loss 0.0372\n",
      "2025-06-01 14:50:22 [INFO]: epoch 555: training loss 0.0314\n",
      "2025-06-01 14:50:22 [INFO]: epoch 556: training loss 0.0370\n",
      "2025-06-01 14:50:22 [INFO]: epoch 557: training loss 0.0392\n",
      "2025-06-01 14:50:22 [INFO]: epoch 558: training loss 0.0298\n",
      "2025-06-01 14:50:22 [INFO]: epoch 559: training loss 0.0268\n",
      "2025-06-01 14:50:22 [INFO]: epoch 560: training loss 0.0308\n",
      "2025-06-01 14:50:22 [INFO]: epoch 561: training loss 0.0313\n",
      "2025-06-01 14:50:22 [INFO]: epoch 562: training loss 0.0334\n",
      "2025-06-01 14:50:22 [INFO]: epoch 563: training loss 0.0301\n",
      "2025-06-01 14:50:22 [INFO]: epoch 564: training loss 0.0313\n",
      "2025-06-01 14:50:22 [INFO]: epoch 565: training loss 0.0303\n",
      "2025-06-01 14:50:22 [INFO]: epoch 566: training loss 0.0282\n",
      "2025-06-01 14:50:22 [INFO]: epoch 567: training loss 0.0306\n",
      "2025-06-01 14:50:22 [INFO]: epoch 568: training loss 0.0307\n",
      "2025-06-01 14:50:22 [INFO]: epoch 569: training loss 0.0363\n",
      "2025-06-01 14:50:22 [INFO]: epoch 570: training loss 0.0233\n",
      "2025-06-01 14:50:22 [INFO]: epoch 571: training loss 0.0402\n",
      "2025-06-01 14:50:22 [INFO]: epoch 572: training loss 0.0301\n",
      "2025-06-01 14:50:22 [INFO]: epoch 573: training loss 0.0270\n",
      "2025-06-01 14:50:22 [INFO]: epoch 574: training loss 0.0361\n",
      "2025-06-01 14:50:22 [INFO]: epoch 575: training loss 0.0277\n",
      "2025-06-01 14:50:22 [INFO]: epoch 576: training loss 0.0314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-01 14:50:22 [INFO]: epoch 577: training loss 0.0388\n",
      "2025-06-01 14:50:22 [INFO]: epoch 578: training loss 0.0317\n",
      "2025-06-01 14:50:23 [INFO]: epoch 579: training loss 0.0239\n",
      "2025-06-01 14:50:23 [INFO]: epoch 580: training loss 0.0320\n",
      "2025-06-01 14:50:23 [INFO]: epoch 581: training loss 0.0308\n",
      "2025-06-01 14:50:23 [INFO]: epoch 582: training loss 0.0360\n",
      "2025-06-01 14:50:23 [INFO]: epoch 583: training loss 0.0253\n",
      "2025-06-01 14:50:23 [INFO]: epoch 584: training loss 0.0362\n",
      "2025-06-01 14:50:23 [INFO]: epoch 585: training loss 0.0321\n",
      "2025-06-01 14:50:23 [INFO]: epoch 586: training loss 0.0308\n",
      "2025-06-01 14:50:23 [INFO]: epoch 587: training loss 0.0278\n",
      "2025-06-01 14:50:23 [INFO]: epoch 588: training loss 0.0434\n",
      "2025-06-01 14:50:23 [INFO]: epoch 589: training loss 0.0385\n",
      "2025-06-01 14:50:23 [INFO]: epoch 590: training loss 0.0300\n",
      "2025-06-01 14:50:23 [INFO]: epoch 591: training loss 0.0289\n",
      "2025-06-01 14:50:23 [INFO]: epoch 592: training loss 0.0338\n",
      "2025-06-01 14:50:23 [INFO]: epoch 593: training loss 0.0344\n",
      "2025-06-01 14:50:23 [INFO]: epoch 594: training loss 0.0388\n",
      "2025-06-01 14:50:23 [INFO]: epoch 595: training loss 0.0331\n",
      "2025-06-01 14:50:23 [INFO]: epoch 596: training loss 0.0294\n",
      "2025-06-01 14:50:23 [INFO]: epoch 597: training loss 0.0353\n",
      "2025-06-01 14:50:23 [INFO]: epoch 598: training loss 0.0300\n",
      "2025-06-01 14:50:23 [INFO]: epoch 599: training loss 0.0262\n",
      "2025-06-01 14:50:23 [INFO]: epoch 600: training loss 0.0354\n",
      "2025-06-01 14:50:23 [INFO]: epoch 601: training loss 0.0302\n",
      "2025-06-01 14:50:23 [INFO]: epoch 602: training loss 0.0272\n",
      "2025-06-01 14:50:23 [INFO]: epoch 603: training loss 0.0342\n",
      "2025-06-01 14:50:23 [INFO]: epoch 604: training loss 0.0366\n",
      "2025-06-01 14:50:23 [INFO]: epoch 605: training loss 0.0269\n",
      "2025-06-01 14:50:23 [INFO]: epoch 606: training loss 0.0309\n",
      "2025-06-01 14:50:23 [INFO]: epoch 607: training loss 0.0314\n",
      "2025-06-01 14:50:23 [INFO]: epoch 608: training loss 0.0271\n",
      "2025-06-01 14:50:23 [INFO]: epoch 609: training loss 0.0311\n",
      "2025-06-01 14:50:23 [INFO]: epoch 610: training loss 0.0346\n",
      "2025-06-01 14:50:23 [INFO]: epoch 611: training loss 0.0348\n",
      "2025-06-01 14:50:23 [INFO]: epoch 612: training loss 0.0288\n",
      "2025-06-01 14:50:23 [INFO]: epoch 613: training loss 0.0287\n",
      "2025-06-01 14:50:23 [INFO]: epoch 614: training loss 0.0296\n",
      "2025-06-01 14:50:23 [INFO]: epoch 615: training loss 0.0326\n",
      "2025-06-01 14:50:23 [INFO]: epoch 616: training loss 0.0323\n",
      "2025-06-01 14:50:23 [INFO]: epoch 617: training loss 0.0277\n",
      "2025-06-01 14:50:23 [INFO]: epoch 618: training loss 0.0343\n",
      "2025-06-01 14:50:23 [INFO]: epoch 619: training loss 0.0320\n",
      "2025-06-01 14:50:23 [INFO]: epoch 620: training loss 0.0315\n",
      "2025-06-01 14:50:23 [INFO]: epoch 621: training loss 0.0280\n",
      "2025-06-01 14:50:23 [INFO]: epoch 622: training loss 0.0288\n",
      "2025-06-01 14:50:23 [INFO]: epoch 623: training loss 0.0395\n",
      "2025-06-01 14:50:23 [INFO]: epoch 624: training loss 0.0283\n",
      "2025-06-01 14:50:23 [INFO]: epoch 625: training loss 0.0330\n",
      "2025-06-01 14:50:23 [INFO]: epoch 626: training loss 0.0301\n",
      "2025-06-01 14:50:23 [INFO]: epoch 627: training loss 0.0260\n",
      "2025-06-01 14:50:23 [INFO]: epoch 628: training loss 0.0275\n",
      "2025-06-01 14:50:23 [INFO]: epoch 629: training loss 0.0253\n",
      "2025-06-01 14:50:23 [INFO]: epoch 630: training loss 0.0284\n",
      "2025-06-01 14:50:23 [INFO]: epoch 631: training loss 0.0288\n",
      "2025-06-01 14:50:23 [INFO]: epoch 632: training loss 0.0289\n",
      "2025-06-01 14:50:23 [INFO]: epoch 633: training loss 0.0309\n",
      "2025-06-01 14:50:23 [INFO]: epoch 634: training loss 0.0382\n",
      "2025-06-01 14:50:23 [INFO]: epoch 635: training loss 0.0354\n",
      "2025-06-01 14:50:23 [INFO]: epoch 636: training loss 0.0323\n",
      "2025-06-01 14:50:23 [INFO]: epoch 637: training loss 0.0344\n",
      "2025-06-01 14:50:23 [INFO]: epoch 638: training loss 0.0324\n",
      "2025-06-01 14:50:23 [INFO]: epoch 639: training loss 0.0240\n",
      "2025-06-01 14:50:23 [INFO]: epoch 640: training loss 0.0278\n",
      "2025-06-01 14:50:23 [INFO]: epoch 641: training loss 0.0327\n",
      "2025-06-01 14:50:23 [INFO]: epoch 642: training loss 0.0255\n",
      "2025-06-01 14:50:23 [INFO]: epoch 643: training loss 0.0282\n",
      "2025-06-01 14:50:23 [INFO]: epoch 644: training loss 0.0248\n",
      "2025-06-01 14:50:23 [INFO]: epoch 645: training loss 0.0254\n",
      "2025-06-01 14:50:23 [INFO]: epoch 646: training loss 0.0306\n",
      "2025-06-01 14:50:23 [INFO]: epoch 647: training loss 0.0282\n",
      "2025-06-01 14:50:23 [INFO]: epoch 648: training loss 0.0235\n",
      "2025-06-01 14:50:23 [INFO]: epoch 649: training loss 0.0255\n",
      "2025-06-01 14:50:23 [INFO]: epoch 650: training loss 0.0258\n",
      "2025-06-01 14:50:23 [INFO]: epoch 651: training loss 0.0257\n",
      "2025-06-01 14:50:23 [INFO]: epoch 652: training loss 0.0253\n",
      "2025-06-01 14:50:23 [INFO]: epoch 653: training loss 0.0258\n",
      "2025-06-01 14:50:23 [INFO]: epoch 654: training loss 0.0316\n",
      "2025-06-01 14:50:24 [INFO]: epoch 655: training loss 0.0236\n",
      "2025-06-01 14:50:24 [INFO]: epoch 656: training loss 0.0316\n",
      "2025-06-01 14:50:24 [INFO]: epoch 657: training loss 0.0260\n",
      "2025-06-01 14:50:24 [INFO]: epoch 658: training loss 0.0221\n",
      "2025-06-01 14:50:24 [INFO]: epoch 659: training loss 0.0316\n",
      "2025-06-01 14:50:24 [INFO]: epoch 660: training loss 0.0257\n",
      "2025-06-01 14:50:24 [INFO]: epoch 661: training loss 0.0201\n",
      "2025-06-01 14:50:24 [INFO]: epoch 662: training loss 0.0298\n",
      "2025-06-01 14:50:24 [INFO]: epoch 663: training loss 0.0285\n",
      "2025-06-01 14:50:24 [INFO]: epoch 664: training loss 0.0236\n",
      "2025-06-01 14:50:24 [INFO]: epoch 665: training loss 0.0309\n",
      "2025-06-01 14:50:24 [INFO]: epoch 666: training loss 0.0224\n",
      "2025-06-01 14:50:24 [INFO]: epoch 667: training loss 0.0286\n",
      "2025-06-01 14:50:24 [INFO]: epoch 668: training loss 0.0231\n",
      "2025-06-01 14:50:24 [INFO]: epoch 669: training loss 0.0215\n",
      "2025-06-01 14:50:24 [INFO]: epoch 670: training loss 0.0225\n",
      "2025-06-01 14:50:24 [INFO]: epoch 671: training loss 0.0270\n",
      "2025-06-01 14:50:24 [INFO]: epoch 672: training loss 0.0314\n",
      "2025-06-01 14:50:24 [INFO]: epoch 673: training loss 0.0241\n",
      "2025-06-01 14:50:24 [INFO]: epoch 674: training loss 0.0310\n",
      "2025-06-01 14:50:24 [INFO]: epoch 675: training loss 0.0261\n",
      "2025-06-01 14:50:24 [INFO]: epoch 676: training loss 0.0249\n",
      "2025-06-01 14:50:24 [INFO]: epoch 677: training loss 0.0256\n",
      "2025-06-01 14:50:24 [INFO]: epoch 678: training loss 0.0283\n",
      "2025-06-01 14:50:24 [INFO]: epoch 679: training loss 0.0242\n",
      "2025-06-01 14:50:24 [INFO]: epoch 680: training loss 0.0217\n",
      "2025-06-01 14:50:24 [INFO]: epoch 681: training loss 0.0258\n",
      "2025-06-01 14:50:24 [INFO]: epoch 682: training loss 0.0241\n",
      "2025-06-01 14:50:24 [INFO]: epoch 683: training loss 0.0256\n",
      "2025-06-01 14:50:24 [INFO]: epoch 684: training loss 0.0262\n",
      "2025-06-01 14:50:24 [INFO]: epoch 685: training loss 0.0286\n",
      "2025-06-01 14:50:24 [INFO]: epoch 686: training loss 0.0204\n",
      "2025-06-01 14:50:24 [INFO]: epoch 687: training loss 0.0267\n",
      "2025-06-01 14:50:24 [INFO]: epoch 688: training loss 0.0276\n",
      "2025-06-01 14:50:24 [INFO]: epoch 689: training loss 0.0219\n",
      "2025-06-01 14:50:24 [INFO]: epoch 690: training loss 0.0235\n",
      "2025-06-01 14:50:24 [INFO]: epoch 691: training loss 0.0244\n",
      "2025-06-01 14:50:24 [INFO]: epoch 692: training loss 0.0269\n",
      "2025-06-01 14:50:24 [INFO]: epoch 693: training loss 0.0256\n",
      "2025-06-01 14:50:24 [INFO]: epoch 694: training loss 0.0301\n",
      "2025-06-01 14:50:24 [INFO]: epoch 695: training loss 0.0306\n",
      "2025-06-01 14:50:24 [INFO]: epoch 696: training loss 0.0245\n",
      "2025-06-01 14:50:24 [INFO]: epoch 697: training loss 0.0245\n",
      "2025-06-01 14:50:24 [INFO]: epoch 698: training loss 0.0232\n",
      "2025-06-01 14:50:24 [INFO]: epoch 699: training loss 0.0230\n",
      "2025-06-01 14:50:24 [INFO]: epoch 700: training loss 0.0212\n",
      "2025-06-01 14:50:24 [INFO]: epoch 701: training loss 0.0269\n",
      "2025-06-01 14:50:24 [INFO]: epoch 702: training loss 0.0229\n",
      "2025-06-01 14:50:24 [INFO]: epoch 703: training loss 0.0216\n",
      "2025-06-01 14:50:24 [INFO]: epoch 704: training loss 0.0280\n",
      "2025-06-01 14:50:24 [INFO]: epoch 705: training loss 0.0247\n",
      "2025-06-01 14:50:24 [INFO]: epoch 706: training loss 0.0220\n",
      "2025-06-01 14:50:24 [INFO]: epoch 707: training loss 0.0253\n",
      "2025-06-01 14:50:24 [INFO]: epoch 708: training loss 0.0274\n",
      "2025-06-01 14:50:24 [INFO]: epoch 709: training loss 0.0236\n",
      "2025-06-01 14:50:24 [INFO]: epoch 710: training loss 0.0260\n",
      "2025-06-01 14:50:24 [INFO]: epoch 711: training loss 0.0220\n",
      "2025-06-01 14:50:24 [INFO]: epoch 712: training loss 0.0263\n",
      "2025-06-01 14:50:24 [INFO]: epoch 713: training loss 0.0268\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-01 14:50:24 [INFO]: epoch 714: training loss 0.0250\n",
      "2025-06-01 14:50:24 [INFO]: epoch 715: training loss 0.0199\n",
      "2025-06-01 14:50:24 [INFO]: epoch 716: training loss 0.0266\n",
      "2025-06-01 14:50:24 [INFO]: epoch 717: training loss 0.0225\n",
      "2025-06-01 14:50:24 [INFO]: epoch 718: training loss 0.0257\n",
      "2025-06-01 14:50:24 [INFO]: epoch 719: training loss 0.0232\n",
      "2025-06-01 14:50:24 [INFO]: epoch 720: training loss 0.0224\n",
      "2025-06-01 14:50:24 [INFO]: epoch 721: training loss 0.0223\n",
      "2025-06-01 14:50:24 [INFO]: epoch 722: training loss 0.0224\n",
      "2025-06-01 14:50:24 [INFO]: epoch 723: training loss 0.0246\n",
      "2025-06-01 14:50:24 [INFO]: epoch 724: training loss 0.0225\n",
      "2025-06-01 14:50:24 [INFO]: epoch 725: training loss 0.0300\n",
      "2025-06-01 14:50:24 [INFO]: epoch 726: training loss 0.0227\n",
      "2025-06-01 14:50:24 [INFO]: epoch 727: training loss 0.0230\n",
      "2025-06-01 14:50:24 [INFO]: epoch 728: training loss 0.0236\n",
      "2025-06-01 14:50:24 [INFO]: epoch 729: training loss 0.0275\n",
      "2025-06-01 14:50:24 [INFO]: epoch 730: training loss 0.0257\n",
      "2025-06-01 14:50:25 [INFO]: epoch 731: training loss 0.0244\n",
      "2025-06-01 14:50:25 [INFO]: epoch 732: training loss 0.0258\n",
      "2025-06-01 14:50:25 [INFO]: epoch 733: training loss 0.0290\n",
      "2025-06-01 14:50:25 [INFO]: epoch 734: training loss 0.0286\n",
      "2025-06-01 14:50:25 [INFO]: epoch 735: training loss 0.0201\n",
      "2025-06-01 14:50:25 [INFO]: epoch 736: training loss 0.0277\n",
      "2025-06-01 14:50:25 [INFO]: epoch 737: training loss 0.0302\n",
      "2025-06-01 14:50:25 [INFO]: epoch 738: training loss 0.0231\n",
      "2025-06-01 14:50:25 [INFO]: epoch 739: training loss 0.0259\n",
      "2025-06-01 14:50:25 [INFO]: epoch 740: training loss 0.0301\n",
      "2025-06-01 14:50:25 [INFO]: epoch 741: training loss 0.0275\n",
      "2025-06-01 14:50:25 [INFO]: epoch 742: training loss 0.0338\n",
      "2025-06-01 14:50:25 [INFO]: epoch 743: training loss 0.0250\n",
      "2025-06-01 14:50:25 [INFO]: epoch 744: training loss 0.0247\n",
      "2025-06-01 14:50:25 [INFO]: epoch 745: training loss 0.0274\n",
      "2025-06-01 14:50:25 [INFO]: epoch 746: training loss 0.0217\n",
      "2025-06-01 14:50:25 [INFO]: epoch 747: training loss 0.0288\n",
      "2025-06-01 14:50:25 [INFO]: epoch 748: training loss 0.0296\n",
      "2025-06-01 14:50:25 [INFO]: epoch 749: training loss 0.0219\n",
      "2025-06-01 14:50:25 [INFO]: epoch 750: training loss 0.0241\n",
      "2025-06-01 14:50:25 [INFO]: epoch 751: training loss 0.0233\n",
      "2025-06-01 14:50:25 [INFO]: epoch 752: training loss 0.0280\n",
      "2025-06-01 14:50:25 [INFO]: epoch 753: training loss 0.0266\n",
      "2025-06-01 14:50:25 [INFO]: epoch 754: training loss 0.0213\n",
      "2025-06-01 14:50:25 [INFO]: epoch 755: training loss 0.0315\n",
      "2025-06-01 14:50:25 [INFO]: epoch 756: training loss 0.0228\n",
      "2025-06-01 14:50:25 [INFO]: epoch 757: training loss 0.0247\n",
      "2025-06-01 14:50:25 [INFO]: epoch 758: training loss 0.0281\n",
      "2025-06-01 14:50:25 [INFO]: epoch 759: training loss 0.0254\n",
      "2025-06-01 14:50:25 [INFO]: epoch 760: training loss 0.0268\n",
      "2025-06-01 14:50:25 [INFO]: epoch 761: training loss 0.0231\n",
      "2025-06-01 14:50:25 [INFO]: epoch 762: training loss 0.0282\n",
      "2025-06-01 14:50:25 [INFO]: epoch 763: training loss 0.0249\n",
      "2025-06-01 14:50:25 [INFO]: epoch 764: training loss 0.0246\n",
      "2025-06-01 14:50:25 [INFO]: epoch 765: training loss 0.0234\n",
      "2025-06-01 14:50:25 [INFO]: epoch 766: training loss 0.0258\n",
      "2025-06-01 14:50:25 [INFO]: epoch 767: training loss 0.0290\n",
      "2025-06-01 14:50:25 [INFO]: epoch 768: training loss 0.0245\n",
      "2025-06-01 14:50:25 [INFO]: epoch 769: training loss 0.0245\n",
      "2025-06-01 14:50:25 [INFO]: epoch 770: training loss 0.0248\n",
      "2025-06-01 14:50:25 [INFO]: epoch 771: training loss 0.0225\n",
      "2025-06-01 14:50:25 [INFO]: epoch 772: training loss 0.0263\n",
      "2025-06-01 14:50:25 [INFO]: epoch 773: training loss 0.0237\n",
      "2025-06-01 14:50:25 [INFO]: epoch 774: training loss 0.0249\n",
      "2025-06-01 14:50:25 [INFO]: epoch 775: training loss 0.0235\n",
      "2025-06-01 14:50:25 [INFO]: epoch 776: training loss 0.0264\n",
      "2025-06-01 14:50:25 [INFO]: epoch 777: training loss 0.0245\n",
      "2025-06-01 14:50:25 [INFO]: epoch 778: training loss 0.0226\n",
      "2025-06-01 14:50:25 [INFO]: epoch 779: training loss 0.0243\n",
      "2025-06-01 14:50:25 [INFO]: epoch 780: training loss 0.0316\n",
      "2025-06-01 14:50:25 [INFO]: epoch 781: training loss 0.0240\n",
      "2025-06-01 14:50:25 [INFO]: epoch 782: training loss 0.0273\n",
      "2025-06-01 14:50:25 [INFO]: epoch 783: training loss 0.0244\n",
      "2025-06-01 14:50:25 [INFO]: epoch 784: training loss 0.0233\n",
      "2025-06-01 14:50:25 [INFO]: epoch 785: training loss 0.0240\n",
      "2025-06-01 14:50:25 [INFO]: epoch 786: training loss 0.0222\n",
      "2025-06-01 14:50:25 [INFO]: epoch 787: training loss 0.0196\n",
      "2025-06-01 14:50:25 [INFO]: epoch 788: training loss 0.0260\n",
      "2025-06-01 14:50:25 [INFO]: epoch 789: training loss 0.0266\n",
      "2025-06-01 14:50:25 [INFO]: epoch 790: training loss 0.0213\n",
      "2025-06-01 14:50:25 [INFO]: epoch 791: training loss 0.0300\n",
      "2025-06-01 14:50:25 [INFO]: epoch 792: training loss 0.0231\n",
      "2025-06-01 14:50:25 [INFO]: epoch 793: training loss 0.0222\n",
      "2025-06-01 14:50:25 [INFO]: epoch 794: training loss 0.0242\n",
      "2025-06-01 14:50:25 [INFO]: epoch 795: training loss 0.0219\n",
      "2025-06-01 14:50:25 [INFO]: epoch 796: training loss 0.0269\n",
      "2025-06-01 14:50:25 [INFO]: epoch 797: training loss 0.0232\n",
      "2025-06-01 14:50:25 [INFO]: epoch 798: training loss 0.0257\n",
      "2025-06-01 14:50:25 [INFO]: epoch 799: training loss 0.0268\n",
      "2025-06-01 14:50:25 [INFO]: Finished training.\n",
      "2025-06-01 14:50:25 [WARNING]: 🚨DeprecationWarning: The method impute is deprecated. Please use `predict` instead.\n",
      " 33%|████████████████████████████                                                        | 2/6 [00:21<00:42, 10.58s/it]2025-06-01 14:50:25 [INFO]: No given device, using default device: cuda\n",
      "2025-06-01 14:50:25 [WARNING]: saving_path not given. Model files and tensorboard file will not be saved.\n",
      "2025-06-01 14:50:25 [INFO]: Model initialized successfully with the number of trainable parameters: 597,780\n",
      "2025-06-01 14:50:25 [INFO]: epoch 0: training loss 1.4152\n",
      "2025-06-01 14:50:26 [INFO]: epoch 1: training loss 0.7166\n",
      "2025-06-01 14:50:26 [INFO]: epoch 2: training loss 0.7629\n",
      "2025-06-01 14:50:26 [INFO]: epoch 3: training loss 0.6658\n",
      "2025-06-01 14:50:26 [INFO]: epoch 4: training loss 0.6840\n",
      "2025-06-01 14:50:26 [INFO]: epoch 5: training loss 0.6608\n",
      "2025-06-01 14:50:26 [INFO]: epoch 6: training loss 0.6050\n",
      "2025-06-01 14:50:26 [INFO]: epoch 7: training loss 0.5095\n",
      "2025-06-01 14:50:26 [INFO]: epoch 8: training loss 0.4141\n",
      "2025-06-01 14:50:26 [INFO]: epoch 9: training loss 0.3693\n",
      "2025-06-01 14:50:26 [INFO]: epoch 10: training loss 0.3594\n",
      "2025-06-01 14:50:26 [INFO]: epoch 11: training loss 0.4758\n",
      "2025-06-01 14:50:26 [INFO]: epoch 12: training loss 0.3631\n",
      "2025-06-01 14:50:26 [INFO]: epoch 13: training loss 0.3405\n",
      "2025-06-01 14:50:26 [INFO]: epoch 14: training loss 0.3526\n",
      "2025-06-01 14:50:26 [INFO]: epoch 15: training loss 0.3359\n",
      "2025-06-01 14:50:26 [INFO]: epoch 16: training loss 0.3212\n",
      "2025-06-01 14:50:26 [INFO]: epoch 17: training loss 0.3164\n",
      "2025-06-01 14:50:26 [INFO]: epoch 18: training loss 0.3091\n",
      "2025-06-01 14:50:26 [INFO]: epoch 19: training loss 0.3210\n",
      "2025-06-01 14:50:26 [INFO]: epoch 20: training loss 0.3015\n",
      "2025-06-01 14:50:26 [INFO]: epoch 21: training loss 0.2682\n",
      "2025-06-01 14:50:26 [INFO]: epoch 22: training loss 0.2861\n",
      "2025-06-01 14:50:26 [INFO]: epoch 23: training loss 0.2839\n",
      "2025-06-01 14:50:26 [INFO]: epoch 24: training loss 0.3188\n",
      "2025-06-01 14:50:26 [INFO]: epoch 25: training loss 0.3095\n",
      "2025-06-01 14:50:26 [INFO]: epoch 26: training loss 0.2882\n",
      "2025-06-01 14:50:26 [INFO]: epoch 27: training loss 0.3000\n",
      "2025-06-01 14:50:26 [INFO]: epoch 28: training loss 0.2516\n",
      "2025-06-01 14:50:26 [INFO]: epoch 29: training loss 0.2932\n",
      "2025-06-01 14:50:26 [INFO]: epoch 30: training loss 0.2904\n",
      "2025-06-01 14:50:26 [INFO]: epoch 31: training loss 0.2753\n",
      "2025-06-01 14:50:26 [INFO]: epoch 32: training loss 0.2947\n",
      "2025-06-01 14:50:26 [INFO]: epoch 33: training loss 0.2589\n",
      "2025-06-01 14:50:26 [INFO]: epoch 34: training loss 0.2665\n",
      "2025-06-01 14:50:26 [INFO]: epoch 35: training loss 0.2267\n",
      "2025-06-01 14:50:26 [INFO]: epoch 36: training loss 0.2562\n",
      "2025-06-01 14:50:26 [INFO]: epoch 37: training loss 0.2577\n",
      "2025-06-01 14:50:26 [INFO]: epoch 38: training loss 0.2447\n",
      "2025-06-01 14:50:26 [INFO]: epoch 39: training loss 0.2459\n",
      "2025-06-01 14:50:26 [INFO]: epoch 40: training loss 0.2395\n",
      "2025-06-01 14:50:26 [INFO]: epoch 41: training loss 0.2455\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-01 14:50:26 [INFO]: epoch 42: training loss 0.2263\n",
      "2025-06-01 14:50:26 [INFO]: epoch 43: training loss 0.2322\n",
      "2025-06-01 14:50:26 [INFO]: epoch 44: training loss 0.2325\n",
      "2025-06-01 14:50:26 [INFO]: epoch 45: training loss 0.2418\n",
      "2025-06-01 14:50:26 [INFO]: epoch 46: training loss 0.2565\n",
      "2025-06-01 14:50:26 [INFO]: epoch 47: training loss 0.2388\n",
      "2025-06-01 14:50:26 [INFO]: epoch 48: training loss 0.2224\n",
      "2025-06-01 14:50:26 [INFO]: epoch 49: training loss 0.2235\n",
      "2025-06-01 14:50:26 [INFO]: epoch 50: training loss 0.2152\n",
      "2025-06-01 14:50:26 [INFO]: epoch 51: training loss 0.2156\n",
      "2025-06-01 14:50:26 [INFO]: epoch 52: training loss 0.2107\n",
      "2025-06-01 14:50:26 [INFO]: epoch 53: training loss 0.2168\n",
      "2025-06-01 14:50:26 [INFO]: epoch 54: training loss 0.2061\n",
      "2025-06-01 14:50:26 [INFO]: epoch 55: training loss 0.2067\n",
      "2025-06-01 14:50:26 [INFO]: epoch 56: training loss 0.2085\n",
      "2025-06-01 14:50:26 [INFO]: epoch 57: training loss 0.2091\n",
      "2025-06-01 14:50:26 [INFO]: epoch 58: training loss 0.2340\n",
      "2025-06-01 14:50:26 [INFO]: epoch 59: training loss 0.2254\n",
      "2025-06-01 14:50:26 [INFO]: epoch 60: training loss 0.2060\n",
      "2025-06-01 14:50:26 [INFO]: epoch 61: training loss 0.1879\n",
      "2025-06-01 14:50:26 [INFO]: epoch 62: training loss 0.2054\n",
      "2025-06-01 14:50:26 [INFO]: epoch 63: training loss 0.2074\n",
      "2025-06-01 14:50:26 [INFO]: epoch 64: training loss 0.1949\n",
      "2025-06-01 14:50:26 [INFO]: epoch 65: training loss 0.1788\n",
      "2025-06-01 14:50:26 [INFO]: epoch 66: training loss 0.1873\n",
      "2025-06-01 14:50:26 [INFO]: epoch 67: training loss 0.1931\n",
      "2025-06-01 14:50:26 [INFO]: epoch 68: training loss 0.1983\n",
      "2025-06-01 14:50:26 [INFO]: epoch 69: training loss 0.1808\n",
      "2025-06-01 14:50:26 [INFO]: epoch 70: training loss 0.1867\n",
      "2025-06-01 14:50:26 [INFO]: epoch 71: training loss 0.1833\n",
      "2025-06-01 14:50:26 [INFO]: epoch 72: training loss 0.1830\n",
      "2025-06-01 14:50:26 [INFO]: epoch 73: training loss 0.1754\n",
      "2025-06-01 14:50:26 [INFO]: epoch 74: training loss 0.1689\n",
      "2025-06-01 14:50:26 [INFO]: epoch 75: training loss 0.1951\n",
      "2025-06-01 14:50:26 [INFO]: epoch 76: training loss 0.1962\n",
      "2025-06-01 14:50:27 [INFO]: epoch 77: training loss 0.1792\n",
      "2025-06-01 14:50:27 [INFO]: epoch 78: training loss 0.1702\n",
      "2025-06-01 14:50:27 [INFO]: epoch 79: training loss 0.1865\n",
      "2025-06-01 14:50:27 [INFO]: epoch 80: training loss 0.1904\n",
      "2025-06-01 14:50:27 [INFO]: epoch 81: training loss 0.1573\n",
      "2025-06-01 14:50:27 [INFO]: epoch 82: training loss 0.1647\n",
      "2025-06-01 14:50:27 [INFO]: epoch 83: training loss 0.1747\n",
      "2025-06-01 14:50:27 [INFO]: epoch 84: training loss 0.2022\n",
      "2025-06-01 14:50:27 [INFO]: epoch 85: training loss 0.1911\n",
      "2025-06-01 14:50:27 [INFO]: epoch 86: training loss 0.1612\n",
      "2025-06-01 14:50:27 [INFO]: epoch 87: training loss 0.1535\n",
      "2025-06-01 14:50:27 [INFO]: epoch 88: training loss 0.1496\n",
      "2025-06-01 14:50:27 [INFO]: epoch 89: training loss 0.1452\n",
      "2025-06-01 14:50:27 [INFO]: epoch 90: training loss 0.1461\n",
      "2025-06-01 14:50:27 [INFO]: epoch 91: training loss 0.1443\n",
      "2025-06-01 14:50:27 [INFO]: epoch 92: training loss 0.1451\n",
      "2025-06-01 14:50:27 [INFO]: epoch 93: training loss 0.1424\n",
      "2025-06-01 14:50:27 [INFO]: epoch 94: training loss 0.1333\n",
      "2025-06-01 14:50:27 [INFO]: epoch 95: training loss 0.1371\n",
      "2025-06-01 14:50:27 [INFO]: epoch 96: training loss 0.1346\n",
      "2025-06-01 14:50:27 [INFO]: epoch 97: training loss 0.1419\n",
      "2025-06-01 14:50:27 [INFO]: epoch 98: training loss 0.1409\n",
      "2025-06-01 14:50:27 [INFO]: epoch 99: training loss 0.1314\n",
      "2025-06-01 14:50:27 [INFO]: epoch 100: training loss 0.1423\n",
      "2025-06-01 14:50:27 [INFO]: epoch 101: training loss 0.1293\n",
      "2025-06-01 14:50:27 [INFO]: epoch 102: training loss 0.1451\n",
      "2025-06-01 14:50:27 [INFO]: epoch 103: training loss 0.1234\n",
      "2025-06-01 14:50:27 [INFO]: epoch 104: training loss 0.1386\n",
      "2025-06-01 14:50:27 [INFO]: epoch 105: training loss 0.1270\n",
      "2025-06-01 14:50:27 [INFO]: epoch 106: training loss 0.1239\n",
      "2025-06-01 14:50:27 [INFO]: epoch 107: training loss 0.1198\n",
      "2025-06-01 14:50:27 [INFO]: epoch 108: training loss 0.1111\n",
      "2025-06-01 14:50:27 [INFO]: epoch 109: training loss 0.1128\n",
      "2025-06-01 14:50:27 [INFO]: epoch 110: training loss 0.1183\n",
      "2025-06-01 14:50:27 [INFO]: epoch 111: training loss 0.1230\n",
      "2025-06-01 14:50:27 [INFO]: epoch 112: training loss 0.1085\n",
      "2025-06-01 14:50:27 [INFO]: epoch 113: training loss 0.1020\n",
      "2025-06-01 14:50:27 [INFO]: epoch 114: training loss 0.1088\n",
      "2025-06-01 14:50:27 [INFO]: epoch 115: training loss 0.1154\n",
      "2025-06-01 14:50:27 [INFO]: epoch 116: training loss 0.1102\n",
      "2025-06-01 14:50:27 [INFO]: epoch 117: training loss 0.1013\n",
      "2025-06-01 14:50:27 [INFO]: epoch 118: training loss 0.0930\n",
      "2025-06-01 14:50:27 [INFO]: epoch 119: training loss 0.1015\n",
      "2025-06-01 14:50:27 [INFO]: epoch 120: training loss 0.1122\n",
      "2025-06-01 14:50:27 [INFO]: epoch 121: training loss 0.1027\n",
      "2025-06-01 14:50:27 [INFO]: epoch 122: training loss 0.0907\n",
      "2025-06-01 14:50:27 [INFO]: epoch 123: training loss 0.0964\n",
      "2025-06-01 14:50:27 [INFO]: epoch 124: training loss 0.1253\n",
      "2025-06-01 14:50:27 [INFO]: epoch 125: training loss 0.1011\n",
      "2025-06-01 14:50:27 [INFO]: epoch 126: training loss 0.0944\n",
      "2025-06-01 14:50:27 [INFO]: epoch 127: training loss 0.0958\n",
      "2025-06-01 14:50:27 [INFO]: epoch 128: training loss 0.1013\n",
      "2025-06-01 14:50:27 [INFO]: epoch 129: training loss 0.0987\n",
      "2025-06-01 14:50:27 [INFO]: epoch 130: training loss 0.0925\n",
      "2025-06-01 14:50:27 [INFO]: epoch 131: training loss 0.1161\n",
      "2025-06-01 14:50:27 [INFO]: epoch 132: training loss 0.0923\n",
      "2025-06-01 14:50:27 [INFO]: epoch 133: training loss 0.1021\n",
      "2025-06-01 14:50:27 [INFO]: epoch 134: training loss 0.1098\n",
      "2025-06-01 14:50:27 [INFO]: epoch 135: training loss 0.0975\n",
      "2025-06-01 14:50:27 [INFO]: epoch 136: training loss 0.0884\n",
      "2025-06-01 14:50:27 [INFO]: epoch 137: training loss 0.1087\n",
      "2025-06-01 14:50:27 [INFO]: epoch 138: training loss 0.0871\n",
      "2025-06-01 14:50:27 [INFO]: epoch 139: training loss 0.0925\n",
      "2025-06-01 14:50:27 [INFO]: epoch 140: training loss 0.0845\n",
      "2025-06-01 14:50:27 [INFO]: epoch 141: training loss 0.0860\n",
      "2025-06-01 14:50:27 [INFO]: epoch 142: training loss 0.0817\n",
      "2025-06-01 14:50:27 [INFO]: epoch 143: training loss 0.0731\n",
      "2025-06-01 14:50:27 [INFO]: epoch 144: training loss 0.0684\n",
      "2025-06-01 14:50:27 [INFO]: epoch 145: training loss 0.0940\n",
      "2025-06-01 14:50:27 [INFO]: epoch 146: training loss 0.0667\n",
      "2025-06-01 14:50:27 [INFO]: epoch 147: training loss 0.0691\n",
      "2025-06-01 14:50:27 [INFO]: epoch 148: training loss 0.0729\n",
      "2025-06-01 14:50:27 [INFO]: epoch 149: training loss 0.0854\n",
      "2025-06-01 14:50:27 [INFO]: epoch 150: training loss 0.0631\n",
      "2025-06-01 14:50:27 [INFO]: epoch 151: training loss 0.0720\n",
      "2025-06-01 14:50:27 [INFO]: epoch 152: training loss 0.0777\n",
      "2025-06-01 14:50:28 [INFO]: epoch 153: training loss 0.0666\n",
      "2025-06-01 14:50:28 [INFO]: epoch 154: training loss 0.0690\n",
      "2025-06-01 14:50:28 [INFO]: epoch 155: training loss 0.0646\n",
      "2025-06-01 14:50:28 [INFO]: epoch 156: training loss 0.0687\n",
      "2025-06-01 14:50:28 [INFO]: epoch 157: training loss 0.0918\n",
      "2025-06-01 14:50:28 [INFO]: epoch 158: training loss 0.0683\n",
      "2025-06-01 14:50:28 [INFO]: epoch 159: training loss 0.0714\n",
      "2025-06-01 14:50:28 [INFO]: epoch 160: training loss 0.0675\n",
      "2025-06-01 14:50:28 [INFO]: epoch 161: training loss 0.0793\n",
      "2025-06-01 14:50:28 [INFO]: epoch 162: training loss 0.0687\n",
      "2025-06-01 14:50:28 [INFO]: epoch 163: training loss 0.0802\n",
      "2025-06-01 14:50:28 [INFO]: epoch 164: training loss 0.0834\n",
      "2025-06-01 14:50:28 [INFO]: epoch 165: training loss 0.0626\n",
      "2025-06-01 14:50:28 [INFO]: epoch 166: training loss 0.0754\n",
      "2025-06-01 14:50:28 [INFO]: epoch 167: training loss 0.0728\n",
      "2025-06-01 14:50:28 [INFO]: epoch 168: training loss 0.0798\n",
      "2025-06-01 14:50:28 [INFO]: epoch 169: training loss 0.0831\n",
      "2025-06-01 14:50:28 [INFO]: epoch 170: training loss 0.0639\n",
      "2025-06-01 14:50:28 [INFO]: epoch 171: training loss 0.0682\n",
      "2025-06-01 14:50:28 [INFO]: epoch 172: training loss 0.0761\n",
      "2025-06-01 14:50:28 [INFO]: epoch 173: training loss 0.0728\n",
      "2025-06-01 14:50:28 [INFO]: epoch 174: training loss 0.0649\n",
      "2025-06-01 14:50:28 [INFO]: epoch 175: training loss 0.0712\n",
      "2025-06-01 14:50:28 [INFO]: epoch 176: training loss 0.0708\n",
      "2025-06-01 14:50:28 [INFO]: epoch 177: training loss 0.0762\n",
      "2025-06-01 14:50:28 [INFO]: epoch 178: training loss 0.0679\n",
      "2025-06-01 14:50:28 [INFO]: epoch 179: training loss 0.0608\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-01 14:50:28 [INFO]: epoch 180: training loss 0.0674\n",
      "2025-06-01 14:50:28 [INFO]: epoch 181: training loss 0.0541\n",
      "2025-06-01 14:50:28 [INFO]: epoch 182: training loss 0.0607\n",
      "2025-06-01 14:50:28 [INFO]: epoch 183: training loss 0.0609\n",
      "2025-06-01 14:50:28 [INFO]: epoch 184: training loss 0.0676\n",
      "2025-06-01 14:50:28 [INFO]: epoch 185: training loss 0.0695\n",
      "2025-06-01 14:50:28 [INFO]: epoch 186: training loss 0.0662\n",
      "2025-06-01 14:50:28 [INFO]: epoch 187: training loss 0.0613\n",
      "2025-06-01 14:50:28 [INFO]: epoch 188: training loss 0.0583\n",
      "2025-06-01 14:50:28 [INFO]: epoch 189: training loss 0.0608\n",
      "2025-06-01 14:50:28 [INFO]: epoch 190: training loss 0.0688\n",
      "2025-06-01 14:50:28 [INFO]: epoch 191: training loss 0.0629\n",
      "2025-06-01 14:50:28 [INFO]: epoch 192: training loss 0.0685\n",
      "2025-06-01 14:50:28 [INFO]: epoch 193: training loss 0.0699\n",
      "2025-06-01 14:50:28 [INFO]: epoch 194: training loss 0.0597\n",
      "2025-06-01 14:50:28 [INFO]: epoch 195: training loss 0.0579\n",
      "2025-06-01 14:50:28 [INFO]: epoch 196: training loss 0.0621\n",
      "2025-06-01 14:50:28 [INFO]: epoch 197: training loss 0.0622\n",
      "2025-06-01 14:50:28 [INFO]: epoch 198: training loss 0.0554\n",
      "2025-06-01 14:50:28 [INFO]: epoch 199: training loss 0.0592\n",
      "2025-06-01 14:50:28 [INFO]: epoch 200: training loss 0.0693\n",
      "2025-06-01 14:50:28 [INFO]: epoch 201: training loss 0.0650\n",
      "2025-06-01 14:50:28 [INFO]: epoch 202: training loss 0.0723\n",
      "2025-06-01 14:50:28 [INFO]: epoch 203: training loss 0.0691\n",
      "2025-06-01 14:50:28 [INFO]: epoch 204: training loss 0.0711\n",
      "2025-06-01 14:50:28 [INFO]: epoch 205: training loss 0.0775\n",
      "2025-06-01 14:50:28 [INFO]: epoch 206: training loss 0.0713\n",
      "2025-06-01 14:50:28 [INFO]: epoch 207: training loss 0.0663\n",
      "2025-06-01 14:50:28 [INFO]: epoch 208: training loss 0.0587\n",
      "2025-06-01 14:50:28 [INFO]: epoch 209: training loss 0.0659\n",
      "2025-06-01 14:50:28 [INFO]: epoch 210: training loss 0.0754\n",
      "2025-06-01 14:50:28 [INFO]: epoch 211: training loss 0.0541\n",
      "2025-06-01 14:50:28 [INFO]: epoch 212: training loss 0.0514\n",
      "2025-06-01 14:50:28 [INFO]: epoch 213: training loss 0.0621\n",
      "2025-06-01 14:50:28 [INFO]: epoch 214: training loss 0.0623\n",
      "2025-06-01 14:50:28 [INFO]: epoch 215: training loss 0.0519\n",
      "2025-06-01 14:50:28 [INFO]: epoch 216: training loss 0.0567\n",
      "2025-06-01 14:50:28 [INFO]: epoch 217: training loss 0.0571\n",
      "2025-06-01 14:50:28 [INFO]: epoch 218: training loss 0.0535\n",
      "2025-06-01 14:50:28 [INFO]: epoch 219: training loss 0.0573\n",
      "2025-06-01 14:50:28 [INFO]: epoch 220: training loss 0.0522\n",
      "2025-06-01 14:50:28 [INFO]: epoch 221: training loss 0.0494\n",
      "2025-06-01 14:50:28 [INFO]: epoch 222: training loss 0.0509\n",
      "2025-06-01 14:50:28 [INFO]: epoch 223: training loss 0.0498\n",
      "2025-06-01 14:50:28 [INFO]: epoch 224: training loss 0.0521\n",
      "2025-06-01 14:50:28 [INFO]: epoch 225: training loss 0.0540\n",
      "2025-06-01 14:50:28 [INFO]: epoch 226: training loss 0.0602\n",
      "2025-06-01 14:50:28 [INFO]: epoch 227: training loss 0.0491\n",
      "2025-06-01 14:50:28 [INFO]: epoch 228: training loss 0.0521\n",
      "2025-06-01 14:50:28 [INFO]: epoch 229: training loss 0.0620\n",
      "2025-06-01 14:50:29 [INFO]: epoch 230: training loss 0.0470\n",
      "2025-06-01 14:50:29 [INFO]: epoch 231: training loss 0.0596\n",
      "2025-06-01 14:50:29 [INFO]: epoch 232: training loss 0.0488\n",
      "2025-06-01 14:50:29 [INFO]: epoch 233: training loss 0.0447\n",
      "2025-06-01 14:50:29 [INFO]: epoch 234: training loss 0.0490\n",
      "2025-06-01 14:50:29 [INFO]: epoch 235: training loss 0.0487\n",
      "2025-06-01 14:50:29 [INFO]: epoch 236: training loss 0.0522\n",
      "2025-06-01 14:50:29 [INFO]: epoch 237: training loss 0.0534\n",
      "2025-06-01 14:50:29 [INFO]: epoch 238: training loss 0.0596\n",
      "2025-06-01 14:50:29 [INFO]: epoch 239: training loss 0.0504\n",
      "2025-06-01 14:50:29 [INFO]: epoch 240: training loss 0.0495\n",
      "2025-06-01 14:50:29 [INFO]: epoch 241: training loss 0.0501\n",
      "2025-06-01 14:50:29 [INFO]: epoch 242: training loss 0.0551\n",
      "2025-06-01 14:50:29 [INFO]: epoch 243: training loss 0.0440\n",
      "2025-06-01 14:50:29 [INFO]: epoch 244: training loss 0.0524\n",
      "2025-06-01 14:50:29 [INFO]: epoch 245: training loss 0.0439\n",
      "2025-06-01 14:50:29 [INFO]: epoch 246: training loss 0.0540\n",
      "2025-06-01 14:50:29 [INFO]: epoch 247: training loss 0.0632\n",
      "2025-06-01 14:50:29 [INFO]: epoch 248: training loss 0.0550\n",
      "2025-06-01 14:50:29 [INFO]: epoch 249: training loss 0.0530\n",
      "2025-06-01 14:50:29 [INFO]: epoch 250: training loss 0.0558\n",
      "2025-06-01 14:50:29 [INFO]: epoch 251: training loss 0.0595\n",
      "2025-06-01 14:50:29 [INFO]: epoch 252: training loss 0.0539\n",
      "2025-06-01 14:50:29 [INFO]: epoch 253: training loss 0.0752\n",
      "2025-06-01 14:50:29 [INFO]: epoch 254: training loss 0.0525\n",
      "2025-06-01 14:50:29 [INFO]: epoch 255: training loss 0.0484\n",
      "2025-06-01 14:50:29 [INFO]: epoch 256: training loss 0.0676\n",
      "2025-06-01 14:50:29 [INFO]: epoch 257: training loss 0.0439\n",
      "2025-06-01 14:50:29 [INFO]: epoch 258: training loss 0.0518\n",
      "2025-06-01 14:50:29 [INFO]: epoch 259: training loss 0.0524\n",
      "2025-06-01 14:50:29 [INFO]: epoch 260: training loss 0.0458\n",
      "2025-06-01 14:50:29 [INFO]: epoch 261: training loss 0.0477\n",
      "2025-06-01 14:50:29 [INFO]: epoch 262: training loss 0.0474\n",
      "2025-06-01 14:50:29 [INFO]: epoch 263: training loss 0.0446\n",
      "2025-06-01 14:50:29 [INFO]: epoch 264: training loss 0.0462\n",
      "2025-06-01 14:50:29 [INFO]: epoch 265: training loss 0.0404\n",
      "2025-06-01 14:50:29 [INFO]: epoch 266: training loss 0.0452\n",
      "2025-06-01 14:50:29 [INFO]: epoch 267: training loss 0.0516\n",
      "2025-06-01 14:50:29 [INFO]: epoch 268: training loss 0.0445\n",
      "2025-06-01 14:50:29 [INFO]: epoch 269: training loss 0.0435\n",
      "2025-06-01 14:50:29 [INFO]: epoch 270: training loss 0.0453\n",
      "2025-06-01 14:50:29 [INFO]: epoch 271: training loss 0.0393\n",
      "2025-06-01 14:50:29 [INFO]: epoch 272: training loss 0.0387\n",
      "2025-06-01 14:50:29 [INFO]: epoch 273: training loss 0.0440\n",
      "2025-06-01 14:50:29 [INFO]: epoch 274: training loss 0.0425\n",
      "2025-06-01 14:50:29 [INFO]: epoch 275: training loss 0.0431\n",
      "2025-06-01 14:50:29 [INFO]: epoch 276: training loss 0.0458\n",
      "2025-06-01 14:50:29 [INFO]: epoch 277: training loss 0.0407\n",
      "2025-06-01 14:50:29 [INFO]: epoch 278: training loss 0.0338\n",
      "2025-06-01 14:50:29 [INFO]: epoch 279: training loss 0.0542\n",
      "2025-06-01 14:50:29 [INFO]: epoch 280: training loss 0.0496\n",
      "2025-06-01 14:50:29 [INFO]: epoch 281: training loss 0.0426\n",
      "2025-06-01 14:50:29 [INFO]: epoch 282: training loss 0.0518\n",
      "2025-06-01 14:50:29 [INFO]: epoch 283: training loss 0.0525\n",
      "2025-06-01 14:50:29 [INFO]: epoch 284: training loss 0.0467\n",
      "2025-06-01 14:50:29 [INFO]: epoch 285: training loss 0.0499\n",
      "2025-06-01 14:50:29 [INFO]: epoch 286: training loss 0.0641\n",
      "2025-06-01 14:50:29 [INFO]: epoch 287: training loss 0.0517\n",
      "2025-06-01 14:50:29 [INFO]: epoch 288: training loss 0.0499\n",
      "2025-06-01 14:50:29 [INFO]: epoch 289: training loss 0.0495\n",
      "2025-06-01 14:50:29 [INFO]: epoch 290: training loss 0.0492\n",
      "2025-06-01 14:50:29 [INFO]: epoch 291: training loss 0.0499\n",
      "2025-06-01 14:50:29 [INFO]: epoch 292: training loss 0.0516\n",
      "2025-06-01 14:50:29 [INFO]: epoch 293: training loss 0.0424\n",
      "2025-06-01 14:50:29 [INFO]: epoch 294: training loss 0.0404\n",
      "2025-06-01 14:50:29 [INFO]: epoch 295: training loss 0.0505\n",
      "2025-06-01 14:50:29 [INFO]: epoch 296: training loss 0.0422\n",
      "2025-06-01 14:50:29 [INFO]: epoch 297: training loss 0.0459\n",
      "2025-06-01 14:50:29 [INFO]: epoch 298: training loss 0.0495\n",
      "2025-06-01 14:50:29 [INFO]: epoch 299: training loss 0.0466\n",
      "2025-06-01 14:50:29 [INFO]: epoch 300: training loss 0.0373\n",
      "2025-06-01 14:50:29 [INFO]: epoch 301: training loss 0.0477\n",
      "2025-06-01 14:50:29 [INFO]: epoch 302: training loss 0.0589\n",
      "2025-06-01 14:50:29 [INFO]: epoch 303: training loss 0.0497\n",
      "2025-06-01 14:50:29 [INFO]: epoch 304: training loss 0.0440\n",
      "2025-06-01 14:50:29 [INFO]: epoch 305: training loss 0.0435\n",
      "2025-06-01 14:50:29 [INFO]: epoch 306: training loss 0.0613\n",
      "2025-06-01 14:50:30 [INFO]: epoch 307: training loss 0.0599\n",
      "2025-06-01 14:50:30 [INFO]: epoch 308: training loss 0.0431\n",
      "2025-06-01 14:50:30 [INFO]: epoch 309: training loss 0.0538\n",
      "2025-06-01 14:50:30 [INFO]: epoch 310: training loss 0.0517\n",
      "2025-06-01 14:50:30 [INFO]: epoch 311: training loss 0.0391\n",
      "2025-06-01 14:50:30 [INFO]: epoch 312: training loss 0.0409\n",
      "2025-06-01 14:50:30 [INFO]: epoch 313: training loss 0.0511\n",
      "2025-06-01 14:50:30 [INFO]: epoch 314: training loss 0.0486\n",
      "2025-06-01 14:50:30 [INFO]: epoch 315: training loss 0.0460\n",
      "2025-06-01 14:50:30 [INFO]: epoch 316: training loss 0.0395\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-01 14:50:30 [INFO]: epoch 317: training loss 0.0423\n",
      "2025-06-01 14:50:30 [INFO]: epoch 318: training loss 0.0368\n",
      "2025-06-01 14:50:30 [INFO]: epoch 319: training loss 0.0376\n",
      "2025-06-01 14:50:30 [INFO]: epoch 320: training loss 0.0350\n",
      "2025-06-01 14:50:30 [INFO]: epoch 321: training loss 0.0396\n",
      "2025-06-01 14:50:30 [INFO]: epoch 322: training loss 0.0440\n",
      "2025-06-01 14:50:30 [INFO]: epoch 323: training loss 0.0391\n",
      "2025-06-01 14:50:30 [INFO]: epoch 324: training loss 0.0414\n",
      "2025-06-01 14:50:30 [INFO]: epoch 325: training loss 0.0426\n",
      "2025-06-01 14:50:30 [INFO]: epoch 326: training loss 0.0363\n",
      "2025-06-01 14:50:30 [INFO]: epoch 327: training loss 0.0418\n",
      "2025-06-01 14:50:30 [INFO]: epoch 328: training loss 0.0458\n",
      "2025-06-01 14:50:30 [INFO]: epoch 329: training loss 0.0355\n",
      "2025-06-01 14:50:30 [INFO]: epoch 330: training loss 0.0423\n",
      "2025-06-01 14:50:30 [INFO]: epoch 331: training loss 0.0424\n",
      "2025-06-01 14:50:30 [INFO]: epoch 332: training loss 0.0404\n",
      "2025-06-01 14:50:30 [INFO]: epoch 333: training loss 0.0392\n",
      "2025-06-01 14:50:30 [INFO]: epoch 334: training loss 0.0449\n",
      "2025-06-01 14:50:30 [INFO]: epoch 335: training loss 0.0422\n",
      "2025-06-01 14:50:30 [INFO]: epoch 336: training loss 0.0335\n",
      "2025-06-01 14:50:30 [INFO]: epoch 337: training loss 0.0385\n",
      "2025-06-01 14:50:30 [INFO]: epoch 338: training loss 0.0428\n",
      "2025-06-01 14:50:30 [INFO]: epoch 339: training loss 0.0352\n",
      "2025-06-01 14:50:30 [INFO]: epoch 340: training loss 0.0373\n",
      "2025-06-01 14:50:30 [INFO]: epoch 341: training loss 0.0429\n",
      "2025-06-01 14:50:30 [INFO]: epoch 342: training loss 0.0412\n",
      "2025-06-01 14:50:30 [INFO]: epoch 343: training loss 0.0346\n",
      "2025-06-01 14:50:30 [INFO]: epoch 344: training loss 0.0356\n",
      "2025-06-01 14:50:30 [INFO]: epoch 345: training loss 0.0402\n",
      "2025-06-01 14:50:30 [INFO]: epoch 346: training loss 0.0350\n",
      "2025-06-01 14:50:30 [INFO]: epoch 347: training loss 0.0301\n",
      "2025-06-01 14:50:30 [INFO]: epoch 348: training loss 0.0280\n",
      "2025-06-01 14:50:30 [INFO]: epoch 349: training loss 0.0386\n",
      "2025-06-01 14:50:30 [INFO]: epoch 350: training loss 0.0312\n",
      "2025-06-01 14:50:30 [INFO]: epoch 351: training loss 0.0309\n",
      "2025-06-01 14:50:30 [INFO]: epoch 352: training loss 0.0362\n",
      "2025-06-01 14:50:30 [INFO]: epoch 353: training loss 0.0335\n",
      "2025-06-01 14:50:30 [INFO]: epoch 354: training loss 0.0341\n",
      "2025-06-01 14:50:30 [INFO]: epoch 355: training loss 0.0344\n",
      "2025-06-01 14:50:30 [INFO]: epoch 356: training loss 0.0395\n",
      "2025-06-01 14:50:30 [INFO]: epoch 357: training loss 0.0350\n",
      "2025-06-01 14:50:30 [INFO]: epoch 358: training loss 0.0327\n",
      "2025-06-01 14:50:30 [INFO]: epoch 359: training loss 0.0331\n",
      "2025-06-01 14:50:30 [INFO]: epoch 360: training loss 0.0365\n",
      "2025-06-01 14:50:30 [INFO]: epoch 361: training loss 0.0344\n",
      "2025-06-01 14:50:30 [INFO]: epoch 362: training loss 0.0303\n",
      "2025-06-01 14:50:30 [INFO]: epoch 363: training loss 0.0359\n",
      "2025-06-01 14:50:30 [INFO]: epoch 364: training loss 0.0275\n",
      "2025-06-01 14:50:30 [INFO]: epoch 365: training loss 0.0315\n",
      "2025-06-01 14:50:30 [INFO]: epoch 366: training loss 0.0294\n",
      "2025-06-01 14:50:30 [INFO]: epoch 367: training loss 0.0357\n",
      "2025-06-01 14:50:30 [INFO]: epoch 368: training loss 0.0317\n",
      "2025-06-01 14:50:30 [INFO]: epoch 369: training loss 0.0311\n",
      "2025-06-01 14:50:30 [INFO]: epoch 370: training loss 0.0304\n",
      "2025-06-01 14:50:30 [INFO]: epoch 371: training loss 0.0341\n",
      "2025-06-01 14:50:30 [INFO]: epoch 372: training loss 0.0326\n",
      "2025-06-01 14:50:30 [INFO]: epoch 373: training loss 0.0300\n",
      "2025-06-01 14:50:30 [INFO]: epoch 374: training loss 0.0357\n",
      "2025-06-01 14:50:30 [INFO]: epoch 375: training loss 0.0328\n",
      "2025-06-01 14:50:30 [INFO]: epoch 376: training loss 0.0281\n",
      "2025-06-01 14:50:30 [INFO]: epoch 377: training loss 0.0239\n",
      "2025-06-01 14:50:30 [INFO]: epoch 378: training loss 0.0293\n",
      "2025-06-01 14:50:30 [INFO]: epoch 379: training loss 0.0332\n",
      "2025-06-01 14:50:30 [INFO]: epoch 380: training loss 0.0287\n",
      "2025-06-01 14:50:30 [INFO]: epoch 381: training loss 0.0353\n",
      "2025-06-01 14:50:30 [INFO]: epoch 382: training loss 0.0293\n",
      "2025-06-01 14:50:30 [INFO]: epoch 383: training loss 0.0272\n",
      "2025-06-01 14:50:31 [INFO]: epoch 384: training loss 0.0282\n",
      "2025-06-01 14:50:31 [INFO]: epoch 385: training loss 0.0312\n",
      "2025-06-01 14:50:31 [INFO]: epoch 386: training loss 0.0262\n",
      "2025-06-01 14:50:31 [INFO]: epoch 387: training loss 0.0304\n",
      "2025-06-01 14:50:31 [INFO]: epoch 388: training loss 0.0315\n",
      "2025-06-01 14:50:31 [INFO]: epoch 389: training loss 0.0316\n",
      "2025-06-01 14:50:31 [INFO]: epoch 390: training loss 0.0378\n",
      "2025-06-01 14:50:31 [INFO]: epoch 391: training loss 0.0352\n",
      "2025-06-01 14:50:31 [INFO]: epoch 392: training loss 0.0371\n",
      "2025-06-01 14:50:31 [INFO]: epoch 393: training loss 0.0286\n",
      "2025-06-01 14:50:31 [INFO]: epoch 394: training loss 0.0360\n",
      "2025-06-01 14:50:31 [INFO]: epoch 395: training loss 0.0333\n",
      "2025-06-01 14:50:31 [INFO]: epoch 396: training loss 0.0343\n",
      "2025-06-01 14:50:31 [INFO]: epoch 397: training loss 0.0254\n",
      "2025-06-01 14:50:31 [INFO]: epoch 398: training loss 0.0338\n",
      "2025-06-01 14:50:31 [INFO]: epoch 399: training loss 0.0319\n",
      "2025-06-01 14:50:31 [INFO]: epoch 400: training loss 0.0368\n",
      "2025-06-01 14:50:31 [INFO]: epoch 401: training loss 0.0316\n",
      "2025-06-01 14:50:31 [INFO]: epoch 402: training loss 0.0339\n",
      "2025-06-01 14:50:31 [INFO]: epoch 403: training loss 0.0331\n",
      "2025-06-01 14:50:31 [INFO]: epoch 404: training loss 0.0345\n",
      "2025-06-01 14:50:31 [INFO]: epoch 405: training loss 0.0349\n",
      "2025-06-01 14:50:31 [INFO]: epoch 406: training loss 0.0358\n",
      "2025-06-01 14:50:31 [INFO]: epoch 407: training loss 0.0323\n",
      "2025-06-01 14:50:31 [INFO]: epoch 408: training loss 0.0347\n",
      "2025-06-01 14:50:31 [INFO]: epoch 409: training loss 0.0418\n",
      "2025-06-01 14:50:31 [INFO]: epoch 410: training loss 0.0317\n",
      "2025-06-01 14:50:31 [INFO]: epoch 411: training loss 0.0353\n",
      "2025-06-01 14:50:31 [INFO]: epoch 412: training loss 0.0370\n",
      "2025-06-01 14:50:31 [INFO]: epoch 413: training loss 0.0431\n",
      "2025-06-01 14:50:31 [INFO]: epoch 414: training loss 0.0366\n",
      "2025-06-01 14:50:31 [INFO]: epoch 415: training loss 0.0346\n",
      "2025-06-01 14:50:31 [INFO]: epoch 416: training loss 0.0374\n",
      "2025-06-01 14:50:31 [INFO]: epoch 417: training loss 0.0385\n",
      "2025-06-01 14:50:31 [INFO]: epoch 418: training loss 0.0372\n",
      "2025-06-01 14:50:31 [INFO]: epoch 419: training loss 0.0340\n",
      "2025-06-01 14:50:31 [INFO]: epoch 420: training loss 0.0357\n",
      "2025-06-01 14:50:31 [INFO]: epoch 421: training loss 0.0320\n",
      "2025-06-01 14:50:31 [INFO]: epoch 422: training loss 0.0262\n",
      "2025-06-01 14:50:31 [INFO]: epoch 423: training loss 0.0348\n",
      "2025-06-01 14:50:31 [INFO]: epoch 424: training loss 0.0417\n",
      "2025-06-01 14:50:31 [INFO]: epoch 425: training loss 0.0331\n",
      "2025-06-01 14:50:31 [INFO]: epoch 426: training loss 0.0357\n",
      "2025-06-01 14:50:31 [INFO]: epoch 427: training loss 0.0308\n",
      "2025-06-01 14:50:31 [INFO]: epoch 428: training loss 0.0346\n",
      "2025-06-01 14:50:31 [INFO]: epoch 429: training loss 0.0430\n",
      "2025-06-01 14:50:31 [INFO]: epoch 430: training loss 0.0348\n",
      "2025-06-01 14:50:31 [INFO]: epoch 431: training loss 0.0318\n",
      "2025-06-01 14:50:31 [INFO]: epoch 432: training loss 0.0437\n",
      "2025-06-01 14:50:31 [INFO]: epoch 433: training loss 0.0314\n",
      "2025-06-01 14:50:31 [INFO]: epoch 434: training loss 0.0322\n",
      "2025-06-01 14:50:31 [INFO]: epoch 435: training loss 0.0280\n",
      "2025-06-01 14:50:31 [INFO]: epoch 436: training loss 0.0271\n",
      "2025-06-01 14:50:31 [INFO]: epoch 437: training loss 0.0304\n",
      "2025-06-01 14:50:31 [INFO]: epoch 438: training loss 0.0293\n",
      "2025-06-01 14:50:31 [INFO]: epoch 439: training loss 0.0298\n",
      "2025-06-01 14:50:31 [INFO]: epoch 440: training loss 0.0259\n",
      "2025-06-01 14:50:31 [INFO]: epoch 441: training loss 0.0289\n",
      "2025-06-01 14:50:31 [INFO]: epoch 442: training loss 0.0284\n",
      "2025-06-01 14:50:31 [INFO]: epoch 443: training loss 0.0243\n",
      "2025-06-01 14:50:31 [INFO]: epoch 444: training loss 0.0258\n",
      "2025-06-01 14:50:31 [INFO]: epoch 445: training loss 0.0263\n",
      "2025-06-01 14:50:31 [INFO]: epoch 446: training loss 0.0307\n",
      "2025-06-01 14:50:31 [INFO]: epoch 447: training loss 0.0285\n",
      "2025-06-01 14:50:31 [INFO]: epoch 448: training loss 0.0316\n",
      "2025-06-01 14:50:31 [INFO]: epoch 449: training loss 0.0299\n",
      "2025-06-01 14:50:31 [INFO]: epoch 450: training loss 0.0281\n",
      "2025-06-01 14:50:31 [INFO]: epoch 451: training loss 0.0327\n",
      "2025-06-01 14:50:31 [INFO]: epoch 452: training loss 0.0277\n",
      "2025-06-01 14:50:31 [INFO]: epoch 453: training loss 0.0280\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-01 14:50:31 [INFO]: epoch 454: training loss 0.0328\n",
      "2025-06-01 14:50:31 [INFO]: epoch 455: training loss 0.0307\n",
      "2025-06-01 14:50:31 [INFO]: epoch 456: training loss 0.0282\n",
      "2025-06-01 14:50:31 [INFO]: epoch 457: training loss 0.0364\n",
      "2025-06-01 14:50:31 [INFO]: epoch 458: training loss 0.0274\n",
      "2025-06-01 14:50:31 [INFO]: epoch 459: training loss 0.0245\n",
      "2025-06-01 14:50:32 [INFO]: epoch 460: training loss 0.0330\n",
      "2025-06-01 14:50:32 [INFO]: epoch 461: training loss 0.0349\n",
      "2025-06-01 14:50:32 [INFO]: epoch 462: training loss 0.0370\n",
      "2025-06-01 14:50:32 [INFO]: epoch 463: training loss 0.0333\n",
      "2025-06-01 14:50:32 [INFO]: epoch 464: training loss 0.0323\n",
      "2025-06-01 14:50:32 [INFO]: epoch 465: training loss 0.0366\n",
      "2025-06-01 14:50:32 [INFO]: epoch 466: training loss 0.0333\n",
      "2025-06-01 14:50:32 [INFO]: epoch 467: training loss 0.0296\n",
      "2025-06-01 14:50:32 [INFO]: epoch 468: training loss 0.0316\n",
      "2025-06-01 14:50:32 [INFO]: epoch 469: training loss 0.0346\n",
      "2025-06-01 14:50:32 [INFO]: epoch 470: training loss 0.0314\n",
      "2025-06-01 14:50:32 [INFO]: epoch 471: training loss 0.0364\n",
      "2025-06-01 14:50:32 [INFO]: epoch 472: training loss 0.0343\n",
      "2025-06-01 14:50:32 [INFO]: epoch 473: training loss 0.0378\n",
      "2025-06-01 14:50:32 [INFO]: epoch 474: training loss 0.0324\n",
      "2025-06-01 14:50:32 [INFO]: epoch 475: training loss 0.0296\n",
      "2025-06-01 14:50:32 [INFO]: epoch 476: training loss 0.0459\n",
      "2025-06-01 14:50:32 [INFO]: epoch 477: training loss 0.0346\n",
      "2025-06-01 14:50:32 [INFO]: epoch 478: training loss 0.0264\n",
      "2025-06-01 14:50:32 [INFO]: epoch 479: training loss 0.0293\n",
      "2025-06-01 14:50:32 [INFO]: epoch 480: training loss 0.0326\n",
      "2025-06-01 14:50:32 [INFO]: epoch 481: training loss 0.0354\n",
      "2025-06-01 14:50:32 [INFO]: epoch 482: training loss 0.0274\n",
      "2025-06-01 14:50:32 [INFO]: epoch 483: training loss 0.0280\n",
      "2025-06-01 14:50:32 [INFO]: epoch 484: training loss 0.0313\n",
      "2025-06-01 14:50:32 [INFO]: epoch 485: training loss 0.0275\n",
      "2025-06-01 14:50:32 [INFO]: epoch 486: training loss 0.0312\n",
      "2025-06-01 14:50:32 [INFO]: epoch 487: training loss 0.0352\n",
      "2025-06-01 14:50:32 [INFO]: epoch 488: training loss 0.0314\n",
      "2025-06-01 14:50:32 [INFO]: epoch 489: training loss 0.0337\n",
      "2025-06-01 14:50:32 [INFO]: epoch 490: training loss 0.0316\n",
      "2025-06-01 14:50:32 [INFO]: epoch 491: training loss 0.0284\n",
      "2025-06-01 14:50:32 [INFO]: epoch 492: training loss 0.0287\n",
      "2025-06-01 14:50:32 [INFO]: epoch 493: training loss 0.0296\n",
      "2025-06-01 14:50:32 [INFO]: epoch 494: training loss 0.0259\n",
      "2025-06-01 14:50:32 [INFO]: epoch 495: training loss 0.0313\n",
      "2025-06-01 14:50:32 [INFO]: epoch 496: training loss 0.0267\n",
      "2025-06-01 14:50:32 [INFO]: epoch 497: training loss 0.0300\n",
      "2025-06-01 14:50:32 [INFO]: epoch 498: training loss 0.0266\n",
      "2025-06-01 14:50:32 [INFO]: epoch 499: training loss 0.0242\n",
      "2025-06-01 14:50:32 [INFO]: epoch 500: training loss 0.0310\n",
      "2025-06-01 14:50:32 [INFO]: epoch 501: training loss 0.0268\n",
      "2025-06-01 14:50:32 [INFO]: epoch 502: training loss 0.0256\n",
      "2025-06-01 14:50:32 [INFO]: epoch 503: training loss 0.0369\n",
      "2025-06-01 14:50:32 [INFO]: epoch 504: training loss 0.0337\n",
      "2025-06-01 14:50:32 [INFO]: epoch 505: training loss 0.0360\n",
      "2025-06-01 14:50:32 [INFO]: epoch 506: training loss 0.0334\n",
      "2025-06-01 14:50:32 [INFO]: epoch 507: training loss 0.0293\n",
      "2025-06-01 14:50:32 [INFO]: epoch 508: training loss 0.0354\n",
      "2025-06-01 14:50:32 [INFO]: epoch 509: training loss 0.0316\n",
      "2025-06-01 14:50:32 [INFO]: epoch 510: training loss 0.0235\n",
      "2025-06-01 14:50:32 [INFO]: epoch 511: training loss 0.0300\n",
      "2025-06-01 14:50:32 [INFO]: epoch 512: training loss 0.0282\n",
      "2025-06-01 14:50:32 [INFO]: epoch 513: training loss 0.0261\n",
      "2025-06-01 14:50:32 [INFO]: epoch 514: training loss 0.0239\n",
      "2025-06-01 14:50:32 [INFO]: epoch 515: training loss 0.0363\n",
      "2025-06-01 14:50:32 [INFO]: epoch 516: training loss 0.0306\n",
      "2025-06-01 14:50:32 [INFO]: epoch 517: training loss 0.0293\n",
      "2025-06-01 14:50:32 [INFO]: epoch 518: training loss 0.0373\n",
      "2025-06-01 14:50:32 [INFO]: epoch 519: training loss 0.0272\n",
      "2025-06-01 14:50:32 [INFO]: epoch 520: training loss 0.0409\n",
      "2025-06-01 14:50:32 [INFO]: epoch 521: training loss 0.0346\n",
      "2025-06-01 14:50:32 [INFO]: epoch 522: training loss 0.0276\n",
      "2025-06-01 14:50:32 [INFO]: epoch 523: training loss 0.0361\n",
      "2025-06-01 14:50:32 [INFO]: epoch 524: training loss 0.0327\n",
      "2025-06-01 14:50:32 [INFO]: epoch 525: training loss 0.0211\n",
      "2025-06-01 14:50:32 [INFO]: epoch 526: training loss 0.0292\n",
      "2025-06-01 14:50:32 [INFO]: epoch 527: training loss 0.0237\n",
      "2025-06-01 14:50:32 [INFO]: epoch 528: training loss 0.0226\n",
      "2025-06-01 14:50:32 [INFO]: epoch 529: training loss 0.0253\n",
      "2025-06-01 14:50:32 [INFO]: epoch 530: training loss 0.0260\n",
      "2025-06-01 14:50:32 [INFO]: epoch 531: training loss 0.0235\n",
      "2025-06-01 14:50:32 [INFO]: epoch 532: training loss 0.0245\n",
      "2025-06-01 14:50:32 [INFO]: epoch 533: training loss 0.0275\n",
      "2025-06-01 14:50:32 [INFO]: epoch 534: training loss 0.0244\n",
      "2025-06-01 14:50:32 [INFO]: epoch 535: training loss 0.0271\n",
      "2025-06-01 14:50:32 [INFO]: epoch 536: training loss 0.0271\n",
      "2025-06-01 14:50:33 [INFO]: epoch 537: training loss 0.0220\n",
      "2025-06-01 14:50:33 [INFO]: epoch 538: training loss 0.0245\n",
      "2025-06-01 14:50:33 [INFO]: epoch 539: training loss 0.0265\n",
      "2025-06-01 14:50:33 [INFO]: epoch 540: training loss 0.0252\n",
      "2025-06-01 14:50:33 [INFO]: epoch 541: training loss 0.0248\n",
      "2025-06-01 14:50:33 [INFO]: epoch 542: training loss 0.0246\n",
      "2025-06-01 14:50:33 [INFO]: epoch 543: training loss 0.0243\n",
      "2025-06-01 14:50:33 [INFO]: epoch 544: training loss 0.0245\n",
      "2025-06-01 14:50:33 [INFO]: epoch 545: training loss 0.0251\n",
      "2025-06-01 14:50:33 [INFO]: epoch 546: training loss 0.0255\n",
      "2025-06-01 14:50:33 [INFO]: epoch 547: training loss 0.0233\n",
      "2025-06-01 14:50:33 [INFO]: epoch 548: training loss 0.0253\n",
      "2025-06-01 14:50:33 [INFO]: epoch 549: training loss 0.0204\n",
      "2025-06-01 14:50:33 [INFO]: epoch 550: training loss 0.0258\n",
      "2025-06-01 14:50:33 [INFO]: epoch 551: training loss 0.0246\n",
      "2025-06-01 14:50:33 [INFO]: epoch 552: training loss 0.0252\n",
      "2025-06-01 14:50:33 [INFO]: epoch 553: training loss 0.0271\n",
      "2025-06-01 14:50:33 [INFO]: epoch 554: training loss 0.0254\n",
      "2025-06-01 14:50:33 [INFO]: epoch 555: training loss 0.0271\n",
      "2025-06-01 14:50:33 [INFO]: epoch 556: training loss 0.0304\n",
      "2025-06-01 14:50:33 [INFO]: epoch 557: training loss 0.0252\n",
      "2025-06-01 14:50:33 [INFO]: epoch 558: training loss 0.0334\n",
      "2025-06-01 14:50:33 [INFO]: epoch 559: training loss 0.0284\n",
      "2025-06-01 14:50:33 [INFO]: epoch 560: training loss 0.0244\n",
      "2025-06-01 14:50:33 [INFO]: epoch 561: training loss 0.0334\n",
      "2025-06-01 14:50:33 [INFO]: epoch 562: training loss 0.0266\n",
      "2025-06-01 14:50:33 [INFO]: epoch 563: training loss 0.0297\n",
      "2025-06-01 14:50:33 [INFO]: epoch 564: training loss 0.0267\n",
      "2025-06-01 14:50:33 [INFO]: epoch 565: training loss 0.0248\n",
      "2025-06-01 14:50:33 [INFO]: epoch 566: training loss 0.0275\n",
      "2025-06-01 14:50:33 [INFO]: epoch 567: training loss 0.0298\n",
      "2025-06-01 14:50:33 [INFO]: epoch 568: training loss 0.0294\n",
      "2025-06-01 14:50:33 [INFO]: epoch 569: training loss 0.0322\n",
      "2025-06-01 14:50:33 [INFO]: epoch 570: training loss 0.0280\n",
      "2025-06-01 14:50:33 [INFO]: epoch 571: training loss 0.0326\n",
      "2025-06-01 14:50:33 [INFO]: epoch 572: training loss 0.0332\n",
      "2025-06-01 14:50:33 [INFO]: epoch 573: training loss 0.0323\n",
      "2025-06-01 14:50:33 [INFO]: epoch 574: training loss 0.0247\n",
      "2025-06-01 14:50:33 [INFO]: epoch 575: training loss 0.0328\n",
      "2025-06-01 14:50:33 [INFO]: epoch 576: training loss 0.0264\n",
      "2025-06-01 14:50:33 [INFO]: epoch 577: training loss 0.0261\n",
      "2025-06-01 14:50:33 [INFO]: epoch 578: training loss 0.0293\n",
      "2025-06-01 14:50:33 [INFO]: epoch 579: training loss 0.0233\n",
      "2025-06-01 14:50:33 [INFO]: epoch 580: training loss 0.0238\n",
      "2025-06-01 14:50:33 [INFO]: epoch 581: training loss 0.0269\n",
      "2025-06-01 14:50:33 [INFO]: epoch 582: training loss 0.0242\n",
      "2025-06-01 14:50:33 [INFO]: epoch 583: training loss 0.0246\n",
      "2025-06-01 14:50:33 [INFO]: epoch 584: training loss 0.0253\n",
      "2025-06-01 14:50:33 [INFO]: epoch 585: training loss 0.0249\n",
      "2025-06-01 14:50:33 [INFO]: epoch 586: training loss 0.0238\n",
      "2025-06-01 14:50:33 [INFO]: epoch 587: training loss 0.0209\n",
      "2025-06-01 14:50:33 [INFO]: epoch 588: training loss 0.0237\n",
      "2025-06-01 14:50:33 [INFO]: epoch 589: training loss 0.0272\n",
      "2025-06-01 14:50:33 [INFO]: epoch 590: training loss 0.0281\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-01 14:50:33 [INFO]: epoch 591: training loss 0.0234\n",
      "2025-06-01 14:50:33 [INFO]: epoch 592: training loss 0.0225\n",
      "2025-06-01 14:50:33 [INFO]: epoch 593: training loss 0.0252\n",
      "2025-06-01 14:50:33 [INFO]: epoch 594: training loss 0.0231\n",
      "2025-06-01 14:50:33 [INFO]: epoch 595: training loss 0.0274\n",
      "2025-06-01 14:50:33 [INFO]: epoch 596: training loss 0.0248\n",
      "2025-06-01 14:50:33 [INFO]: epoch 597: training loss 0.0235\n",
      "2025-06-01 14:50:33 [INFO]: epoch 598: training loss 0.0256\n",
      "2025-06-01 14:50:33 [INFO]: epoch 599: training loss 0.0290\n",
      "2025-06-01 14:50:33 [INFO]: epoch 600: training loss 0.0205\n",
      "2025-06-01 14:50:33 [INFO]: epoch 601: training loss 0.0232\n",
      "2025-06-01 14:50:33 [INFO]: epoch 602: training loss 0.0255\n",
      "2025-06-01 14:50:33 [INFO]: epoch 603: training loss 0.0283\n",
      "2025-06-01 14:50:33 [INFO]: epoch 604: training loss 0.0276\n",
      "2025-06-01 14:50:33 [INFO]: epoch 605: training loss 0.0319\n",
      "2025-06-01 14:50:33 [INFO]: epoch 606: training loss 0.0222\n",
      "2025-06-01 14:50:33 [INFO]: epoch 607: training loss 0.0282\n",
      "2025-06-01 14:50:33 [INFO]: epoch 608: training loss 0.0304\n",
      "2025-06-01 14:50:33 [INFO]: epoch 609: training loss 0.0243\n",
      "2025-06-01 14:50:33 [INFO]: epoch 610: training loss 0.0281\n",
      "2025-06-01 14:50:33 [INFO]: epoch 611: training loss 0.0270\n",
      "2025-06-01 14:50:33 [INFO]: epoch 612: training loss 0.0288\n",
      "2025-06-01 14:50:34 [INFO]: epoch 613: training loss 0.0298\n",
      "2025-06-01 14:50:34 [INFO]: epoch 614: training loss 0.0215\n",
      "2025-06-01 14:50:34 [INFO]: epoch 615: training loss 0.0234\n",
      "2025-06-01 14:50:34 [INFO]: epoch 616: training loss 0.0238\n",
      "2025-06-01 14:50:34 [INFO]: epoch 617: training loss 0.0250\n",
      "2025-06-01 14:50:34 [INFO]: epoch 618: training loss 0.0254\n",
      "2025-06-01 14:50:34 [INFO]: epoch 619: training loss 0.0255\n",
      "2025-06-01 14:50:34 [INFO]: epoch 620: training loss 0.0241\n",
      "2025-06-01 14:50:34 [INFO]: epoch 621: training loss 0.0235\n",
      "2025-06-01 14:50:34 [INFO]: epoch 622: training loss 0.0228\n",
      "2025-06-01 14:50:34 [INFO]: epoch 623: training loss 0.0281\n",
      "2025-06-01 14:50:34 [INFO]: epoch 624: training loss 0.0285\n",
      "2025-06-01 14:50:34 [INFO]: epoch 625: training loss 0.0272\n",
      "2025-06-01 14:50:34 [INFO]: epoch 626: training loss 0.0242\n",
      "2025-06-01 14:50:34 [INFO]: epoch 627: training loss 0.0262\n",
      "2025-06-01 14:50:34 [INFO]: epoch 628: training loss 0.0293\n",
      "2025-06-01 14:50:34 [INFO]: epoch 629: training loss 0.0245\n",
      "2025-06-01 14:50:34 [INFO]: epoch 630: training loss 0.0273\n",
      "2025-06-01 14:50:34 [INFO]: epoch 631: training loss 0.0271\n",
      "2025-06-01 14:50:34 [INFO]: epoch 632: training loss 0.0285\n",
      "2025-06-01 14:50:34 [INFO]: epoch 633: training loss 0.0283\n",
      "2025-06-01 14:50:34 [INFO]: epoch 634: training loss 0.0247\n",
      "2025-06-01 14:50:34 [INFO]: epoch 635: training loss 0.0270\n",
      "2025-06-01 14:50:34 [INFO]: epoch 636: training loss 0.0224\n",
      "2025-06-01 14:50:34 [INFO]: epoch 637: training loss 0.0248\n",
      "2025-06-01 14:50:34 [INFO]: epoch 638: training loss 0.0298\n",
      "2025-06-01 14:50:34 [INFO]: epoch 639: training loss 0.0232\n",
      "2025-06-01 14:50:34 [INFO]: epoch 640: training loss 0.0239\n",
      "2025-06-01 14:50:34 [INFO]: epoch 641: training loss 0.0291\n",
      "2025-06-01 14:50:34 [INFO]: epoch 642: training loss 0.0210\n",
      "2025-06-01 14:50:34 [INFO]: epoch 643: training loss 0.0197\n",
      "2025-06-01 14:50:34 [INFO]: epoch 644: training loss 0.0255\n",
      "2025-06-01 14:50:34 [INFO]: epoch 645: training loss 0.0230\n",
      "2025-06-01 14:50:34 [INFO]: epoch 646: training loss 0.0242\n",
      "2025-06-01 14:50:34 [INFO]: epoch 647: training loss 0.0216\n",
      "2025-06-01 14:50:34 [INFO]: epoch 648: training loss 0.0243\n",
      "2025-06-01 14:50:34 [INFO]: epoch 649: training loss 0.0255\n",
      "2025-06-01 14:50:34 [INFO]: epoch 650: training loss 0.0214\n",
      "2025-06-01 14:50:34 [INFO]: epoch 651: training loss 0.0210\n",
      "2025-06-01 14:50:34 [INFO]: epoch 652: training loss 0.0217\n",
      "2025-06-01 14:50:34 [INFO]: epoch 653: training loss 0.0246\n",
      "2025-06-01 14:50:34 [INFO]: epoch 654: training loss 0.0239\n",
      "2025-06-01 14:50:34 [INFO]: epoch 655: training loss 0.0227\n",
      "2025-06-01 14:50:34 [INFO]: epoch 656: training loss 0.0241\n",
      "2025-06-01 14:50:34 [INFO]: epoch 657: training loss 0.0257\n",
      "2025-06-01 14:50:34 [INFO]: epoch 658: training loss 0.0241\n",
      "2025-06-01 14:50:34 [INFO]: epoch 659: training loss 0.0194\n",
      "2025-06-01 14:50:34 [INFO]: epoch 660: training loss 0.0239\n",
      "2025-06-01 14:50:34 [INFO]: epoch 661: training loss 0.0206\n",
      "2025-06-01 14:50:34 [INFO]: epoch 662: training loss 0.0221\n",
      "2025-06-01 14:50:34 [INFO]: epoch 663: training loss 0.0207\n",
      "2025-06-01 14:50:34 [INFO]: epoch 664: training loss 0.0218\n",
      "2025-06-01 14:50:34 [INFO]: epoch 665: training loss 0.0206\n",
      "2025-06-01 14:50:34 [INFO]: epoch 666: training loss 0.0182\n",
      "2025-06-01 14:50:34 [INFO]: epoch 667: training loss 0.0217\n",
      "2025-06-01 14:50:34 [INFO]: epoch 668: training loss 0.0195\n",
      "2025-06-01 14:50:34 [INFO]: epoch 669: training loss 0.0231\n",
      "2025-06-01 14:50:34 [INFO]: epoch 670: training loss 0.0248\n",
      "2025-06-01 14:50:34 [INFO]: epoch 671: training loss 0.0259\n",
      "2025-06-01 14:50:34 [INFO]: epoch 672: training loss 0.0285\n",
      "2025-06-01 14:50:34 [INFO]: epoch 673: training loss 0.0238\n",
      "2025-06-01 14:50:34 [INFO]: epoch 674: training loss 0.0249\n",
      "2025-06-01 14:50:34 [INFO]: epoch 675: training loss 0.0257\n",
      "2025-06-01 14:50:34 [INFO]: epoch 676: training loss 0.0202\n",
      "2025-06-01 14:50:34 [INFO]: epoch 677: training loss 0.0241\n",
      "2025-06-01 14:50:34 [INFO]: epoch 678: training loss 0.0240\n",
      "2025-06-01 14:50:34 [INFO]: epoch 679: training loss 0.0223\n",
      "2025-06-01 14:50:34 [INFO]: epoch 680: training loss 0.0200\n",
      "2025-06-01 14:50:34 [INFO]: epoch 681: training loss 0.0228\n",
      "2025-06-01 14:50:34 [INFO]: epoch 682: training loss 0.0177\n",
      "2025-06-01 14:50:34 [INFO]: epoch 683: training loss 0.0228\n",
      "2025-06-01 14:50:34 [INFO]: epoch 684: training loss 0.0233\n",
      "2025-06-01 14:50:34 [INFO]: epoch 685: training loss 0.0263\n",
      "2025-06-01 14:50:34 [INFO]: epoch 686: training loss 0.0204\n",
      "2025-06-01 14:50:34 [INFO]: epoch 687: training loss 0.0201\n",
      "2025-06-01 14:50:34 [INFO]: epoch 688: training loss 0.0246\n",
      "2025-06-01 14:50:34 [INFO]: epoch 689: training loss 0.0207\n",
      "2025-06-01 14:50:35 [INFO]: epoch 690: training loss 0.0207\n",
      "2025-06-01 14:50:35 [INFO]: epoch 691: training loss 0.0196\n",
      "2025-06-01 14:50:35 [INFO]: epoch 692: training loss 0.0230\n",
      "2025-06-01 14:50:35 [INFO]: epoch 693: training loss 0.0231\n",
      "2025-06-01 14:50:35 [INFO]: epoch 694: training loss 0.0259\n",
      "2025-06-01 14:50:35 [INFO]: epoch 695: training loss 0.0232\n",
      "2025-06-01 14:50:35 [INFO]: epoch 696: training loss 0.0209\n",
      "2025-06-01 14:50:35 [INFO]: epoch 697: training loss 0.0239\n",
      "2025-06-01 14:50:35 [INFO]: epoch 698: training loss 0.0235\n",
      "2025-06-01 14:50:35 [INFO]: epoch 699: training loss 0.0215\n",
      "2025-06-01 14:50:35 [INFO]: epoch 700: training loss 0.0206\n",
      "2025-06-01 14:50:35 [INFO]: epoch 701: training loss 0.0193\n",
      "2025-06-01 14:50:35 [INFO]: epoch 702: training loss 0.0213\n",
      "2025-06-01 14:50:35 [INFO]: epoch 703: training loss 0.0207\n",
      "2025-06-01 14:50:35 [INFO]: epoch 704: training loss 0.0208\n",
      "2025-06-01 14:50:35 [INFO]: epoch 705: training loss 0.0259\n",
      "2025-06-01 14:50:35 [INFO]: epoch 706: training loss 0.0218\n",
      "2025-06-01 14:50:35 [INFO]: epoch 707: training loss 0.0274\n",
      "2025-06-01 14:50:35 [INFO]: epoch 708: training loss 0.0219\n",
      "2025-06-01 14:50:35 [INFO]: epoch 709: training loss 0.0283\n",
      "2025-06-01 14:50:35 [INFO]: epoch 710: training loss 0.0219\n",
      "2025-06-01 14:50:35 [INFO]: epoch 711: training loss 0.0244\n",
      "2025-06-01 14:50:35 [INFO]: epoch 712: training loss 0.0294\n",
      "2025-06-01 14:50:35 [INFO]: epoch 713: training loss 0.0268\n",
      "2025-06-01 14:50:35 [INFO]: epoch 714: training loss 0.0209\n",
      "2025-06-01 14:50:35 [INFO]: epoch 715: training loss 0.0228\n",
      "2025-06-01 14:50:35 [INFO]: epoch 716: training loss 0.0252\n",
      "2025-06-01 14:50:35 [INFO]: epoch 717: training loss 0.0209\n",
      "2025-06-01 14:50:35 [INFO]: epoch 718: training loss 0.0233\n",
      "2025-06-01 14:50:35 [INFO]: epoch 719: training loss 0.0297\n",
      "2025-06-01 14:50:35 [INFO]: epoch 720: training loss 0.0218\n",
      "2025-06-01 14:50:35 [INFO]: epoch 721: training loss 0.0214\n",
      "2025-06-01 14:50:35 [INFO]: epoch 722: training loss 0.0206\n",
      "2025-06-01 14:50:35 [INFO]: epoch 723: training loss 0.0248\n",
      "2025-06-01 14:50:35 [INFO]: epoch 724: training loss 0.0260\n",
      "2025-06-01 14:50:35 [INFO]: epoch 725: training loss 0.0227\n",
      "2025-06-01 14:50:35 [INFO]: epoch 726: training loss 0.0211\n",
      "2025-06-01 14:50:35 [INFO]: epoch 727: training loss 0.0192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-01 14:50:35 [INFO]: epoch 728: training loss 0.0183\n",
      "2025-06-01 14:50:35 [INFO]: epoch 729: training loss 0.0269\n",
      "2025-06-01 14:50:35 [INFO]: epoch 730: training loss 0.0234\n",
      "2025-06-01 14:50:35 [INFO]: epoch 731: training loss 0.0322\n",
      "2025-06-01 14:50:35 [INFO]: epoch 732: training loss 0.0312\n",
      "2025-06-01 14:50:35 [INFO]: epoch 733: training loss 0.0229\n",
      "2025-06-01 14:50:35 [INFO]: epoch 734: training loss 0.0245\n",
      "2025-06-01 14:50:35 [INFO]: epoch 735: training loss 0.0259\n",
      "2025-06-01 14:50:35 [INFO]: epoch 736: training loss 0.0187\n",
      "2025-06-01 14:50:35 [INFO]: epoch 737: training loss 0.0377\n",
      "2025-06-01 14:50:35 [INFO]: epoch 738: training loss 0.0252\n",
      "2025-06-01 14:50:35 [INFO]: epoch 739: training loss 0.0247\n",
      "2025-06-01 14:50:35 [INFO]: epoch 740: training loss 0.0247\n",
      "2025-06-01 14:50:35 [INFO]: epoch 741: training loss 0.0216\n",
      "2025-06-01 14:50:35 [INFO]: epoch 742: training loss 0.0213\n",
      "2025-06-01 14:50:35 [INFO]: epoch 743: training loss 0.0260\n",
      "2025-06-01 14:50:35 [INFO]: epoch 744: training loss 0.0212\n",
      "2025-06-01 14:50:35 [INFO]: epoch 745: training loss 0.0265\n",
      "2025-06-01 14:50:35 [INFO]: epoch 746: training loss 0.0267\n",
      "2025-06-01 14:50:35 [INFO]: epoch 747: training loss 0.0202\n",
      "2025-06-01 14:50:35 [INFO]: epoch 748: training loss 0.0232\n",
      "2025-06-01 14:50:35 [INFO]: epoch 749: training loss 0.0221\n",
      "2025-06-01 14:50:35 [INFO]: epoch 750: training loss 0.0201\n",
      "2025-06-01 14:50:35 [INFO]: epoch 751: training loss 0.0218\n",
      "2025-06-01 14:50:35 [INFO]: epoch 752: training loss 0.0198\n",
      "2025-06-01 14:50:35 [INFO]: epoch 753: training loss 0.0243\n",
      "2025-06-01 14:50:35 [INFO]: epoch 754: training loss 0.0214\n",
      "2025-06-01 14:50:35 [INFO]: epoch 755: training loss 0.0192\n",
      "2025-06-01 14:50:35 [INFO]: epoch 756: training loss 0.0212\n",
      "2025-06-01 14:50:35 [INFO]: epoch 757: training loss 0.0188\n",
      "2025-06-01 14:50:35 [INFO]: epoch 758: training loss 0.0192\n",
      "2025-06-01 14:50:35 [INFO]: epoch 759: training loss 0.0247\n",
      "2025-06-01 14:50:35 [INFO]: epoch 760: training loss 0.0265\n",
      "2025-06-01 14:50:35 [INFO]: epoch 761: training loss 0.0255\n",
      "2025-06-01 14:50:35 [INFO]: epoch 762: training loss 0.0224\n",
      "2025-06-01 14:50:35 [INFO]: epoch 763: training loss 0.0222\n",
      "2025-06-01 14:50:35 [INFO]: epoch 764: training loss 0.0217\n",
      "2025-06-01 14:50:35 [INFO]: epoch 765: training loss 0.0209\n",
      "2025-06-01 14:50:35 [INFO]: epoch 766: training loss 0.0211\n",
      "2025-06-01 14:50:36 [INFO]: epoch 767: training loss 0.0210\n",
      "2025-06-01 14:50:36 [INFO]: epoch 768: training loss 0.0208\n",
      "2025-06-01 14:50:36 [INFO]: epoch 769: training loss 0.0226\n",
      "2025-06-01 14:50:36 [INFO]: epoch 770: training loss 0.0203\n",
      "2025-06-01 14:50:36 [INFO]: epoch 771: training loss 0.0226\n",
      "2025-06-01 14:50:36 [INFO]: epoch 772: training loss 0.0219\n",
      "2025-06-01 14:50:36 [INFO]: epoch 773: training loss 0.0220\n",
      "2025-06-01 14:50:36 [INFO]: epoch 774: training loss 0.0201\n",
      "2025-06-01 14:50:36 [INFO]: epoch 775: training loss 0.0235\n",
      "2025-06-01 14:50:36 [INFO]: epoch 776: training loss 0.0211\n",
      "2025-06-01 14:50:36 [INFO]: epoch 777: training loss 0.0193\n",
      "2025-06-01 14:50:36 [INFO]: epoch 778: training loss 0.0217\n",
      "2025-06-01 14:50:36 [INFO]: epoch 779: training loss 0.0217\n",
      "2025-06-01 14:50:36 [INFO]: epoch 780: training loss 0.0220\n",
      "2025-06-01 14:50:36 [INFO]: epoch 781: training loss 0.0229\n",
      "2025-06-01 14:50:36 [INFO]: epoch 782: training loss 0.0185\n",
      "2025-06-01 14:50:36 [INFO]: epoch 783: training loss 0.0212\n",
      "2025-06-01 14:50:36 [INFO]: epoch 784: training loss 0.0203\n",
      "2025-06-01 14:50:36 [INFO]: epoch 785: training loss 0.0237\n",
      "2025-06-01 14:50:36 [INFO]: epoch 786: training loss 0.0208\n",
      "2025-06-01 14:50:36 [INFO]: epoch 787: training loss 0.0192\n",
      "2025-06-01 14:50:36 [INFO]: epoch 788: training loss 0.0183\n",
      "2025-06-01 14:50:36 [INFO]: epoch 789: training loss 0.0204\n",
      "2025-06-01 14:50:36 [INFO]: epoch 790: training loss 0.0210\n",
      "2025-06-01 14:50:36 [INFO]: epoch 791: training loss 0.0192\n",
      "2025-06-01 14:50:36 [INFO]: epoch 792: training loss 0.0211\n",
      "2025-06-01 14:50:36 [INFO]: epoch 793: training loss 0.0223\n",
      "2025-06-01 14:50:36 [INFO]: epoch 794: training loss 0.0197\n",
      "2025-06-01 14:50:36 [INFO]: epoch 795: training loss 0.0212\n",
      "2025-06-01 14:50:36 [INFO]: epoch 796: training loss 0.0197\n",
      "2025-06-01 14:50:36 [INFO]: epoch 797: training loss 0.0208\n",
      "2025-06-01 14:50:36 [INFO]: epoch 798: training loss 0.0208\n",
      "2025-06-01 14:50:36 [INFO]: epoch 799: training loss 0.0187\n",
      "2025-06-01 14:50:36 [INFO]: Finished training.\n",
      "2025-06-01 14:50:36 [WARNING]: 🚨DeprecationWarning: The method impute is deprecated. Please use `predict` instead.\n",
      " 50%|██████████████████████████████████████████                                          | 3/6 [00:31<00:31, 10.56s/it]2025-06-01 14:50:36 [INFO]: No given device, using default device: cuda\n",
      "2025-06-01 14:50:36 [WARNING]: saving_path not given. Model files and tensorboard file will not be saved.\n",
      "2025-06-01 14:50:36 [INFO]: Model initialized successfully with the number of trainable parameters: 597,780\n",
      "2025-06-01 14:50:36 [INFO]: epoch 0: training loss 1.0958\n",
      "2025-06-01 14:50:36 [INFO]: epoch 1: training loss 0.6954\n",
      "2025-06-01 14:50:36 [INFO]: epoch 2: training loss 0.7202\n",
      "2025-06-01 14:50:36 [INFO]: epoch 3: training loss 0.6266\n",
      "2025-06-01 14:50:36 [INFO]: epoch 4: training loss 0.5628\n",
      "2025-06-01 14:50:36 [INFO]: epoch 5: training loss 0.5280\n",
      "2025-06-01 14:50:36 [INFO]: epoch 6: training loss 0.4715\n",
      "2025-06-01 14:50:36 [INFO]: epoch 7: training loss 0.4161\n",
      "2025-06-01 14:50:36 [INFO]: epoch 8: training loss 0.4213\n",
      "2025-06-01 14:50:36 [INFO]: epoch 9: training loss 0.4161\n",
      "2025-06-01 14:50:36 [INFO]: epoch 10: training loss 0.4025\n",
      "2025-06-01 14:50:36 [INFO]: epoch 11: training loss 0.3590\n",
      "2025-06-01 14:50:36 [INFO]: epoch 12: training loss 0.3214\n",
      "2025-06-01 14:50:36 [INFO]: epoch 13: training loss 0.2933\n",
      "2025-06-01 14:50:36 [INFO]: epoch 14: training loss 0.3379\n",
      "2025-06-01 14:50:36 [INFO]: epoch 15: training loss 0.3379\n",
      "2025-06-01 14:50:36 [INFO]: epoch 16: training loss 0.3085\n",
      "2025-06-01 14:50:36 [INFO]: epoch 17: training loss 0.3220\n",
      "2025-06-01 14:50:36 [INFO]: epoch 18: training loss 0.3222\n",
      "2025-06-01 14:50:36 [INFO]: epoch 19: training loss 0.2959\n",
      "2025-06-01 14:50:36 [INFO]: epoch 20: training loss 0.2723\n",
      "2025-06-01 14:50:36 [INFO]: epoch 21: training loss 0.3012\n",
      "2025-06-01 14:50:36 [INFO]: epoch 22: training loss 0.2902\n",
      "2025-06-01 14:50:36 [INFO]: epoch 23: training loss 0.2708\n",
      "2025-06-01 14:50:36 [INFO]: epoch 24: training loss 0.2841\n",
      "2025-06-01 14:50:36 [INFO]: epoch 25: training loss 0.2845\n",
      "2025-06-01 14:50:36 [INFO]: epoch 26: training loss 0.2709\n",
      "2025-06-01 14:50:36 [INFO]: epoch 27: training loss 0.2958\n",
      "2025-06-01 14:50:36 [INFO]: epoch 28: training loss 0.2716\n",
      "2025-06-01 14:50:36 [INFO]: epoch 29: training loss 0.2455\n",
      "2025-06-01 14:50:36 [INFO]: epoch 30: training loss 0.2415\n",
      "2025-06-01 14:50:36 [INFO]: epoch 31: training loss 0.2712\n",
      "2025-06-01 14:50:36 [INFO]: epoch 32: training loss 0.2776\n",
      "2025-06-01 14:50:36 [INFO]: epoch 33: training loss 0.2560\n",
      "2025-06-01 14:50:36 [INFO]: epoch 34: training loss 0.2903\n",
      "2025-06-01 14:50:36 [INFO]: epoch 35: training loss 0.2476\n",
      "2025-06-01 14:50:37 [INFO]: epoch 36: training loss 0.2599\n",
      "2025-06-01 14:50:37 [INFO]: epoch 37: training loss 0.2291\n",
      "2025-06-01 14:50:37 [INFO]: epoch 38: training loss 0.2308\n",
      "2025-06-01 14:50:37 [INFO]: epoch 39: training loss 0.2293\n",
      "2025-06-01 14:50:37 [INFO]: epoch 40: training loss 0.2245\n",
      "2025-06-01 14:50:37 [INFO]: epoch 41: training loss 0.2438\n",
      "2025-06-01 14:50:37 [INFO]: epoch 42: training loss 0.2203\n",
      "2025-06-01 14:50:37 [INFO]: epoch 43: training loss 0.2146\n",
      "2025-06-01 14:50:37 [INFO]: epoch 44: training loss 0.2436\n",
      "2025-06-01 14:50:37 [INFO]: epoch 45: training loss 0.2221\n",
      "2025-06-01 14:50:37 [INFO]: epoch 46: training loss 0.2272\n",
      "2025-06-01 14:50:37 [INFO]: epoch 47: training loss 0.2445\n",
      "2025-06-01 14:50:37 [INFO]: epoch 48: training loss 0.2303\n",
      "2025-06-01 14:50:37 [INFO]: epoch 49: training loss 0.2016\n",
      "2025-06-01 14:50:37 [INFO]: epoch 50: training loss 0.2232\n",
      "2025-06-01 14:50:37 [INFO]: epoch 51: training loss 0.2063\n",
      "2025-06-01 14:50:37 [INFO]: epoch 52: training loss 0.1924\n",
      "2025-06-01 14:50:37 [INFO]: epoch 53: training loss 0.2154\n",
      "2025-06-01 14:50:37 [INFO]: epoch 54: training loss 0.1819\n",
      "2025-06-01 14:50:37 [INFO]: epoch 55: training loss 0.1871\n",
      "2025-06-01 14:50:37 [INFO]: epoch 56: training loss 0.1938\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-01 14:50:37 [INFO]: epoch 57: training loss 0.1804\n",
      "2025-06-01 14:50:37 [INFO]: epoch 58: training loss 0.1903\n",
      "2025-06-01 14:50:37 [INFO]: epoch 59: training loss 0.1993\n",
      "2025-06-01 14:50:37 [INFO]: epoch 60: training loss 0.2032\n",
      "2025-06-01 14:50:37 [INFO]: epoch 61: training loss 0.1856\n",
      "2025-06-01 14:50:37 [INFO]: epoch 62: training loss 0.2073\n",
      "2025-06-01 14:50:37 [INFO]: epoch 63: training loss 0.1896\n",
      "2025-06-01 14:50:37 [INFO]: epoch 64: training loss 0.1809\n",
      "2025-06-01 14:50:37 [INFO]: epoch 65: training loss 0.1731\n",
      "2025-06-01 14:50:37 [INFO]: epoch 66: training loss 0.1606\n",
      "2025-06-01 14:50:37 [INFO]: epoch 67: training loss 0.1695\n",
      "2025-06-01 14:50:37 [INFO]: epoch 68: training loss 0.1641\n",
      "2025-06-01 14:50:37 [INFO]: epoch 69: training loss 0.1674\n",
      "2025-06-01 14:50:37 [INFO]: epoch 70: training loss 0.1582\n",
      "2025-06-01 14:50:37 [INFO]: epoch 71: training loss 0.1619\n",
      "2025-06-01 14:50:37 [INFO]: epoch 72: training loss 0.1609\n",
      "2025-06-01 14:50:37 [INFO]: epoch 73: training loss 0.1802\n",
      "2025-06-01 14:50:37 [INFO]: epoch 74: training loss 0.1591\n",
      "2025-06-01 14:50:37 [INFO]: epoch 75: training loss 0.1626\n",
      "2025-06-01 14:50:37 [INFO]: epoch 76: training loss 0.1670\n",
      "2025-06-01 14:50:37 [INFO]: epoch 77: training loss 0.1629\n",
      "2025-06-01 14:50:37 [INFO]: epoch 78: training loss 0.1523\n",
      "2025-06-01 14:50:37 [INFO]: epoch 79: training loss 0.1499\n",
      "2025-06-01 14:50:37 [INFO]: epoch 80: training loss 0.1734\n",
      "2025-06-01 14:50:37 [INFO]: epoch 81: training loss 0.1666\n",
      "2025-06-01 14:50:37 [INFO]: epoch 82: training loss 0.1649\n",
      "2025-06-01 14:50:37 [INFO]: epoch 83: training loss 0.1470\n",
      "2025-06-01 14:50:37 [INFO]: epoch 84: training loss 0.1597\n",
      "2025-06-01 14:50:37 [INFO]: epoch 85: training loss 0.1575\n",
      "2025-06-01 14:50:37 [INFO]: epoch 86: training loss 0.1798\n",
      "2025-06-01 14:50:37 [INFO]: epoch 87: training loss 0.1546\n",
      "2025-06-01 14:50:37 [INFO]: epoch 88: training loss 0.1460\n",
      "2025-06-01 14:50:37 [INFO]: epoch 89: training loss 0.1431\n",
      "2025-06-01 14:50:37 [INFO]: epoch 90: training loss 0.1551\n",
      "2025-06-01 14:50:37 [INFO]: epoch 91: training loss 0.1583\n",
      "2025-06-01 14:50:37 [INFO]: epoch 92: training loss 0.1602\n",
      "2025-06-01 14:50:37 [INFO]: epoch 93: training loss 0.1446\n",
      "2025-06-01 14:50:37 [INFO]: epoch 94: training loss 0.1533\n",
      "2025-06-01 14:50:37 [INFO]: epoch 95: training loss 0.1520\n",
      "2025-06-01 14:50:37 [INFO]: epoch 96: training loss 0.1522\n",
      "2025-06-01 14:50:37 [INFO]: epoch 97: training loss 0.1513\n",
      "2025-06-01 14:50:37 [INFO]: epoch 98: training loss 0.1386\n",
      "2025-06-01 14:50:37 [INFO]: epoch 99: training loss 0.1378\n",
      "2025-06-01 14:50:37 [INFO]: epoch 100: training loss 0.1372\n",
      "2025-06-01 14:50:37 [INFO]: epoch 101: training loss 0.1545\n",
      "2025-06-01 14:50:37 [INFO]: epoch 102: training loss 0.1372\n",
      "2025-06-01 14:50:37 [INFO]: epoch 103: training loss 0.1358\n",
      "2025-06-01 14:50:37 [INFO]: epoch 104: training loss 0.1462\n",
      "2025-06-01 14:50:37 [INFO]: epoch 105: training loss 0.1400\n",
      "2025-06-01 14:50:37 [INFO]: epoch 106: training loss 0.1233\n",
      "2025-06-01 14:50:37 [INFO]: epoch 107: training loss 0.1565\n",
      "2025-06-01 14:50:37 [INFO]: epoch 108: training loss 0.1246\n",
      "2025-06-01 14:50:37 [INFO]: epoch 109: training loss 0.1330\n",
      "2025-06-01 14:50:37 [INFO]: epoch 110: training loss 0.1348\n",
      "2025-06-01 14:50:37 [INFO]: epoch 111: training loss 0.1413\n",
      "2025-06-01 14:50:38 [INFO]: epoch 112: training loss 0.1272\n",
      "2025-06-01 14:50:38 [INFO]: epoch 113: training loss 0.1106\n",
      "2025-06-01 14:50:38 [INFO]: epoch 114: training loss 0.1408\n",
      "2025-06-01 14:50:38 [INFO]: epoch 115: training loss 0.1375\n",
      "2025-06-01 14:50:38 [INFO]: epoch 116: training loss 0.1268\n",
      "2025-06-01 14:50:38 [INFO]: epoch 117: training loss 0.1231\n",
      "2025-06-01 14:50:38 [INFO]: epoch 118: training loss 0.1171\n",
      "2025-06-01 14:50:38 [INFO]: epoch 119: training loss 0.1286\n",
      "2025-06-01 14:50:38 [INFO]: epoch 120: training loss 0.1185\n",
      "2025-06-01 14:50:38 [INFO]: epoch 121: training loss 0.1140\n",
      "2025-06-01 14:50:38 [INFO]: epoch 122: training loss 0.1261\n",
      "2025-06-01 14:50:38 [INFO]: epoch 123: training loss 0.1392\n",
      "2025-06-01 14:50:38 [INFO]: epoch 124: training loss 0.1265\n",
      "2025-06-01 14:50:38 [INFO]: epoch 125: training loss 0.1202\n",
      "2025-06-01 14:50:38 [INFO]: epoch 126: training loss 0.1044\n",
      "2025-06-01 14:50:38 [INFO]: epoch 127: training loss 0.1067\n",
      "2025-06-01 14:50:38 [INFO]: epoch 128: training loss 0.1030\n",
      "2025-06-01 14:50:38 [INFO]: epoch 129: training loss 0.1106\n",
      "2025-06-01 14:50:38 [INFO]: epoch 130: training loss 0.1104\n",
      "2025-06-01 14:50:38 [INFO]: epoch 131: training loss 0.1090\n",
      "2025-06-01 14:50:38 [INFO]: epoch 132: training loss 0.1130\n",
      "2025-06-01 14:50:38 [INFO]: epoch 133: training loss 0.1076\n",
      "2025-06-01 14:50:38 [INFO]: epoch 134: training loss 0.0976\n",
      "2025-06-01 14:50:38 [INFO]: epoch 135: training loss 0.1105\n",
      "2025-06-01 14:50:38 [INFO]: epoch 136: training loss 0.1118\n",
      "2025-06-01 14:50:38 [INFO]: epoch 137: training loss 0.1027\n",
      "2025-06-01 14:50:38 [INFO]: epoch 138: training loss 0.1042\n",
      "2025-06-01 14:50:38 [INFO]: epoch 139: training loss 0.1100\n",
      "2025-06-01 14:50:38 [INFO]: epoch 140: training loss 0.1057\n",
      "2025-06-01 14:50:38 [INFO]: epoch 141: training loss 0.1142\n",
      "2025-06-01 14:50:38 [INFO]: epoch 142: training loss 0.0959\n",
      "2025-06-01 14:50:38 [INFO]: epoch 143: training loss 0.1066\n",
      "2025-06-01 14:50:38 [INFO]: epoch 144: training loss 0.1191\n",
      "2025-06-01 14:50:38 [INFO]: epoch 145: training loss 0.1230\n",
      "2025-06-01 14:50:38 [INFO]: epoch 146: training loss 0.0906\n",
      "2025-06-01 14:50:38 [INFO]: epoch 147: training loss 0.1104\n",
      "2025-06-01 14:50:38 [INFO]: epoch 148: training loss 0.1069\n",
      "2025-06-01 14:50:38 [INFO]: epoch 149: training loss 0.0944\n",
      "2025-06-01 14:50:38 [INFO]: epoch 150: training loss 0.0907\n",
      "2025-06-01 14:50:38 [INFO]: epoch 151: training loss 0.0993\n",
      "2025-06-01 14:50:38 [INFO]: epoch 152: training loss 0.1204\n",
      "2025-06-01 14:50:38 [INFO]: epoch 153: training loss 0.1008\n",
      "2025-06-01 14:50:38 [INFO]: epoch 154: training loss 0.0917\n",
      "2025-06-01 14:50:38 [INFO]: epoch 155: training loss 0.0959\n",
      "2025-06-01 14:50:38 [INFO]: epoch 156: training loss 0.1032\n",
      "2025-06-01 14:50:38 [INFO]: epoch 157: training loss 0.0915\n",
      "2025-06-01 14:50:38 [INFO]: epoch 158: training loss 0.0867\n",
      "2025-06-01 14:50:38 [INFO]: epoch 159: training loss 0.1011\n",
      "2025-06-01 14:50:38 [INFO]: epoch 160: training loss 0.0952\n",
      "2025-06-01 14:50:38 [INFO]: epoch 161: training loss 0.0923\n",
      "2025-06-01 14:50:38 [INFO]: epoch 162: training loss 0.0984\n",
      "2025-06-01 14:50:38 [INFO]: epoch 163: training loss 0.0903\n",
      "2025-06-01 14:50:38 [INFO]: epoch 164: training loss 0.0970\n",
      "2025-06-01 14:50:38 [INFO]: epoch 165: training loss 0.0866\n",
      "2025-06-01 14:50:38 [INFO]: epoch 166: training loss 0.0965\n",
      "2025-06-01 14:50:38 [INFO]: epoch 167: training loss 0.0948\n",
      "2025-06-01 14:50:38 [INFO]: epoch 168: training loss 0.0858\n",
      "2025-06-01 14:50:38 [INFO]: epoch 169: training loss 0.0830\n",
      "2025-06-01 14:50:38 [INFO]: epoch 170: training loss 0.0845\n",
      "2025-06-01 14:50:38 [INFO]: epoch 171: training loss 0.0859\n",
      "2025-06-01 14:50:38 [INFO]: epoch 172: training loss 0.0908\n",
      "2025-06-01 14:50:38 [INFO]: epoch 173: training loss 0.0898\n",
      "2025-06-01 14:50:38 [INFO]: epoch 174: training loss 0.0857\n",
      "2025-06-01 14:50:38 [INFO]: epoch 175: training loss 0.0845\n",
      "2025-06-01 14:50:38 [INFO]: epoch 176: training loss 0.0953\n",
      "2025-06-01 14:50:38 [INFO]: epoch 177: training loss 0.0897\n",
      "2025-06-01 14:50:38 [INFO]: epoch 178: training loss 0.0848\n",
      "2025-06-01 14:50:38 [INFO]: epoch 179: training loss 0.0781\n",
      "2025-06-01 14:50:38 [INFO]: epoch 180: training loss 0.0825\n",
      "2025-06-01 14:50:38 [INFO]: epoch 181: training loss 0.0865\n",
      "2025-06-01 14:50:38 [INFO]: epoch 182: training loss 0.0783\n",
      "2025-06-01 14:50:38 [INFO]: epoch 183: training loss 0.0820\n",
      "2025-06-01 14:50:38 [INFO]: epoch 184: training loss 0.0824\n",
      "2025-06-01 14:50:38 [INFO]: epoch 185: training loss 0.0858\n",
      "2025-06-01 14:50:38 [INFO]: epoch 186: training loss 0.0749\n",
      "2025-06-01 14:50:38 [INFO]: epoch 187: training loss 0.0821\n",
      "2025-06-01 14:50:39 [INFO]: epoch 188: training loss 0.0862\n",
      "2025-06-01 14:50:39 [INFO]: epoch 189: training loss 0.0952\n",
      "2025-06-01 14:50:39 [INFO]: epoch 190: training loss 0.0842\n",
      "2025-06-01 14:50:39 [INFO]: epoch 191: training loss 0.0865\n",
      "2025-06-01 14:50:39 [INFO]: epoch 192: training loss 0.0860\n",
      "2025-06-01 14:50:39 [INFO]: epoch 193: training loss 0.0841\n",
      "2025-06-01 14:50:39 [INFO]: epoch 194: training loss 0.0767\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-01 14:50:39 [INFO]: epoch 195: training loss 0.0813\n",
      "2025-06-01 14:50:39 [INFO]: epoch 196: training loss 0.0796\n",
      "2025-06-01 14:50:39 [INFO]: epoch 197: training loss 0.0722\n",
      "2025-06-01 14:50:39 [INFO]: epoch 198: training loss 0.0683\n",
      "2025-06-01 14:50:39 [INFO]: epoch 199: training loss 0.0889\n",
      "2025-06-01 14:50:39 [INFO]: epoch 200: training loss 0.1014\n",
      "2025-06-01 14:50:39 [INFO]: epoch 201: training loss 0.0695\n",
      "2025-06-01 14:50:39 [INFO]: epoch 202: training loss 0.0679\n",
      "2025-06-01 14:50:39 [INFO]: epoch 203: training loss 0.0793\n",
      "2025-06-01 14:50:39 [INFO]: epoch 204: training loss 0.0902\n",
      "2025-06-01 14:50:39 [INFO]: epoch 205: training loss 0.0671\n",
      "2025-06-01 14:50:39 [INFO]: epoch 206: training loss 0.0873\n",
      "2025-06-01 14:50:39 [INFO]: epoch 207: training loss 0.0610\n",
      "2025-06-01 14:50:39 [INFO]: epoch 208: training loss 0.0599\n",
      "2025-06-01 14:50:39 [INFO]: epoch 209: training loss 0.0735\n",
      "2025-06-01 14:50:39 [INFO]: epoch 210: training loss 0.0628\n",
      "2025-06-01 14:50:39 [INFO]: epoch 211: training loss 0.0667\n",
      "2025-06-01 14:50:39 [INFO]: epoch 212: training loss 0.0735\n",
      "2025-06-01 14:50:39 [INFO]: epoch 213: training loss 0.0678\n",
      "2025-06-01 14:50:39 [INFO]: epoch 214: training loss 0.0765\n",
      "2025-06-01 14:50:39 [INFO]: epoch 215: training loss 0.0695\n",
      "2025-06-01 14:50:39 [INFO]: epoch 216: training loss 0.0711\n",
      "2025-06-01 14:50:39 [INFO]: epoch 217: training loss 0.0624\n",
      "2025-06-01 14:50:39 [INFO]: epoch 218: training loss 0.0655\n",
      "2025-06-01 14:50:39 [INFO]: epoch 219: training loss 0.0689\n",
      "2025-06-01 14:50:39 [INFO]: epoch 220: training loss 0.0665\n",
      "2025-06-01 14:50:39 [INFO]: epoch 221: training loss 0.0746\n",
      "2025-06-01 14:50:39 [INFO]: epoch 222: training loss 0.0649\n",
      "2025-06-01 14:50:39 [INFO]: epoch 223: training loss 0.0808\n",
      "2025-06-01 14:50:39 [INFO]: epoch 224: training loss 0.0729\n",
      "2025-06-01 14:50:39 [INFO]: epoch 225: training loss 0.0713\n",
      "2025-06-01 14:50:39 [INFO]: epoch 226: training loss 0.0839\n",
      "2025-06-01 14:50:39 [INFO]: epoch 227: training loss 0.0807\n",
      "2025-06-01 14:50:39 [INFO]: epoch 228: training loss 0.0663\n",
      "2025-06-01 14:50:39 [INFO]: epoch 229: training loss 0.0692\n",
      "2025-06-01 14:50:39 [INFO]: epoch 230: training loss 0.0592\n",
      "2025-06-01 14:50:39 [INFO]: epoch 231: training loss 0.0676\n",
      "2025-06-01 14:50:39 [INFO]: epoch 232: training loss 0.0616\n",
      "2025-06-01 14:50:39 [INFO]: epoch 233: training loss 0.0700\n",
      "2025-06-01 14:50:39 [INFO]: epoch 234: training loss 0.0748\n",
      "2025-06-01 14:50:39 [INFO]: epoch 235: training loss 0.0691\n",
      "2025-06-01 14:50:39 [INFO]: epoch 236: training loss 0.0567\n",
      "2025-06-01 14:50:39 [INFO]: epoch 237: training loss 0.0669\n",
      "2025-06-01 14:50:39 [INFO]: epoch 238: training loss 0.0664\n",
      "2025-06-01 14:50:39 [INFO]: epoch 239: training loss 0.0603\n",
      "2025-06-01 14:50:39 [INFO]: epoch 240: training loss 0.0655\n",
      "2025-06-01 14:50:39 [INFO]: epoch 241: training loss 0.0680\n",
      "2025-06-01 14:50:39 [INFO]: epoch 242: training loss 0.0589\n",
      "2025-06-01 14:50:39 [INFO]: epoch 243: training loss 0.0586\n",
      "2025-06-01 14:50:39 [INFO]: epoch 244: training loss 0.0644\n",
      "2025-06-01 14:50:39 [INFO]: epoch 245: training loss 0.0861\n",
      "2025-06-01 14:50:39 [INFO]: epoch 246: training loss 0.0908\n",
      "2025-06-01 14:50:39 [INFO]: epoch 247: training loss 0.0566\n",
      "2025-06-01 14:50:39 [INFO]: epoch 248: training loss 0.0588\n",
      "2025-06-01 14:50:39 [INFO]: epoch 249: training loss 0.0750\n",
      "2025-06-01 14:50:39 [INFO]: epoch 250: training loss 0.0628\n",
      "2025-06-01 14:50:39 [INFO]: epoch 251: training loss 0.0638\n",
      "2025-06-01 14:50:39 [INFO]: epoch 252: training loss 0.0720\n",
      "2025-06-01 14:50:39 [INFO]: epoch 253: training loss 0.0636\n",
      "2025-06-01 14:50:39 [INFO]: epoch 254: training loss 0.0628\n",
      "2025-06-01 14:50:39 [INFO]: epoch 255: training loss 0.0554\n",
      "2025-06-01 14:50:39 [INFO]: epoch 256: training loss 0.0468\n",
      "2025-06-01 14:50:39 [INFO]: epoch 257: training loss 0.0649\n",
      "2025-06-01 14:50:39 [INFO]: epoch 258: training loss 0.0582\n",
      "2025-06-01 14:50:39 [INFO]: epoch 259: training loss 0.0496\n",
      "2025-06-01 14:50:39 [INFO]: epoch 260: training loss 0.0479\n",
      "2025-06-01 14:50:39 [INFO]: epoch 261: training loss 0.0551\n",
      "2025-06-01 14:50:39 [INFO]: epoch 262: training loss 0.0469\n",
      "2025-06-01 14:50:39 [INFO]: epoch 263: training loss 0.0449\n",
      "2025-06-01 14:50:40 [INFO]: epoch 264: training loss 0.0625\n",
      "2025-06-01 14:50:40 [INFO]: epoch 265: training loss 0.0449\n",
      "2025-06-01 14:50:40 [INFO]: epoch 266: training loss 0.0524\n",
      "2025-06-01 14:50:40 [INFO]: epoch 267: training loss 0.0517\n",
      "2025-06-01 14:50:40 [INFO]: epoch 268: training loss 0.0624\n",
      "2025-06-01 14:50:40 [INFO]: epoch 269: training loss 0.0729\n",
      "2025-06-01 14:50:40 [INFO]: epoch 270: training loss 0.0528\n",
      "2025-06-01 14:50:40 [INFO]: epoch 271: training loss 0.0488\n",
      "2025-06-01 14:50:40 [INFO]: epoch 272: training loss 0.0622\n",
      "2025-06-01 14:50:40 [INFO]: epoch 273: training loss 0.0617\n",
      "2025-06-01 14:50:40 [INFO]: epoch 274: training loss 0.0621\n",
      "2025-06-01 14:50:40 [INFO]: epoch 275: training loss 0.0624\n",
      "2025-06-01 14:50:40 [INFO]: epoch 276: training loss 0.0781\n",
      "2025-06-01 14:50:40 [INFO]: epoch 277: training loss 0.0613\n",
      "2025-06-01 14:50:40 [INFO]: epoch 278: training loss 0.0559\n",
      "2025-06-01 14:50:40 [INFO]: epoch 279: training loss 0.0709\n",
      "2025-06-01 14:50:40 [INFO]: epoch 280: training loss 0.0667\n",
      "2025-06-01 14:50:40 [INFO]: epoch 281: training loss 0.0458\n",
      "2025-06-01 14:50:40 [INFO]: epoch 282: training loss 0.0671\n",
      "2025-06-01 14:50:40 [INFO]: epoch 283: training loss 0.0664\n",
      "2025-06-01 14:50:40 [INFO]: epoch 284: training loss 0.0667\n",
      "2025-06-01 14:50:40 [INFO]: epoch 285: training loss 0.0598\n",
      "2025-06-01 14:50:40 [INFO]: epoch 286: training loss 0.0585\n",
      "2025-06-01 14:50:40 [INFO]: epoch 287: training loss 0.0478\n",
      "2025-06-01 14:50:40 [INFO]: epoch 288: training loss 0.0507\n",
      "2025-06-01 14:50:40 [INFO]: epoch 289: training loss 0.0569\n",
      "2025-06-01 14:50:40 [INFO]: epoch 290: training loss 0.0510\n",
      "2025-06-01 14:50:40 [INFO]: epoch 291: training loss 0.0499\n",
      "2025-06-01 14:50:40 [INFO]: epoch 292: training loss 0.0608\n",
      "2025-06-01 14:50:40 [INFO]: epoch 293: training loss 0.0772\n",
      "2025-06-01 14:50:40 [INFO]: epoch 294: training loss 0.0518\n",
      "2025-06-01 14:50:40 [INFO]: epoch 295: training loss 0.0482\n",
      "2025-06-01 14:50:40 [INFO]: epoch 296: training loss 0.0620\n",
      "2025-06-01 14:50:40 [INFO]: epoch 297: training loss 0.0638\n",
      "2025-06-01 14:50:40 [INFO]: epoch 298: training loss 0.0422\n",
      "2025-06-01 14:50:40 [INFO]: epoch 299: training loss 0.0538\n",
      "2025-06-01 14:50:40 [INFO]: epoch 300: training loss 0.0490\n",
      "2025-06-01 14:50:40 [INFO]: epoch 301: training loss 0.0479\n",
      "2025-06-01 14:50:40 [INFO]: epoch 302: training loss 0.0481\n",
      "2025-06-01 14:50:40 [INFO]: epoch 303: training loss 0.0491\n",
      "2025-06-01 14:50:40 [INFO]: epoch 304: training loss 0.0460\n",
      "2025-06-01 14:50:40 [INFO]: epoch 305: training loss 0.0519\n",
      "2025-06-01 14:50:40 [INFO]: epoch 306: training loss 0.0575\n",
      "2025-06-01 14:50:40 [INFO]: epoch 307: training loss 0.0414\n",
      "2025-06-01 14:50:40 [INFO]: epoch 308: training loss 0.0427\n",
      "2025-06-01 14:50:40 [INFO]: epoch 309: training loss 0.0479\n",
      "2025-06-01 14:50:40 [INFO]: epoch 310: training loss 0.0498\n",
      "2025-06-01 14:50:40 [INFO]: epoch 311: training loss 0.0436\n",
      "2025-06-01 14:50:40 [INFO]: epoch 312: training loss 0.0509\n",
      "2025-06-01 14:50:40 [INFO]: epoch 313: training loss 0.0452\n",
      "2025-06-01 14:50:40 [INFO]: epoch 314: training loss 0.0452\n",
      "2025-06-01 14:50:40 [INFO]: epoch 315: training loss 0.0465\n",
      "2025-06-01 14:50:40 [INFO]: epoch 316: training loss 0.0432\n",
      "2025-06-01 14:50:40 [INFO]: epoch 317: training loss 0.0482\n",
      "2025-06-01 14:50:40 [INFO]: epoch 318: training loss 0.0490\n",
      "2025-06-01 14:50:40 [INFO]: epoch 319: training loss 0.0491\n",
      "2025-06-01 14:50:40 [INFO]: epoch 320: training loss 0.0433\n",
      "2025-06-01 14:50:40 [INFO]: epoch 321: training loss 0.0413\n",
      "2025-06-01 14:50:40 [INFO]: epoch 322: training loss 0.0545\n",
      "2025-06-01 14:50:40 [INFO]: epoch 323: training loss 0.0498\n",
      "2025-06-01 14:50:40 [INFO]: epoch 324: training loss 0.0399\n",
      "2025-06-01 14:50:40 [INFO]: epoch 325: training loss 0.0488\n",
      "2025-06-01 14:50:40 [INFO]: epoch 326: training loss 0.0467\n",
      "2025-06-01 14:50:40 [INFO]: epoch 327: training loss 0.0447\n",
      "2025-06-01 14:50:40 [INFO]: epoch 328: training loss 0.0408\n",
      "2025-06-01 14:50:40 [INFO]: epoch 329: training loss 0.0456\n",
      "2025-06-01 14:50:40 [INFO]: epoch 330: training loss 0.0460\n",
      "2025-06-01 14:50:40 [INFO]: epoch 331: training loss 0.0351\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-01 14:50:40 [INFO]: epoch 332: training loss 0.0487\n",
      "2025-06-01 14:50:40 [INFO]: epoch 333: training loss 0.0465\n",
      "2025-06-01 14:50:40 [INFO]: epoch 334: training loss 0.0471\n",
      "2025-06-01 14:50:40 [INFO]: epoch 335: training loss 0.0470\n",
      "2025-06-01 14:50:40 [INFO]: epoch 336: training loss 0.0416\n",
      "2025-06-01 14:50:40 [INFO]: epoch 337: training loss 0.0432\n",
      "2025-06-01 14:50:40 [INFO]: epoch 338: training loss 0.0512\n",
      "2025-06-01 14:50:40 [INFO]: epoch 339: training loss 0.0425\n",
      "2025-06-01 14:50:40 [INFO]: epoch 340: training loss 0.0453\n",
      "2025-06-01 14:50:41 [INFO]: epoch 341: training loss 0.0451\n",
      "2025-06-01 14:50:41 [INFO]: epoch 342: training loss 0.0489\n",
      "2025-06-01 14:50:41 [INFO]: epoch 343: training loss 0.0423\n",
      "2025-06-01 14:50:41 [INFO]: epoch 344: training loss 0.0402\n",
      "2025-06-01 14:50:41 [INFO]: epoch 345: training loss 0.0474\n",
      "2025-06-01 14:50:41 [INFO]: epoch 346: training loss 0.0488\n",
      "2025-06-01 14:50:41 [INFO]: epoch 347: training loss 0.0426\n",
      "2025-06-01 14:50:41 [INFO]: epoch 348: training loss 0.0356\n",
      "2025-06-01 14:50:41 [INFO]: epoch 349: training loss 0.0470\n",
      "2025-06-01 14:50:41 [INFO]: epoch 350: training loss 0.0482\n",
      "2025-06-01 14:50:41 [INFO]: epoch 351: training loss 0.0466\n",
      "2025-06-01 14:50:41 [INFO]: epoch 352: training loss 0.0546\n",
      "2025-06-01 14:50:41 [INFO]: epoch 353: training loss 0.0478\n",
      "2025-06-01 14:50:41 [INFO]: epoch 354: training loss 0.0395\n",
      "2025-06-01 14:50:41 [INFO]: epoch 355: training loss 0.0462\n",
      "2025-06-01 14:50:41 [INFO]: epoch 356: training loss 0.0437\n",
      "2025-06-01 14:50:41 [INFO]: epoch 357: training loss 0.0477\n",
      "2025-06-01 14:50:41 [INFO]: epoch 358: training loss 0.0402\n",
      "2025-06-01 14:50:41 [INFO]: epoch 359: training loss 0.0371\n",
      "2025-06-01 14:50:41 [INFO]: epoch 360: training loss 0.0490\n",
      "2025-06-01 14:50:41 [INFO]: epoch 361: training loss 0.0504\n",
      "2025-06-01 14:50:41 [INFO]: epoch 362: training loss 0.0443\n",
      "2025-06-01 14:50:41 [INFO]: epoch 363: training loss 0.0411\n",
      "2025-06-01 14:50:41 [INFO]: epoch 364: training loss 0.0468\n",
      "2025-06-01 14:50:41 [INFO]: epoch 365: training loss 0.0404\n",
      "2025-06-01 14:50:41 [INFO]: epoch 366: training loss 0.0417\n",
      "2025-06-01 14:50:41 [INFO]: epoch 367: training loss 0.0352\n",
      "2025-06-01 14:50:41 [INFO]: epoch 368: training loss 0.0322\n",
      "2025-06-01 14:50:41 [INFO]: epoch 369: training loss 0.0487\n",
      "2025-06-01 14:50:41 [INFO]: epoch 370: training loss 0.0424\n",
      "2025-06-01 14:50:41 [INFO]: epoch 371: training loss 0.0385\n",
      "2025-06-01 14:50:41 [INFO]: epoch 372: training loss 0.0462\n",
      "2025-06-01 14:50:41 [INFO]: epoch 373: training loss 0.0471\n",
      "2025-06-01 14:50:41 [INFO]: epoch 374: training loss 0.0399\n",
      "2025-06-01 14:50:41 [INFO]: epoch 375: training loss 0.0429\n",
      "2025-06-01 14:50:41 [INFO]: epoch 376: training loss 0.0520\n",
      "2025-06-01 14:50:41 [INFO]: epoch 377: training loss 0.0415\n",
      "2025-06-01 14:50:41 [INFO]: epoch 378: training loss 0.0452\n",
      "2025-06-01 14:50:41 [INFO]: epoch 379: training loss 0.0474\n",
      "2025-06-01 14:50:41 [INFO]: epoch 380: training loss 0.0357\n",
      "2025-06-01 14:50:41 [INFO]: epoch 381: training loss 0.0452\n",
      "2025-06-01 14:50:41 [INFO]: epoch 382: training loss 0.0330\n",
      "2025-06-01 14:50:41 [INFO]: epoch 383: training loss 0.0440\n",
      "2025-06-01 14:50:41 [INFO]: epoch 384: training loss 0.0446\n",
      "2025-06-01 14:50:41 [INFO]: epoch 385: training loss 0.0381\n",
      "2025-06-01 14:50:41 [INFO]: epoch 386: training loss 0.0355\n",
      "2025-06-01 14:50:41 [INFO]: epoch 387: training loss 0.0429\n",
      "2025-06-01 14:50:41 [INFO]: epoch 388: training loss 0.0332\n",
      "2025-06-01 14:50:41 [INFO]: epoch 389: training loss 0.0423\n",
      "2025-06-01 14:50:41 [INFO]: epoch 390: training loss 0.0459\n",
      "2025-06-01 14:50:41 [INFO]: epoch 391: training loss 0.0376\n",
      "2025-06-01 14:50:41 [INFO]: epoch 392: training loss 0.0456\n",
      "2025-06-01 14:50:41 [INFO]: epoch 393: training loss 0.0423\n",
      "2025-06-01 14:50:41 [INFO]: epoch 394: training loss 0.0358\n",
      "2025-06-01 14:50:41 [INFO]: epoch 395: training loss 0.0464\n",
      "2025-06-01 14:50:41 [INFO]: epoch 396: training loss 0.0402\n",
      "2025-06-01 14:50:41 [INFO]: epoch 397: training loss 0.0377\n",
      "2025-06-01 14:50:41 [INFO]: epoch 398: training loss 0.0425\n",
      "2025-06-01 14:50:41 [INFO]: epoch 399: training loss 0.0362\n",
      "2025-06-01 14:50:41 [INFO]: epoch 400: training loss 0.0446\n",
      "2025-06-01 14:50:41 [INFO]: epoch 401: training loss 0.0479\n",
      "2025-06-01 14:50:41 [INFO]: epoch 402: training loss 0.0320\n",
      "2025-06-01 14:50:41 [INFO]: epoch 403: training loss 0.0395\n",
      "2025-06-01 14:50:41 [INFO]: epoch 404: training loss 0.0365\n",
      "2025-06-01 14:50:41 [INFO]: epoch 405: training loss 0.0337\n",
      "2025-06-01 14:50:41 [INFO]: epoch 406: training loss 0.0380\n",
      "2025-06-01 14:50:41 [INFO]: epoch 407: training loss 0.0391\n",
      "2025-06-01 14:50:41 [INFO]: epoch 408: training loss 0.0360\n",
      "2025-06-01 14:50:41 [INFO]: epoch 409: training loss 0.0333\n",
      "2025-06-01 14:50:41 [INFO]: epoch 410: training loss 0.0322\n",
      "2025-06-01 14:50:41 [INFO]: epoch 411: training loss 0.0316\n",
      "2025-06-01 14:50:41 [INFO]: epoch 412: training loss 0.0389\n",
      "2025-06-01 14:50:41 [INFO]: epoch 413: training loss 0.0378\n",
      "2025-06-01 14:50:41 [INFO]: epoch 414: training loss 0.0338\n",
      "2025-06-01 14:50:41 [INFO]: epoch 415: training loss 0.0396\n",
      "2025-06-01 14:50:41 [INFO]: epoch 416: training loss 0.0464\n",
      "2025-06-01 14:50:42 [INFO]: epoch 417: training loss 0.0331\n",
      "2025-06-01 14:50:42 [INFO]: epoch 418: training loss 0.0495\n",
      "2025-06-01 14:50:42 [INFO]: epoch 419: training loss 0.0480\n",
      "2025-06-01 14:50:42 [INFO]: epoch 420: training loss 0.0407\n",
      "2025-06-01 14:50:42 [INFO]: epoch 421: training loss 0.0411\n",
      "2025-06-01 14:50:42 [INFO]: epoch 422: training loss 0.0409\n",
      "2025-06-01 14:50:42 [INFO]: epoch 423: training loss 0.0376\n",
      "2025-06-01 14:50:42 [INFO]: epoch 424: training loss 0.0449\n",
      "2025-06-01 14:50:42 [INFO]: epoch 425: training loss 0.0362\n",
      "2025-06-01 14:50:42 [INFO]: epoch 426: training loss 0.0342\n",
      "2025-06-01 14:50:42 [INFO]: epoch 427: training loss 0.0354\n",
      "2025-06-01 14:50:42 [INFO]: epoch 428: training loss 0.0329\n",
      "2025-06-01 14:50:42 [INFO]: epoch 429: training loss 0.0317\n",
      "2025-06-01 14:50:42 [INFO]: epoch 430: training loss 0.0309\n",
      "2025-06-01 14:50:42 [INFO]: epoch 431: training loss 0.0330\n",
      "2025-06-01 14:50:42 [INFO]: epoch 432: training loss 0.0285\n",
      "2025-06-01 14:50:42 [INFO]: epoch 433: training loss 0.0330\n",
      "2025-06-01 14:50:42 [INFO]: epoch 434: training loss 0.0324\n",
      "2025-06-01 14:50:42 [INFO]: epoch 435: training loss 0.0328\n",
      "2025-06-01 14:50:42 [INFO]: epoch 436: training loss 0.0342\n",
      "2025-06-01 14:50:42 [INFO]: epoch 437: training loss 0.0338\n",
      "2025-06-01 14:50:42 [INFO]: epoch 438: training loss 0.0341\n",
      "2025-06-01 14:50:42 [INFO]: epoch 439: training loss 0.0316\n",
      "2025-06-01 14:50:42 [INFO]: epoch 440: training loss 0.0384\n",
      "2025-06-01 14:50:42 [INFO]: epoch 441: training loss 0.0307\n",
      "2025-06-01 14:50:42 [INFO]: epoch 442: training loss 0.0337\n",
      "2025-06-01 14:50:42 [INFO]: epoch 443: training loss 0.0382\n",
      "2025-06-01 14:50:42 [INFO]: epoch 444: training loss 0.0313\n",
      "2025-06-01 14:50:42 [INFO]: epoch 445: training loss 0.0307\n",
      "2025-06-01 14:50:42 [INFO]: epoch 446: training loss 0.0381\n",
      "2025-06-01 14:50:42 [INFO]: epoch 447: training loss 0.0314\n",
      "2025-06-01 14:50:42 [INFO]: epoch 448: training loss 0.0321\n",
      "2025-06-01 14:50:42 [INFO]: epoch 449: training loss 0.0289\n",
      "2025-06-01 14:50:42 [INFO]: epoch 450: training loss 0.0334\n",
      "2025-06-01 14:50:42 [INFO]: epoch 451: training loss 0.0325\n",
      "2025-06-01 14:50:42 [INFO]: epoch 452: training loss 0.0265\n",
      "2025-06-01 14:50:42 [INFO]: epoch 453: training loss 0.0287\n",
      "2025-06-01 14:50:42 [INFO]: epoch 454: training loss 0.0300\n",
      "2025-06-01 14:50:42 [INFO]: epoch 455: training loss 0.0309\n",
      "2025-06-01 14:50:42 [INFO]: epoch 456: training loss 0.0350\n",
      "2025-06-01 14:50:42 [INFO]: epoch 457: training loss 0.0273\n",
      "2025-06-01 14:50:42 [INFO]: epoch 458: training loss 0.0295\n",
      "2025-06-01 14:50:42 [INFO]: epoch 459: training loss 0.0330\n",
      "2025-06-01 14:50:42 [INFO]: epoch 460: training loss 0.0284\n",
      "2025-06-01 14:50:42 [INFO]: epoch 461: training loss 0.0276\n",
      "2025-06-01 14:50:42 [INFO]: epoch 462: training loss 0.0335\n",
      "2025-06-01 14:50:42 [INFO]: epoch 463: training loss 0.0346\n",
      "2025-06-01 14:50:42 [INFO]: epoch 464: training loss 0.0325\n",
      "2025-06-01 14:50:42 [INFO]: epoch 465: training loss 0.0317\n",
      "2025-06-01 14:50:42 [INFO]: epoch 466: training loss 0.0292\n",
      "2025-06-01 14:50:42 [INFO]: epoch 467: training loss 0.0295\n",
      "2025-06-01 14:50:42 [INFO]: epoch 468: training loss 0.0298\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-01 14:50:42 [INFO]: epoch 469: training loss 0.0261\n",
      "2025-06-01 14:50:42 [INFO]: epoch 470: training loss 0.0306\n",
      "2025-06-01 14:50:42 [INFO]: epoch 471: training loss 0.0323\n",
      "2025-06-01 14:50:42 [INFO]: epoch 472: training loss 0.0339\n",
      "2025-06-01 14:50:42 [INFO]: epoch 473: training loss 0.0328\n",
      "2025-06-01 14:50:42 [INFO]: epoch 474: training loss 0.0331\n",
      "2025-06-01 14:50:42 [INFO]: epoch 475: training loss 0.0356\n",
      "2025-06-01 14:50:42 [INFO]: epoch 476: training loss 0.0290\n",
      "2025-06-01 14:50:42 [INFO]: epoch 477: training loss 0.0257\n",
      "2025-06-01 14:50:42 [INFO]: epoch 478: training loss 0.0295\n",
      "2025-06-01 14:50:42 [INFO]: epoch 479: training loss 0.0280\n",
      "2025-06-01 14:50:42 [INFO]: epoch 480: training loss 0.0308\n",
      "2025-06-01 14:50:42 [INFO]: epoch 481: training loss 0.0275\n",
      "2025-06-01 14:50:42 [INFO]: epoch 482: training loss 0.0335\n",
      "2025-06-01 14:50:42 [INFO]: epoch 483: training loss 0.0349\n",
      "2025-06-01 14:50:42 [INFO]: epoch 484: training loss 0.0324\n",
      "2025-06-01 14:50:42 [INFO]: epoch 485: training loss 0.0348\n",
      "2025-06-01 14:50:42 [INFO]: epoch 486: training loss 0.0361\n",
      "2025-06-01 14:50:42 [INFO]: epoch 487: training loss 0.0259\n",
      "2025-06-01 14:50:42 [INFO]: epoch 488: training loss 0.0348\n",
      "2025-06-01 14:50:42 [INFO]: epoch 489: training loss 0.0331\n",
      "2025-06-01 14:50:42 [INFO]: epoch 490: training loss 0.0324\n",
      "2025-06-01 14:50:42 [INFO]: epoch 491: training loss 0.0315\n",
      "2025-06-01 14:50:42 [INFO]: epoch 492: training loss 0.0393\n",
      "2025-06-01 14:50:42 [INFO]: epoch 493: training loss 0.0372\n",
      "2025-06-01 14:50:43 [INFO]: epoch 494: training loss 0.0307\n",
      "2025-06-01 14:50:43 [INFO]: epoch 495: training loss 0.0358\n",
      "2025-06-01 14:50:43 [INFO]: epoch 496: training loss 0.0448\n",
      "2025-06-01 14:50:43 [INFO]: epoch 497: training loss 0.0266\n",
      "2025-06-01 14:50:43 [INFO]: epoch 498: training loss 0.0339\n",
      "2025-06-01 14:50:43 [INFO]: epoch 499: training loss 0.0432\n",
      "2025-06-01 14:50:43 [INFO]: epoch 500: training loss 0.0343\n",
      "2025-06-01 14:50:43 [INFO]: epoch 501: training loss 0.0292\n",
      "2025-06-01 14:50:43 [INFO]: epoch 502: training loss 0.0334\n",
      "2025-06-01 14:50:43 [INFO]: epoch 503: training loss 0.0287\n",
      "2025-06-01 14:50:43 [INFO]: epoch 504: training loss 0.0305\n",
      "2025-06-01 14:50:43 [INFO]: epoch 505: training loss 0.0329\n",
      "2025-06-01 14:50:43 [INFO]: epoch 506: training loss 0.0247\n",
      "2025-06-01 14:50:43 [INFO]: epoch 507: training loss 0.0304\n",
      "2025-06-01 14:50:43 [INFO]: epoch 508: training loss 0.0353\n",
      "2025-06-01 14:50:43 [INFO]: epoch 509: training loss 0.0269\n",
      "2025-06-01 14:50:43 [INFO]: epoch 510: training loss 0.0327\n",
      "2025-06-01 14:50:43 [INFO]: epoch 511: training loss 0.0369\n",
      "2025-06-01 14:50:43 [INFO]: epoch 512: training loss 0.0281\n",
      "2025-06-01 14:50:43 [INFO]: epoch 513: training loss 0.0306\n",
      "2025-06-01 14:50:43 [INFO]: epoch 514: training loss 0.0267\n",
      "2025-06-01 14:50:43 [INFO]: epoch 515: training loss 0.0259\n",
      "2025-06-01 14:50:43 [INFO]: epoch 516: training loss 0.0430\n",
      "2025-06-01 14:50:43 [INFO]: epoch 517: training loss 0.0352\n",
      "2025-06-01 14:50:43 [INFO]: epoch 518: training loss 0.0287\n",
      "2025-06-01 14:50:43 [INFO]: epoch 519: training loss 0.0486\n",
      "2025-06-01 14:50:43 [INFO]: epoch 520: training loss 0.0358\n",
      "2025-06-01 14:50:43 [INFO]: epoch 521: training loss 0.0270\n",
      "2025-06-01 14:50:43 [INFO]: epoch 522: training loss 0.0398\n",
      "2025-06-01 14:50:43 [INFO]: epoch 523: training loss 0.0315\n",
      "2025-06-01 14:50:43 [INFO]: epoch 524: training loss 0.0261\n",
      "2025-06-01 14:50:43 [INFO]: epoch 525: training loss 0.0293\n",
      "2025-06-01 14:50:43 [INFO]: epoch 526: training loss 0.0311\n",
      "2025-06-01 14:50:43 [INFO]: epoch 527: training loss 0.0285\n",
      "2025-06-01 14:50:43 [INFO]: epoch 528: training loss 0.0338\n",
      "2025-06-01 14:50:43 [INFO]: epoch 529: training loss 0.0343\n",
      "2025-06-01 14:50:43 [INFO]: epoch 530: training loss 0.0394\n",
      "2025-06-01 14:50:43 [INFO]: epoch 531: training loss 0.0469\n",
      "2025-06-01 14:50:43 [INFO]: epoch 532: training loss 0.0331\n",
      "2025-06-01 14:50:43 [INFO]: epoch 533: training loss 0.0372\n",
      "2025-06-01 14:50:43 [INFO]: epoch 534: training loss 0.0312\n",
      "2025-06-01 14:50:43 [INFO]: epoch 535: training loss 0.0299\n",
      "2025-06-01 14:50:43 [INFO]: epoch 536: training loss 0.0300\n",
      "2025-06-01 14:50:43 [INFO]: epoch 537: training loss 0.0293\n",
      "2025-06-01 14:50:43 [INFO]: epoch 538: training loss 0.0277\n",
      "2025-06-01 14:50:43 [INFO]: epoch 539: training loss 0.0281\n",
      "2025-06-01 14:50:43 [INFO]: epoch 540: training loss 0.0332\n",
      "2025-06-01 14:50:43 [INFO]: epoch 541: training loss 0.0336\n",
      "2025-06-01 14:50:43 [INFO]: epoch 542: training loss 0.0263\n",
      "2025-06-01 14:50:43 [INFO]: epoch 543: training loss 0.0324\n",
      "2025-06-01 14:50:43 [INFO]: epoch 544: training loss 0.0270\n",
      "2025-06-01 14:50:43 [INFO]: epoch 545: training loss 0.0330\n",
      "2025-06-01 14:50:43 [INFO]: epoch 546: training loss 0.0320\n",
      "2025-06-01 14:50:43 [INFO]: epoch 547: training loss 0.0309\n",
      "2025-06-01 14:50:43 [INFO]: epoch 548: training loss 0.0327\n",
      "2025-06-01 14:50:43 [INFO]: epoch 549: training loss 0.0330\n",
      "2025-06-01 14:50:43 [INFO]: epoch 550: training loss 0.0272\n",
      "2025-06-01 14:50:43 [INFO]: epoch 551: training loss 0.0285\n",
      "2025-06-01 14:50:43 [INFO]: epoch 552: training loss 0.0325\n",
      "2025-06-01 14:50:43 [INFO]: epoch 553: training loss 0.0372\n",
      "2025-06-01 14:50:43 [INFO]: epoch 554: training loss 0.0286\n",
      "2025-06-01 14:50:43 [INFO]: epoch 555: training loss 0.0327\n",
      "2025-06-01 14:50:43 [INFO]: epoch 556: training loss 0.0373\n",
      "2025-06-01 14:50:43 [INFO]: epoch 557: training loss 0.0330\n",
      "2025-06-01 14:50:43 [INFO]: epoch 558: training loss 0.0264\n",
      "2025-06-01 14:50:43 [INFO]: epoch 559: training loss 0.0355\n",
      "2025-06-01 14:50:43 [INFO]: epoch 560: training loss 0.0365\n",
      "2025-06-01 14:50:43 [INFO]: epoch 561: training loss 0.0286\n",
      "2025-06-01 14:50:43 [INFO]: epoch 562: training loss 0.0305\n",
      "2025-06-01 14:50:43 [INFO]: epoch 563: training loss 0.0343\n",
      "2025-06-01 14:50:43 [INFO]: epoch 564: training loss 0.0310\n",
      "2025-06-01 14:50:43 [INFO]: epoch 565: training loss 0.0263\n",
      "2025-06-01 14:50:43 [INFO]: epoch 566: training loss 0.0325\n",
      "2025-06-01 14:50:43 [INFO]: epoch 567: training loss 0.0358\n",
      "2025-06-01 14:50:43 [INFO]: epoch 568: training loss 0.0252\n",
      "2025-06-01 14:50:43 [INFO]: epoch 569: training loss 0.0276\n",
      "2025-06-01 14:50:44 [INFO]: epoch 570: training loss 0.0311\n",
      "2025-06-01 14:50:44 [INFO]: epoch 571: training loss 0.0313\n",
      "2025-06-01 14:50:44 [INFO]: epoch 572: training loss 0.0331\n",
      "2025-06-01 14:50:44 [INFO]: epoch 573: training loss 0.0266\n",
      "2025-06-01 14:50:44 [INFO]: epoch 574: training loss 0.0295\n",
      "2025-06-01 14:50:44 [INFO]: epoch 575: training loss 0.0308\n",
      "2025-06-01 14:50:44 [INFO]: epoch 576: training loss 0.0350\n",
      "2025-06-01 14:50:44 [INFO]: epoch 577: training loss 0.0349\n",
      "2025-06-01 14:50:44 [INFO]: epoch 578: training loss 0.0264\n",
      "2025-06-01 14:50:44 [INFO]: epoch 579: training loss 0.0310\n",
      "2025-06-01 14:50:44 [INFO]: epoch 580: training loss 0.0316\n",
      "2025-06-01 14:50:44 [INFO]: epoch 581: training loss 0.0267\n",
      "2025-06-01 14:50:44 [INFO]: epoch 582: training loss 0.0259\n",
      "2025-06-01 14:50:44 [INFO]: epoch 583: training loss 0.0246\n",
      "2025-06-01 14:50:44 [INFO]: epoch 584: training loss 0.0286\n",
      "2025-06-01 14:50:44 [INFO]: epoch 585: training loss 0.0310\n",
      "2025-06-01 14:50:44 [INFO]: epoch 586: training loss 0.0295\n",
      "2025-06-01 14:50:44 [INFO]: epoch 587: training loss 0.0278\n",
      "2025-06-01 14:50:44 [INFO]: epoch 588: training loss 0.0419\n",
      "2025-06-01 14:50:44 [INFO]: epoch 589: training loss 0.0325\n",
      "2025-06-01 14:50:44 [INFO]: epoch 590: training loss 0.0256\n",
      "2025-06-01 14:50:44 [INFO]: epoch 591: training loss 0.0440\n",
      "2025-06-01 14:50:44 [INFO]: epoch 592: training loss 0.0375\n",
      "2025-06-01 14:50:44 [INFO]: epoch 593: training loss 0.0271\n",
      "2025-06-01 14:50:44 [INFO]: epoch 594: training loss 0.0421\n",
      "2025-06-01 14:50:44 [INFO]: epoch 595: training loss 0.0357\n",
      "2025-06-01 14:50:44 [INFO]: epoch 596: training loss 0.0270\n",
      "2025-06-01 14:50:44 [INFO]: epoch 597: training loss 0.0313\n",
      "2025-06-01 14:50:44 [INFO]: epoch 598: training loss 0.0398\n",
      "2025-06-01 14:50:44 [INFO]: epoch 599: training loss 0.0248\n",
      "2025-06-01 14:50:44 [INFO]: epoch 600: training loss 0.0281\n",
      "2025-06-01 14:50:44 [INFO]: epoch 601: training loss 0.0332\n",
      "2025-06-01 14:50:44 [INFO]: epoch 602: training loss 0.0281\n",
      "2025-06-01 14:50:44 [INFO]: epoch 603: training loss 0.0253\n",
      "2025-06-01 14:50:44 [INFO]: epoch 604: training loss 0.0350\n",
      "2025-06-01 14:50:44 [INFO]: epoch 605: training loss 0.0275\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-01 14:50:44 [INFO]: epoch 606: training loss 0.0234\n",
      "2025-06-01 14:50:44 [INFO]: epoch 607: training loss 0.0252\n",
      "2025-06-01 14:50:44 [INFO]: epoch 608: training loss 0.0338\n",
      "2025-06-01 14:50:44 [INFO]: epoch 609: training loss 0.0304\n",
      "2025-06-01 14:50:44 [INFO]: epoch 610: training loss 0.0245\n",
      "2025-06-01 14:50:44 [INFO]: epoch 611: training loss 0.0243\n",
      "2025-06-01 14:50:44 [INFO]: epoch 612: training loss 0.0285\n",
      "2025-06-01 14:50:44 [INFO]: epoch 613: training loss 0.0357\n",
      "2025-06-01 14:50:44 [INFO]: epoch 614: training loss 0.0238\n",
      "2025-06-01 14:50:44 [INFO]: epoch 615: training loss 0.0281\n",
      "2025-06-01 14:50:44 [INFO]: epoch 616: training loss 0.0334\n",
      "2025-06-01 14:50:44 [INFO]: epoch 617: training loss 0.0226\n",
      "2025-06-01 14:50:44 [INFO]: epoch 618: training loss 0.0310\n",
      "2025-06-01 14:50:44 [INFO]: epoch 619: training loss 0.0284\n",
      "2025-06-01 14:50:44 [INFO]: epoch 620: training loss 0.0277\n",
      "2025-06-01 14:50:44 [INFO]: epoch 621: training loss 0.0262\n",
      "2025-06-01 14:50:44 [INFO]: epoch 622: training loss 0.0244\n",
      "2025-06-01 14:50:44 [INFO]: epoch 623: training loss 0.0250\n",
      "2025-06-01 14:50:44 [INFO]: epoch 624: training loss 0.0276\n",
      "2025-06-01 14:50:44 [INFO]: epoch 625: training loss 0.0249\n",
      "2025-06-01 14:50:44 [INFO]: epoch 626: training loss 0.0239\n",
      "2025-06-01 14:50:44 [INFO]: epoch 627: training loss 0.0268\n",
      "2025-06-01 14:50:44 [INFO]: epoch 628: training loss 0.0203\n",
      "2025-06-01 14:50:44 [INFO]: epoch 629: training loss 0.0219\n",
      "2025-06-01 14:50:44 [INFO]: epoch 630: training loss 0.0254\n",
      "2025-06-01 14:50:44 [INFO]: epoch 631: training loss 0.0212\n",
      "2025-06-01 14:50:44 [INFO]: epoch 632: training loss 0.0223\n",
      "2025-06-01 14:50:44 [INFO]: epoch 633: training loss 0.0295\n",
      "2025-06-01 14:50:44 [INFO]: epoch 634: training loss 0.0222\n",
      "2025-06-01 14:50:44 [INFO]: epoch 635: training loss 0.0244\n",
      "2025-06-01 14:50:44 [INFO]: epoch 636: training loss 0.0273\n",
      "2025-06-01 14:50:44 [INFO]: epoch 637: training loss 0.0224\n",
      "2025-06-01 14:50:44 [INFO]: epoch 638: training loss 0.0220\n",
      "2025-06-01 14:50:44 [INFO]: epoch 639: training loss 0.0301\n",
      "2025-06-01 14:50:44 [INFO]: epoch 640: training loss 0.0245\n",
      "2025-06-01 14:50:44 [INFO]: epoch 641: training loss 0.0241\n",
      "2025-06-01 14:50:44 [INFO]: epoch 642: training loss 0.0313\n",
      "2025-06-01 14:50:44 [INFO]: epoch 643: training loss 0.0260\n",
      "2025-06-01 14:50:44 [INFO]: epoch 644: training loss 0.0302\n",
      "2025-06-01 14:50:44 [INFO]: epoch 645: training loss 0.0271\n",
      "2025-06-01 14:50:45 [INFO]: epoch 646: training loss 0.0201\n",
      "2025-06-01 14:50:45 [INFO]: epoch 647: training loss 0.0313\n",
      "2025-06-01 14:50:45 [INFO]: epoch 648: training loss 0.0246\n",
      "2025-06-01 14:50:45 [INFO]: epoch 649: training loss 0.0242\n",
      "2025-06-01 14:50:45 [INFO]: epoch 650: training loss 0.0287\n",
      "2025-06-01 14:50:45 [INFO]: epoch 651: training loss 0.0287\n",
      "2025-06-01 14:50:45 [INFO]: epoch 652: training loss 0.0231\n",
      "2025-06-01 14:50:45 [INFO]: epoch 653: training loss 0.0261\n",
      "2025-06-01 14:50:45 [INFO]: epoch 654: training loss 0.0298\n",
      "2025-06-01 14:50:45 [INFO]: epoch 655: training loss 0.0288\n",
      "2025-06-01 14:50:45 [INFO]: epoch 656: training loss 0.0263\n",
      "2025-06-01 14:50:45 [INFO]: epoch 657: training loss 0.0242\n",
      "2025-06-01 14:50:45 [INFO]: epoch 658: training loss 0.0346\n",
      "2025-06-01 14:50:45 [INFO]: epoch 659: training loss 0.0262\n",
      "2025-06-01 14:50:45 [INFO]: epoch 660: training loss 0.0311\n",
      "2025-06-01 14:50:45 [INFO]: epoch 661: training loss 0.0249\n",
      "2025-06-01 14:50:45 [INFO]: epoch 662: training loss 0.0304\n",
      "2025-06-01 14:50:45 [INFO]: epoch 663: training loss 0.0318\n",
      "2025-06-01 14:50:45 [INFO]: epoch 664: training loss 0.0333\n",
      "2025-06-01 14:50:45 [INFO]: epoch 665: training loss 0.0310\n",
      "2025-06-01 14:50:45 [INFO]: epoch 666: training loss 0.0263\n",
      "2025-06-01 14:50:45 [INFO]: epoch 667: training loss 0.0338\n",
      "2025-06-01 14:50:45 [INFO]: epoch 668: training loss 0.0300\n",
      "2025-06-01 14:50:45 [INFO]: epoch 669: training loss 0.0290\n",
      "2025-06-01 14:50:45 [INFO]: epoch 670: training loss 0.0262\n",
      "2025-06-01 14:50:45 [INFO]: epoch 671: training loss 0.0286\n",
      "2025-06-01 14:50:45 [INFO]: epoch 672: training loss 0.0267\n",
      "2025-06-01 14:50:45 [INFO]: epoch 673: training loss 0.0239\n",
      "2025-06-01 14:50:45 [INFO]: epoch 674: training loss 0.0263\n",
      "2025-06-01 14:50:45 [INFO]: epoch 675: training loss 0.0230\n",
      "2025-06-01 14:50:45 [INFO]: epoch 676: training loss 0.0299\n",
      "2025-06-01 14:50:45 [INFO]: epoch 677: training loss 0.0206\n",
      "2025-06-01 14:50:45 [INFO]: epoch 678: training loss 0.0288\n",
      "2025-06-01 14:50:45 [INFO]: epoch 679: training loss 0.0261\n",
      "2025-06-01 14:50:45 [INFO]: epoch 680: training loss 0.0258\n",
      "2025-06-01 14:50:45 [INFO]: epoch 681: training loss 0.0241\n",
      "2025-06-01 14:50:45 [INFO]: epoch 682: training loss 0.0231\n",
      "2025-06-01 14:50:45 [INFO]: epoch 683: training loss 0.0247\n",
      "2025-06-01 14:50:45 [INFO]: epoch 684: training loss 0.0323\n",
      "2025-06-01 14:50:45 [INFO]: epoch 685: training loss 0.0291\n",
      "2025-06-01 14:50:45 [INFO]: epoch 686: training loss 0.0221\n",
      "2025-06-01 14:50:45 [INFO]: epoch 687: training loss 0.0275\n",
      "2025-06-01 14:50:45 [INFO]: epoch 688: training loss 0.0248\n",
      "2025-06-01 14:50:45 [INFO]: epoch 689: training loss 0.0264\n",
      "2025-06-01 14:50:45 [INFO]: epoch 690: training loss 0.0252\n",
      "2025-06-01 14:50:45 [INFO]: epoch 691: training loss 0.0231\n",
      "2025-06-01 14:50:45 [INFO]: epoch 692: training loss 0.0272\n",
      "2025-06-01 14:50:45 [INFO]: epoch 693: training loss 0.0313\n",
      "2025-06-01 14:50:45 [INFO]: epoch 694: training loss 0.0252\n",
      "2025-06-01 14:50:45 [INFO]: epoch 695: training loss 0.0300\n",
      "2025-06-01 14:50:45 [INFO]: epoch 696: training loss 0.0256\n",
      "2025-06-01 14:50:45 [INFO]: epoch 697: training loss 0.0252\n",
      "2025-06-01 14:50:45 [INFO]: epoch 698: training loss 0.0267\n",
      "2025-06-01 14:50:45 [INFO]: epoch 699: training loss 0.0248\n",
      "2025-06-01 14:50:45 [INFO]: epoch 700: training loss 0.0269\n",
      "2025-06-01 14:50:45 [INFO]: epoch 701: training loss 0.0217\n",
      "2025-06-01 14:50:45 [INFO]: epoch 702: training loss 0.0209\n",
      "2025-06-01 14:50:45 [INFO]: epoch 703: training loss 0.0205\n",
      "2025-06-01 14:50:45 [INFO]: epoch 704: training loss 0.0411\n",
      "2025-06-01 14:50:45 [INFO]: epoch 705: training loss 0.0271\n",
      "2025-06-01 14:50:45 [INFO]: epoch 706: training loss 0.0257\n",
      "2025-06-01 14:50:45 [INFO]: epoch 707: training loss 0.0306\n",
      "2025-06-01 14:50:45 [INFO]: epoch 708: training loss 0.0273\n",
      "2025-06-01 14:50:45 [INFO]: epoch 709: training loss 0.0219\n",
      "2025-06-01 14:50:45 [INFO]: epoch 710: training loss 0.0242\n",
      "2025-06-01 14:50:45 [INFO]: epoch 711: training loss 0.0231\n",
      "2025-06-01 14:50:45 [INFO]: epoch 712: training loss 0.0221\n",
      "2025-06-01 14:50:45 [INFO]: epoch 713: training loss 0.0225\n",
      "2025-06-01 14:50:45 [INFO]: epoch 714: training loss 0.0246\n",
      "2025-06-01 14:50:45 [INFO]: epoch 715: training loss 0.0244\n",
      "2025-06-01 14:50:45 [INFO]: epoch 716: training loss 0.0244\n",
      "2025-06-01 14:50:45 [INFO]: epoch 717: training loss 0.0246\n",
      "2025-06-01 14:50:45 [INFO]: epoch 718: training loss 0.0208\n",
      "2025-06-01 14:50:45 [INFO]: epoch 719: training loss 0.0228\n",
      "2025-06-01 14:50:45 [INFO]: epoch 720: training loss 0.0204\n",
      "2025-06-01 14:50:45 [INFO]: epoch 721: training loss 0.0254\n",
      "2025-06-01 14:50:46 [INFO]: epoch 722: training loss 0.0257\n",
      "2025-06-01 14:50:46 [INFO]: epoch 723: training loss 0.0233\n",
      "2025-06-01 14:50:46 [INFO]: epoch 724: training loss 0.0244\n",
      "2025-06-01 14:50:46 [INFO]: epoch 725: training loss 0.0241\n",
      "2025-06-01 14:50:46 [INFO]: epoch 726: training loss 0.0215\n",
      "2025-06-01 14:50:46 [INFO]: epoch 727: training loss 0.0261\n",
      "2025-06-01 14:50:46 [INFO]: epoch 728: training loss 0.0235\n",
      "2025-06-01 14:50:46 [INFO]: epoch 729: training loss 0.0234\n",
      "2025-06-01 14:50:46 [INFO]: epoch 730: training loss 0.0248\n",
      "2025-06-01 14:50:46 [INFO]: epoch 731: training loss 0.0231\n",
      "2025-06-01 14:50:46 [INFO]: epoch 732: training loss 0.0249\n",
      "2025-06-01 14:50:46 [INFO]: epoch 733: training loss 0.0238\n",
      "2025-06-01 14:50:46 [INFO]: epoch 734: training loss 0.0217\n",
      "2025-06-01 14:50:46 [INFO]: epoch 735: training loss 0.0280\n",
      "2025-06-01 14:50:46 [INFO]: epoch 736: training loss 0.0270\n",
      "2025-06-01 14:50:46 [INFO]: epoch 737: training loss 0.0241\n",
      "2025-06-01 14:50:46 [INFO]: epoch 738: training loss 0.0208\n",
      "2025-06-01 14:50:46 [INFO]: epoch 739: training loss 0.0213\n",
      "2025-06-01 14:50:46 [INFO]: epoch 740: training loss 0.0246\n",
      "2025-06-01 14:50:46 [INFO]: epoch 741: training loss 0.0239\n",
      "2025-06-01 14:50:46 [INFO]: epoch 742: training loss 0.0208\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-01 14:50:46 [INFO]: epoch 743: training loss 0.0245\n",
      "2025-06-01 14:50:46 [INFO]: epoch 744: training loss 0.0229\n",
      "2025-06-01 14:50:46 [INFO]: epoch 745: training loss 0.0223\n",
      "2025-06-01 14:50:46 [INFO]: epoch 746: training loss 0.0227\n",
      "2025-06-01 14:50:46 [INFO]: epoch 747: training loss 0.0193\n",
      "2025-06-01 14:50:46 [INFO]: epoch 748: training loss 0.0264\n",
      "2025-06-01 14:50:46 [INFO]: epoch 749: training loss 0.0276\n",
      "2025-06-01 14:50:46 [INFO]: epoch 750: training loss 0.0239\n",
      "2025-06-01 14:50:46 [INFO]: epoch 751: training loss 0.0258\n",
      "2025-06-01 14:50:46 [INFO]: epoch 752: training loss 0.0266\n",
      "2025-06-01 14:50:46 [INFO]: epoch 753: training loss 0.0225\n",
      "2025-06-01 14:50:46 [INFO]: epoch 754: training loss 0.0217\n",
      "2025-06-01 14:50:46 [INFO]: epoch 755: training loss 0.0247\n",
      "2025-06-01 14:50:46 [INFO]: epoch 756: training loss 0.0238\n",
      "2025-06-01 14:50:46 [INFO]: epoch 757: training loss 0.0220\n",
      "2025-06-01 14:50:46 [INFO]: epoch 758: training loss 0.0283\n",
      "2025-06-01 14:50:46 [INFO]: epoch 759: training loss 0.0267\n",
      "2025-06-01 14:50:46 [INFO]: epoch 760: training loss 0.0220\n",
      "2025-06-01 14:50:46 [INFO]: epoch 761: training loss 0.0253\n",
      "2025-06-01 14:50:46 [INFO]: epoch 762: training loss 0.0273\n",
      "2025-06-01 14:50:46 [INFO]: epoch 763: training loss 0.0267\n",
      "2025-06-01 14:50:46 [INFO]: epoch 764: training loss 0.0241\n",
      "2025-06-01 14:50:46 [INFO]: epoch 765: training loss 0.0198\n",
      "2025-06-01 14:50:46 [INFO]: epoch 766: training loss 0.0229\n",
      "2025-06-01 14:50:46 [INFO]: epoch 767: training loss 0.0205\n",
      "2025-06-01 14:50:46 [INFO]: epoch 768: training loss 0.0208\n",
      "2025-06-01 14:50:46 [INFO]: epoch 769: training loss 0.0223\n",
      "2025-06-01 14:50:46 [INFO]: epoch 770: training loss 0.0224\n",
      "2025-06-01 14:50:46 [INFO]: epoch 771: training loss 0.0214\n",
      "2025-06-01 14:50:46 [INFO]: epoch 772: training loss 0.0192\n",
      "2025-06-01 14:50:46 [INFO]: epoch 773: training loss 0.0261\n",
      "2025-06-01 14:50:46 [INFO]: epoch 774: training loss 0.0237\n",
      "2025-06-01 14:50:46 [INFO]: epoch 775: training loss 0.0177\n",
      "2025-06-01 14:50:46 [INFO]: epoch 776: training loss 0.0288\n",
      "2025-06-01 14:50:46 [INFO]: epoch 777: training loss 0.0192\n",
      "2025-06-01 14:50:46 [INFO]: epoch 778: training loss 0.0208\n",
      "2025-06-01 14:50:46 [INFO]: epoch 779: training loss 0.0216\n",
      "2025-06-01 14:50:46 [INFO]: epoch 780: training loss 0.0208\n",
      "2025-06-01 14:50:46 [INFO]: epoch 781: training loss 0.0194\n",
      "2025-06-01 14:50:46 [INFO]: epoch 782: training loss 0.0214\n",
      "2025-06-01 14:50:46 [INFO]: epoch 783: training loss 0.0232\n",
      "2025-06-01 14:50:46 [INFO]: epoch 784: training loss 0.0179\n",
      "2025-06-01 14:50:46 [INFO]: epoch 785: training loss 0.0200\n",
      "2025-06-01 14:50:46 [INFO]: epoch 786: training loss 0.0202\n",
      "2025-06-01 14:50:46 [INFO]: epoch 787: training loss 0.0235\n",
      "2025-06-01 14:50:46 [INFO]: epoch 788: training loss 0.0225\n",
      "2025-06-01 14:50:46 [INFO]: epoch 789: training loss 0.0323\n",
      "2025-06-01 14:50:46 [INFO]: epoch 790: training loss 0.0239\n",
      "2025-06-01 14:50:46 [INFO]: epoch 791: training loss 0.0225\n",
      "2025-06-01 14:50:46 [INFO]: epoch 792: training loss 0.0341\n",
      "2025-06-01 14:50:46 [INFO]: epoch 793: training loss 0.0270\n",
      "2025-06-01 14:50:46 [INFO]: epoch 794: training loss 0.0264\n",
      "2025-06-01 14:50:46 [INFO]: epoch 795: training loss 0.0288\n",
      "2025-06-01 14:50:46 [INFO]: epoch 796: training loss 0.0241\n",
      "2025-06-01 14:50:46 [INFO]: epoch 797: training loss 0.0238\n",
      "2025-06-01 14:50:47 [INFO]: epoch 798: training loss 0.0242\n",
      "2025-06-01 14:50:47 [INFO]: epoch 799: training loss 0.0253\n",
      "2025-06-01 14:50:47 [INFO]: Finished training.\n",
      "2025-06-01 14:50:47 [WARNING]: 🚨DeprecationWarning: The method impute is deprecated. Please use `predict` instead.\n",
      " 67%|████████████████████████████████████████████████████████                            | 4/6 [00:42<00:21, 10.58s/it]2025-06-01 14:50:47 [INFO]: No given device, using default device: cuda\n",
      "2025-06-01 14:50:47 [WARNING]: saving_path not given. Model files and tensorboard file will not be saved.\n",
      "2025-06-01 14:50:47 [INFO]: Model initialized successfully with the number of trainable parameters: 597,780\n",
      "2025-06-01 14:50:47 [INFO]: epoch 0: training loss 1.3621\n",
      "2025-06-01 14:50:47 [INFO]: epoch 1: training loss 0.5851\n",
      "2025-06-01 14:50:47 [INFO]: epoch 2: training loss 0.6895\n",
      "2025-06-01 14:50:47 [INFO]: epoch 3: training loss 0.6817\n",
      "2025-06-01 14:50:47 [INFO]: epoch 4: training loss 0.6127\n",
      "2025-06-01 14:50:47 [INFO]: epoch 5: training loss 0.5180\n",
      "2025-06-01 14:50:47 [INFO]: epoch 6: training loss 0.5179\n",
      "2025-06-01 14:50:47 [INFO]: epoch 7: training loss 0.4786\n",
      "2025-06-01 14:50:47 [INFO]: epoch 8: training loss 0.4547\n",
      "2025-06-01 14:50:47 [INFO]: epoch 9: training loss 0.4426\n",
      "2025-06-01 14:50:47 [INFO]: epoch 10: training loss 0.4411\n",
      "2025-06-01 14:50:47 [INFO]: epoch 11: training loss 0.5150\n",
      "2025-06-01 14:50:47 [INFO]: epoch 12: training loss 0.5072\n",
      "2025-06-01 14:50:47 [INFO]: epoch 13: training loss 0.3857\n",
      "2025-06-01 14:50:47 [INFO]: epoch 14: training loss 0.3724\n",
      "2025-06-01 14:50:47 [INFO]: epoch 15: training loss 0.3769\n",
      "2025-06-01 14:50:47 [INFO]: epoch 16: training loss 0.3553\n",
      "2025-06-01 14:50:47 [INFO]: epoch 17: training loss 0.3325\n",
      "2025-06-01 14:50:47 [INFO]: epoch 18: training loss 0.3443\n",
      "2025-06-01 14:50:47 [INFO]: epoch 19: training loss 0.3777\n",
      "2025-06-01 14:50:47 [INFO]: epoch 20: training loss 0.3950\n",
      "2025-06-01 14:50:47 [INFO]: epoch 21: training loss 0.3528\n",
      "2025-06-01 14:50:47 [INFO]: epoch 22: training loss 0.3282\n",
      "2025-06-01 14:50:47 [INFO]: epoch 23: training loss 0.3375\n",
      "2025-06-01 14:50:47 [INFO]: epoch 24: training loss 0.3601\n",
      "2025-06-01 14:50:47 [INFO]: epoch 25: training loss 0.3131\n",
      "2025-06-01 14:50:47 [INFO]: epoch 26: training loss 0.3386\n",
      "2025-06-01 14:50:47 [INFO]: epoch 27: training loss 0.3513\n",
      "2025-06-01 14:50:47 [INFO]: epoch 28: training loss 0.3280\n",
      "2025-06-01 14:50:47 [INFO]: epoch 29: training loss 0.2907\n",
      "2025-06-01 14:50:47 [INFO]: epoch 30: training loss 0.3303\n",
      "2025-06-01 14:50:47 [INFO]: epoch 31: training loss 0.3176\n",
      "2025-06-01 14:50:47 [INFO]: epoch 32: training loss 0.3400\n",
      "2025-06-01 14:50:47 [INFO]: epoch 33: training loss 0.3227\n",
      "2025-06-01 14:50:47 [INFO]: epoch 34: training loss 0.3010\n",
      "2025-06-01 14:50:47 [INFO]: epoch 35: training loss 0.3091\n",
      "2025-06-01 14:50:47 [INFO]: epoch 36: training loss 0.3009\n",
      "2025-06-01 14:50:47 [INFO]: epoch 37: training loss 0.2875\n",
      "2025-06-01 14:50:47 [INFO]: epoch 38: training loss 0.2951\n",
      "2025-06-01 14:50:47 [INFO]: epoch 39: training loss 0.2882\n",
      "2025-06-01 14:50:47 [INFO]: epoch 40: training loss 0.2940\n",
      "2025-06-01 14:50:47 [INFO]: epoch 41: training loss 0.2903\n",
      "2025-06-01 14:50:47 [INFO]: epoch 42: training loss 0.2941\n",
      "2025-06-01 14:50:47 [INFO]: epoch 43: training loss 0.2835\n",
      "2025-06-01 14:50:47 [INFO]: epoch 44: training loss 0.2797\n",
      "2025-06-01 14:50:47 [INFO]: epoch 45: training loss 0.2631\n",
      "2025-06-01 14:50:47 [INFO]: epoch 46: training loss 0.2612\n",
      "2025-06-01 14:50:47 [INFO]: epoch 47: training loss 0.2537\n",
      "2025-06-01 14:50:47 [INFO]: epoch 48: training loss 0.2461\n",
      "2025-06-01 14:50:47 [INFO]: epoch 49: training loss 0.2601\n",
      "2025-06-01 14:50:47 [INFO]: epoch 50: training loss 0.2469\n",
      "2025-06-01 14:50:47 [INFO]: epoch 51: training loss 0.2388\n",
      "2025-06-01 14:50:47 [INFO]: epoch 52: training loss 0.2459\n",
      "2025-06-01 14:50:47 [INFO]: epoch 53: training loss 0.2493\n",
      "2025-06-01 14:50:47 [INFO]: epoch 54: training loss 0.2635\n",
      "2025-06-01 14:50:47 [INFO]: epoch 55: training loss 0.2538\n",
      "2025-06-01 14:50:47 [INFO]: epoch 56: training loss 0.2318\n",
      "2025-06-01 14:50:47 [INFO]: epoch 57: training loss 0.2473\n",
      "2025-06-01 14:50:47 [INFO]: epoch 58: training loss 0.2362\n",
      "2025-06-01 14:50:47 [INFO]: epoch 59: training loss 0.2398\n",
      "2025-06-01 14:50:47 [INFO]: epoch 60: training loss 0.2396\n",
      "2025-06-01 14:50:47 [INFO]: epoch 61: training loss 0.2688\n",
      "2025-06-01 14:50:47 [INFO]: epoch 62: training loss 0.2503\n",
      "2025-06-01 14:50:47 [INFO]: epoch 63: training loss 0.2269\n",
      "2025-06-01 14:50:48 [INFO]: epoch 64: training loss 0.2479\n",
      "2025-06-01 14:50:48 [INFO]: epoch 65: training loss 0.2600\n",
      "2025-06-01 14:50:48 [INFO]: epoch 66: training loss 0.2451\n",
      "2025-06-01 14:50:48 [INFO]: epoch 67: training loss 0.2525\n",
      "2025-06-01 14:50:48 [INFO]: epoch 68: training loss 0.2390\n",
      "2025-06-01 14:50:48 [INFO]: epoch 69: training loss 0.2060\n",
      "2025-06-01 14:50:48 [INFO]: epoch 70: training loss 0.2063\n",
      "2025-06-01 14:50:48 [INFO]: epoch 71: training loss 0.2269\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-01 14:50:48 [INFO]: epoch 72: training loss 0.2225\n",
      "2025-06-01 14:50:48 [INFO]: epoch 73: training loss 0.2090\n",
      "2025-06-01 14:50:48 [INFO]: epoch 74: training loss 0.1995\n",
      "2025-06-01 14:50:48 [INFO]: epoch 75: training loss 0.2168\n",
      "2025-06-01 14:50:48 [INFO]: epoch 76: training loss 0.2285\n",
      "2025-06-01 14:50:48 [INFO]: epoch 77: training loss 0.2067\n",
      "2025-06-01 14:50:48 [INFO]: epoch 78: training loss 0.1947\n",
      "2025-06-01 14:50:48 [INFO]: epoch 79: training loss 0.2000\n",
      "2025-06-01 14:50:48 [INFO]: epoch 80: training loss 0.1948\n",
      "2025-06-01 14:50:48 [INFO]: epoch 81: training loss 0.2103\n",
      "2025-06-01 14:50:48 [INFO]: epoch 82: training loss 0.2089\n",
      "2025-06-01 14:50:48 [INFO]: epoch 83: training loss 0.1916\n",
      "2025-06-01 14:50:48 [INFO]: epoch 84: training loss 0.1869\n",
      "2025-06-01 14:50:48 [INFO]: epoch 85: training loss 0.2060\n",
      "2025-06-01 14:50:48 [INFO]: epoch 86: training loss 0.1974\n",
      "2025-06-01 14:50:48 [INFO]: epoch 87: training loss 0.1828\n",
      "2025-06-01 14:50:48 [INFO]: epoch 88: training loss 0.1838\n",
      "2025-06-01 14:50:48 [INFO]: epoch 89: training loss 0.1967\n",
      "2025-06-01 14:50:48 [INFO]: epoch 90: training loss 0.1722\n",
      "2025-06-01 14:50:48 [INFO]: epoch 91: training loss 0.1584\n",
      "2025-06-01 14:50:48 [INFO]: epoch 92: training loss 0.1805\n",
      "2025-06-01 14:50:48 [INFO]: epoch 93: training loss 0.1798\n",
      "2025-06-01 14:50:48 [INFO]: epoch 94: training loss 0.1865\n",
      "2025-06-01 14:50:48 [INFO]: epoch 95: training loss 0.1686\n",
      "2025-06-01 14:50:48 [INFO]: epoch 96: training loss 0.1695\n",
      "2025-06-01 14:50:48 [INFO]: epoch 97: training loss 0.1807\n",
      "2025-06-01 14:50:48 [INFO]: epoch 98: training loss 0.1686\n",
      "2025-06-01 14:50:48 [INFO]: epoch 99: training loss 0.1795\n",
      "2025-06-01 14:50:48 [INFO]: epoch 100: training loss 0.1629\n",
      "2025-06-01 14:50:48 [INFO]: epoch 101: training loss 0.1693\n",
      "2025-06-01 14:50:48 [INFO]: epoch 102: training loss 0.1474\n",
      "2025-06-01 14:50:48 [INFO]: epoch 103: training loss 0.1494\n",
      "2025-06-01 14:50:48 [INFO]: epoch 104: training loss 0.1456\n",
      "2025-06-01 14:50:48 [INFO]: epoch 105: training loss 0.1510\n",
      "2025-06-01 14:50:48 [INFO]: epoch 106: training loss 0.1559\n",
      "2025-06-01 14:50:48 [INFO]: epoch 107: training loss 0.1528\n",
      "2025-06-01 14:50:48 [INFO]: epoch 108: training loss 0.1542\n",
      "2025-06-01 14:50:48 [INFO]: epoch 109: training loss 0.1415\n",
      "2025-06-01 14:50:48 [INFO]: epoch 110: training loss 0.1445\n",
      "2025-06-01 14:50:48 [INFO]: epoch 111: training loss 0.1616\n",
      "2025-06-01 14:50:48 [INFO]: epoch 112: training loss 0.1591\n",
      "2025-06-01 14:50:48 [INFO]: epoch 113: training loss 0.1431\n",
      "2025-06-01 14:50:48 [INFO]: epoch 114: training loss 0.1346\n",
      "2025-06-01 14:50:48 [INFO]: epoch 115: training loss 0.1414\n",
      "2025-06-01 14:50:48 [INFO]: epoch 116: training loss 0.1456\n",
      "2025-06-01 14:50:48 [INFO]: epoch 117: training loss 0.1611\n",
      "2025-06-01 14:50:48 [INFO]: epoch 118: training loss 0.1383\n",
      "2025-06-01 14:50:48 [INFO]: epoch 119: training loss 0.1353\n",
      "2025-06-01 14:50:48 [INFO]: epoch 120: training loss 0.1362\n",
      "2025-06-01 14:50:48 [INFO]: epoch 121: training loss 0.1463\n",
      "2025-06-01 14:50:48 [INFO]: epoch 122: training loss 0.1521\n",
      "2025-06-01 14:50:48 [INFO]: epoch 123: training loss 0.1384\n",
      "2025-06-01 14:50:48 [INFO]: epoch 124: training loss 0.1264\n",
      "2025-06-01 14:50:48 [INFO]: epoch 125: training loss 0.1405\n",
      "2025-06-01 14:50:48 [INFO]: epoch 126: training loss 0.1557\n",
      "2025-06-01 14:50:48 [INFO]: epoch 127: training loss 0.1396\n",
      "2025-06-01 14:50:48 [INFO]: epoch 128: training loss 0.1378\n",
      "2025-06-01 14:50:48 [INFO]: epoch 129: training loss 0.1254\n",
      "2025-06-01 14:50:48 [INFO]: epoch 130: training loss 0.1256\n",
      "2025-06-01 14:50:48 [INFO]: epoch 131: training loss 0.1418\n",
      "2025-06-01 14:50:48 [INFO]: epoch 132: training loss 0.1335\n",
      "2025-06-01 14:50:48 [INFO]: epoch 133: training loss 0.1094\n",
      "2025-06-01 14:50:48 [INFO]: epoch 134: training loss 0.1119\n",
      "2025-06-01 14:50:48 [INFO]: epoch 135: training loss 0.1145\n",
      "2025-06-01 14:50:48 [INFO]: epoch 136: training loss 0.1128\n",
      "2025-06-01 14:50:48 [INFO]: epoch 137: training loss 0.1135\n",
      "2025-06-01 14:50:48 [INFO]: epoch 138: training loss 0.1187\n",
      "2025-06-01 14:50:48 [INFO]: epoch 139: training loss 0.1243\n",
      "2025-06-01 14:50:49 [INFO]: epoch 140: training loss 0.1050\n",
      "2025-06-01 14:50:49 [INFO]: epoch 141: training loss 0.1057\n",
      "2025-06-01 14:50:49 [INFO]: epoch 142: training loss 0.1032\n",
      "2025-06-01 14:50:49 [INFO]: epoch 143: training loss 0.1077\n",
      "2025-06-01 14:50:49 [INFO]: epoch 144: training loss 0.1162\n",
      "2025-06-01 14:50:49 [INFO]: epoch 145: training loss 0.0922\n",
      "2025-06-01 14:50:49 [INFO]: epoch 146: training loss 0.1011\n",
      "2025-06-01 14:50:49 [INFO]: epoch 147: training loss 0.1082\n",
      "2025-06-01 14:50:49 [INFO]: epoch 148: training loss 0.0988\n",
      "2025-06-01 14:50:49 [INFO]: epoch 149: training loss 0.1026\n",
      "2025-06-01 14:50:49 [INFO]: epoch 150: training loss 0.0927\n",
      "2025-06-01 14:50:49 [INFO]: epoch 151: training loss 0.1068\n",
      "2025-06-01 14:50:49 [INFO]: epoch 152: training loss 0.0921\n",
      "2025-06-01 14:50:49 [INFO]: epoch 153: training loss 0.0967\n",
      "2025-06-01 14:50:49 [INFO]: epoch 154: training loss 0.1031\n",
      "2025-06-01 14:50:49 [INFO]: epoch 155: training loss 0.0915\n",
      "2025-06-01 14:50:49 [INFO]: epoch 156: training loss 0.0907\n",
      "2025-06-01 14:50:49 [INFO]: epoch 157: training loss 0.1052\n",
      "2025-06-01 14:50:49 [INFO]: epoch 158: training loss 0.0891\n",
      "2025-06-01 14:50:49 [INFO]: epoch 159: training loss 0.0961\n",
      "2025-06-01 14:50:49 [INFO]: epoch 160: training loss 0.0997\n",
      "2025-06-01 14:50:49 [INFO]: epoch 161: training loss 0.0888\n",
      "2025-06-01 14:50:49 [INFO]: epoch 162: training loss 0.1104\n",
      "2025-06-01 14:50:49 [INFO]: epoch 163: training loss 0.0973\n",
      "2025-06-01 14:50:49 [INFO]: epoch 164: training loss 0.0877\n",
      "2025-06-01 14:50:49 [INFO]: epoch 165: training loss 0.0889\n",
      "2025-06-01 14:50:49 [INFO]: epoch 166: training loss 0.0816\n",
      "2025-06-01 14:50:49 [INFO]: epoch 167: training loss 0.0976\n",
      "2025-06-01 14:50:49 [INFO]: epoch 168: training loss 0.0958\n",
      "2025-06-01 14:50:49 [INFO]: epoch 169: training loss 0.0967\n",
      "2025-06-01 14:50:49 [INFO]: epoch 170: training loss 0.1044\n",
      "2025-06-01 14:50:49 [INFO]: epoch 171: training loss 0.0696\n",
      "2025-06-01 14:50:49 [INFO]: epoch 172: training loss 0.0803\n",
      "2025-06-01 14:50:49 [INFO]: epoch 173: training loss 0.0846\n",
      "2025-06-01 14:50:49 [INFO]: epoch 174: training loss 0.0794\n",
      "2025-06-01 14:50:49 [INFO]: epoch 175: training loss 0.0679\n",
      "2025-06-01 14:50:49 [INFO]: epoch 176: training loss 0.0823\n",
      "2025-06-01 14:50:49 [INFO]: epoch 177: training loss 0.0738\n",
      "2025-06-01 14:50:49 [INFO]: epoch 178: training loss 0.0784\n",
      "2025-06-01 14:50:49 [INFO]: epoch 179: training loss 0.0770\n",
      "2025-06-01 14:50:49 [INFO]: epoch 180: training loss 0.0590\n",
      "2025-06-01 14:50:49 [INFO]: epoch 181: training loss 0.0733\n",
      "2025-06-01 14:50:49 [INFO]: epoch 182: training loss 0.0803\n",
      "2025-06-01 14:50:49 [INFO]: epoch 183: training loss 0.0865\n",
      "2025-06-01 14:50:49 [INFO]: epoch 184: training loss 0.0861\n",
      "2025-06-01 14:50:49 [INFO]: epoch 185: training loss 0.0639\n",
      "2025-06-01 14:50:49 [INFO]: epoch 186: training loss 0.0634\n",
      "2025-06-01 14:50:49 [INFO]: epoch 187: training loss 0.0776\n",
      "2025-06-01 14:50:49 [INFO]: epoch 188: training loss 0.0678\n",
      "2025-06-01 14:50:49 [INFO]: epoch 189: training loss 0.0527\n",
      "2025-06-01 14:50:49 [INFO]: epoch 190: training loss 0.0694\n",
      "2025-06-01 14:50:49 [INFO]: epoch 191: training loss 0.0760\n",
      "2025-06-01 14:50:49 [INFO]: epoch 192: training loss 0.0715\n",
      "2025-06-01 14:50:49 [INFO]: epoch 193: training loss 0.0655\n",
      "2025-06-01 14:50:49 [INFO]: epoch 194: training loss 0.0667\n",
      "2025-06-01 14:50:49 [INFO]: epoch 195: training loss 0.0731\n",
      "2025-06-01 14:50:49 [INFO]: epoch 196: training loss 0.0598\n",
      "2025-06-01 14:50:49 [INFO]: epoch 197: training loss 0.0678\n",
      "2025-06-01 14:50:49 [INFO]: epoch 198: training loss 0.0605\n",
      "2025-06-01 14:50:49 [INFO]: epoch 199: training loss 0.0668\n",
      "2025-06-01 14:50:49 [INFO]: epoch 200: training loss 0.0776\n",
      "2025-06-01 14:50:49 [INFO]: epoch 201: training loss 0.0689\n",
      "2025-06-01 14:50:49 [INFO]: epoch 202: training loss 0.0720\n",
      "2025-06-01 14:50:49 [INFO]: epoch 203: training loss 0.0628\n",
      "2025-06-01 14:50:49 [INFO]: epoch 204: training loss 0.0700\n",
      "2025-06-01 14:50:49 [INFO]: epoch 205: training loss 0.0887\n",
      "2025-06-01 14:50:49 [INFO]: epoch 206: training loss 0.0670\n",
      "2025-06-01 14:50:49 [INFO]: epoch 207: training loss 0.0574\n",
      "2025-06-01 14:50:49 [INFO]: epoch 208: training loss 0.0705\n",
      "2025-06-01 14:50:49 [INFO]: epoch 209: training loss 0.0724\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-01 14:50:49 [INFO]: epoch 210: training loss 0.0683\n",
      "2025-06-01 14:50:49 [INFO]: epoch 211: training loss 0.0711\n",
      "2025-06-01 14:50:49 [INFO]: epoch 212: training loss 0.0666\n",
      "2025-06-01 14:50:49 [INFO]: epoch 213: training loss 0.0629\n",
      "2025-06-01 14:50:49 [INFO]: epoch 214: training loss 0.0597\n",
      "2025-06-01 14:50:49 [INFO]: epoch 215: training loss 0.0709\n",
      "2025-06-01 14:50:49 [INFO]: epoch 216: training loss 0.0717\n",
      "2025-06-01 14:50:50 [INFO]: epoch 217: training loss 0.0605\n",
      "2025-06-01 14:50:50 [INFO]: epoch 218: training loss 0.0531\n",
      "2025-06-01 14:50:50 [INFO]: epoch 219: training loss 0.0617\n",
      "2025-06-01 14:50:50 [INFO]: epoch 220: training loss 0.0744\n",
      "2025-06-01 14:50:50 [INFO]: epoch 221: training loss 0.0617\n",
      "2025-06-01 14:50:50 [INFO]: epoch 222: training loss 0.0586\n",
      "2025-06-01 14:50:50 [INFO]: epoch 223: training loss 0.0704\n",
      "2025-06-01 14:50:50 [INFO]: epoch 224: training loss 0.0657\n",
      "2025-06-01 14:50:50 [INFO]: epoch 225: training loss 0.0712\n",
      "2025-06-01 14:50:50 [INFO]: epoch 226: training loss 0.0738\n",
      "2025-06-01 14:50:50 [INFO]: epoch 227: training loss 0.0581\n",
      "2025-06-01 14:50:50 [INFO]: epoch 228: training loss 0.0465\n",
      "2025-06-01 14:50:50 [INFO]: epoch 229: training loss 0.0538\n",
      "2025-06-01 14:50:50 [INFO]: epoch 230: training loss 0.0607\n",
      "2025-06-01 14:50:50 [INFO]: epoch 231: training loss 0.0602\n",
      "2025-06-01 14:50:50 [INFO]: epoch 232: training loss 0.0513\n",
      "2025-06-01 14:50:50 [INFO]: epoch 233: training loss 0.0497\n",
      "2025-06-01 14:50:50 [INFO]: epoch 234: training loss 0.0649\n",
      "2025-06-01 14:50:50 [INFO]: epoch 235: training loss 0.0596\n",
      "2025-06-01 14:50:50 [INFO]: epoch 236: training loss 0.0594\n",
      "2025-06-01 14:50:50 [INFO]: epoch 237: training loss 0.0556\n",
      "2025-06-01 14:50:50 [INFO]: epoch 238: training loss 0.0519\n",
      "2025-06-01 14:50:50 [INFO]: epoch 239: training loss 0.0504\n",
      "2025-06-01 14:50:50 [INFO]: epoch 240: training loss 0.0603\n",
      "2025-06-01 14:50:50 [INFO]: epoch 241: training loss 0.0530\n",
      "2025-06-01 14:50:50 [INFO]: epoch 242: training loss 0.0537\n",
      "2025-06-01 14:50:50 [INFO]: epoch 243: training loss 0.0526\n",
      "2025-06-01 14:50:50 [INFO]: epoch 244: training loss 0.0483\n",
      "2025-06-01 14:50:50 [INFO]: epoch 245: training loss 0.0508\n",
      "2025-06-01 14:50:50 [INFO]: epoch 246: training loss 0.0502\n",
      "2025-06-01 14:50:50 [INFO]: epoch 247: training loss 0.0521\n",
      "2025-06-01 14:50:50 [INFO]: epoch 248: training loss 0.0510\n",
      "2025-06-01 14:50:50 [INFO]: epoch 249: training loss 0.0705\n",
      "2025-06-01 14:50:50 [INFO]: epoch 250: training loss 0.0528\n",
      "2025-06-01 14:50:50 [INFO]: epoch 251: training loss 0.0546\n",
      "2025-06-01 14:50:50 [INFO]: epoch 252: training loss 0.0514\n",
      "2025-06-01 14:50:50 [INFO]: epoch 253: training loss 0.0459\n",
      "2025-06-01 14:50:50 [INFO]: epoch 254: training loss 0.0465\n",
      "2025-06-01 14:50:50 [INFO]: epoch 255: training loss 0.0460\n",
      "2025-06-01 14:50:50 [INFO]: epoch 256: training loss 0.0458\n",
      "2025-06-01 14:50:50 [INFO]: epoch 257: training loss 0.0414\n",
      "2025-06-01 14:50:50 [INFO]: epoch 258: training loss 0.0561\n",
      "2025-06-01 14:50:50 [INFO]: epoch 259: training loss 0.0551\n",
      "2025-06-01 14:50:50 [INFO]: epoch 260: training loss 0.0465\n",
      "2025-06-01 14:50:50 [INFO]: epoch 261: training loss 0.0446\n",
      "2025-06-01 14:50:50 [INFO]: epoch 262: training loss 0.0519\n",
      "2025-06-01 14:50:50 [INFO]: epoch 263: training loss 0.0517\n",
      "2025-06-01 14:50:50 [INFO]: epoch 264: training loss 0.0476\n",
      "2025-06-01 14:50:50 [INFO]: epoch 265: training loss 0.0524\n",
      "2025-06-01 14:50:50 [INFO]: epoch 266: training loss 0.0422\n",
      "2025-06-01 14:50:50 [INFO]: epoch 267: training loss 0.0503\n",
      "2025-06-01 14:50:50 [INFO]: epoch 268: training loss 0.0429\n",
      "2025-06-01 14:50:50 [INFO]: epoch 269: training loss 0.0477\n",
      "2025-06-01 14:50:50 [INFO]: epoch 270: training loss 0.0489\n",
      "2025-06-01 14:50:50 [INFO]: epoch 271: training loss 0.0451\n",
      "2025-06-01 14:50:50 [INFO]: epoch 272: training loss 0.0445\n",
      "2025-06-01 14:50:50 [INFO]: epoch 273: training loss 0.0415\n",
      "2025-06-01 14:50:50 [INFO]: epoch 274: training loss 0.0376\n",
      "2025-06-01 14:50:50 [INFO]: epoch 275: training loss 0.0450\n",
      "2025-06-01 14:50:50 [INFO]: epoch 276: training loss 0.0523\n",
      "2025-06-01 14:50:50 [INFO]: epoch 277: training loss 0.0427\n",
      "2025-06-01 14:50:50 [INFO]: epoch 278: training loss 0.0523\n",
      "2025-06-01 14:50:50 [INFO]: epoch 279: training loss 0.0417\n",
      "2025-06-01 14:50:50 [INFO]: epoch 280: training loss 0.0415\n",
      "2025-06-01 14:50:50 [INFO]: epoch 281: training loss 0.0431\n",
      "2025-06-01 14:50:50 [INFO]: epoch 282: training loss 0.0452\n",
      "2025-06-01 14:50:50 [INFO]: epoch 283: training loss 0.0509\n",
      "2025-06-01 14:50:50 [INFO]: epoch 284: training loss 0.0471\n",
      "2025-06-01 14:50:50 [INFO]: epoch 285: training loss 0.0469\n",
      "2025-06-01 14:50:50 [INFO]: epoch 286: training loss 0.0428\n",
      "2025-06-01 14:50:50 [INFO]: epoch 287: training loss 0.0466\n",
      "2025-06-01 14:50:50 [INFO]: epoch 288: training loss 0.0457\n",
      "2025-06-01 14:50:50 [INFO]: epoch 289: training loss 0.0437\n",
      "2025-06-01 14:50:50 [INFO]: epoch 290: training loss 0.0479\n",
      "2025-06-01 14:50:50 [INFO]: epoch 291: training loss 0.0393\n",
      "2025-06-01 14:50:50 [INFO]: epoch 292: training loss 0.0453\n",
      "2025-06-01 14:50:51 [INFO]: epoch 293: training loss 0.0468\n",
      "2025-06-01 14:50:51 [INFO]: epoch 294: training loss 0.0383\n",
      "2025-06-01 14:50:51 [INFO]: epoch 295: training loss 0.0457\n",
      "2025-06-01 14:50:51 [INFO]: epoch 296: training loss 0.0543\n",
      "2025-06-01 14:50:51 [INFO]: epoch 297: training loss 0.0456\n",
      "2025-06-01 14:50:51 [INFO]: epoch 298: training loss 0.0441\n",
      "2025-06-01 14:50:51 [INFO]: epoch 299: training loss 0.0423\n",
      "2025-06-01 14:50:51 [INFO]: epoch 300: training loss 0.0460\n",
      "2025-06-01 14:50:51 [INFO]: epoch 301: training loss 0.0434\n",
      "2025-06-01 14:50:51 [INFO]: epoch 302: training loss 0.0368\n",
      "2025-06-01 14:50:51 [INFO]: epoch 303: training loss 0.0414\n",
      "2025-06-01 14:50:51 [INFO]: epoch 304: training loss 0.0459\n",
      "2025-06-01 14:50:51 [INFO]: epoch 305: training loss 0.0387\n",
      "2025-06-01 14:50:51 [INFO]: epoch 306: training loss 0.0382\n",
      "2025-06-01 14:50:51 [INFO]: epoch 307: training loss 0.0450\n",
      "2025-06-01 14:50:51 [INFO]: epoch 308: training loss 0.0403\n",
      "2025-06-01 14:50:51 [INFO]: epoch 309: training loss 0.0425\n",
      "2025-06-01 14:50:51 [INFO]: epoch 310: training loss 0.0382\n",
      "2025-06-01 14:50:51 [INFO]: epoch 311: training loss 0.0460\n",
      "2025-06-01 14:50:51 [INFO]: epoch 312: training loss 0.0449\n",
      "2025-06-01 14:50:51 [INFO]: epoch 313: training loss 0.0406\n",
      "2025-06-01 14:50:51 [INFO]: epoch 314: training loss 0.0431\n",
      "2025-06-01 14:50:51 [INFO]: epoch 315: training loss 0.0564\n",
      "2025-06-01 14:50:51 [INFO]: epoch 316: training loss 0.0443\n",
      "2025-06-01 14:50:51 [INFO]: epoch 317: training loss 0.0383\n",
      "2025-06-01 14:50:51 [INFO]: epoch 318: training loss 0.0499\n",
      "2025-06-01 14:50:51 [INFO]: epoch 319: training loss 0.0388\n",
      "2025-06-01 14:50:51 [INFO]: epoch 320: training loss 0.0409\n",
      "2025-06-01 14:50:51 [INFO]: epoch 321: training loss 0.0528\n",
      "2025-06-01 14:50:51 [INFO]: epoch 322: training loss 0.0489\n",
      "2025-06-01 14:50:51 [INFO]: epoch 323: training loss 0.0336\n",
      "2025-06-01 14:50:51 [INFO]: epoch 324: training loss 0.0544\n",
      "2025-06-01 14:50:51 [INFO]: epoch 325: training loss 0.0490\n",
      "2025-06-01 14:50:51 [INFO]: epoch 326: training loss 0.0378\n",
      "2025-06-01 14:50:51 [INFO]: epoch 327: training loss 0.0425\n",
      "2025-06-01 14:50:51 [INFO]: epoch 328: training loss 0.0428\n",
      "2025-06-01 14:50:51 [INFO]: epoch 329: training loss 0.0431\n",
      "2025-06-01 14:50:51 [INFO]: epoch 330: training loss 0.0346\n",
      "2025-06-01 14:50:51 [INFO]: epoch 331: training loss 0.0307\n",
      "2025-06-01 14:50:51 [INFO]: epoch 332: training loss 0.0405\n",
      "2025-06-01 14:50:51 [INFO]: epoch 333: training loss 0.0488\n",
      "2025-06-01 14:50:51 [INFO]: epoch 334: training loss 0.0463\n",
      "2025-06-01 14:50:51 [INFO]: epoch 335: training loss 0.0333\n",
      "2025-06-01 14:50:51 [INFO]: epoch 336: training loss 0.0424\n",
      "2025-06-01 14:50:51 [INFO]: epoch 337: training loss 0.0420\n",
      "2025-06-01 14:50:51 [INFO]: epoch 338: training loss 0.0438\n",
      "2025-06-01 14:50:51 [INFO]: epoch 339: training loss 0.0335\n",
      "2025-06-01 14:50:51 [INFO]: epoch 340: training loss 0.0401\n",
      "2025-06-01 14:50:51 [INFO]: epoch 341: training loss 0.0353\n",
      "2025-06-01 14:50:51 [INFO]: epoch 342: training loss 0.0324\n",
      "2025-06-01 14:50:51 [INFO]: epoch 343: training loss 0.0340\n",
      "2025-06-01 14:50:51 [INFO]: epoch 344: training loss 0.0436\n",
      "2025-06-01 14:50:51 [INFO]: epoch 345: training loss 0.0405\n",
      "2025-06-01 14:50:51 [INFO]: epoch 346: training loss 0.0386\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-01 14:50:51 [INFO]: epoch 347: training loss 0.0482\n",
      "2025-06-01 14:50:51 [INFO]: epoch 348: training loss 0.0400\n",
      "2025-06-01 14:50:51 [INFO]: epoch 349: training loss 0.0438\n",
      "2025-06-01 14:50:51 [INFO]: epoch 350: training loss 0.0405\n",
      "2025-06-01 14:50:51 [INFO]: epoch 351: training loss 0.0377\n",
      "2025-06-01 14:50:51 [INFO]: epoch 352: training loss 0.0385\n",
      "2025-06-01 14:50:51 [INFO]: epoch 353: training loss 0.0516\n",
      "2025-06-01 14:50:51 [INFO]: epoch 354: training loss 0.0321\n",
      "2025-06-01 14:50:51 [INFO]: epoch 355: training loss 0.0335\n",
      "2025-06-01 14:50:51 [INFO]: epoch 356: training loss 0.0405\n",
      "2025-06-01 14:50:51 [INFO]: epoch 357: training loss 0.0426\n",
      "2025-06-01 14:50:51 [INFO]: epoch 358: training loss 0.0378\n",
      "2025-06-01 14:50:51 [INFO]: epoch 359: training loss 0.0384\n",
      "2025-06-01 14:50:51 [INFO]: epoch 360: training loss 0.0448\n",
      "2025-06-01 14:50:51 [INFO]: epoch 361: training loss 0.0382\n",
      "2025-06-01 14:50:51 [INFO]: epoch 362: training loss 0.0383\n",
      "2025-06-01 14:50:51 [INFO]: epoch 363: training loss 0.0403\n",
      "2025-06-01 14:50:51 [INFO]: epoch 364: training loss 0.0379\n",
      "2025-06-01 14:50:51 [INFO]: epoch 365: training loss 0.0318\n",
      "2025-06-01 14:50:51 [INFO]: epoch 366: training loss 0.0352\n",
      "2025-06-01 14:50:51 [INFO]: epoch 367: training loss 0.0351\n",
      "2025-06-01 14:50:51 [INFO]: epoch 368: training loss 0.0340\n",
      "2025-06-01 14:50:52 [INFO]: epoch 369: training loss 0.0389\n",
      "2025-06-01 14:50:52 [INFO]: epoch 370: training loss 0.0390\n",
      "2025-06-01 14:50:52 [INFO]: epoch 371: training loss 0.0306\n",
      "2025-06-01 14:50:52 [INFO]: epoch 372: training loss 0.0367\n",
      "2025-06-01 14:50:52 [INFO]: epoch 373: training loss 0.0389\n",
      "2025-06-01 14:50:52 [INFO]: epoch 374: training loss 0.0307\n",
      "2025-06-01 14:50:52 [INFO]: epoch 375: training loss 0.0301\n",
      "2025-06-01 14:50:52 [INFO]: epoch 376: training loss 0.0379\n",
      "2025-06-01 14:50:52 [INFO]: epoch 377: training loss 0.0477\n",
      "2025-06-01 14:50:52 [INFO]: epoch 378: training loss 0.0428\n",
      "2025-06-01 14:50:52 [INFO]: epoch 379: training loss 0.0454\n",
      "2025-06-01 14:50:52 [INFO]: epoch 380: training loss 0.0437\n",
      "2025-06-01 14:50:52 [INFO]: epoch 381: training loss 0.0395\n",
      "2025-06-01 14:50:52 [INFO]: epoch 382: training loss 0.0504\n",
      "2025-06-01 14:50:52 [INFO]: epoch 383: training loss 0.0601\n",
      "2025-06-01 14:50:52 [INFO]: epoch 384: training loss 0.0473\n",
      "2025-06-01 14:50:52 [INFO]: epoch 385: training loss 0.0320\n",
      "2025-06-01 14:50:52 [INFO]: epoch 386: training loss 0.0382\n",
      "2025-06-01 14:50:52 [INFO]: epoch 387: training loss 0.0444\n",
      "2025-06-01 14:50:52 [INFO]: epoch 388: training loss 0.0384\n",
      "2025-06-01 14:50:52 [INFO]: epoch 389: training loss 0.0338\n",
      "2025-06-01 14:50:52 [INFO]: epoch 390: training loss 0.0327\n",
      "2025-06-01 14:50:52 [INFO]: epoch 391: training loss 0.0321\n",
      "2025-06-01 14:50:52 [INFO]: epoch 392: training loss 0.0358\n",
      "2025-06-01 14:50:52 [INFO]: epoch 393: training loss 0.0244\n",
      "2025-06-01 14:50:52 [INFO]: epoch 394: training loss 0.0349\n",
      "2025-06-01 14:50:52 [INFO]: epoch 395: training loss 0.0336\n",
      "2025-06-01 14:50:52 [INFO]: epoch 396: training loss 0.0310\n",
      "2025-06-01 14:50:52 [INFO]: epoch 397: training loss 0.0351\n",
      "2025-06-01 14:50:52 [INFO]: epoch 398: training loss 0.0423\n",
      "2025-06-01 14:50:52 [INFO]: epoch 399: training loss 0.0374\n",
      "2025-06-01 14:50:52 [INFO]: epoch 400: training loss 0.0277\n",
      "2025-06-01 14:50:52 [INFO]: epoch 401: training loss 0.0367\n",
      "2025-06-01 14:50:52 [INFO]: epoch 402: training loss 0.0369\n",
      "2025-06-01 14:50:52 [INFO]: epoch 403: training loss 0.0307\n",
      "2025-06-01 14:50:52 [INFO]: epoch 404: training loss 0.0317\n",
      "2025-06-01 14:50:52 [INFO]: epoch 405: training loss 0.0316\n",
      "2025-06-01 14:50:52 [INFO]: epoch 406: training loss 0.0319\n",
      "2025-06-01 14:50:52 [INFO]: epoch 407: training loss 0.0295\n",
      "2025-06-01 14:50:52 [INFO]: epoch 408: training loss 0.0331\n",
      "2025-06-01 14:50:52 [INFO]: epoch 409: training loss 0.0320\n",
      "2025-06-01 14:50:52 [INFO]: epoch 410: training loss 0.0313\n",
      "2025-06-01 14:50:52 [INFO]: epoch 411: training loss 0.0326\n",
      "2025-06-01 14:50:52 [INFO]: epoch 412: training loss 0.0306\n",
      "2025-06-01 14:50:52 [INFO]: epoch 413: training loss 0.0289\n",
      "2025-06-01 14:50:52 [INFO]: epoch 414: training loss 0.0309\n",
      "2025-06-01 14:50:52 [INFO]: epoch 415: training loss 0.0336\n",
      "2025-06-01 14:50:52 [INFO]: epoch 416: training loss 0.0264\n",
      "2025-06-01 14:50:52 [INFO]: epoch 417: training loss 0.0320\n",
      "2025-06-01 14:50:52 [INFO]: epoch 418: training loss 0.0309\n",
      "2025-06-01 14:50:52 [INFO]: epoch 419: training loss 0.0320\n",
      "2025-06-01 14:50:52 [INFO]: epoch 420: training loss 0.0303\n",
      "2025-06-01 14:50:52 [INFO]: epoch 421: training loss 0.0324\n",
      "2025-06-01 14:50:52 [INFO]: epoch 422: training loss 0.0302\n",
      "2025-06-01 14:50:52 [INFO]: epoch 423: training loss 0.0293\n",
      "2025-06-01 14:50:52 [INFO]: epoch 424: training loss 0.0318\n",
      "2025-06-01 14:50:52 [INFO]: epoch 425: training loss 0.0285\n",
      "2025-06-01 14:50:52 [INFO]: epoch 426: training loss 0.0279\n",
      "2025-06-01 14:50:52 [INFO]: epoch 427: training loss 0.0320\n",
      "2025-06-01 14:50:52 [INFO]: epoch 428: training loss 0.0316\n",
      "2025-06-01 14:50:52 [INFO]: epoch 429: training loss 0.0402\n",
      "2025-06-01 14:50:52 [INFO]: epoch 430: training loss 0.0292\n",
      "2025-06-01 14:50:52 [INFO]: epoch 431: training loss 0.0315\n",
      "2025-06-01 14:50:52 [INFO]: epoch 432: training loss 0.0373\n",
      "2025-06-01 14:50:52 [INFO]: epoch 433: training loss 0.0264\n",
      "2025-06-01 14:50:52 [INFO]: epoch 434: training loss 0.0287\n",
      "2025-06-01 14:50:52 [INFO]: epoch 435: training loss 0.0371\n",
      "2025-06-01 14:50:52 [INFO]: epoch 436: training loss 0.0243\n",
      "2025-06-01 14:50:52 [INFO]: epoch 437: training loss 0.0285\n",
      "2025-06-01 14:50:52 [INFO]: epoch 438: training loss 0.0253\n",
      "2025-06-01 14:50:52 [INFO]: epoch 439: training loss 0.0317\n",
      "2025-06-01 14:50:52 [INFO]: epoch 440: training loss 0.0306\n",
      "2025-06-01 14:50:52 [INFO]: epoch 441: training loss 0.0270\n",
      "2025-06-01 14:50:52 [INFO]: epoch 442: training loss 0.0312\n",
      "2025-06-01 14:50:52 [INFO]: epoch 443: training loss 0.0279\n",
      "2025-06-01 14:50:52 [INFO]: epoch 444: training loss 0.0281\n",
      "2025-06-01 14:50:53 [INFO]: epoch 445: training loss 0.0320\n",
      "2025-06-01 14:50:53 [INFO]: epoch 446: training loss 0.0327\n",
      "2025-06-01 14:50:53 [INFO]: epoch 447: training loss 0.0332\n",
      "2025-06-01 14:50:53 [INFO]: epoch 448: training loss 0.0334\n",
      "2025-06-01 14:50:53 [INFO]: epoch 449: training loss 0.0309\n",
      "2025-06-01 14:50:53 [INFO]: epoch 450: training loss 0.0349\n",
      "2025-06-01 14:50:53 [INFO]: epoch 451: training loss 0.0327\n",
      "2025-06-01 14:50:53 [INFO]: epoch 452: training loss 0.0290\n",
      "2025-06-01 14:50:53 [INFO]: epoch 453: training loss 0.0317\n",
      "2025-06-01 14:50:53 [INFO]: epoch 454: training loss 0.0301\n",
      "2025-06-01 14:50:53 [INFO]: epoch 455: training loss 0.0358\n",
      "2025-06-01 14:50:53 [INFO]: epoch 456: training loss 0.0361\n",
      "2025-06-01 14:50:53 [INFO]: epoch 457: training loss 0.0310\n",
      "2025-06-01 14:50:53 [INFO]: epoch 458: training loss 0.0271\n",
      "2025-06-01 14:50:53 [INFO]: epoch 459: training loss 0.0354\n",
      "2025-06-01 14:50:53 [INFO]: epoch 460: training loss 0.0263\n",
      "2025-06-01 14:50:53 [INFO]: epoch 461: training loss 0.0317\n",
      "2025-06-01 14:50:53 [INFO]: epoch 462: training loss 0.0357\n",
      "2025-06-01 14:50:53 [INFO]: epoch 463: training loss 0.0310\n",
      "2025-06-01 14:50:53 [INFO]: epoch 464: training loss 0.0318\n",
      "2025-06-01 14:50:53 [INFO]: epoch 465: training loss 0.0403\n",
      "2025-06-01 14:50:53 [INFO]: epoch 466: training loss 0.0306\n",
      "2025-06-01 14:50:53 [INFO]: epoch 467: training loss 0.0398\n",
      "2025-06-01 14:50:53 [INFO]: epoch 468: training loss 0.0386\n",
      "2025-06-01 14:50:53 [INFO]: epoch 469: training loss 0.0300\n",
      "2025-06-01 14:50:53 [INFO]: epoch 470: training loss 0.0340\n",
      "2025-06-01 14:50:53 [INFO]: epoch 471: training loss 0.0329\n",
      "2025-06-01 14:50:53 [INFO]: epoch 472: training loss 0.0366\n",
      "2025-06-01 14:50:53 [INFO]: epoch 473: training loss 0.0389\n",
      "2025-06-01 14:50:53 [INFO]: epoch 474: training loss 0.0299\n",
      "2025-06-01 14:50:53 [INFO]: epoch 475: training loss 0.0348\n",
      "2025-06-01 14:50:53 [INFO]: epoch 476: training loss 0.0385\n",
      "2025-06-01 14:50:53 [INFO]: epoch 477: training loss 0.0340\n",
      "2025-06-01 14:50:53 [INFO]: epoch 478: training loss 0.0328\n",
      "2025-06-01 14:50:53 [INFO]: epoch 479: training loss 0.0402\n",
      "2025-06-01 14:50:53 [INFO]: epoch 480: training loss 0.0367\n",
      "2025-06-01 14:50:53 [INFO]: epoch 481: training loss 0.0394\n",
      "2025-06-01 14:50:53 [INFO]: epoch 482: training loss 0.0332\n",
      "2025-06-01 14:50:53 [INFO]: epoch 483: training loss 0.0393\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-01 14:50:53 [INFO]: epoch 484: training loss 0.0334\n",
      "2025-06-01 14:50:53 [INFO]: epoch 485: training loss 0.0307\n",
      "2025-06-01 14:50:53 [INFO]: epoch 486: training loss 0.0353\n",
      "2025-06-01 14:50:53 [INFO]: epoch 487: training loss 0.0272\n",
      "2025-06-01 14:50:53 [INFO]: epoch 488: training loss 0.0324\n",
      "2025-06-01 14:50:53 [INFO]: epoch 489: training loss 0.0402\n",
      "2025-06-01 14:50:53 [INFO]: epoch 490: training loss 0.0268\n",
      "2025-06-01 14:50:53 [INFO]: epoch 491: training loss 0.0293\n",
      "2025-06-01 14:50:53 [INFO]: epoch 492: training loss 0.0359\n",
      "2025-06-01 14:50:53 [INFO]: epoch 493: training loss 0.0295\n",
      "2025-06-01 14:50:53 [INFO]: epoch 494: training loss 0.0335\n",
      "2025-06-01 14:50:53 [INFO]: epoch 495: training loss 0.0362\n",
      "2025-06-01 14:50:53 [INFO]: epoch 496: training loss 0.0257\n",
      "2025-06-01 14:50:53 [INFO]: epoch 497: training loss 0.0344\n",
      "2025-06-01 14:50:53 [INFO]: epoch 498: training loss 0.0375\n",
      "2025-06-01 14:50:53 [INFO]: epoch 499: training loss 0.0278\n",
      "2025-06-01 14:50:53 [INFO]: epoch 500: training loss 0.0252\n",
      "2025-06-01 14:50:53 [INFO]: epoch 501: training loss 0.0358\n",
      "2025-06-01 14:50:53 [INFO]: epoch 502: training loss 0.0313\n",
      "2025-06-01 14:50:53 [INFO]: epoch 503: training loss 0.0297\n",
      "2025-06-01 14:50:53 [INFO]: epoch 504: training loss 0.0325\n",
      "2025-06-01 14:50:53 [INFO]: epoch 505: training loss 0.0272\n",
      "2025-06-01 14:50:53 [INFO]: epoch 506: training loss 0.0248\n",
      "2025-06-01 14:50:53 [INFO]: epoch 507: training loss 0.0297\n",
      "2025-06-01 14:50:53 [INFO]: epoch 508: training loss 0.0269\n",
      "2025-06-01 14:50:53 [INFO]: epoch 509: training loss 0.0277\n",
      "2025-06-01 14:50:53 [INFO]: epoch 510: training loss 0.0282\n",
      "2025-06-01 14:50:53 [INFO]: epoch 511: training loss 0.0314\n",
      "2025-06-01 14:50:53 [INFO]: epoch 512: training loss 0.0246\n",
      "2025-06-01 14:50:53 [INFO]: epoch 513: training loss 0.0255\n",
      "2025-06-01 14:50:53 [INFO]: epoch 514: training loss 0.0290\n",
      "2025-06-01 14:50:53 [INFO]: epoch 515: training loss 0.0258\n",
      "2025-06-01 14:50:53 [INFO]: epoch 516: training loss 0.0257\n",
      "2025-06-01 14:50:53 [INFO]: epoch 517: training loss 0.0294\n",
      "2025-06-01 14:50:53 [INFO]: epoch 518: training loss 0.0290\n",
      "2025-06-01 14:50:53 [INFO]: epoch 519: training loss 0.0289\n",
      "2025-06-01 14:50:53 [INFO]: epoch 520: training loss 0.0262\n",
      "2025-06-01 14:50:54 [INFO]: epoch 521: training loss 0.0295\n",
      "2025-06-01 14:50:54 [INFO]: epoch 522: training loss 0.0340\n",
      "2025-06-01 14:50:54 [INFO]: epoch 523: training loss 0.0292\n",
      "2025-06-01 14:50:54 [INFO]: epoch 524: training loss 0.0292\n",
      "2025-06-01 14:50:54 [INFO]: epoch 525: training loss 0.0408\n",
      "2025-06-01 14:50:54 [INFO]: epoch 526: training loss 0.0311\n",
      "2025-06-01 14:50:54 [INFO]: epoch 527: training loss 0.0295\n",
      "2025-06-01 14:50:54 [INFO]: epoch 528: training loss 0.0292\n",
      "2025-06-01 14:50:54 [INFO]: epoch 529: training loss 0.0341\n",
      "2025-06-01 14:50:54 [INFO]: epoch 530: training loss 0.0299\n",
      "2025-06-01 14:50:54 [INFO]: epoch 531: training loss 0.0357\n",
      "2025-06-01 14:50:54 [INFO]: epoch 532: training loss 0.0312\n",
      "2025-06-01 14:50:54 [INFO]: epoch 533: training loss 0.0279\n",
      "2025-06-01 14:50:54 [INFO]: epoch 534: training loss 0.0323\n",
      "2025-06-01 14:50:54 [INFO]: epoch 535: training loss 0.0339\n",
      "2025-06-01 14:50:54 [INFO]: epoch 536: training loss 0.0253\n",
      "2025-06-01 14:50:54 [INFO]: epoch 537: training loss 0.0282\n",
      "2025-06-01 14:50:54 [INFO]: epoch 538: training loss 0.0273\n",
      "2025-06-01 14:50:54 [INFO]: epoch 539: training loss 0.0248\n",
      "2025-06-01 14:50:54 [INFO]: epoch 540: training loss 0.0306\n",
      "2025-06-01 14:50:54 [INFO]: epoch 541: training loss 0.0307\n",
      "2025-06-01 14:50:54 [INFO]: epoch 542: training loss 0.0251\n",
      "2025-06-01 14:50:54 [INFO]: epoch 543: training loss 0.0297\n",
      "2025-06-01 14:50:54 [INFO]: epoch 544: training loss 0.0296\n",
      "2025-06-01 14:50:54 [INFO]: epoch 545: training loss 0.0283\n",
      "2025-06-01 14:50:54 [INFO]: epoch 546: training loss 0.0262\n",
      "2025-06-01 14:50:54 [INFO]: epoch 547: training loss 0.0280\n",
      "2025-06-01 14:50:54 [INFO]: epoch 548: training loss 0.0275\n",
      "2025-06-01 14:50:54 [INFO]: epoch 549: training loss 0.0290\n",
      "2025-06-01 14:50:54 [INFO]: epoch 550: training loss 0.0280\n",
      "2025-06-01 14:50:54 [INFO]: epoch 551: training loss 0.0264\n",
      "2025-06-01 14:50:54 [INFO]: epoch 552: training loss 0.0293\n",
      "2025-06-01 14:50:54 [INFO]: epoch 553: training loss 0.0366\n",
      "2025-06-01 14:50:54 [INFO]: epoch 554: training loss 0.0303\n",
      "2025-06-01 14:50:54 [INFO]: epoch 555: training loss 0.0304\n",
      "2025-06-01 14:50:54 [INFO]: epoch 556: training loss 0.0345\n",
      "2025-06-01 14:50:54 [INFO]: epoch 557: training loss 0.0329\n",
      "2025-06-01 14:50:54 [INFO]: epoch 558: training loss 0.0347\n",
      "2025-06-01 14:50:54 [INFO]: epoch 559: training loss 0.0353\n",
      "2025-06-01 14:50:54 [INFO]: epoch 560: training loss 0.0301\n",
      "2025-06-01 14:50:54 [INFO]: epoch 561: training loss 0.0323\n",
      "2025-06-01 14:50:54 [INFO]: epoch 562: training loss 0.0290\n",
      "2025-06-01 14:50:54 [INFO]: epoch 563: training loss 0.0350\n",
      "2025-06-01 14:50:54 [INFO]: epoch 564: training loss 0.0292\n",
      "2025-06-01 14:50:54 [INFO]: epoch 565: training loss 0.0265\n",
      "2025-06-01 14:50:54 [INFO]: epoch 566: training loss 0.0282\n",
      "2025-06-01 14:50:54 [INFO]: epoch 567: training loss 0.0306\n",
      "2025-06-01 14:50:54 [INFO]: epoch 568: training loss 0.0266\n",
      "2025-06-01 14:50:54 [INFO]: epoch 569: training loss 0.0297\n",
      "2025-06-01 14:50:54 [INFO]: epoch 570: training loss 0.0292\n",
      "2025-06-01 14:50:54 [INFO]: epoch 571: training loss 0.0346\n",
      "2025-06-01 14:50:54 [INFO]: epoch 572: training loss 0.0314\n",
      "2025-06-01 14:50:54 [INFO]: epoch 573: training loss 0.0281\n",
      "2025-06-01 14:50:54 [INFO]: epoch 574: training loss 0.0337\n",
      "2025-06-01 14:50:54 [INFO]: epoch 575: training loss 0.0364\n",
      "2025-06-01 14:50:54 [INFO]: epoch 576: training loss 0.0237\n",
      "2025-06-01 14:50:54 [INFO]: epoch 577: training loss 0.0273\n",
      "2025-06-01 14:50:54 [INFO]: epoch 578: training loss 0.0317\n",
      "2025-06-01 14:50:54 [INFO]: epoch 579: training loss 0.0260\n",
      "2025-06-01 14:50:54 [INFO]: epoch 580: training loss 0.0252\n",
      "2025-06-01 14:50:54 [INFO]: epoch 581: training loss 0.0286\n",
      "2025-06-01 14:50:54 [INFO]: epoch 582: training loss 0.0278\n",
      "2025-06-01 14:50:54 [INFO]: epoch 583: training loss 0.0258\n",
      "2025-06-01 14:50:54 [INFO]: epoch 584: training loss 0.0309\n",
      "2025-06-01 14:50:54 [INFO]: epoch 585: training loss 0.0326\n",
      "2025-06-01 14:50:54 [INFO]: epoch 586: training loss 0.0291\n",
      "2025-06-01 14:50:54 [INFO]: epoch 587: training loss 0.0333\n",
      "2025-06-01 14:50:54 [INFO]: epoch 588: training loss 0.0263\n",
      "2025-06-01 14:50:54 [INFO]: epoch 589: training loss 0.0257\n",
      "2025-06-01 14:50:54 [INFO]: epoch 590: training loss 0.0297\n",
      "2025-06-01 14:50:54 [INFO]: epoch 591: training loss 0.0318\n",
      "2025-06-01 14:50:54 [INFO]: epoch 592: training loss 0.0222\n",
      "2025-06-01 14:50:54 [INFO]: epoch 593: training loss 0.0244\n",
      "2025-06-01 14:50:54 [INFO]: epoch 594: training loss 0.0300\n",
      "2025-06-01 14:50:54 [INFO]: epoch 595: training loss 0.0254\n",
      "2025-06-01 14:50:54 [INFO]: epoch 596: training loss 0.0279\n",
      "2025-06-01 14:50:54 [INFO]: epoch 597: training loss 0.0325\n",
      "2025-06-01 14:50:55 [INFO]: epoch 598: training loss 0.0294\n",
      "2025-06-01 14:50:55 [INFO]: epoch 599: training loss 0.0231\n",
      "2025-06-01 14:50:55 [INFO]: epoch 600: training loss 0.0279\n",
      "2025-06-01 14:50:55 [INFO]: epoch 601: training loss 0.0318\n",
      "2025-06-01 14:50:55 [INFO]: epoch 602: training loss 0.0223\n",
      "2025-06-01 14:50:55 [INFO]: epoch 603: training loss 0.0256\n",
      "2025-06-01 14:50:55 [INFO]: epoch 604: training loss 0.0367\n",
      "2025-06-01 14:50:55 [INFO]: epoch 605: training loss 0.0274\n",
      "2025-06-01 14:50:55 [INFO]: epoch 606: training loss 0.0288\n",
      "2025-06-01 14:50:55 [INFO]: epoch 607: training loss 0.0308\n",
      "2025-06-01 14:50:55 [INFO]: epoch 608: training loss 0.0310\n",
      "2025-06-01 14:50:55 [INFO]: epoch 609: training loss 0.0249\n",
      "2025-06-01 14:50:55 [INFO]: epoch 610: training loss 0.0351\n",
      "2025-06-01 14:50:55 [INFO]: epoch 611: training loss 0.0383\n",
      "2025-06-01 14:50:55 [INFO]: epoch 612: training loss 0.0277\n",
      "2025-06-01 14:50:55 [INFO]: epoch 613: training loss 0.0253\n",
      "2025-06-01 14:50:55 [INFO]: epoch 614: training loss 0.0351\n",
      "2025-06-01 14:50:55 [INFO]: epoch 615: training loss 0.0318\n",
      "2025-06-01 14:50:55 [INFO]: epoch 616: training loss 0.0270\n",
      "2025-06-01 14:50:55 [INFO]: epoch 617: training loss 0.0418\n",
      "2025-06-01 14:50:55 [INFO]: epoch 618: training loss 0.0267\n",
      "2025-06-01 14:50:55 [INFO]: epoch 619: training loss 0.0236\n",
      "2025-06-01 14:50:55 [INFO]: epoch 620: training loss 0.0302\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-01 14:50:55 [INFO]: epoch 621: training loss 0.0248\n",
      "2025-06-01 14:50:55 [INFO]: epoch 622: training loss 0.0262\n",
      "2025-06-01 14:50:55 [INFO]: epoch 623: training loss 0.0252\n",
      "2025-06-01 14:50:55 [INFO]: epoch 624: training loss 0.0298\n",
      "2025-06-01 14:50:55 [INFO]: epoch 625: training loss 0.0253\n",
      "2025-06-01 14:50:55 [INFO]: epoch 626: training loss 0.0257\n",
      "2025-06-01 14:50:55 [INFO]: epoch 627: training loss 0.0264\n",
      "2025-06-01 14:50:55 [INFO]: epoch 628: training loss 0.0265\n",
      "2025-06-01 14:50:55 [INFO]: epoch 629: training loss 0.0317\n",
      "2025-06-01 14:50:55 [INFO]: epoch 630: training loss 0.0244\n",
      "2025-06-01 14:50:55 [INFO]: epoch 631: training loss 0.0313\n",
      "2025-06-01 14:50:55 [INFO]: epoch 632: training loss 0.0311\n",
      "2025-06-01 14:50:55 [INFO]: epoch 633: training loss 0.0257\n",
      "2025-06-01 14:50:55 [INFO]: epoch 634: training loss 0.0276\n",
      "2025-06-01 14:50:55 [INFO]: epoch 635: training loss 0.0320\n",
      "2025-06-01 14:50:55 [INFO]: epoch 636: training loss 0.0249\n",
      "2025-06-01 14:50:55 [INFO]: epoch 637: training loss 0.0261\n",
      "2025-06-01 14:50:55 [INFO]: epoch 638: training loss 0.0267\n",
      "2025-06-01 14:50:55 [INFO]: epoch 639: training loss 0.0206\n",
      "2025-06-01 14:50:55 [INFO]: epoch 640: training loss 0.0254\n",
      "2025-06-01 14:50:55 [INFO]: epoch 641: training loss 0.0269\n",
      "2025-06-01 14:50:55 [INFO]: epoch 642: training loss 0.0227\n",
      "2025-06-01 14:50:55 [INFO]: epoch 643: training loss 0.0243\n",
      "2025-06-01 14:50:55 [INFO]: epoch 644: training loss 0.0325\n",
      "2025-06-01 14:50:55 [INFO]: epoch 645: training loss 0.0240\n",
      "2025-06-01 14:50:55 [INFO]: epoch 646: training loss 0.0271\n",
      "2025-06-01 14:50:55 [INFO]: epoch 647: training loss 0.0228\n",
      "2025-06-01 14:50:55 [INFO]: epoch 648: training loss 0.0230\n",
      "2025-06-01 14:50:55 [INFO]: epoch 649: training loss 0.0352\n",
      "2025-06-01 14:50:55 [INFO]: epoch 650: training loss 0.0289\n",
      "2025-06-01 14:50:55 [INFO]: epoch 651: training loss 0.0248\n",
      "2025-06-01 14:50:55 [INFO]: epoch 652: training loss 0.0378\n",
      "2025-06-01 14:50:55 [INFO]: epoch 653: training loss 0.0322\n",
      "2025-06-01 14:50:55 [INFO]: epoch 654: training loss 0.0288\n",
      "2025-06-01 14:50:55 [INFO]: epoch 655: training loss 0.0242\n",
      "2025-06-01 14:50:55 [INFO]: epoch 656: training loss 0.0285\n",
      "2025-06-01 14:50:55 [INFO]: epoch 657: training loss 0.0233\n",
      "2025-06-01 14:50:55 [INFO]: epoch 658: training loss 0.0298\n",
      "2025-06-01 14:50:55 [INFO]: epoch 659: training loss 0.0262\n",
      "2025-06-01 14:50:55 [INFO]: epoch 660: training loss 0.0230\n",
      "2025-06-01 14:50:55 [INFO]: epoch 661: training loss 0.0253\n",
      "2025-06-01 14:50:55 [INFO]: epoch 662: training loss 0.0249\n",
      "2025-06-01 14:50:55 [INFO]: epoch 663: training loss 0.0213\n",
      "2025-06-01 14:50:55 [INFO]: epoch 664: training loss 0.0244\n",
      "2025-06-01 14:50:55 [INFO]: epoch 665: training loss 0.0214\n",
      "2025-06-01 14:50:55 [INFO]: epoch 666: training loss 0.0279\n",
      "2025-06-01 14:50:55 [INFO]: epoch 667: training loss 0.0322\n",
      "2025-06-01 14:50:55 [INFO]: epoch 668: training loss 0.0243\n",
      "2025-06-01 14:50:55 [INFO]: epoch 669: training loss 0.0214\n",
      "2025-06-01 14:50:55 [INFO]: epoch 670: training loss 0.0246\n",
      "2025-06-01 14:50:55 [INFO]: epoch 671: training loss 0.0224\n",
      "2025-06-01 14:50:55 [INFO]: epoch 672: training loss 0.0275\n",
      "2025-06-01 14:50:55 [INFO]: epoch 673: training loss 0.0216\n",
      "2025-06-01 14:50:56 [INFO]: epoch 674: training loss 0.0226\n",
      "2025-06-01 14:50:56 [INFO]: epoch 675: training loss 0.0226\n",
      "2025-06-01 14:50:56 [INFO]: epoch 676: training loss 0.0272\n",
      "2025-06-01 14:50:56 [INFO]: epoch 677: training loss 0.0249\n",
      "2025-06-01 14:50:56 [INFO]: epoch 678: training loss 0.0261\n",
      "2025-06-01 14:50:56 [INFO]: epoch 679: training loss 0.0251\n",
      "2025-06-01 14:50:56 [INFO]: epoch 680: training loss 0.0245\n",
      "2025-06-01 14:50:56 [INFO]: epoch 681: training loss 0.0264\n",
      "2025-06-01 14:50:56 [INFO]: epoch 682: training loss 0.0244\n",
      "2025-06-01 14:50:56 [INFO]: epoch 683: training loss 0.0256\n",
      "2025-06-01 14:50:56 [INFO]: epoch 684: training loss 0.0205\n",
      "2025-06-01 14:50:56 [INFO]: epoch 685: training loss 0.0248\n",
      "2025-06-01 14:50:56 [INFO]: epoch 686: training loss 0.0292\n",
      "2025-06-01 14:50:56 [INFO]: epoch 687: training loss 0.0232\n",
      "2025-06-01 14:50:56 [INFO]: epoch 688: training loss 0.0254\n",
      "2025-06-01 14:50:56 [INFO]: epoch 689: training loss 0.0269\n",
      "2025-06-01 14:50:56 [INFO]: epoch 690: training loss 0.0256\n",
      "2025-06-01 14:50:56 [INFO]: epoch 691: training loss 0.0212\n",
      "2025-06-01 14:50:56 [INFO]: epoch 692: training loss 0.0234\n",
      "2025-06-01 14:50:56 [INFO]: epoch 693: training loss 0.0237\n",
      "2025-06-01 14:50:56 [INFO]: epoch 694: training loss 0.0274\n",
      "2025-06-01 14:50:56 [INFO]: epoch 695: training loss 0.0204\n",
      "2025-06-01 14:50:56 [INFO]: epoch 696: training loss 0.0271\n",
      "2025-06-01 14:50:56 [INFO]: epoch 697: training loss 0.0299\n",
      "2025-06-01 14:50:56 [INFO]: epoch 698: training loss 0.0227\n",
      "2025-06-01 14:50:56 [INFO]: epoch 699: training loss 0.0211\n",
      "2025-06-01 14:50:56 [INFO]: epoch 700: training loss 0.0275\n",
      "2025-06-01 14:50:56 [INFO]: epoch 701: training loss 0.0312\n",
      "2025-06-01 14:50:56 [INFO]: epoch 702: training loss 0.0265\n",
      "2025-06-01 14:50:56 [INFO]: epoch 703: training loss 0.0211\n",
      "2025-06-01 14:50:56 [INFO]: epoch 704: training loss 0.0306\n",
      "2025-06-01 14:50:56 [INFO]: epoch 705: training loss 0.0303\n",
      "2025-06-01 14:50:56 [INFO]: epoch 706: training loss 0.0253\n",
      "2025-06-01 14:50:56 [INFO]: epoch 707: training loss 0.0241\n",
      "2025-06-01 14:50:56 [INFO]: epoch 708: training loss 0.0261\n",
      "2025-06-01 14:50:56 [INFO]: epoch 709: training loss 0.0331\n",
      "2025-06-01 14:50:56 [INFO]: epoch 710: training loss 0.0232\n",
      "2025-06-01 14:50:56 [INFO]: epoch 711: training loss 0.0215\n",
      "2025-06-01 14:50:56 [INFO]: epoch 712: training loss 0.0279\n",
      "2025-06-01 14:50:56 [INFO]: epoch 713: training loss 0.0290\n",
      "2025-06-01 14:50:56 [INFO]: epoch 714: training loss 0.0201\n",
      "2025-06-01 14:50:56 [INFO]: epoch 715: training loss 0.0185\n",
      "2025-06-01 14:50:56 [INFO]: epoch 716: training loss 0.0334\n",
      "2025-06-01 14:50:56 [INFO]: epoch 717: training loss 0.0293\n",
      "2025-06-01 14:50:56 [INFO]: epoch 718: training loss 0.0218\n",
      "2025-06-01 14:50:56 [INFO]: epoch 719: training loss 0.0276\n",
      "2025-06-01 14:50:56 [INFO]: epoch 720: training loss 0.0284\n",
      "2025-06-01 14:50:56 [INFO]: epoch 721: training loss 0.0243\n",
      "2025-06-01 14:50:56 [INFO]: epoch 722: training loss 0.0206\n",
      "2025-06-01 14:50:56 [INFO]: epoch 723: training loss 0.0231\n",
      "2025-06-01 14:50:56 [INFO]: epoch 724: training loss 0.0199\n",
      "2025-06-01 14:50:56 [INFO]: epoch 725: training loss 0.0226\n",
      "2025-06-01 14:50:56 [INFO]: epoch 726: training loss 0.0254\n",
      "2025-06-01 14:50:56 [INFO]: epoch 727: training loss 0.0240\n",
      "2025-06-01 14:50:56 [INFO]: epoch 728: training loss 0.0161\n",
      "2025-06-01 14:50:56 [INFO]: epoch 729: training loss 0.0271\n",
      "2025-06-01 14:50:56 [INFO]: epoch 730: training loss 0.0249\n",
      "2025-06-01 14:50:56 [INFO]: epoch 731: training loss 0.0260\n",
      "2025-06-01 14:50:56 [INFO]: epoch 732: training loss 0.0223\n",
      "2025-06-01 14:50:56 [INFO]: epoch 733: training loss 0.0252\n",
      "2025-06-01 14:50:56 [INFO]: epoch 734: training loss 0.0256\n",
      "2025-06-01 14:50:56 [INFO]: epoch 735: training loss 0.0254\n",
      "2025-06-01 14:50:56 [INFO]: epoch 736: training loss 0.0224\n",
      "2025-06-01 14:50:56 [INFO]: epoch 737: training loss 0.0271\n",
      "2025-06-01 14:50:56 [INFO]: epoch 738: training loss 0.0314\n",
      "2025-06-01 14:50:56 [INFO]: epoch 739: training loss 0.0238\n",
      "2025-06-01 14:50:56 [INFO]: epoch 740: training loss 0.0253\n",
      "2025-06-01 14:50:56 [INFO]: epoch 741: training loss 0.0261\n",
      "2025-06-01 14:50:56 [INFO]: epoch 742: training loss 0.0264\n",
      "2025-06-01 14:50:56 [INFO]: epoch 743: training loss 0.0212\n",
      "2025-06-01 14:50:56 [INFO]: epoch 744: training loss 0.0248\n",
      "2025-06-01 14:50:56 [INFO]: epoch 745: training loss 0.0214\n",
      "2025-06-01 14:50:56 [INFO]: epoch 746: training loss 0.0271\n",
      "2025-06-01 14:50:56 [INFO]: epoch 747: training loss 0.0262\n",
      "2025-06-01 14:50:56 [INFO]: epoch 748: training loss 0.0217\n",
      "2025-06-01 14:50:57 [INFO]: epoch 749: training loss 0.0223\n",
      "2025-06-01 14:50:57 [INFO]: epoch 750: training loss 0.0233\n",
      "2025-06-01 14:50:57 [INFO]: epoch 751: training loss 0.0217\n",
      "2025-06-01 14:50:57 [INFO]: epoch 752: training loss 0.0199\n",
      "2025-06-01 14:50:57 [INFO]: epoch 753: training loss 0.0232\n",
      "2025-06-01 14:50:57 [INFO]: epoch 754: training loss 0.0195\n",
      "2025-06-01 14:50:57 [INFO]: epoch 755: training loss 0.0243\n",
      "2025-06-01 14:50:57 [INFO]: epoch 756: training loss 0.0219\n",
      "2025-06-01 14:50:57 [INFO]: epoch 757: training loss 0.0202\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-01 14:50:57 [INFO]: epoch 758: training loss 0.0239\n",
      "2025-06-01 14:50:57 [INFO]: epoch 759: training loss 0.0225\n",
      "2025-06-01 14:50:57 [INFO]: epoch 760: training loss 0.0200\n",
      "2025-06-01 14:50:57 [INFO]: epoch 761: training loss 0.0231\n",
      "2025-06-01 14:50:57 [INFO]: epoch 762: training loss 0.0233\n",
      "2025-06-01 14:50:57 [INFO]: epoch 763: training loss 0.0241\n",
      "2025-06-01 14:50:57 [INFO]: epoch 764: training loss 0.0227\n",
      "2025-06-01 14:50:57 [INFO]: epoch 765: training loss 0.0250\n",
      "2025-06-01 14:50:57 [INFO]: epoch 766: training loss 0.0226\n",
      "2025-06-01 14:50:57 [INFO]: epoch 767: training loss 0.0210\n",
      "2025-06-01 14:50:57 [INFO]: epoch 768: training loss 0.0258\n",
      "2025-06-01 14:50:57 [INFO]: epoch 769: training loss 0.0246\n",
      "2025-06-01 14:50:57 [INFO]: epoch 770: training loss 0.0217\n",
      "2025-06-01 14:50:57 [INFO]: epoch 771: training loss 0.0184\n",
      "2025-06-01 14:50:57 [INFO]: epoch 772: training loss 0.0219\n",
      "2025-06-01 14:50:57 [INFO]: epoch 773: training loss 0.0227\n",
      "2025-06-01 14:50:57 [INFO]: epoch 774: training loss 0.0232\n",
      "2025-06-01 14:50:57 [INFO]: epoch 775: training loss 0.0248\n",
      "2025-06-01 14:50:57 [INFO]: epoch 776: training loss 0.0226\n",
      "2025-06-01 14:50:57 [INFO]: epoch 777: training loss 0.0217\n",
      "2025-06-01 14:50:57 [INFO]: epoch 778: training loss 0.0244\n",
      "2025-06-01 14:50:57 [INFO]: epoch 779: training loss 0.0242\n",
      "2025-06-01 14:50:57 [INFO]: epoch 780: training loss 0.0201\n",
      "2025-06-01 14:50:57 [INFO]: epoch 781: training loss 0.0182\n",
      "2025-06-01 14:50:57 [INFO]: epoch 782: training loss 0.0259\n",
      "2025-06-01 14:50:57 [INFO]: epoch 783: training loss 0.0202\n",
      "2025-06-01 14:50:57 [INFO]: epoch 784: training loss 0.0255\n",
      "2025-06-01 14:50:57 [INFO]: epoch 785: training loss 0.0255\n",
      "2025-06-01 14:50:57 [INFO]: epoch 786: training loss 0.0294\n",
      "2025-06-01 14:50:57 [INFO]: epoch 787: training loss 0.0273\n",
      "2025-06-01 14:50:57 [INFO]: epoch 788: training loss 0.0359\n",
      "2025-06-01 14:50:57 [INFO]: epoch 789: training loss 0.0321\n",
      "2025-06-01 14:50:57 [INFO]: epoch 790: training loss 0.0223\n",
      "2025-06-01 14:50:57 [INFO]: epoch 791: training loss 0.0248\n",
      "2025-06-01 14:50:57 [INFO]: epoch 792: training loss 0.0274\n",
      "2025-06-01 14:50:57 [INFO]: epoch 793: training loss 0.0201\n",
      "2025-06-01 14:50:57 [INFO]: epoch 794: training loss 0.0269\n",
      "2025-06-01 14:50:57 [INFO]: epoch 795: training loss 0.0288\n",
      "2025-06-01 14:50:57 [INFO]: epoch 796: training loss 0.0250\n",
      "2025-06-01 14:50:57 [INFO]: epoch 797: training loss 0.0161\n",
      "2025-06-01 14:50:57 [INFO]: epoch 798: training loss 0.0255\n",
      "2025-06-01 14:50:57 [INFO]: epoch 799: training loss 0.0259\n",
      "2025-06-01 14:50:57 [INFO]: Finished training.\n",
      "2025-06-01 14:50:57 [WARNING]: 🚨DeprecationWarning: The method impute is deprecated. Please use `predict` instead.\n",
      " 83%|██████████████████████████████████████████████████████████████████████              | 5/6 [00:52<00:10, 10.60s/it]2025-06-01 14:50:57 [INFO]: No given device, using default device: cuda\n",
      "2025-06-01 14:50:57 [WARNING]: saving_path not given. Model files and tensorboard file will not be saved.\n",
      "2025-06-01 14:50:57 [INFO]: Model initialized successfully with the number of trainable parameters: 597,780\n",
      "2025-06-01 14:50:57 [INFO]: epoch 0: training loss 1.4585\n",
      "2025-06-01 14:50:57 [INFO]: epoch 1: training loss 0.6256\n",
      "2025-06-01 14:50:57 [INFO]: epoch 2: training loss 0.6597\n",
      "2025-06-01 14:50:57 [INFO]: epoch 3: training loss 0.5200\n",
      "2025-06-01 14:50:57 [INFO]: epoch 4: training loss 0.5206\n",
      "2025-06-01 14:50:57 [INFO]: epoch 5: training loss 0.5299\n",
      "2025-06-01 14:50:57 [INFO]: epoch 6: training loss 0.5430\n",
      "2025-06-01 14:50:57 [INFO]: epoch 7: training loss 0.4847\n",
      "2025-06-01 14:50:57 [INFO]: epoch 8: training loss 0.3687\n",
      "2025-06-01 14:50:57 [INFO]: epoch 9: training loss 0.3017\n",
      "2025-06-01 14:50:57 [INFO]: epoch 10: training loss 0.3074\n",
      "2025-06-01 14:50:57 [INFO]: epoch 11: training loss 0.4320\n",
      "2025-06-01 14:50:57 [INFO]: epoch 12: training loss 0.3814\n",
      "2025-06-01 14:50:57 [INFO]: epoch 13: training loss 0.3106\n",
      "2025-06-01 14:50:57 [INFO]: epoch 14: training loss 0.2999\n",
      "2025-06-01 14:50:57 [INFO]: epoch 15: training loss 0.2877\n",
      "2025-06-01 14:50:57 [INFO]: epoch 16: training loss 0.3075\n",
      "2025-06-01 14:50:57 [INFO]: epoch 17: training loss 0.2919\n",
      "2025-06-01 14:50:58 [INFO]: epoch 18: training loss 0.2748\n",
      "2025-06-01 14:50:58 [INFO]: epoch 19: training loss 0.2666\n",
      "2025-06-01 14:50:58 [INFO]: epoch 20: training loss 0.2411\n",
      "2025-06-01 14:50:58 [INFO]: epoch 21: training loss 0.2419\n",
      "2025-06-01 14:50:58 [INFO]: epoch 22: training loss 0.2251\n",
      "2025-06-01 14:50:58 [INFO]: epoch 23: training loss 0.2358\n",
      "2025-06-01 14:50:58 [INFO]: epoch 24: training loss 0.2225\n",
      "2025-06-01 14:50:58 [INFO]: epoch 25: training loss 0.2002\n",
      "2025-06-01 14:50:58 [INFO]: epoch 26: training loss 0.2365\n",
      "2025-06-01 14:50:58 [INFO]: epoch 27: training loss 0.2383\n",
      "2025-06-01 14:50:58 [INFO]: epoch 28: training loss 0.1931\n",
      "2025-06-01 14:50:58 [INFO]: epoch 29: training loss 0.2105\n",
      "2025-06-01 14:50:58 [INFO]: epoch 30: training loss 0.1823\n",
      "2025-06-01 14:50:58 [INFO]: epoch 31: training loss 0.1950\n",
      "2025-06-01 14:50:58 [INFO]: epoch 32: training loss 0.2082\n",
      "2025-06-01 14:50:58 [INFO]: epoch 33: training loss 0.1933\n",
      "2025-06-01 14:50:58 [INFO]: epoch 34: training loss 0.1814\n",
      "2025-06-01 14:50:58 [INFO]: epoch 35: training loss 0.1760\n",
      "2025-06-01 14:50:58 [INFO]: epoch 36: training loss 0.1912\n",
      "2025-06-01 14:50:58 [INFO]: epoch 37: training loss 0.1835\n",
      "2025-06-01 14:50:58 [INFO]: epoch 38: training loss 0.1633\n",
      "2025-06-01 14:50:58 [INFO]: epoch 39: training loss 0.1654\n",
      "2025-06-01 14:50:58 [INFO]: epoch 40: training loss 0.1792\n",
      "2025-06-01 14:50:58 [INFO]: epoch 41: training loss 0.1608\n",
      "2025-06-01 14:50:58 [INFO]: epoch 42: training loss 0.1502\n",
      "2025-06-01 14:50:58 [INFO]: epoch 43: training loss 0.1476\n",
      "2025-06-01 14:50:58 [INFO]: epoch 44: training loss 0.1769\n",
      "2025-06-01 14:50:58 [INFO]: epoch 45: training loss 0.1760\n",
      "2025-06-01 14:50:58 [INFO]: epoch 46: training loss 0.1641\n",
      "2025-06-01 14:50:58 [INFO]: epoch 47: training loss 0.1418\n",
      "2025-06-01 14:50:58 [INFO]: epoch 48: training loss 0.1430\n",
      "2025-06-01 14:50:58 [INFO]: epoch 49: training loss 0.1453\n",
      "2025-06-01 14:50:58 [INFO]: epoch 50: training loss 0.1511\n",
      "2025-06-01 14:50:58 [INFO]: epoch 51: training loss 0.1431\n",
      "2025-06-01 14:50:58 [INFO]: epoch 52: training loss 0.1479\n",
      "2025-06-01 14:50:58 [INFO]: epoch 53: training loss 0.1407\n",
      "2025-06-01 14:50:58 [INFO]: epoch 54: training loss 0.1427\n",
      "2025-06-01 14:50:58 [INFO]: epoch 55: training loss 0.1353\n",
      "2025-06-01 14:50:58 [INFO]: epoch 56: training loss 0.1253\n",
      "2025-06-01 14:50:58 [INFO]: epoch 57: training loss 0.1299\n",
      "2025-06-01 14:50:58 [INFO]: epoch 58: training loss 0.1195\n",
      "2025-06-01 14:50:58 [INFO]: epoch 59: training loss 0.1166\n",
      "2025-06-01 14:50:58 [INFO]: epoch 60: training loss 0.1246\n",
      "2025-06-01 14:50:58 [INFO]: epoch 61: training loss 0.1204\n",
      "2025-06-01 14:50:58 [INFO]: epoch 62: training loss 0.1141\n",
      "2025-06-01 14:50:58 [INFO]: epoch 63: training loss 0.1172\n",
      "2025-06-01 14:50:58 [INFO]: epoch 64: training loss 0.1231\n",
      "2025-06-01 14:50:58 [INFO]: epoch 65: training loss 0.1237\n",
      "2025-06-01 14:50:58 [INFO]: epoch 66: training loss 0.1197\n",
      "2025-06-01 14:50:58 [INFO]: epoch 67: training loss 0.1197\n",
      "2025-06-01 14:50:58 [INFO]: epoch 68: training loss 0.1180\n",
      "2025-06-01 14:50:58 [INFO]: epoch 69: training loss 0.1149\n",
      "2025-06-01 14:50:58 [INFO]: epoch 70: training loss 0.1075\n",
      "2025-06-01 14:50:58 [INFO]: epoch 71: training loss 0.1217\n",
      "2025-06-01 14:50:58 [INFO]: epoch 72: training loss 0.1228\n",
      "2025-06-01 14:50:58 [INFO]: epoch 73: training loss 0.1169\n",
      "2025-06-01 14:50:58 [INFO]: epoch 74: training loss 0.1216\n",
      "2025-06-01 14:50:58 [INFO]: epoch 75: training loss 0.1087\n",
      "2025-06-01 14:50:58 [INFO]: epoch 76: training loss 0.1011\n",
      "2025-06-01 14:50:58 [INFO]: epoch 77: training loss 0.1099\n",
      "2025-06-01 14:50:58 [INFO]: epoch 78: training loss 0.1137\n",
      "2025-06-01 14:50:58 [INFO]: epoch 79: training loss 0.1091\n",
      "2025-06-01 14:50:58 [INFO]: epoch 80: training loss 0.1009\n",
      "2025-06-01 14:50:58 [INFO]: epoch 81: training loss 0.1077\n",
      "2025-06-01 14:50:58 [INFO]: epoch 82: training loss 0.1007\n",
      "2025-06-01 14:50:58 [INFO]: epoch 83: training loss 0.1081\n",
      "2025-06-01 14:50:58 [INFO]: epoch 84: training loss 0.1219\n",
      "2025-06-01 14:50:58 [INFO]: epoch 85: training loss 0.1071\n",
      "2025-06-01 14:50:58 [INFO]: epoch 86: training loss 0.1086\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-01 14:50:58 [INFO]: epoch 87: training loss 0.1244\n",
      "2025-06-01 14:50:58 [INFO]: epoch 88: training loss 0.1158\n",
      "2025-06-01 14:50:58 [INFO]: epoch 89: training loss 0.1157\n",
      "2025-06-01 14:50:58 [INFO]: epoch 90: training loss 0.1215\n",
      "2025-06-01 14:50:58 [INFO]: epoch 91: training loss 0.1104\n",
      "2025-06-01 14:50:58 [INFO]: epoch 92: training loss 0.1119\n",
      "2025-06-01 14:50:58 [INFO]: epoch 93: training loss 0.1156\n",
      "2025-06-01 14:50:59 [INFO]: epoch 94: training loss 0.1076\n",
      "2025-06-01 14:50:59 [INFO]: epoch 95: training loss 0.0948\n",
      "2025-06-01 14:50:59 [INFO]: epoch 96: training loss 0.1365\n",
      "2025-06-01 14:50:59 [INFO]: epoch 97: training loss 0.1477\n",
      "2025-06-01 14:50:59 [INFO]: epoch 98: training loss 0.1219\n",
      "2025-06-01 14:50:59 [INFO]: epoch 99: training loss 0.1108\n",
      "2025-06-01 14:50:59 [INFO]: epoch 100: training loss 0.1017\n",
      "2025-06-01 14:50:59 [INFO]: epoch 101: training loss 0.0971\n",
      "2025-06-01 14:50:59 [INFO]: epoch 102: training loss 0.1198\n",
      "2025-06-01 14:50:59 [INFO]: epoch 103: training loss 0.1178\n",
      "2025-06-01 14:50:59 [INFO]: epoch 104: training loss 0.1196\n",
      "2025-06-01 14:50:59 [INFO]: epoch 105: training loss 0.1151\n",
      "2025-06-01 14:50:59 [INFO]: epoch 106: training loss 0.1197\n",
      "2025-06-01 14:50:59 [INFO]: epoch 107: training loss 0.1102\n",
      "2025-06-01 14:50:59 [INFO]: epoch 108: training loss 0.0950\n",
      "2025-06-01 14:50:59 [INFO]: epoch 109: training loss 0.0996\n",
      "2025-06-01 14:50:59 [INFO]: epoch 110: training loss 0.0972\n",
      "2025-06-01 14:50:59 [INFO]: epoch 111: training loss 0.0902\n",
      "2025-06-01 14:50:59 [INFO]: epoch 112: training loss 0.0944\n",
      "2025-06-01 14:50:59 [INFO]: epoch 113: training loss 0.0993\n",
      "2025-06-01 14:50:59 [INFO]: epoch 114: training loss 0.1007\n",
      "2025-06-01 14:50:59 [INFO]: epoch 115: training loss 0.0784\n",
      "2025-06-01 14:50:59 [INFO]: epoch 116: training loss 0.0874\n",
      "2025-06-01 14:50:59 [INFO]: epoch 117: training loss 0.0933\n",
      "2025-06-01 14:50:59 [INFO]: epoch 118: training loss 0.0850\n",
      "2025-06-01 14:50:59 [INFO]: epoch 119: training loss 0.0974\n",
      "2025-06-01 14:50:59 [INFO]: epoch 120: training loss 0.0887\n",
      "2025-06-01 14:50:59 [INFO]: epoch 121: training loss 0.0889\n",
      "2025-06-01 14:50:59 [INFO]: epoch 122: training loss 0.0960\n",
      "2025-06-01 14:50:59 [INFO]: epoch 123: training loss 0.0891\n",
      "2025-06-01 14:50:59 [INFO]: epoch 124: training loss 0.1009\n",
      "2025-06-01 14:50:59 [INFO]: epoch 125: training loss 0.0870\n",
      "2025-06-01 14:50:59 [INFO]: epoch 126: training loss 0.0973\n",
      "2025-06-01 14:50:59 [INFO]: epoch 127: training loss 0.0863\n",
      "2025-06-01 14:50:59 [INFO]: epoch 128: training loss 0.0926\n",
      "2025-06-01 14:50:59 [INFO]: epoch 129: training loss 0.1141\n",
      "2025-06-01 14:50:59 [INFO]: epoch 130: training loss 0.0949\n",
      "2025-06-01 14:50:59 [INFO]: epoch 131: training loss 0.0806\n",
      "2025-06-01 14:50:59 [INFO]: epoch 132: training loss 0.0999\n",
      "2025-06-01 14:50:59 [INFO]: epoch 133: training loss 0.1011\n",
      "2025-06-01 14:50:59 [INFO]: epoch 134: training loss 0.0957\n",
      "2025-06-01 14:50:59 [INFO]: epoch 135: training loss 0.0976\n",
      "2025-06-01 14:50:59 [INFO]: epoch 136: training loss 0.0809\n",
      "2025-06-01 14:50:59 [INFO]: epoch 137: training loss 0.0852\n",
      "2025-06-01 14:50:59 [INFO]: epoch 138: training loss 0.0938\n",
      "2025-06-01 14:50:59 [INFO]: epoch 139: training loss 0.0756\n",
      "2025-06-01 14:50:59 [INFO]: epoch 140: training loss 0.0739\n",
      "2025-06-01 14:50:59 [INFO]: epoch 141: training loss 0.0797\n",
      "2025-06-01 14:50:59 [INFO]: epoch 142: training loss 0.0862\n",
      "2025-06-01 14:50:59 [INFO]: epoch 143: training loss 0.0676\n",
      "2025-06-01 14:50:59 [INFO]: epoch 144: training loss 0.0879\n",
      "2025-06-01 14:50:59 [INFO]: epoch 145: training loss 0.1016\n",
      "2025-06-01 14:50:59 [INFO]: epoch 146: training loss 0.0875\n",
      "2025-06-01 14:50:59 [INFO]: epoch 147: training loss 0.0846\n",
      "2025-06-01 14:50:59 [INFO]: epoch 148: training loss 0.0797\n",
      "2025-06-01 14:50:59 [INFO]: epoch 149: training loss 0.1007\n",
      "2025-06-01 14:50:59 [INFO]: epoch 150: training loss 0.0828\n",
      "2025-06-01 14:50:59 [INFO]: epoch 151: training loss 0.0816\n",
      "2025-06-01 14:50:59 [INFO]: epoch 152: training loss 0.0841\n",
      "2025-06-01 14:50:59 [INFO]: epoch 153: training loss 0.0898\n",
      "2025-06-01 14:50:59 [INFO]: epoch 154: training loss 0.0720\n",
      "2025-06-01 14:50:59 [INFO]: epoch 155: training loss 0.0877\n",
      "2025-06-01 14:50:59 [INFO]: epoch 156: training loss 0.1026\n",
      "2025-06-01 14:50:59 [INFO]: epoch 157: training loss 0.0733\n",
      "2025-06-01 14:50:59 [INFO]: epoch 158: training loss 0.0857\n",
      "2025-06-01 14:50:59 [INFO]: epoch 159: training loss 0.0931\n",
      "2025-06-01 14:50:59 [INFO]: epoch 160: training loss 0.0725\n",
      "2025-06-01 14:50:59 [INFO]: epoch 161: training loss 0.0903\n",
      "2025-06-01 14:50:59 [INFO]: epoch 162: training loss 0.0946\n",
      "2025-06-01 14:50:59 [INFO]: epoch 163: training loss 0.1076\n",
      "2025-06-01 14:50:59 [INFO]: epoch 164: training loss 0.0822\n",
      "2025-06-01 14:50:59 [INFO]: epoch 165: training loss 0.0726\n",
      "2025-06-01 14:50:59 [INFO]: epoch 166: training loss 0.0820\n",
      "2025-06-01 14:50:59 [INFO]: epoch 167: training loss 0.0853\n",
      "2025-06-01 14:51:00 [INFO]: epoch 168: training loss 0.0640\n",
      "2025-06-01 14:51:00 [INFO]: epoch 169: training loss 0.0832\n",
      "2025-06-01 14:51:00 [INFO]: epoch 170: training loss 0.0857\n",
      "2025-06-01 14:51:00 [INFO]: epoch 171: training loss 0.0765\n",
      "2025-06-01 14:51:00 [INFO]: epoch 172: training loss 0.0974\n",
      "2025-06-01 14:51:00 [INFO]: epoch 173: training loss 0.0940\n",
      "2025-06-01 14:51:00 [INFO]: epoch 174: training loss 0.0760\n",
      "2025-06-01 14:51:00 [INFO]: epoch 175: training loss 0.0783\n",
      "2025-06-01 14:51:00 [INFO]: epoch 176: training loss 0.0981\n",
      "2025-06-01 14:51:00 [INFO]: epoch 177: training loss 0.1021\n",
      "2025-06-01 14:51:00 [INFO]: epoch 178: training loss 0.0854\n",
      "2025-06-01 14:51:00 [INFO]: epoch 179: training loss 0.0859\n",
      "2025-06-01 14:51:00 [INFO]: epoch 180: training loss 0.0792\n",
      "2025-06-01 14:51:00 [INFO]: epoch 181: training loss 0.0666\n",
      "2025-06-01 14:51:00 [INFO]: epoch 182: training loss 0.0813\n",
      "2025-06-01 14:51:00 [INFO]: epoch 183: training loss 0.0880\n",
      "2025-06-01 14:51:00 [INFO]: epoch 184: training loss 0.0791\n",
      "2025-06-01 14:51:00 [INFO]: epoch 185: training loss 0.0843\n",
      "2025-06-01 14:51:00 [INFO]: epoch 186: training loss 0.0926\n",
      "2025-06-01 14:51:00 [INFO]: epoch 187: training loss 0.0894\n",
      "2025-06-01 14:51:00 [INFO]: epoch 188: training loss 0.0634\n",
      "2025-06-01 14:51:00 [INFO]: epoch 189: training loss 0.0752\n",
      "2025-06-01 14:51:00 [INFO]: epoch 190: training loss 0.0811\n",
      "2025-06-01 14:51:00 [INFO]: epoch 191: training loss 0.0839\n",
      "2025-06-01 14:51:00 [INFO]: epoch 192: training loss 0.0696\n",
      "2025-06-01 14:51:00 [INFO]: epoch 193: training loss 0.0722\n",
      "2025-06-01 14:51:00 [INFO]: epoch 194: training loss 0.1018\n",
      "2025-06-01 14:51:00 [INFO]: epoch 195: training loss 0.0821\n",
      "2025-06-01 14:51:00 [INFO]: epoch 196: training loss 0.0806\n",
      "2025-06-01 14:51:00 [INFO]: epoch 197: training loss 0.0898\n",
      "2025-06-01 14:51:00 [INFO]: epoch 198: training loss 0.0826\n",
      "2025-06-01 14:51:00 [INFO]: epoch 199: training loss 0.0708\n",
      "2025-06-01 14:51:00 [INFO]: epoch 200: training loss 0.0724\n",
      "2025-06-01 14:51:00 [INFO]: epoch 201: training loss 0.0865\n",
      "2025-06-01 14:51:00 [INFO]: epoch 202: training loss 0.0772\n",
      "2025-06-01 14:51:00 [INFO]: epoch 203: training loss 0.0671\n",
      "2025-06-01 14:51:00 [INFO]: epoch 204: training loss 0.0698\n",
      "2025-06-01 14:51:00 [INFO]: epoch 205: training loss 0.0717\n",
      "2025-06-01 14:51:00 [INFO]: epoch 206: training loss 0.0738\n",
      "2025-06-01 14:51:00 [INFO]: epoch 207: training loss 0.0641\n",
      "2025-06-01 14:51:00 [INFO]: epoch 208: training loss 0.0626\n",
      "2025-06-01 14:51:00 [INFO]: epoch 209: training loss 0.0641\n",
      "2025-06-01 14:51:00 [INFO]: epoch 210: training loss 0.0699\n",
      "2025-06-01 14:51:00 [INFO]: epoch 211: training loss 0.0666\n",
      "2025-06-01 14:51:00 [INFO]: epoch 212: training loss 0.0577\n",
      "2025-06-01 14:51:00 [INFO]: epoch 213: training loss 0.0644\n",
      "2025-06-01 14:51:00 [INFO]: epoch 214: training loss 0.0816\n",
      "2025-06-01 14:51:00 [INFO]: epoch 215: training loss 0.0729\n",
      "2025-06-01 14:51:00 [INFO]: epoch 216: training loss 0.0539\n",
      "2025-06-01 14:51:00 [INFO]: epoch 217: training loss 0.0705\n",
      "2025-06-01 14:51:00 [INFO]: epoch 218: training loss 0.0725\n",
      "2025-06-01 14:51:00 [INFO]: epoch 219: training loss 0.0730\n",
      "2025-06-01 14:51:00 [INFO]: epoch 220: training loss 0.0617\n",
      "2025-06-01 14:51:00 [INFO]: epoch 221: training loss 0.0600\n",
      "2025-06-01 14:51:00 [INFO]: epoch 222: training loss 0.0657\n",
      "2025-06-01 14:51:00 [INFO]: epoch 223: training loss 0.0826\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-01 14:51:00 [INFO]: epoch 224: training loss 0.0682\n",
      "2025-06-01 14:51:00 [INFO]: epoch 225: training loss 0.0674\n",
      "2025-06-01 14:51:00 [INFO]: epoch 226: training loss 0.0562\n",
      "2025-06-01 14:51:00 [INFO]: epoch 227: training loss 0.0657\n",
      "2025-06-01 14:51:00 [INFO]: epoch 228: training loss 0.0716\n",
      "2025-06-01 14:51:00 [INFO]: epoch 229: training loss 0.0608\n",
      "2025-06-01 14:51:00 [INFO]: epoch 230: training loss 0.0590\n",
      "2025-06-01 14:51:00 [INFO]: epoch 231: training loss 0.0565\n",
      "2025-06-01 14:51:00 [INFO]: epoch 232: training loss 0.0543\n",
      "2025-06-01 14:51:00 [INFO]: epoch 233: training loss 0.0699\n",
      "2025-06-01 14:51:00 [INFO]: epoch 234: training loss 0.0754\n",
      "2025-06-01 14:51:00 [INFO]: epoch 235: training loss 0.0579\n",
      "2025-06-01 14:51:00 [INFO]: epoch 236: training loss 0.0664\n",
      "2025-06-01 14:51:00 [INFO]: epoch 237: training loss 0.0699\n",
      "2025-06-01 14:51:00 [INFO]: epoch 238: training loss 0.0592\n",
      "2025-06-01 14:51:00 [INFO]: epoch 239: training loss 0.0573\n",
      "2025-06-01 14:51:00 [INFO]: epoch 240: training loss 0.0601\n",
      "2025-06-01 14:51:00 [INFO]: epoch 241: training loss 0.0717\n",
      "2025-06-01 14:51:00 [INFO]: epoch 242: training loss 0.0521\n",
      "2025-06-01 14:51:01 [INFO]: epoch 243: training loss 0.0602\n",
      "2025-06-01 14:51:01 [INFO]: epoch 244: training loss 0.0568\n",
      "2025-06-01 14:51:01 [INFO]: epoch 245: training loss 0.0654\n",
      "2025-06-01 14:51:01 [INFO]: epoch 246: training loss 0.0665\n",
      "2025-06-01 14:51:01 [INFO]: epoch 247: training loss 0.0530\n",
      "2025-06-01 14:51:01 [INFO]: epoch 248: training loss 0.0616\n",
      "2025-06-01 14:51:01 [INFO]: epoch 249: training loss 0.0500\n",
      "2025-06-01 14:51:01 [INFO]: epoch 250: training loss 0.0640\n",
      "2025-06-01 14:51:01 [INFO]: epoch 251: training loss 0.0614\n",
      "2025-06-01 14:51:01 [INFO]: epoch 252: training loss 0.0608\n",
      "2025-06-01 14:51:01 [INFO]: epoch 253: training loss 0.0532\n",
      "2025-06-01 14:51:01 [INFO]: epoch 254: training loss 0.0561\n",
      "2025-06-01 14:51:01 [INFO]: epoch 255: training loss 0.0462\n",
      "2025-06-01 14:51:01 [INFO]: epoch 256: training loss 0.0536\n",
      "2025-06-01 14:51:01 [INFO]: epoch 257: training loss 0.0585\n",
      "2025-06-01 14:51:01 [INFO]: epoch 258: training loss 0.0563\n",
      "2025-06-01 14:51:01 [INFO]: epoch 259: training loss 0.0640\n",
      "2025-06-01 14:51:01 [INFO]: epoch 260: training loss 0.0522\n",
      "2025-06-01 14:51:01 [INFO]: epoch 261: training loss 0.0494\n",
      "2025-06-01 14:51:01 [INFO]: epoch 262: training loss 0.0584\n",
      "2025-06-01 14:51:01 [INFO]: epoch 263: training loss 0.0565\n",
      "2025-06-01 14:51:01 [INFO]: epoch 264: training loss 0.0537\n",
      "2025-06-01 14:51:01 [INFO]: epoch 265: training loss 0.0520\n",
      "2025-06-01 14:51:01 [INFO]: epoch 266: training loss 0.0566\n",
      "2025-06-01 14:51:01 [INFO]: epoch 267: training loss 0.0554\n",
      "2025-06-01 14:51:01 [INFO]: epoch 268: training loss 0.0510\n",
      "2025-06-01 14:51:01 [INFO]: epoch 269: training loss 0.0495\n",
      "2025-06-01 14:51:01 [INFO]: epoch 270: training loss 0.0510\n",
      "2025-06-01 14:51:01 [INFO]: epoch 271: training loss 0.0485\n",
      "2025-06-01 14:51:01 [INFO]: epoch 272: training loss 0.0470\n",
      "2025-06-01 14:51:01 [INFO]: epoch 273: training loss 0.0566\n",
      "2025-06-01 14:51:01 [INFO]: epoch 274: training loss 0.0654\n",
      "2025-06-01 14:51:01 [INFO]: epoch 275: training loss 0.0583\n",
      "2025-06-01 14:51:01 [INFO]: epoch 276: training loss 0.0492\n",
      "2025-06-01 14:51:01 [INFO]: epoch 277: training loss 0.0615\n",
      "2025-06-01 14:51:01 [INFO]: epoch 278: training loss 0.0683\n",
      "2025-06-01 14:51:01 [INFO]: epoch 279: training loss 0.0595\n",
      "2025-06-01 14:51:01 [INFO]: epoch 280: training loss 0.0471\n",
      "2025-06-01 14:51:01 [INFO]: epoch 281: training loss 0.0557\n",
      "2025-06-01 14:51:01 [INFO]: epoch 282: training loss 0.0563\n",
      "2025-06-01 14:51:01 [INFO]: epoch 283: training loss 0.0521\n",
      "2025-06-01 14:51:01 [INFO]: epoch 284: training loss 0.0700\n",
      "2025-06-01 14:51:01 [INFO]: epoch 285: training loss 0.0517\n",
      "2025-06-01 14:51:01 [INFO]: epoch 286: training loss 0.0507\n",
      "2025-06-01 14:51:01 [INFO]: epoch 287: training loss 0.0638\n",
      "2025-06-01 14:51:01 [INFO]: epoch 288: training loss 0.0496\n",
      "2025-06-01 14:51:01 [INFO]: epoch 289: training loss 0.0487\n",
      "2025-06-01 14:51:01 [INFO]: epoch 290: training loss 0.0504\n",
      "2025-06-01 14:51:01 [INFO]: epoch 291: training loss 0.0573\n",
      "2025-06-01 14:51:01 [INFO]: epoch 292: training loss 0.0455\n",
      "2025-06-01 14:51:01 [INFO]: epoch 293: training loss 0.0405\n",
      "2025-06-01 14:51:01 [INFO]: epoch 294: training loss 0.0469\n",
      "2025-06-01 14:51:01 [INFO]: epoch 295: training loss 0.0553\n",
      "2025-06-01 14:51:01 [INFO]: epoch 296: training loss 0.0422\n",
      "2025-06-01 14:51:01 [INFO]: epoch 297: training loss 0.0501\n",
      "2025-06-01 14:51:01 [INFO]: epoch 298: training loss 0.0491\n",
      "2025-06-01 14:51:01 [INFO]: epoch 299: training loss 0.0441\n",
      "2025-06-01 14:51:01 [INFO]: epoch 300: training loss 0.0469\n",
      "2025-06-01 14:51:01 [INFO]: epoch 301: training loss 0.0479\n",
      "2025-06-01 14:51:01 [INFO]: epoch 302: training loss 0.0453\n",
      "2025-06-01 14:51:01 [INFO]: epoch 303: training loss 0.0472\n",
      "2025-06-01 14:51:01 [INFO]: epoch 304: training loss 0.0443\n",
      "2025-06-01 14:51:01 [INFO]: epoch 305: training loss 0.0440\n",
      "2025-06-01 14:51:01 [INFO]: epoch 306: training loss 0.0520\n",
      "2025-06-01 14:51:01 [INFO]: epoch 307: training loss 0.0439\n",
      "2025-06-01 14:51:01 [INFO]: epoch 308: training loss 0.0506\n",
      "2025-06-01 14:51:01 [INFO]: epoch 309: training loss 0.0368\n",
      "2025-06-01 14:51:01 [INFO]: epoch 310: training loss 0.0439\n",
      "2025-06-01 14:51:01 [INFO]: epoch 311: training loss 0.0488\n",
      "2025-06-01 14:51:01 [INFO]: epoch 312: training loss 0.0453\n",
      "2025-06-01 14:51:01 [INFO]: epoch 313: training loss 0.0410\n",
      "2025-06-01 14:51:01 [INFO]: epoch 314: training loss 0.0465\n",
      "2025-06-01 14:51:01 [INFO]: epoch 315: training loss 0.0498\n",
      "2025-06-01 14:51:01 [INFO]: epoch 316: training loss 0.0577\n",
      "2025-06-01 14:51:01 [INFO]: epoch 317: training loss 0.0522\n",
      "2025-06-01 14:51:01 [INFO]: epoch 318: training loss 0.0530\n",
      "2025-06-01 14:51:02 [INFO]: epoch 319: training loss 0.0537\n",
      "2025-06-01 14:51:02 [INFO]: epoch 320: training loss 0.0517\n",
      "2025-06-01 14:51:02 [INFO]: epoch 321: training loss 0.0448\n",
      "2025-06-01 14:51:02 [INFO]: epoch 322: training loss 0.0500\n",
      "2025-06-01 14:51:02 [INFO]: epoch 323: training loss 0.0469\n",
      "2025-06-01 14:51:02 [INFO]: epoch 324: training loss 0.0442\n",
      "2025-06-01 14:51:02 [INFO]: epoch 325: training loss 0.0575\n",
      "2025-06-01 14:51:02 [INFO]: epoch 326: training loss 0.0458\n",
      "2025-06-01 14:51:02 [INFO]: epoch 327: training loss 0.0482\n",
      "2025-06-01 14:51:02 [INFO]: epoch 328: training loss 0.0442\n",
      "2025-06-01 14:51:02 [INFO]: epoch 329: training loss 0.0450\n",
      "2025-06-01 14:51:02 [INFO]: epoch 330: training loss 0.0506\n",
      "2025-06-01 14:51:02 [INFO]: epoch 331: training loss 0.0452\n",
      "2025-06-01 14:51:02 [INFO]: epoch 332: training loss 0.0481\n",
      "2025-06-01 14:51:02 [INFO]: epoch 333: training loss 0.0438\n",
      "2025-06-01 14:51:02 [INFO]: epoch 334: training loss 0.0378\n",
      "2025-06-01 14:51:02 [INFO]: epoch 335: training loss 0.0456\n",
      "2025-06-01 14:51:02 [INFO]: epoch 336: training loss 0.0482\n",
      "2025-06-01 14:51:02 [INFO]: epoch 337: training loss 0.0464\n",
      "2025-06-01 14:51:02 [INFO]: epoch 338: training loss 0.0445\n",
      "2025-06-01 14:51:02 [INFO]: epoch 339: training loss 0.0520\n",
      "2025-06-01 14:51:02 [INFO]: epoch 340: training loss 0.0514\n",
      "2025-06-01 14:51:02 [INFO]: epoch 341: training loss 0.0420\n",
      "2025-06-01 14:51:02 [INFO]: epoch 342: training loss 0.0426\n",
      "2025-06-01 14:51:02 [INFO]: epoch 343: training loss 0.0442\n",
      "2025-06-01 14:51:02 [INFO]: epoch 344: training loss 0.0490\n",
      "2025-06-01 14:51:02 [INFO]: epoch 345: training loss 0.0363\n",
      "2025-06-01 14:51:02 [INFO]: epoch 346: training loss 0.0463\n",
      "2025-06-01 14:51:02 [INFO]: epoch 347: training loss 0.0462\n",
      "2025-06-01 14:51:02 [INFO]: epoch 348: training loss 0.0426\n",
      "2025-06-01 14:51:02 [INFO]: epoch 349: training loss 0.0402\n",
      "2025-06-01 14:51:02 [INFO]: epoch 350: training loss 0.0468\n",
      "2025-06-01 14:51:02 [INFO]: epoch 351: training loss 0.0432\n",
      "2025-06-01 14:51:02 [INFO]: epoch 352: training loss 0.0470\n",
      "2025-06-01 14:51:02 [INFO]: epoch 353: training loss 0.0400\n",
      "2025-06-01 14:51:02 [INFO]: epoch 354: training loss 0.0425\n",
      "2025-06-01 14:51:02 [INFO]: epoch 355: training loss 0.0443\n",
      "2025-06-01 14:51:02 [INFO]: epoch 356: training loss 0.0389\n",
      "2025-06-01 14:51:02 [INFO]: epoch 357: training loss 0.0431\n",
      "2025-06-01 14:51:02 [INFO]: epoch 358: training loss 0.0421\n",
      "2025-06-01 14:51:02 [INFO]: epoch 359: training loss 0.0389\n",
      "2025-06-01 14:51:02 [INFO]: epoch 360: training loss 0.0447\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-01 14:51:02 [INFO]: epoch 361: training loss 0.0369\n",
      "2025-06-01 14:51:02 [INFO]: epoch 362: training loss 0.0455\n",
      "2025-06-01 14:51:02 [INFO]: epoch 363: training loss 0.0435\n",
      "2025-06-01 14:51:02 [INFO]: epoch 364: training loss 0.0436\n",
      "2025-06-01 14:51:02 [INFO]: epoch 365: training loss 0.0422\n",
      "2025-06-01 14:51:02 [INFO]: epoch 366: training loss 0.0468\n",
      "2025-06-01 14:51:02 [INFO]: epoch 367: training loss 0.0468\n",
      "2025-06-01 14:51:02 [INFO]: epoch 368: training loss 0.0414\n",
      "2025-06-01 14:51:02 [INFO]: epoch 369: training loss 0.0416\n",
      "2025-06-01 14:51:02 [INFO]: epoch 370: training loss 0.0498\n",
      "2025-06-01 14:51:02 [INFO]: epoch 371: training loss 0.0518\n",
      "2025-06-01 14:51:02 [INFO]: epoch 372: training loss 0.0438\n",
      "2025-06-01 14:51:02 [INFO]: epoch 373: training loss 0.0472\n",
      "2025-06-01 14:51:02 [INFO]: epoch 374: training loss 0.0354\n",
      "2025-06-01 14:51:02 [INFO]: epoch 375: training loss 0.0368\n",
      "2025-06-01 14:51:02 [INFO]: epoch 376: training loss 0.0483\n",
      "2025-06-01 14:51:02 [INFO]: epoch 377: training loss 0.0324\n",
      "2025-06-01 14:51:02 [INFO]: epoch 378: training loss 0.0406\n",
      "2025-06-01 14:51:02 [INFO]: epoch 379: training loss 0.0404\n",
      "2025-06-01 14:51:02 [INFO]: epoch 380: training loss 0.0445\n",
      "2025-06-01 14:51:02 [INFO]: epoch 381: training loss 0.0458\n",
      "2025-06-01 14:51:02 [INFO]: epoch 382: training loss 0.0319\n",
      "2025-06-01 14:51:02 [INFO]: epoch 383: training loss 0.0431\n",
      "2025-06-01 14:51:02 [INFO]: epoch 384: training loss 0.0501\n",
      "2025-06-01 14:51:02 [INFO]: epoch 385: training loss 0.0442\n",
      "2025-06-01 14:51:02 [INFO]: epoch 386: training loss 0.0427\n",
      "2025-06-01 14:51:02 [INFO]: epoch 387: training loss 0.0444\n",
      "2025-06-01 14:51:02 [INFO]: epoch 388: training loss 0.0486\n",
      "2025-06-01 14:51:02 [INFO]: epoch 389: training loss 0.0364\n",
      "2025-06-01 14:51:02 [INFO]: epoch 390: training loss 0.0381\n",
      "2025-06-01 14:51:02 [INFO]: epoch 391: training loss 0.0463\n",
      "2025-06-01 14:51:02 [INFO]: epoch 392: training loss 0.0366\n",
      "2025-06-01 14:51:02 [INFO]: epoch 393: training loss 0.0526\n",
      "2025-06-01 14:51:02 [INFO]: epoch 394: training loss 0.0473\n",
      "2025-06-01 14:51:03 [INFO]: epoch 395: training loss 0.0445\n",
      "2025-06-01 14:51:03 [INFO]: epoch 396: training loss 0.0432\n",
      "2025-06-01 14:51:03 [INFO]: epoch 397: training loss 0.0412\n",
      "2025-06-01 14:51:03 [INFO]: epoch 398: training loss 0.0445\n",
      "2025-06-01 14:51:03 [INFO]: epoch 399: training loss 0.0369\n",
      "2025-06-01 14:51:03 [INFO]: epoch 400: training loss 0.0301\n",
      "2025-06-01 14:51:03 [INFO]: epoch 401: training loss 0.0391\n",
      "2025-06-01 14:51:03 [INFO]: epoch 402: training loss 0.0414\n",
      "2025-06-01 14:51:03 [INFO]: epoch 403: training loss 0.0432\n",
      "2025-06-01 14:51:03 [INFO]: epoch 404: training loss 0.0394\n",
      "2025-06-01 14:51:03 [INFO]: epoch 405: training loss 0.0503\n",
      "2025-06-01 14:51:03 [INFO]: epoch 406: training loss 0.0448\n",
      "2025-06-01 14:51:03 [INFO]: epoch 407: training loss 0.0476\n",
      "2025-06-01 14:51:03 [INFO]: epoch 408: training loss 0.0404\n",
      "2025-06-01 14:51:03 [INFO]: epoch 409: training loss 0.0451\n",
      "2025-06-01 14:51:03 [INFO]: epoch 410: training loss 0.0481\n",
      "2025-06-01 14:51:03 [INFO]: epoch 411: training loss 0.0427\n",
      "2025-06-01 14:51:03 [INFO]: epoch 412: training loss 0.0498\n",
      "2025-06-01 14:51:03 [INFO]: epoch 413: training loss 0.0439\n",
      "2025-06-01 14:51:03 [INFO]: epoch 414: training loss 0.0371\n",
      "2025-06-01 14:51:03 [INFO]: epoch 415: training loss 0.0470\n",
      "2025-06-01 14:51:03 [INFO]: epoch 416: training loss 0.0439\n",
      "2025-06-01 14:51:03 [INFO]: epoch 417: training loss 0.0407\n",
      "2025-06-01 14:51:03 [INFO]: epoch 418: training loss 0.0461\n",
      "2025-06-01 14:51:03 [INFO]: epoch 419: training loss 0.0350\n",
      "2025-06-01 14:51:03 [INFO]: epoch 420: training loss 0.0372\n",
      "2025-06-01 14:51:03 [INFO]: epoch 421: training loss 0.0463\n",
      "2025-06-01 14:51:03 [INFO]: epoch 422: training loss 0.0428\n",
      "2025-06-01 14:51:03 [INFO]: epoch 423: training loss 0.0370\n",
      "2025-06-01 14:51:03 [INFO]: epoch 424: training loss 0.0401\n",
      "2025-06-01 14:51:03 [INFO]: epoch 425: training loss 0.0370\n",
      "2025-06-01 14:51:03 [INFO]: epoch 426: training loss 0.0309\n",
      "2025-06-01 14:51:03 [INFO]: epoch 427: training loss 0.0363\n",
      "2025-06-01 14:51:03 [INFO]: epoch 428: training loss 0.0357\n",
      "2025-06-01 14:51:03 [INFO]: epoch 429: training loss 0.0363\n",
      "2025-06-01 14:51:03 [INFO]: epoch 430: training loss 0.0362\n",
      "2025-06-01 14:51:03 [INFO]: epoch 431: training loss 0.0299\n",
      "2025-06-01 14:51:03 [INFO]: epoch 432: training loss 0.0354\n",
      "2025-06-01 14:51:03 [INFO]: epoch 433: training loss 0.0398\n",
      "2025-06-01 14:51:03 [INFO]: epoch 434: training loss 0.0321\n",
      "2025-06-01 14:51:03 [INFO]: epoch 435: training loss 0.0474\n",
      "2025-06-01 14:51:03 [INFO]: epoch 436: training loss 0.0410\n",
      "2025-06-01 14:51:03 [INFO]: epoch 437: training loss 0.0411\n",
      "2025-06-01 14:51:03 [INFO]: epoch 438: training loss 0.0348\n",
      "2025-06-01 14:51:03 [INFO]: epoch 439: training loss 0.0301\n",
      "2025-06-01 14:51:03 [INFO]: epoch 440: training loss 0.0329\n",
      "2025-06-01 14:51:03 [INFO]: epoch 441: training loss 0.0360\n",
      "2025-06-01 14:51:03 [INFO]: epoch 442: training loss 0.0318\n",
      "2025-06-01 14:51:03 [INFO]: epoch 443: training loss 0.0462\n",
      "2025-06-01 14:51:03 [INFO]: epoch 444: training loss 0.0333\n",
      "2025-06-01 14:51:03 [INFO]: epoch 445: training loss 0.0392\n",
      "2025-06-01 14:51:03 [INFO]: epoch 446: training loss 0.0312\n",
      "2025-06-01 14:51:03 [INFO]: epoch 447: training loss 0.0288\n",
      "2025-06-01 14:51:03 [INFO]: epoch 448: training loss 0.0446\n",
      "2025-06-01 14:51:03 [INFO]: epoch 449: training loss 0.0328\n",
      "2025-06-01 14:51:03 [INFO]: epoch 450: training loss 0.0304\n",
      "2025-06-01 14:51:03 [INFO]: epoch 451: training loss 0.0318\n",
      "2025-06-01 14:51:03 [INFO]: epoch 452: training loss 0.0315\n",
      "2025-06-01 14:51:03 [INFO]: epoch 453: training loss 0.0306\n",
      "2025-06-01 14:51:03 [INFO]: epoch 454: training loss 0.0282\n",
      "2025-06-01 14:51:03 [INFO]: epoch 455: training loss 0.0275\n",
      "2025-06-01 14:51:03 [INFO]: epoch 456: training loss 0.0349\n",
      "2025-06-01 14:51:03 [INFO]: epoch 457: training loss 0.0318\n",
      "2025-06-01 14:51:03 [INFO]: epoch 458: training loss 0.0407\n",
      "2025-06-01 14:51:03 [INFO]: epoch 459: training loss 0.0365\n",
      "2025-06-01 14:51:03 [INFO]: epoch 460: training loss 0.0319\n",
      "2025-06-01 14:51:03 [INFO]: epoch 461: training loss 0.0414\n",
      "2025-06-01 14:51:03 [INFO]: epoch 462: training loss 0.0402\n",
      "2025-06-01 14:51:03 [INFO]: epoch 463: training loss 0.0371\n",
      "2025-06-01 14:51:03 [INFO]: epoch 464: training loss 0.0416\n",
      "2025-06-01 14:51:03 [INFO]: epoch 465: training loss 0.0333\n",
      "2025-06-01 14:51:03 [INFO]: epoch 466: training loss 0.0354\n",
      "2025-06-01 14:51:03 [INFO]: epoch 467: training loss 0.0328\n",
      "2025-06-01 14:51:03 [INFO]: epoch 468: training loss 0.0437\n",
      "2025-06-01 14:51:03 [INFO]: epoch 469: training loss 0.0335\n",
      "2025-06-01 14:51:03 [INFO]: epoch 470: training loss 0.0306\n",
      "2025-06-01 14:51:03 [INFO]: epoch 471: training loss 0.0354\n",
      "2025-06-01 14:51:04 [INFO]: epoch 472: training loss 0.0397\n",
      "2025-06-01 14:51:04 [INFO]: epoch 473: training loss 0.0332\n",
      "2025-06-01 14:51:04 [INFO]: epoch 474: training loss 0.0334\n",
      "2025-06-01 14:51:04 [INFO]: epoch 475: training loss 0.0401\n",
      "2025-06-01 14:51:04 [INFO]: epoch 476: training loss 0.0383\n",
      "2025-06-01 14:51:04 [INFO]: epoch 477: training loss 0.0331\n",
      "2025-06-01 14:51:04 [INFO]: epoch 478: training loss 0.0323\n",
      "2025-06-01 14:51:04 [INFO]: epoch 479: training loss 0.0350\n",
      "2025-06-01 14:51:04 [INFO]: epoch 480: training loss 0.0367\n",
      "2025-06-01 14:51:04 [INFO]: epoch 481: training loss 0.0333\n",
      "2025-06-01 14:51:04 [INFO]: epoch 482: training loss 0.0330\n",
      "2025-06-01 14:51:04 [INFO]: epoch 483: training loss 0.0340\n",
      "2025-06-01 14:51:04 [INFO]: epoch 484: training loss 0.0301\n",
      "2025-06-01 14:51:04 [INFO]: epoch 485: training loss 0.0293\n",
      "2025-06-01 14:51:04 [INFO]: epoch 486: training loss 0.0319\n",
      "2025-06-01 14:51:04 [INFO]: epoch 487: training loss 0.0340\n",
      "2025-06-01 14:51:04 [INFO]: epoch 488: training loss 0.0308\n",
      "2025-06-01 14:51:04 [INFO]: epoch 489: training loss 0.0313\n",
      "2025-06-01 14:51:04 [INFO]: epoch 490: training loss 0.0305\n",
      "2025-06-01 14:51:04 [INFO]: epoch 491: training loss 0.0326\n",
      "2025-06-01 14:51:04 [INFO]: epoch 492: training loss 0.0310\n",
      "2025-06-01 14:51:04 [INFO]: epoch 493: training loss 0.0366\n",
      "2025-06-01 14:51:04 [INFO]: epoch 494: training loss 0.0331\n",
      "2025-06-01 14:51:04 [INFO]: epoch 495: training loss 0.0295\n",
      "2025-06-01 14:51:04 [INFO]: epoch 496: training loss 0.0340\n",
      "2025-06-01 14:51:04 [INFO]: epoch 497: training loss 0.0305\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-01 14:51:04 [INFO]: epoch 498: training loss 0.0352\n",
      "2025-06-01 14:51:04 [INFO]: epoch 499: training loss 0.0308\n",
      "2025-06-01 14:51:04 [INFO]: epoch 500: training loss 0.0353\n",
      "2025-06-01 14:51:04 [INFO]: epoch 501: training loss 0.0297\n",
      "2025-06-01 14:51:04 [INFO]: epoch 502: training loss 0.0330\n",
      "2025-06-01 14:51:04 [INFO]: epoch 503: training loss 0.0373\n",
      "2025-06-01 14:51:04 [INFO]: epoch 504: training loss 0.0334\n",
      "2025-06-01 14:51:04 [INFO]: epoch 505: training loss 0.0419\n",
      "2025-06-01 14:51:04 [INFO]: epoch 506: training loss 0.0425\n",
      "2025-06-01 14:51:04 [INFO]: epoch 507: training loss 0.0274\n",
      "2025-06-01 14:51:04 [INFO]: epoch 508: training loss 0.0455\n",
      "2025-06-01 14:51:04 [INFO]: epoch 509: training loss 0.0360\n",
      "2025-06-01 14:51:04 [INFO]: epoch 510: training loss 0.0344\n",
      "2025-06-01 14:51:04 [INFO]: epoch 511: training loss 0.0400\n",
      "2025-06-01 14:51:04 [INFO]: epoch 512: training loss 0.0357\n",
      "2025-06-01 14:51:04 [INFO]: epoch 513: training loss 0.0338\n",
      "2025-06-01 14:51:04 [INFO]: epoch 514: training loss 0.0315\n",
      "2025-06-01 14:51:04 [INFO]: epoch 515: training loss 0.0328\n",
      "2025-06-01 14:51:04 [INFO]: epoch 516: training loss 0.0301\n",
      "2025-06-01 14:51:04 [INFO]: epoch 517: training loss 0.0267\n",
      "2025-06-01 14:51:04 [INFO]: epoch 518: training loss 0.0383\n",
      "2025-06-01 14:51:04 [INFO]: epoch 519: training loss 0.0328\n",
      "2025-06-01 14:51:04 [INFO]: epoch 520: training loss 0.0242\n",
      "2025-06-01 14:51:04 [INFO]: epoch 521: training loss 0.0309\n",
      "2025-06-01 14:51:04 [INFO]: epoch 522: training loss 0.0317\n",
      "2025-06-01 14:51:04 [INFO]: epoch 523: training loss 0.0263\n",
      "2025-06-01 14:51:04 [INFO]: epoch 524: training loss 0.0302\n",
      "2025-06-01 14:51:04 [INFO]: epoch 525: training loss 0.0378\n",
      "2025-06-01 14:51:04 [INFO]: epoch 526: training loss 0.0287\n",
      "2025-06-01 14:51:04 [INFO]: epoch 527: training loss 0.0303\n",
      "2025-06-01 14:51:04 [INFO]: epoch 528: training loss 0.0351\n",
      "2025-06-01 14:51:04 [INFO]: epoch 529: training loss 0.0287\n",
      "2025-06-01 14:51:04 [INFO]: epoch 530: training loss 0.0261\n",
      "2025-06-01 14:51:04 [INFO]: epoch 531: training loss 0.0393\n",
      "2025-06-01 14:51:04 [INFO]: epoch 532: training loss 0.0269\n",
      "2025-06-01 14:51:04 [INFO]: epoch 533: training loss 0.0309\n",
      "2025-06-01 14:51:04 [INFO]: epoch 534: training loss 0.0411\n",
      "2025-06-01 14:51:04 [INFO]: epoch 535: training loss 0.0365\n",
      "2025-06-01 14:51:04 [INFO]: epoch 536: training loss 0.0304\n",
      "2025-06-01 14:51:04 [INFO]: epoch 537: training loss 0.0491\n",
      "2025-06-01 14:51:04 [INFO]: epoch 538: training loss 0.0385\n",
      "2025-06-01 14:51:04 [INFO]: epoch 539: training loss 0.0302\n",
      "2025-06-01 14:51:04 [INFO]: epoch 540: training loss 0.0319\n",
      "2025-06-01 14:51:04 [INFO]: epoch 541: training loss 0.0314\n",
      "2025-06-01 14:51:04 [INFO]: epoch 542: training loss 0.0286\n",
      "2025-06-01 14:51:04 [INFO]: epoch 543: training loss 0.0365\n",
      "2025-06-01 14:51:04 [INFO]: epoch 544: training loss 0.0409\n",
      "2025-06-01 14:51:04 [INFO]: epoch 545: training loss 0.0419\n",
      "2025-06-01 14:51:04 [INFO]: epoch 546: training loss 0.0398\n",
      "2025-06-01 14:51:04 [INFO]: epoch 547: training loss 0.0344\n",
      "2025-06-01 14:51:04 [INFO]: epoch 548: training loss 0.0381\n",
      "2025-06-01 14:51:05 [INFO]: epoch 549: training loss 0.0372\n",
      "2025-06-01 14:51:05 [INFO]: epoch 550: training loss 0.0285\n",
      "2025-06-01 14:51:05 [INFO]: epoch 551: training loss 0.0378\n",
      "2025-06-01 14:51:05 [INFO]: epoch 552: training loss 0.0354\n",
      "2025-06-01 14:51:05 [INFO]: epoch 553: training loss 0.0266\n",
      "2025-06-01 14:51:05 [INFO]: epoch 554: training loss 0.0426\n",
      "2025-06-01 14:51:05 [INFO]: epoch 555: training loss 0.0397\n",
      "2025-06-01 14:51:05 [INFO]: epoch 556: training loss 0.0283\n",
      "2025-06-01 14:51:05 [INFO]: epoch 557: training loss 0.0323\n",
      "2025-06-01 14:51:05 [INFO]: epoch 558: training loss 0.0338\n",
      "2025-06-01 14:51:05 [INFO]: epoch 559: training loss 0.0289\n",
      "2025-06-01 14:51:05 [INFO]: epoch 560: training loss 0.0346\n",
      "2025-06-01 14:51:05 [INFO]: epoch 561: training loss 0.0313\n",
      "2025-06-01 14:51:05 [INFO]: epoch 562: training loss 0.0300\n",
      "2025-06-01 14:51:05 [INFO]: epoch 563: training loss 0.0335\n",
      "2025-06-01 14:51:05 [INFO]: epoch 564: training loss 0.0276\n",
      "2025-06-01 14:51:05 [INFO]: epoch 565: training loss 0.0237\n",
      "2025-06-01 14:51:05 [INFO]: epoch 566: training loss 0.0376\n",
      "2025-06-01 14:51:05 [INFO]: epoch 567: training loss 0.0264\n",
      "2025-06-01 14:51:05 [INFO]: epoch 568: training loss 0.0309\n",
      "2025-06-01 14:51:05 [INFO]: epoch 569: training loss 0.0257\n",
      "2025-06-01 14:51:05 [INFO]: epoch 570: training loss 0.0237\n",
      "2025-06-01 14:51:05 [INFO]: epoch 571: training loss 0.0264\n",
      "2025-06-01 14:51:05 [INFO]: epoch 572: training loss 0.0259\n",
      "2025-06-01 14:51:05 [INFO]: epoch 573: training loss 0.0303\n",
      "2025-06-01 14:51:05 [INFO]: epoch 574: training loss 0.0240\n",
      "2025-06-01 14:51:05 [INFO]: epoch 575: training loss 0.0273\n",
      "2025-06-01 14:51:05 [INFO]: epoch 576: training loss 0.0311\n",
      "2025-06-01 14:51:05 [INFO]: epoch 577: training loss 0.0269\n",
      "2025-06-01 14:51:05 [INFO]: epoch 578: training loss 0.0259\n",
      "2025-06-01 14:51:05 [INFO]: epoch 579: training loss 0.0349\n",
      "2025-06-01 14:51:05 [INFO]: epoch 580: training loss 0.0311\n",
      "2025-06-01 14:51:05 [INFO]: epoch 581: training loss 0.0280\n",
      "2025-06-01 14:51:05 [INFO]: epoch 582: training loss 0.0369\n",
      "2025-06-01 14:51:05 [INFO]: epoch 583: training loss 0.0311\n",
      "2025-06-01 14:51:05 [INFO]: epoch 584: training loss 0.0300\n",
      "2025-06-01 14:51:05 [INFO]: epoch 585: training loss 0.0425\n",
      "2025-06-01 14:51:05 [INFO]: epoch 586: training loss 0.0356\n",
      "2025-06-01 14:51:05 [INFO]: epoch 587: training loss 0.0388\n",
      "2025-06-01 14:51:05 [INFO]: epoch 588: training loss 0.0319\n",
      "2025-06-01 14:51:05 [INFO]: epoch 589: training loss 0.0316\n",
      "2025-06-01 14:51:05 [INFO]: epoch 590: training loss 0.0333\n",
      "2025-06-01 14:51:05 [INFO]: epoch 591: training loss 0.0326\n",
      "2025-06-01 14:51:05 [INFO]: epoch 592: training loss 0.0267\n",
      "2025-06-01 14:51:05 [INFO]: epoch 593: training loss 0.0271\n",
      "2025-06-01 14:51:05 [INFO]: epoch 594: training loss 0.0276\n",
      "2025-06-01 14:51:05 [INFO]: epoch 595: training loss 0.0278\n",
      "2025-06-01 14:51:05 [INFO]: epoch 596: training loss 0.0293\n",
      "2025-06-01 14:51:05 [INFO]: epoch 597: training loss 0.0305\n",
      "2025-06-01 14:51:05 [INFO]: epoch 598: training loss 0.0364\n",
      "2025-06-01 14:51:05 [INFO]: epoch 599: training loss 0.0266\n",
      "2025-06-01 14:51:05 [INFO]: epoch 600: training loss 0.0286\n",
      "2025-06-01 14:51:05 [INFO]: epoch 601: training loss 0.0259\n",
      "2025-06-01 14:51:05 [INFO]: epoch 602: training loss 0.0258\n",
      "2025-06-01 14:51:05 [INFO]: epoch 603: training loss 0.0238\n",
      "2025-06-01 14:51:05 [INFO]: epoch 604: training loss 0.0305\n",
      "2025-06-01 14:51:05 [INFO]: epoch 605: training loss 0.0280\n",
      "2025-06-01 14:51:05 [INFO]: epoch 606: training loss 0.0267\n",
      "2025-06-01 14:51:05 [INFO]: epoch 607: training loss 0.0255\n",
      "2025-06-01 14:51:05 [INFO]: epoch 608: training loss 0.0235\n",
      "2025-06-01 14:51:05 [INFO]: epoch 609: training loss 0.0234\n",
      "2025-06-01 14:51:05 [INFO]: epoch 610: training loss 0.0281\n",
      "2025-06-01 14:51:05 [INFO]: epoch 611: training loss 0.0286\n",
      "2025-06-01 14:51:05 [INFO]: epoch 612: training loss 0.0266\n",
      "2025-06-01 14:51:05 [INFO]: epoch 613: training loss 0.0240\n",
      "2025-06-01 14:51:05 [INFO]: epoch 614: training loss 0.0255\n",
      "2025-06-01 14:51:05 [INFO]: epoch 615: training loss 0.0255\n",
      "2025-06-01 14:51:05 [INFO]: epoch 616: training loss 0.0247\n",
      "2025-06-01 14:51:05 [INFO]: epoch 617: training loss 0.0267\n",
      "2025-06-01 14:51:05 [INFO]: epoch 618: training loss 0.0213\n",
      "2025-06-01 14:51:05 [INFO]: epoch 619: training loss 0.0326\n",
      "2025-06-01 14:51:05 [INFO]: epoch 620: training loss 0.0239\n",
      "2025-06-01 14:51:05 [INFO]: epoch 621: training loss 0.0263\n",
      "2025-06-01 14:51:05 [INFO]: epoch 622: training loss 0.0261\n",
      "2025-06-01 14:51:05 [INFO]: epoch 623: training loss 0.0277\n",
      "2025-06-01 14:51:05 [INFO]: epoch 624: training loss 0.0265\n",
      "2025-06-01 14:51:06 [INFO]: epoch 625: training loss 0.0215\n",
      "2025-06-01 14:51:06 [INFO]: epoch 626: training loss 0.0232\n",
      "2025-06-01 14:51:06 [INFO]: epoch 627: training loss 0.0313\n",
      "2025-06-01 14:51:06 [INFO]: epoch 628: training loss 0.0263\n",
      "2025-06-01 14:51:06 [INFO]: epoch 629: training loss 0.0215\n",
      "2025-06-01 14:51:06 [INFO]: epoch 630: training loss 0.0260\n",
      "2025-06-01 14:51:06 [INFO]: epoch 631: training loss 0.0280\n",
      "2025-06-01 14:51:06 [INFO]: epoch 632: training loss 0.0247\n",
      "2025-06-01 14:51:06 [INFO]: epoch 633: training loss 0.0266\n",
      "2025-06-01 14:51:06 [INFO]: epoch 634: training loss 0.0257\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-01 14:51:06 [INFO]: epoch 635: training loss 0.0288\n",
      "2025-06-01 14:51:06 [INFO]: epoch 636: training loss 0.0286\n",
      "2025-06-01 14:51:06 [INFO]: epoch 637: training loss 0.0286\n",
      "2025-06-01 14:51:06 [INFO]: epoch 638: training loss 0.0272\n",
      "2025-06-01 14:51:06 [INFO]: epoch 639: training loss 0.0255\n",
      "2025-06-01 14:51:06 [INFO]: epoch 640: training loss 0.0312\n",
      "2025-06-01 14:51:06 [INFO]: epoch 641: training loss 0.0276\n",
      "2025-06-01 14:51:06 [INFO]: epoch 642: training loss 0.0218\n",
      "2025-06-01 14:51:06 [INFO]: epoch 643: training loss 0.0271\n",
      "2025-06-01 14:51:06 [INFO]: epoch 644: training loss 0.0253\n",
      "2025-06-01 14:51:06 [INFO]: epoch 645: training loss 0.0269\n",
      "2025-06-01 14:51:06 [INFO]: epoch 646: training loss 0.0281\n",
      "2025-06-01 14:51:06 [INFO]: epoch 647: training loss 0.0299\n",
      "2025-06-01 14:51:06 [INFO]: epoch 648: training loss 0.0263\n",
      "2025-06-01 14:51:06 [INFO]: epoch 649: training loss 0.0274\n",
      "2025-06-01 14:51:06 [INFO]: epoch 650: training loss 0.0256\n",
      "2025-06-01 14:51:06 [INFO]: epoch 651: training loss 0.0253\n",
      "2025-06-01 14:51:06 [INFO]: epoch 652: training loss 0.0300\n",
      "2025-06-01 14:51:06 [INFO]: epoch 653: training loss 0.0275\n",
      "2025-06-01 14:51:06 [INFO]: epoch 654: training loss 0.0294\n",
      "2025-06-01 14:51:06 [INFO]: epoch 655: training loss 0.0294\n",
      "2025-06-01 14:51:06 [INFO]: epoch 656: training loss 0.0354\n",
      "2025-06-01 14:51:06 [INFO]: epoch 657: training loss 0.0320\n",
      "2025-06-01 14:51:06 [INFO]: epoch 658: training loss 0.0240\n",
      "2025-06-01 14:51:06 [INFO]: epoch 659: training loss 0.0303\n",
      "2025-06-01 14:51:06 [INFO]: epoch 660: training loss 0.0371\n",
      "2025-06-01 14:51:06 [INFO]: epoch 661: training loss 0.0281\n",
      "2025-06-01 14:51:06 [INFO]: epoch 662: training loss 0.0204\n",
      "2025-06-01 14:51:06 [INFO]: epoch 663: training loss 0.0373\n",
      "2025-06-01 14:51:06 [INFO]: epoch 664: training loss 0.0308\n",
      "2025-06-01 14:51:06 [INFO]: epoch 665: training loss 0.0264\n",
      "2025-06-01 14:51:06 [INFO]: epoch 666: training loss 0.0300\n",
      "2025-06-01 14:51:06 [INFO]: epoch 667: training loss 0.0336\n",
      "2025-06-01 14:51:06 [INFO]: epoch 668: training loss 0.0274\n",
      "2025-06-01 14:51:06 [INFO]: epoch 669: training loss 0.0218\n",
      "2025-06-01 14:51:06 [INFO]: epoch 670: training loss 0.0302\n",
      "2025-06-01 14:51:06 [INFO]: epoch 671: training loss 0.0263\n",
      "2025-06-01 14:51:06 [INFO]: epoch 672: training loss 0.0256\n",
      "2025-06-01 14:51:06 [INFO]: epoch 673: training loss 0.0260\n",
      "2025-06-01 14:51:06 [INFO]: epoch 674: training loss 0.0310\n",
      "2025-06-01 14:51:06 [INFO]: epoch 675: training loss 0.0272\n",
      "2025-06-01 14:51:06 [INFO]: epoch 676: training loss 0.0276\n",
      "2025-06-01 14:51:06 [INFO]: epoch 677: training loss 0.0339\n",
      "2025-06-01 14:51:06 [INFO]: epoch 678: training loss 0.0289\n",
      "2025-06-01 14:51:06 [INFO]: epoch 679: training loss 0.0268\n",
      "2025-06-01 14:51:06 [INFO]: epoch 680: training loss 0.0294\n",
      "2025-06-01 14:51:06 [INFO]: epoch 681: training loss 0.0257\n",
      "2025-06-01 14:51:06 [INFO]: epoch 682: training loss 0.0240\n",
      "2025-06-01 14:51:06 [INFO]: epoch 683: training loss 0.0307\n",
      "2025-06-01 14:51:06 [INFO]: epoch 684: training loss 0.0283\n",
      "2025-06-01 14:51:06 [INFO]: epoch 685: training loss 0.0258\n",
      "2025-06-01 14:51:06 [INFO]: epoch 686: training loss 0.0252\n",
      "2025-06-01 14:51:06 [INFO]: epoch 687: training loss 0.0292\n",
      "2025-06-01 14:51:06 [INFO]: epoch 688: training loss 0.0309\n",
      "2025-06-01 14:51:06 [INFO]: epoch 689: training loss 0.0262\n",
      "2025-06-01 14:51:06 [INFO]: epoch 690: training loss 0.0245\n",
      "2025-06-01 14:51:06 [INFO]: epoch 691: training loss 0.0253\n",
      "2025-06-01 14:51:06 [INFO]: epoch 692: training loss 0.0285\n",
      "2025-06-01 14:51:06 [INFO]: epoch 693: training loss 0.0317\n",
      "2025-06-01 14:51:06 [INFO]: epoch 694: training loss 0.0263\n",
      "2025-06-01 14:51:06 [INFO]: epoch 695: training loss 0.0246\n",
      "2025-06-01 14:51:06 [INFO]: epoch 696: training loss 0.0302\n",
      "2025-06-01 14:51:06 [INFO]: epoch 697: training loss 0.0301\n",
      "2025-06-01 14:51:06 [INFO]: epoch 698: training loss 0.0234\n",
      "2025-06-01 14:51:06 [INFO]: epoch 699: training loss 0.0242\n",
      "2025-06-01 14:51:06 [INFO]: epoch 700: training loss 0.0285\n",
      "2025-06-01 14:51:07 [INFO]: epoch 701: training loss 0.0229\n",
      "2025-06-01 14:51:07 [INFO]: epoch 702: training loss 0.0255\n",
      "2025-06-01 14:51:07 [INFO]: epoch 703: training loss 0.0271\n",
      "2025-06-01 14:51:07 [INFO]: epoch 704: training loss 0.0247\n",
      "2025-06-01 14:51:07 [INFO]: epoch 705: training loss 0.0305\n",
      "2025-06-01 14:51:07 [INFO]: epoch 706: training loss 0.0244\n",
      "2025-06-01 14:51:07 [INFO]: epoch 707: training loss 0.0289\n",
      "2025-06-01 14:51:07 [INFO]: epoch 708: training loss 0.0283\n",
      "2025-06-01 14:51:07 [INFO]: epoch 709: training loss 0.0254\n",
      "2025-06-01 14:51:07 [INFO]: epoch 710: training loss 0.0250\n",
      "2025-06-01 14:51:07 [INFO]: epoch 711: training loss 0.0281\n",
      "2025-06-01 14:51:07 [INFO]: epoch 712: training loss 0.0254\n",
      "2025-06-01 14:51:07 [INFO]: epoch 713: training loss 0.0226\n",
      "2025-06-01 14:51:07 [INFO]: epoch 714: training loss 0.0241\n",
      "2025-06-01 14:51:07 [INFO]: epoch 715: training loss 0.0261\n",
      "2025-06-01 14:51:07 [INFO]: epoch 716: training loss 0.0243\n",
      "2025-06-01 14:51:07 [INFO]: epoch 717: training loss 0.0223\n",
      "2025-06-01 14:51:07 [INFO]: epoch 718: training loss 0.0319\n",
      "2025-06-01 14:51:07 [INFO]: epoch 719: training loss 0.0219\n",
      "2025-06-01 14:51:07 [INFO]: epoch 720: training loss 0.0211\n",
      "2025-06-01 14:51:07 [INFO]: epoch 721: training loss 0.0254\n",
      "2025-06-01 14:51:07 [INFO]: epoch 722: training loss 0.0262\n",
      "2025-06-01 14:51:07 [INFO]: epoch 723: training loss 0.0250\n",
      "2025-06-01 14:51:07 [INFO]: epoch 724: training loss 0.0251\n",
      "2025-06-01 14:51:07 [INFO]: epoch 725: training loss 0.0229\n",
      "2025-06-01 14:51:07 [INFO]: epoch 726: training loss 0.0196\n",
      "2025-06-01 14:51:07 [INFO]: epoch 727: training loss 0.0296\n",
      "2025-06-01 14:51:07 [INFO]: epoch 728: training loss 0.0247\n",
      "2025-06-01 14:51:07 [INFO]: epoch 729: training loss 0.0212\n",
      "2025-06-01 14:51:07 [INFO]: epoch 730: training loss 0.0251\n",
      "2025-06-01 14:51:07 [INFO]: epoch 731: training loss 0.0293\n",
      "2025-06-01 14:51:07 [INFO]: epoch 732: training loss 0.0265\n",
      "2025-06-01 14:51:07 [INFO]: epoch 733: training loss 0.0222\n",
      "2025-06-01 14:51:07 [INFO]: epoch 734: training loss 0.0244\n",
      "2025-06-01 14:51:07 [INFO]: epoch 735: training loss 0.0281\n",
      "2025-06-01 14:51:07 [INFO]: epoch 736: training loss 0.0247\n",
      "2025-06-01 14:51:07 [INFO]: epoch 737: training loss 0.0264\n",
      "2025-06-01 14:51:07 [INFO]: epoch 738: training loss 0.0267\n",
      "2025-06-01 14:51:07 [INFO]: epoch 739: training loss 0.0277\n",
      "2025-06-01 14:51:07 [INFO]: epoch 740: training loss 0.0271\n",
      "2025-06-01 14:51:07 [INFO]: epoch 741: training loss 0.0280\n",
      "2025-06-01 14:51:07 [INFO]: epoch 742: training loss 0.0241\n",
      "2025-06-01 14:51:07 [INFO]: epoch 743: training loss 0.0223\n",
      "2025-06-01 14:51:07 [INFO]: epoch 744: training loss 0.0233\n",
      "2025-06-01 14:51:07 [INFO]: epoch 745: training loss 0.0250\n",
      "2025-06-01 14:51:07 [INFO]: epoch 746: training loss 0.0240\n",
      "2025-06-01 14:51:07 [INFO]: epoch 747: training loss 0.0243\n",
      "2025-06-01 14:51:07 [INFO]: epoch 748: training loss 0.0225\n",
      "2025-06-01 14:51:07 [INFO]: epoch 749: training loss 0.0237\n",
      "2025-06-01 14:51:07 [INFO]: epoch 750: training loss 0.0241\n",
      "2025-06-01 14:51:07 [INFO]: epoch 751: training loss 0.0268\n",
      "2025-06-01 14:51:07 [INFO]: epoch 752: training loss 0.0254\n",
      "2025-06-01 14:51:07 [INFO]: epoch 753: training loss 0.0208\n",
      "2025-06-01 14:51:07 [INFO]: epoch 754: training loss 0.0249\n",
      "2025-06-01 14:51:07 [INFO]: epoch 755: training loss 0.0236\n",
      "2025-06-01 14:51:07 [INFO]: epoch 756: training loss 0.0223\n",
      "2025-06-01 14:51:07 [INFO]: epoch 757: training loss 0.0245\n",
      "2025-06-01 14:51:07 [INFO]: epoch 758: training loss 0.0252\n",
      "2025-06-01 14:51:07 [INFO]: epoch 759: training loss 0.0245\n",
      "2025-06-01 14:51:07 [INFO]: epoch 760: training loss 0.0214\n",
      "2025-06-01 14:51:07 [INFO]: epoch 761: training loss 0.0239\n",
      "2025-06-01 14:51:07 [INFO]: epoch 762: training loss 0.0251\n",
      "2025-06-01 14:51:07 [INFO]: epoch 763: training loss 0.0238\n",
      "2025-06-01 14:51:07 [INFO]: epoch 764: training loss 0.0219\n",
      "2025-06-01 14:51:07 [INFO]: epoch 765: training loss 0.0262\n",
      "2025-06-01 14:51:07 [INFO]: epoch 766: training loss 0.0251\n",
      "2025-06-01 14:51:07 [INFO]: epoch 767: training loss 0.0255\n",
      "2025-06-01 14:51:07 [INFO]: epoch 768: training loss 0.0204\n",
      "2025-06-01 14:51:07 [INFO]: epoch 769: training loss 0.0249\n",
      "2025-06-01 14:51:07 [INFO]: epoch 770: training loss 0.0231\n",
      "2025-06-01 14:51:07 [INFO]: epoch 771: training loss 0.0204\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-01 14:51:07 [INFO]: epoch 772: training loss 0.0273\n",
      "2025-06-01 14:51:07 [INFO]: epoch 773: training loss 0.0251\n",
      "2025-06-01 14:51:07 [INFO]: epoch 774: training loss 0.0203\n",
      "2025-06-01 14:51:07 [INFO]: epoch 775: training loss 0.0239\n",
      "2025-06-01 14:51:07 [INFO]: epoch 776: training loss 0.0234\n",
      "2025-06-01 14:51:08 [INFO]: epoch 777: training loss 0.0272\n",
      "2025-06-01 14:51:08 [INFO]: epoch 778: training loss 0.0235\n",
      "2025-06-01 14:51:08 [INFO]: epoch 779: training loss 0.0228\n",
      "2025-06-01 14:51:08 [INFO]: epoch 780: training loss 0.0270\n",
      "2025-06-01 14:51:08 [INFO]: epoch 781: training loss 0.0227\n",
      "2025-06-01 14:51:08 [INFO]: epoch 782: training loss 0.0204\n",
      "2025-06-01 14:51:08 [INFO]: epoch 783: training loss 0.0209\n",
      "2025-06-01 14:51:08 [INFO]: epoch 784: training loss 0.0225\n",
      "2025-06-01 14:51:08 [INFO]: epoch 785: training loss 0.0235\n",
      "2025-06-01 14:51:08 [INFO]: epoch 786: training loss 0.0199\n",
      "2025-06-01 14:51:08 [INFO]: epoch 787: training loss 0.0251\n",
      "2025-06-01 14:51:08 [INFO]: epoch 788: training loss 0.0224\n",
      "2025-06-01 14:51:08 [INFO]: epoch 789: training loss 0.0223\n",
      "2025-06-01 14:51:08 [INFO]: epoch 790: training loss 0.0209\n",
      "2025-06-01 14:51:08 [INFO]: epoch 791: training loss 0.0189\n",
      "2025-06-01 14:51:08 [INFO]: epoch 792: training loss 0.0187\n",
      "2025-06-01 14:51:08 [INFO]: epoch 793: training loss 0.0215\n",
      "2025-06-01 14:51:08 [INFO]: epoch 794: training loss 0.0222\n",
      "2025-06-01 14:51:08 [INFO]: epoch 795: training loss 0.0240\n",
      "2025-06-01 14:51:08 [INFO]: epoch 796: training loss 0.0282\n",
      "2025-06-01 14:51:08 [INFO]: epoch 797: training loss 0.0259\n",
      "2025-06-01 14:51:08 [INFO]: epoch 798: training loss 0.0229\n",
      "2025-06-01 14:51:08 [INFO]: epoch 799: training loss 0.0263\n",
      "2025-06-01 14:51:08 [INFO]: Finished training.\n",
      "2025-06-01 14:51:08 [WARNING]: 🚨DeprecationWarning: The method impute is deprecated. Please use `predict` instead.\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 6/6 [01:03<00:00, 10.60s/it]\n"
     ]
    }
   ],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "imputation_list_1 = []\n",
    "imputation_list_2 = []\n",
    "\n",
    "for model in range(1,3):\n",
    "    \n",
    "    if model ==1:\n",
    "        epoch = 400\n",
    "    else:\n",
    "        epoch = 800\n",
    "        \n",
    "    for i in tqdm( range(6) ):             # 运行5次计算标准差\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        saits = SAITS(n_steps=30, n_features=6, n_layers=2, d_model=128, d_inner=64, n_heads=4, d_k=64, d_v=64, dropout=0.1, batch_size=10, epochs = epoch)\n",
    "\n",
    "        dataset = {\"X\": X_missed_1}\n",
    "\n",
    "        saits.fit(dataset)               \n",
    "        imputation = saits.impute(dataset)  \n",
    "        \n",
    "        if model ==1:\n",
    "            imputation_list_1.append(imputation)\n",
    "        else:\n",
    "            imputation_list_2.append(imputation)\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1fdf364f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "6\n",
      "(6, 30, 6)\n",
      "(6, 30, 6)\n"
     ]
    }
   ],
   "source": [
    "print(len(imputation_list_1))\n",
    "print(len(imputation_list_2))\n",
    "\n",
    "imp_1 = np.array(imputation_list_1).reshape((6,30,6)) \n",
    "print(imp_1.shape)\n",
    "\n",
    "imp_2 = np.array(imputation_list_2).reshape((6,30,6)) \n",
    "print(imp_2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "18465988",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30, 6)\n",
      "(30, 6)\n"
     ]
    }
   ],
   "source": [
    "res_1 = torch.from_numpy(imp_1)\n",
    "\n",
    "res1_005 = torch.quantile( res_1, 0.05, dim=0 ).cpu().numpy()\n",
    "res1_05 = torch.quantile( res_1, 0.5, dim=0 ).cpu().numpy()\n",
    "res1_095 = torch.quantile( res_1, 0.95, dim=0 ).cpu().numpy()\n",
    "res1_x = torch.quantile( res_1, 0.6, dim=0 ).cpu().numpy()\n",
    "print(res1_005.shape)\n",
    "\n",
    "\n",
    "\n",
    "res_2 = torch.from_numpy(imp_2)\n",
    "\n",
    "res2_005 = torch.quantile( res_2, 0.05, dim=0 ).cpu().numpy()\n",
    "res2_05 = torch.quantile( res_2, 0.5, dim=0 ).cpu().numpy()\n",
    "res2_095 = torch.quantile( res_2, 0.95, dim=0 ).cpu().numpy()\n",
    "res2_x = torch.quantile( res_2, 0.6, dim=0 ).cpu().numpy()\n",
    "print(res2_005.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "114541b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1716077ee50>]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 700x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(figsize=(7,4))\n",
    "\n",
    "X_missed = X_missed_1\n",
    "\n",
    "plt.subplot(2,2,1)\n",
    "plt.plot(res2_x[:,3], c='k', label='imp', alpha=0.8) \n",
    "plt.fill_between( range(0,30), res1_005[:,3], res1_095[:,3], color='#ff7f0e', alpha=0.4)  # alpha是透明程度\n",
    "plt.fill_between( range(0,30), res2_005[:,3], res2_095[:,3], color='#ff7f0e', alpha=0.8)  \n",
    "plt.plot(X_missed[0][:,3].cpu().numpy(), c='#1f77b4', marker='o',alpha=1, markersize='4') # 观测值的蓝色把之前的值覆盖了 生成区域的值还是之前的0.5分位数的\n",
    "\n",
    "\n",
    "# plt.savefig('分位数',bbox_inches='tight',dpi=500)\n",
    "# plt.savefig('分位数.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "cb728b10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 30, 6])\n",
      "tensor([0.7323,    nan,    nan, 0.7433, 0.7333,    nan], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "'第二个样本'\n",
    "zero_shot = np.array(norm_data[80:110])    # 40-70     seed1 100-130\n",
    "zero_shot = torch.from_numpy(zero_shot).float().cuda()\n",
    "zero_shot = zero_shot.unsqueeze(dim=0)\n",
    "print(zero_shot.shape)\n",
    "\n",
    "\n",
    "\n",
    "np.random.seed(5)\n",
    "# torch.manual_seed(4)\n",
    "# torch.cuda.manual_seed_all(4)\n",
    "\n",
    "X_ori, X_missed_2, missing_mask, indicating_mask = mcar(zero_shot, 0.3, np.nan)\n",
    "print(X_missed[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4910faff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                            | 0/6 [00:00<?, ?it/s]2025-06-01 14:51:08 [INFO]: No given device, using default device: cuda\n",
      "2025-06-01 14:51:08 [WARNING]: saving_path not given. Model files and tensorboard file will not be saved.\n",
      "2025-06-01 14:51:08 [INFO]: Model initialized successfully with the number of trainable parameters: 597,780\n",
      "2025-06-01 14:51:08 [INFO]: epoch 0: training loss 1.1037\n",
      "2025-06-01 14:51:08 [INFO]: epoch 1: training loss 0.6266\n",
      "2025-06-01 14:51:08 [INFO]: epoch 2: training loss 0.6093\n",
      "2025-06-01 14:51:08 [INFO]: epoch 3: training loss 0.5992\n",
      "2025-06-01 14:51:08 [INFO]: epoch 4: training loss 0.5469\n",
      "2025-06-01 14:51:08 [INFO]: epoch 5: training loss 0.3870\n",
      "2025-06-01 14:51:08 [INFO]: epoch 6: training loss 0.3811\n",
      "2025-06-01 14:51:08 [INFO]: epoch 7: training loss 0.3733\n",
      "2025-06-01 14:51:08 [INFO]: epoch 8: training loss 0.3973\n",
      "2025-06-01 14:51:08 [INFO]: epoch 9: training loss 0.3993\n",
      "2025-06-01 14:51:08 [INFO]: epoch 10: training loss 0.3930\n",
      "2025-06-01 14:51:08 [INFO]: epoch 11: training loss 0.3625\n",
      "2025-06-01 14:51:08 [INFO]: epoch 12: training loss 0.3525\n",
      "2025-06-01 14:51:08 [INFO]: epoch 13: training loss 0.3541\n",
      "2025-06-01 14:51:08 [INFO]: epoch 14: training loss 0.3672\n",
      "2025-06-01 14:51:08 [INFO]: epoch 15: training loss 0.3244\n",
      "2025-06-01 14:51:08 [INFO]: epoch 16: training loss 0.3171\n",
      "2025-06-01 14:51:08 [INFO]: epoch 17: training loss 0.2931\n",
      "2025-06-01 14:51:08 [INFO]: epoch 18: training loss 0.2993\n",
      "2025-06-01 14:51:08 [INFO]: epoch 19: training loss 0.3368\n",
      "2025-06-01 14:51:08 [INFO]: epoch 20: training loss 0.3442\n",
      "2025-06-01 14:51:08 [INFO]: epoch 21: training loss 0.2889\n",
      "2025-06-01 14:51:08 [INFO]: epoch 22: training loss 0.2749\n",
      "2025-06-01 14:51:08 [INFO]: epoch 23: training loss 0.2845\n",
      "2025-06-01 14:51:08 [INFO]: epoch 24: training loss 0.2683\n",
      "2025-06-01 14:51:08 [INFO]: epoch 25: training loss 0.2831\n",
      "2025-06-01 14:51:08 [INFO]: epoch 26: training loss 0.2713\n",
      "2025-06-01 14:51:09 [INFO]: epoch 27: training loss 0.2757\n",
      "2025-06-01 14:51:09 [INFO]: epoch 28: training loss 0.2637\n",
      "2025-06-01 14:51:09 [INFO]: epoch 29: training loss 0.2735\n",
      "2025-06-01 14:51:09 [INFO]: epoch 30: training loss 0.2895\n",
      "2025-06-01 14:51:09 [INFO]: epoch 31: training loss 0.2715\n",
      "2025-06-01 14:51:09 [INFO]: epoch 32: training loss 0.2813\n",
      "2025-06-01 14:51:09 [INFO]: epoch 33: training loss 0.2425\n",
      "2025-06-01 14:51:09 [INFO]: epoch 34: training loss 0.2765\n",
      "2025-06-01 14:51:09 [INFO]: epoch 35: training loss 0.2674\n",
      "2025-06-01 14:51:09 [INFO]: epoch 36: training loss 0.2650\n",
      "2025-06-01 14:51:09 [INFO]: epoch 37: training loss 0.2833\n",
      "2025-06-01 14:51:09 [INFO]: epoch 38: training loss 0.2515\n",
      "2025-06-01 14:51:09 [INFO]: epoch 39: training loss 0.2668\n",
      "2025-06-01 14:51:09 [INFO]: epoch 40: training loss 0.2620\n",
      "2025-06-01 14:51:09 [INFO]: epoch 41: training loss 0.2580\n",
      "2025-06-01 14:51:09 [INFO]: epoch 42: training loss 0.2448\n",
      "2025-06-01 14:51:09 [INFO]: epoch 43: training loss 0.2679\n",
      "2025-06-01 14:51:09 [INFO]: epoch 44: training loss 0.2556\n",
      "2025-06-01 14:51:09 [INFO]: epoch 45: training loss 0.2451\n",
      "2025-06-01 14:51:09 [INFO]: epoch 46: training loss 0.2472\n",
      "2025-06-01 14:51:09 [INFO]: epoch 47: training loss 0.2592\n",
      "2025-06-01 14:51:09 [INFO]: epoch 48: training loss 0.2707\n",
      "2025-06-01 14:51:09 [INFO]: epoch 49: training loss 0.2409\n",
      "2025-06-01 14:51:09 [INFO]: epoch 50: training loss 0.2403\n",
      "2025-06-01 14:51:09 [INFO]: epoch 51: training loss 0.2410\n",
      "2025-06-01 14:51:09 [INFO]: epoch 52: training loss 0.2485\n",
      "2025-06-01 14:51:09 [INFO]: epoch 53: training loss 0.2660\n",
      "2025-06-01 14:51:09 [INFO]: epoch 54: training loss 0.2449\n",
      "2025-06-01 14:51:09 [INFO]: epoch 55: training loss 0.2571\n",
      "2025-06-01 14:51:09 [INFO]: epoch 56: training loss 0.2314\n",
      "2025-06-01 14:51:09 [INFO]: epoch 57: training loss 0.2259\n",
      "2025-06-01 14:51:09 [INFO]: epoch 58: training loss 0.2314\n",
      "2025-06-01 14:51:09 [INFO]: epoch 59: training loss 0.2536\n",
      "2025-06-01 14:51:09 [INFO]: epoch 60: training loss 0.2567\n",
      "2025-06-01 14:51:09 [INFO]: epoch 61: training loss 0.2691\n",
      "2025-06-01 14:51:09 [INFO]: epoch 62: training loss 0.2375\n",
      "2025-06-01 14:51:09 [INFO]: epoch 63: training loss 0.2414\n",
      "2025-06-01 14:51:09 [INFO]: epoch 64: training loss 0.2385\n",
      "2025-06-01 14:51:09 [INFO]: epoch 65: training loss 0.2224\n",
      "2025-06-01 14:51:09 [INFO]: epoch 66: training loss 0.2549\n",
      "2025-06-01 14:51:09 [INFO]: epoch 67: training loss 0.2518\n",
      "2025-06-01 14:51:09 [INFO]: epoch 68: training loss 0.2261\n",
      "2025-06-01 14:51:09 [INFO]: epoch 69: training loss 0.2455\n",
      "2025-06-01 14:51:09 [INFO]: epoch 70: training loss 0.2328\n",
      "2025-06-01 14:51:09 [INFO]: epoch 71: training loss 0.2449\n",
      "2025-06-01 14:51:09 [INFO]: epoch 72: training loss 0.2269\n",
      "2025-06-01 14:51:09 [INFO]: epoch 73: training loss 0.2239\n",
      "2025-06-01 14:51:09 [INFO]: epoch 74: training loss 0.2182\n",
      "2025-06-01 14:51:09 [INFO]: epoch 75: training loss 0.2267\n",
      "2025-06-01 14:51:09 [INFO]: epoch 76: training loss 0.2331\n",
      "2025-06-01 14:51:09 [INFO]: epoch 77: training loss 0.2278\n",
      "2025-06-01 14:51:09 [INFO]: epoch 78: training loss 0.2153\n",
      "2025-06-01 14:51:09 [INFO]: epoch 79: training loss 0.2224\n",
      "2025-06-01 14:51:09 [INFO]: epoch 80: training loss 0.2275\n",
      "2025-06-01 14:51:09 [INFO]: epoch 81: training loss 0.2224\n",
      "2025-06-01 14:51:09 [INFO]: epoch 82: training loss 0.2360\n",
      "2025-06-01 14:51:09 [INFO]: epoch 83: training loss 0.2138\n",
      "2025-06-01 14:51:09 [INFO]: epoch 84: training loss 0.2262\n",
      "2025-06-01 14:51:09 [INFO]: epoch 85: training loss 0.2021\n",
      "2025-06-01 14:51:09 [INFO]: epoch 86: training loss 0.2099\n",
      "2025-06-01 14:51:09 [INFO]: epoch 87: training loss 0.2103\n",
      "2025-06-01 14:51:09 [INFO]: epoch 88: training loss 0.2224\n",
      "2025-06-01 14:51:09 [INFO]: epoch 89: training loss 0.2027\n",
      "2025-06-01 14:51:09 [INFO]: epoch 90: training loss 0.2051\n",
      "2025-06-01 14:51:09 [INFO]: epoch 91: training loss 0.2090\n",
      "2025-06-01 14:51:09 [INFO]: epoch 92: training loss 0.1866\n",
      "2025-06-01 14:51:09 [INFO]: epoch 93: training loss 0.2209\n",
      "2025-06-01 14:51:09 [INFO]: epoch 94: training loss 0.2247\n",
      "2025-06-01 14:51:09 [INFO]: epoch 95: training loss 0.2152\n",
      "2025-06-01 14:51:09 [INFO]: epoch 96: training loss 0.2087\n",
      "2025-06-01 14:51:09 [INFO]: epoch 97: training loss 0.2312\n",
      "2025-06-01 14:51:09 [INFO]: epoch 98: training loss 0.1963\n",
      "2025-06-01 14:51:09 [INFO]: epoch 99: training loss 0.2106\n",
      "2025-06-01 14:51:09 [INFO]: epoch 100: training loss 0.2006\n",
      "2025-06-01 14:51:10 [INFO]: epoch 101: training loss 0.1936\n",
      "2025-06-01 14:51:10 [INFO]: epoch 102: training loss 0.2001\n",
      "2025-06-01 14:51:10 [INFO]: epoch 103: training loss 0.1984\n",
      "2025-06-01 14:51:10 [INFO]: epoch 104: training loss 0.2018\n",
      "2025-06-01 14:51:10 [INFO]: epoch 105: training loss 0.2331\n",
      "2025-06-01 14:51:10 [INFO]: epoch 106: training loss 0.2245\n",
      "2025-06-01 14:51:10 [INFO]: epoch 107: training loss 0.1902\n",
      "2025-06-01 14:51:10 [INFO]: epoch 108: training loss 0.2074\n",
      "2025-06-01 14:51:10 [INFO]: epoch 109: training loss 0.1897\n",
      "2025-06-01 14:51:10 [INFO]: epoch 110: training loss 0.1956\n",
      "2025-06-01 14:51:10 [INFO]: epoch 111: training loss 0.2161\n",
      "2025-06-01 14:51:10 [INFO]: epoch 112: training loss 0.2059\n",
      "2025-06-01 14:51:10 [INFO]: epoch 113: training loss 0.2082\n",
      "2025-06-01 14:51:10 [INFO]: epoch 114: training loss 0.1939\n",
      "2025-06-01 14:51:10 [INFO]: epoch 115: training loss 0.2110\n",
      "2025-06-01 14:51:10 [INFO]: epoch 116: training loss 0.1952\n",
      "2025-06-01 14:51:10 [INFO]: epoch 117: training loss 0.2009\n",
      "2025-06-01 14:51:10 [INFO]: epoch 118: training loss 0.1819\n",
      "2025-06-01 14:51:10 [INFO]: epoch 119: training loss 0.1758\n",
      "2025-06-01 14:51:10 [INFO]: epoch 120: training loss 0.1931\n",
      "2025-06-01 14:51:10 [INFO]: epoch 121: training loss 0.1826\n",
      "2025-06-01 14:51:10 [INFO]: epoch 122: training loss 0.1964\n",
      "2025-06-01 14:51:10 [INFO]: epoch 123: training loss 0.1859\n",
      "2025-06-01 14:51:10 [INFO]: epoch 124: training loss 0.1983\n",
      "2025-06-01 14:51:10 [INFO]: epoch 125: training loss 0.2010\n",
      "2025-06-01 14:51:10 [INFO]: epoch 126: training loss 0.1968\n",
      "2025-06-01 14:51:10 [INFO]: epoch 127: training loss 0.1776\n",
      "2025-06-01 14:51:10 [INFO]: epoch 128: training loss 0.1747\n",
      "2025-06-01 14:51:10 [INFO]: epoch 129: training loss 0.1935\n",
      "2025-06-01 14:51:10 [INFO]: epoch 130: training loss 0.2033\n",
      "2025-06-01 14:51:10 [INFO]: epoch 131: training loss 0.1827\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-01 14:51:10 [INFO]: epoch 132: training loss 0.1783\n",
      "2025-06-01 14:51:10 [INFO]: epoch 133: training loss 0.1850\n",
      "2025-06-01 14:51:10 [INFO]: epoch 134: training loss 0.1729\n",
      "2025-06-01 14:51:10 [INFO]: epoch 135: training loss 0.1815\n",
      "2025-06-01 14:51:10 [INFO]: epoch 136: training loss 0.1772\n",
      "2025-06-01 14:51:10 [INFO]: epoch 137: training loss 0.1807\n",
      "2025-06-01 14:51:10 [INFO]: epoch 138: training loss 0.2026\n",
      "2025-06-01 14:51:10 [INFO]: epoch 139: training loss 0.1898\n",
      "2025-06-01 14:51:10 [INFO]: epoch 140: training loss 0.1861\n",
      "2025-06-01 14:51:10 [INFO]: epoch 141: training loss 0.1819\n",
      "2025-06-01 14:51:10 [INFO]: epoch 142: training loss 0.1750\n",
      "2025-06-01 14:51:10 [INFO]: epoch 143: training loss 0.1616\n",
      "2025-06-01 14:51:10 [INFO]: epoch 144: training loss 0.1675\n",
      "2025-06-01 14:51:10 [INFO]: epoch 145: training loss 0.1692\n",
      "2025-06-01 14:51:10 [INFO]: epoch 146: training loss 0.1808\n",
      "2025-06-01 14:51:10 [INFO]: epoch 147: training loss 0.1752\n",
      "2025-06-01 14:51:10 [INFO]: epoch 148: training loss 0.1681\n",
      "2025-06-01 14:51:10 [INFO]: epoch 149: training loss 0.1679\n",
      "2025-06-01 14:51:10 [INFO]: epoch 150: training loss 0.1563\n",
      "2025-06-01 14:51:10 [INFO]: epoch 151: training loss 0.1659\n",
      "2025-06-01 14:51:10 [INFO]: epoch 152: training loss 0.1733\n",
      "2025-06-01 14:51:10 [INFO]: epoch 153: training loss 0.1805\n",
      "2025-06-01 14:51:10 [INFO]: epoch 154: training loss 0.1664\n",
      "2025-06-01 14:51:10 [INFO]: epoch 155: training loss 0.1601\n",
      "2025-06-01 14:51:10 [INFO]: epoch 156: training loss 0.1637\n",
      "2025-06-01 14:51:10 [INFO]: epoch 157: training loss 0.1465\n",
      "2025-06-01 14:51:10 [INFO]: epoch 158: training loss 0.1718\n",
      "2025-06-01 14:51:10 [INFO]: epoch 159: training loss 0.1654\n",
      "2025-06-01 14:51:10 [INFO]: epoch 160: training loss 0.1626\n",
      "2025-06-01 14:51:10 [INFO]: epoch 161: training loss 0.1745\n",
      "2025-06-01 14:51:10 [INFO]: epoch 162: training loss 0.1770\n",
      "2025-06-01 14:51:10 [INFO]: epoch 163: training loss 0.1668\n",
      "2025-06-01 14:51:10 [INFO]: epoch 164: training loss 0.1565\n",
      "2025-06-01 14:51:10 [INFO]: epoch 165: training loss 0.1581\n",
      "2025-06-01 14:51:10 [INFO]: epoch 166: training loss 0.1675\n",
      "2025-06-01 14:51:10 [INFO]: epoch 167: training loss 0.1566\n",
      "2025-06-01 14:51:10 [INFO]: epoch 168: training loss 0.1784\n",
      "2025-06-01 14:51:10 [INFO]: epoch 169: training loss 0.1678\n",
      "2025-06-01 14:51:10 [INFO]: epoch 170: training loss 0.1510\n",
      "2025-06-01 14:51:10 [INFO]: epoch 171: training loss 0.1701\n",
      "2025-06-01 14:51:10 [INFO]: epoch 172: training loss 0.1694\n",
      "2025-06-01 14:51:10 [INFO]: epoch 173: training loss 0.1781\n",
      "2025-06-01 14:51:10 [INFO]: epoch 174: training loss 0.1527\n",
      "2025-06-01 14:51:10 [INFO]: epoch 175: training loss 0.1550\n",
      "2025-06-01 14:51:10 [INFO]: epoch 176: training loss 0.1640\n",
      "2025-06-01 14:51:11 [INFO]: epoch 177: training loss 0.1634\n",
      "2025-06-01 14:51:11 [INFO]: epoch 178: training loss 0.1647\n",
      "2025-06-01 14:51:11 [INFO]: epoch 179: training loss 0.1418\n",
      "2025-06-01 14:51:11 [INFO]: epoch 180: training loss 0.1514\n",
      "2025-06-01 14:51:11 [INFO]: epoch 181: training loss 0.1528\n",
      "2025-06-01 14:51:11 [INFO]: epoch 182: training loss 0.1554\n",
      "2025-06-01 14:51:11 [INFO]: epoch 183: training loss 0.1461\n",
      "2025-06-01 14:51:11 [INFO]: epoch 184: training loss 0.1452\n",
      "2025-06-01 14:51:11 [INFO]: epoch 185: training loss 0.1612\n",
      "2025-06-01 14:51:11 [INFO]: epoch 186: training loss 0.1527\n",
      "2025-06-01 14:51:11 [INFO]: epoch 187: training loss 0.1456\n",
      "2025-06-01 14:51:11 [INFO]: epoch 188: training loss 0.1443\n",
      "2025-06-01 14:51:11 [INFO]: epoch 189: training loss 0.1377\n",
      "2025-06-01 14:51:11 [INFO]: epoch 190: training loss 0.1377\n",
      "2025-06-01 14:51:11 [INFO]: epoch 191: training loss 0.1493\n",
      "2025-06-01 14:51:11 [INFO]: epoch 192: training loss 0.1532\n",
      "2025-06-01 14:51:11 [INFO]: epoch 193: training loss 0.1427\n",
      "2025-06-01 14:51:11 [INFO]: epoch 194: training loss 0.1387\n",
      "2025-06-01 14:51:11 [INFO]: epoch 195: training loss 0.1414\n",
      "2025-06-01 14:51:11 [INFO]: epoch 196: training loss 0.1489\n",
      "2025-06-01 14:51:11 [INFO]: epoch 197: training loss 0.1446\n",
      "2025-06-01 14:51:11 [INFO]: epoch 198: training loss 0.1471\n",
      "2025-06-01 14:51:11 [INFO]: epoch 199: training loss 0.1393\n",
      "2025-06-01 14:51:11 [INFO]: epoch 200: training loss 0.1421\n",
      "2025-06-01 14:51:11 [INFO]: epoch 201: training loss 0.1390\n",
      "2025-06-01 14:51:11 [INFO]: epoch 202: training loss 0.1396\n",
      "2025-06-01 14:51:11 [INFO]: epoch 203: training loss 0.1267\n",
      "2025-06-01 14:51:11 [INFO]: epoch 204: training loss 0.1428\n",
      "2025-06-01 14:51:11 [INFO]: epoch 205: training loss 0.1442\n",
      "2025-06-01 14:51:11 [INFO]: epoch 206: training loss 0.1408\n",
      "2025-06-01 14:51:11 [INFO]: epoch 207: training loss 0.1377\n",
      "2025-06-01 14:51:11 [INFO]: epoch 208: training loss 0.1399\n",
      "2025-06-01 14:51:11 [INFO]: epoch 209: training loss 0.1329\n",
      "2025-06-01 14:51:11 [INFO]: epoch 210: training loss 0.1344\n",
      "2025-06-01 14:51:11 [INFO]: epoch 211: training loss 0.1272\n",
      "2025-06-01 14:51:11 [INFO]: epoch 212: training loss 0.1420\n",
      "2025-06-01 14:51:11 [INFO]: epoch 213: training loss 0.1314\n",
      "2025-06-01 14:51:11 [INFO]: epoch 214: training loss 0.1439\n",
      "2025-06-01 14:51:11 [INFO]: epoch 215: training loss 0.1365\n",
      "2025-06-01 14:51:11 [INFO]: epoch 216: training loss 0.1253\n",
      "2025-06-01 14:51:11 [INFO]: epoch 217: training loss 0.1311\n",
      "2025-06-01 14:51:11 [INFO]: epoch 218: training loss 0.1329\n",
      "2025-06-01 14:51:11 [INFO]: epoch 219: training loss 0.1314\n",
      "2025-06-01 14:51:11 [INFO]: epoch 220: training loss 0.1339\n",
      "2025-06-01 14:51:11 [INFO]: epoch 221: training loss 0.1317\n",
      "2025-06-01 14:51:11 [INFO]: epoch 222: training loss 0.1233\n",
      "2025-06-01 14:51:11 [INFO]: epoch 223: training loss 0.1168\n",
      "2025-06-01 14:51:11 [INFO]: epoch 224: training loss 0.1368\n",
      "2025-06-01 14:51:11 [INFO]: epoch 225: training loss 0.1337\n",
      "2025-06-01 14:51:11 [INFO]: epoch 226: training loss 0.1252\n",
      "2025-06-01 14:51:11 [INFO]: epoch 227: training loss 0.1231\n",
      "2025-06-01 14:51:11 [INFO]: epoch 228: training loss 0.1265\n",
      "2025-06-01 14:51:11 [INFO]: epoch 229: training loss 0.1205\n",
      "2025-06-01 14:51:11 [INFO]: epoch 230: training loss 0.1378\n",
      "2025-06-01 14:51:11 [INFO]: epoch 231: training loss 0.1210\n",
      "2025-06-01 14:51:11 [INFO]: epoch 232: training loss 0.1113\n",
      "2025-06-01 14:51:11 [INFO]: epoch 233: training loss 0.1355\n",
      "2025-06-01 14:51:11 [INFO]: epoch 234: training loss 0.1272\n",
      "2025-06-01 14:51:11 [INFO]: epoch 235: training loss 0.1340\n",
      "2025-06-01 14:51:11 [INFO]: epoch 236: training loss 0.1272\n",
      "2025-06-01 14:51:11 [INFO]: epoch 237: training loss 0.1276\n",
      "2025-06-01 14:51:11 [INFO]: epoch 238: training loss 0.1308\n",
      "2025-06-01 14:51:11 [INFO]: epoch 239: training loss 0.1336\n",
      "2025-06-01 14:51:11 [INFO]: epoch 240: training loss 0.1460\n",
      "2025-06-01 14:51:11 [INFO]: epoch 241: training loss 0.1238\n",
      "2025-06-01 14:51:11 [INFO]: epoch 242: training loss 0.1268\n",
      "2025-06-01 14:51:11 [INFO]: epoch 243: training loss 0.1176\n",
      "2025-06-01 14:51:11 [INFO]: epoch 244: training loss 0.1194\n",
      "2025-06-01 14:51:11 [INFO]: epoch 245: training loss 0.1276\n",
      "2025-06-01 14:51:11 [INFO]: epoch 246: training loss 0.1256\n",
      "2025-06-01 14:51:11 [INFO]: epoch 247: training loss 0.1116\n",
      "2025-06-01 14:51:11 [INFO]: epoch 248: training loss 0.1121\n",
      "2025-06-01 14:51:11 [INFO]: epoch 249: training loss 0.1120\n",
      "2025-06-01 14:51:11 [INFO]: epoch 250: training loss 0.1119\n",
      "2025-06-01 14:51:11 [INFO]: epoch 251: training loss 0.1158\n",
      "2025-06-01 14:51:11 [INFO]: epoch 252: training loss 0.1077\n",
      "2025-06-01 14:51:12 [INFO]: epoch 253: training loss 0.1126\n",
      "2025-06-01 14:51:12 [INFO]: epoch 254: training loss 0.1007\n",
      "2025-06-01 14:51:12 [INFO]: epoch 255: training loss 0.0996\n",
      "2025-06-01 14:51:12 [INFO]: epoch 256: training loss 0.1094\n",
      "2025-06-01 14:51:12 [INFO]: epoch 257: training loss 0.1102\n",
      "2025-06-01 14:51:12 [INFO]: epoch 258: training loss 0.1135\n",
      "2025-06-01 14:51:12 [INFO]: epoch 259: training loss 0.1179\n",
      "2025-06-01 14:51:12 [INFO]: epoch 260: training loss 0.1131\n",
      "2025-06-01 14:51:12 [INFO]: epoch 261: training loss 0.1127\n",
      "2025-06-01 14:51:12 [INFO]: epoch 262: training loss 0.1159\n",
      "2025-06-01 14:51:12 [INFO]: epoch 263: training loss 0.1095\n",
      "2025-06-01 14:51:12 [INFO]: epoch 264: training loss 0.1049\n",
      "2025-06-01 14:51:12 [INFO]: epoch 265: training loss 0.1096\n",
      "2025-06-01 14:51:12 [INFO]: epoch 266: training loss 0.1142\n",
      "2025-06-01 14:51:12 [INFO]: epoch 267: training loss 0.1141\n",
      "2025-06-01 14:51:12 [INFO]: epoch 268: training loss 0.1088\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-01 14:51:12 [INFO]: epoch 269: training loss 0.1095\n",
      "2025-06-01 14:51:12 [INFO]: epoch 270: training loss 0.1061\n",
      "2025-06-01 14:51:12 [INFO]: epoch 271: training loss 0.1136\n",
      "2025-06-01 14:51:12 [INFO]: epoch 272: training loss 0.1045\n",
      "2025-06-01 14:51:12 [INFO]: epoch 273: training loss 0.1107\n",
      "2025-06-01 14:51:12 [INFO]: epoch 274: training loss 0.1089\n",
      "2025-06-01 14:51:12 [INFO]: epoch 275: training loss 0.1060\n",
      "2025-06-01 14:51:12 [INFO]: epoch 276: training loss 0.1103\n",
      "2025-06-01 14:51:12 [INFO]: epoch 277: training loss 0.0995\n",
      "2025-06-01 14:51:12 [INFO]: epoch 278: training loss 0.0952\n",
      "2025-06-01 14:51:12 [INFO]: epoch 279: training loss 0.1052\n",
      "2025-06-01 14:51:12 [INFO]: epoch 280: training loss 0.1052\n",
      "2025-06-01 14:51:12 [INFO]: epoch 281: training loss 0.1129\n",
      "2025-06-01 14:51:12 [INFO]: epoch 282: training loss 0.1055\n",
      "2025-06-01 14:51:12 [INFO]: epoch 283: training loss 0.1098\n",
      "2025-06-01 14:51:12 [INFO]: epoch 284: training loss 0.1093\n",
      "2025-06-01 14:51:12 [INFO]: epoch 285: training loss 0.0966\n",
      "2025-06-01 14:51:12 [INFO]: epoch 286: training loss 0.1035\n",
      "2025-06-01 14:51:12 [INFO]: epoch 287: training loss 0.1020\n",
      "2025-06-01 14:51:12 [INFO]: epoch 288: training loss 0.1127\n",
      "2025-06-01 14:51:12 [INFO]: epoch 289: training loss 0.1064\n",
      "2025-06-01 14:51:12 [INFO]: epoch 290: training loss 0.1082\n",
      "2025-06-01 14:51:12 [INFO]: epoch 291: training loss 0.1104\n",
      "2025-06-01 14:51:12 [INFO]: epoch 292: training loss 0.1035\n",
      "2025-06-01 14:51:12 [INFO]: epoch 293: training loss 0.1067\n",
      "2025-06-01 14:51:12 [INFO]: epoch 294: training loss 0.1082\n",
      "2025-06-01 14:51:12 [INFO]: epoch 295: training loss 0.1097\n",
      "2025-06-01 14:51:12 [INFO]: epoch 296: training loss 0.1019\n",
      "2025-06-01 14:51:12 [INFO]: epoch 297: training loss 0.1056\n",
      "2025-06-01 14:51:12 [INFO]: epoch 298: training loss 0.0974\n",
      "2025-06-01 14:51:12 [INFO]: epoch 299: training loss 0.1081\n",
      "2025-06-01 14:51:12 [INFO]: epoch 300: training loss 0.1063\n",
      "2025-06-01 14:51:12 [INFO]: epoch 301: training loss 0.1022\n",
      "2025-06-01 14:51:12 [INFO]: epoch 302: training loss 0.1168\n",
      "2025-06-01 14:51:12 [INFO]: epoch 303: training loss 0.1048\n",
      "2025-06-01 14:51:12 [INFO]: epoch 304: training loss 0.1073\n",
      "2025-06-01 14:51:12 [INFO]: epoch 305: training loss 0.1064\n",
      "2025-06-01 14:51:12 [INFO]: epoch 306: training loss 0.1228\n",
      "2025-06-01 14:51:12 [INFO]: epoch 307: training loss 0.1228\n",
      "2025-06-01 14:51:12 [INFO]: epoch 308: training loss 0.1077\n",
      "2025-06-01 14:51:12 [INFO]: epoch 309: training loss 0.0985\n",
      "2025-06-01 14:51:12 [INFO]: epoch 310: training loss 0.0990\n",
      "2025-06-01 14:51:12 [INFO]: epoch 311: training loss 0.1126\n",
      "2025-06-01 14:51:12 [INFO]: epoch 312: training loss 0.1103\n",
      "2025-06-01 14:51:12 [INFO]: epoch 313: training loss 0.0886\n",
      "2025-06-01 14:51:12 [INFO]: epoch 314: training loss 0.1081\n",
      "2025-06-01 14:51:12 [INFO]: epoch 315: training loss 0.1017\n",
      "2025-06-01 14:51:12 [INFO]: epoch 316: training loss 0.1031\n",
      "2025-06-01 14:51:12 [INFO]: epoch 317: training loss 0.0874\n",
      "2025-06-01 14:51:12 [INFO]: epoch 318: training loss 0.0979\n",
      "2025-06-01 14:51:12 [INFO]: epoch 319: training loss 0.1041\n",
      "2025-06-01 14:51:12 [INFO]: epoch 320: training loss 0.0998\n",
      "2025-06-01 14:51:12 [INFO]: epoch 321: training loss 0.0949\n",
      "2025-06-01 14:51:12 [INFO]: epoch 322: training loss 0.1005\n",
      "2025-06-01 14:51:12 [INFO]: epoch 323: training loss 0.1004\n",
      "2025-06-01 14:51:12 [INFO]: epoch 324: training loss 0.1050\n",
      "2025-06-01 14:51:12 [INFO]: epoch 325: training loss 0.0964\n",
      "2025-06-01 14:51:12 [INFO]: epoch 326: training loss 0.0998\n",
      "2025-06-01 14:51:12 [INFO]: epoch 327: training loss 0.0985\n",
      "2025-06-01 14:51:12 [INFO]: epoch 328: training loss 0.0958\n",
      "2025-06-01 14:51:13 [INFO]: epoch 329: training loss 0.0897\n",
      "2025-06-01 14:51:13 [INFO]: epoch 330: training loss 0.0957\n",
      "2025-06-01 14:51:13 [INFO]: epoch 331: training loss 0.1066\n",
      "2025-06-01 14:51:13 [INFO]: epoch 332: training loss 0.0948\n",
      "2025-06-01 14:51:13 [INFO]: epoch 333: training loss 0.0901\n",
      "2025-06-01 14:51:13 [INFO]: epoch 334: training loss 0.0887\n",
      "2025-06-01 14:51:13 [INFO]: epoch 335: training loss 0.1004\n",
      "2025-06-01 14:51:13 [INFO]: epoch 336: training loss 0.0908\n",
      "2025-06-01 14:51:13 [INFO]: epoch 337: training loss 0.0835\n",
      "2025-06-01 14:51:13 [INFO]: epoch 338: training loss 0.0872\n",
      "2025-06-01 14:51:13 [INFO]: epoch 339: training loss 0.0813\n",
      "2025-06-01 14:51:13 [INFO]: epoch 340: training loss 0.0946\n",
      "2025-06-01 14:51:13 [INFO]: epoch 341: training loss 0.0973\n",
      "2025-06-01 14:51:13 [INFO]: epoch 342: training loss 0.0822\n",
      "2025-06-01 14:51:13 [INFO]: epoch 343: training loss 0.0864\n",
      "2025-06-01 14:51:13 [INFO]: epoch 344: training loss 0.0902\n",
      "2025-06-01 14:51:13 [INFO]: epoch 345: training loss 0.0916\n",
      "2025-06-01 14:51:13 [INFO]: epoch 346: training loss 0.0961\n",
      "2025-06-01 14:51:13 [INFO]: epoch 347: training loss 0.0928\n",
      "2025-06-01 14:51:13 [INFO]: epoch 348: training loss 0.0898\n",
      "2025-06-01 14:51:13 [INFO]: epoch 349: training loss 0.0918\n",
      "2025-06-01 14:51:13 [INFO]: epoch 350: training loss 0.0906\n",
      "2025-06-01 14:51:13 [INFO]: epoch 351: training loss 0.0968\n",
      "2025-06-01 14:51:13 [INFO]: epoch 352: training loss 0.0926\n",
      "2025-06-01 14:51:13 [INFO]: epoch 353: training loss 0.0908\n",
      "2025-06-01 14:51:13 [INFO]: epoch 354: training loss 0.0861\n",
      "2025-06-01 14:51:13 [INFO]: epoch 355: training loss 0.0958\n",
      "2025-06-01 14:51:13 [INFO]: epoch 356: training loss 0.0813\n",
      "2025-06-01 14:51:13 [INFO]: epoch 357: training loss 0.0937\n",
      "2025-06-01 14:51:13 [INFO]: epoch 358: training loss 0.0880\n",
      "2025-06-01 14:51:13 [INFO]: epoch 359: training loss 0.0902\n",
      "2025-06-01 14:51:13 [INFO]: epoch 360: training loss 0.0924\n",
      "2025-06-01 14:51:13 [INFO]: epoch 361: training loss 0.0959\n",
      "2025-06-01 14:51:13 [INFO]: epoch 362: training loss 0.0844\n",
      "2025-06-01 14:51:13 [INFO]: epoch 363: training loss 0.0937\n",
      "2025-06-01 14:51:13 [INFO]: epoch 364: training loss 0.0906\n",
      "2025-06-01 14:51:13 [INFO]: epoch 365: training loss 0.0867\n",
      "2025-06-01 14:51:13 [INFO]: epoch 366: training loss 0.0782\n",
      "2025-06-01 14:51:13 [INFO]: epoch 367: training loss 0.0835\n",
      "2025-06-01 14:51:13 [INFO]: epoch 368: training loss 0.0955\n",
      "2025-06-01 14:51:13 [INFO]: epoch 369: training loss 0.0888\n",
      "2025-06-01 14:51:13 [INFO]: epoch 370: training loss 0.0966\n",
      "2025-06-01 14:51:13 [INFO]: epoch 371: training loss 0.1014\n",
      "2025-06-01 14:51:13 [INFO]: epoch 372: training loss 0.0936\n",
      "2025-06-01 14:51:13 [INFO]: epoch 373: training loss 0.0761\n",
      "2025-06-01 14:51:13 [INFO]: epoch 374: training loss 0.0891\n",
      "2025-06-01 14:51:13 [INFO]: epoch 375: training loss 0.0938\n",
      "2025-06-01 14:51:13 [INFO]: epoch 376: training loss 0.0839\n",
      "2025-06-01 14:51:13 [INFO]: epoch 377: training loss 0.0846\n",
      "2025-06-01 14:51:13 [INFO]: epoch 378: training loss 0.0893\n",
      "2025-06-01 14:51:13 [INFO]: epoch 379: training loss 0.0898\n",
      "2025-06-01 14:51:13 [INFO]: epoch 380: training loss 0.0814\n",
      "2025-06-01 14:51:13 [INFO]: epoch 381: training loss 0.0859\n",
      "2025-06-01 14:51:13 [INFO]: epoch 382: training loss 0.0898\n",
      "2025-06-01 14:51:13 [INFO]: epoch 383: training loss 0.0943\n",
      "2025-06-01 14:51:13 [INFO]: epoch 384: training loss 0.0846\n",
      "2025-06-01 14:51:13 [INFO]: epoch 385: training loss 0.0764\n",
      "2025-06-01 14:51:13 [INFO]: epoch 386: training loss 0.0915\n",
      "2025-06-01 14:51:13 [INFO]: epoch 387: training loss 0.0915\n",
      "2025-06-01 14:51:13 [INFO]: epoch 388: training loss 0.0852\n",
      "2025-06-01 14:51:13 [INFO]: epoch 389: training loss 0.0751\n",
      "2025-06-01 14:51:13 [INFO]: epoch 390: training loss 0.0770\n",
      "2025-06-01 14:51:13 [INFO]: epoch 391: training loss 0.0894\n",
      "2025-06-01 14:51:13 [INFO]: epoch 392: training loss 0.0809\n",
      "2025-06-01 14:51:13 [INFO]: epoch 393: training loss 0.0833\n",
      "2025-06-01 14:51:13 [INFO]: epoch 394: training loss 0.0750\n",
      "2025-06-01 14:51:13 [INFO]: epoch 395: training loss 0.0860\n",
      "2025-06-01 14:51:13 [INFO]: epoch 396: training loss 0.0929\n",
      "2025-06-01 14:51:13 [INFO]: epoch 397: training loss 0.0789\n",
      "2025-06-01 14:51:13 [INFO]: epoch 398: training loss 0.0770\n",
      "2025-06-01 14:51:13 [INFO]: epoch 399: training loss 0.0755\n",
      "2025-06-01 14:51:13 [INFO]: Finished training.\n",
      "2025-06-01 14:51:13 [WARNING]: 🚨DeprecationWarning: The method impute is deprecated. Please use `predict` instead.\n",
      " 17%|██████████████                                                                      | 1/6 [00:05<00:26,  5.38s/it]2025-06-01 14:51:13 [INFO]: No given device, using default device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-01 14:51:13 [WARNING]: saving_path not given. Model files and tensorboard file will not be saved.\n",
      "2025-06-01 14:51:14 [INFO]: Model initialized successfully with the number of trainable parameters: 597,780\n",
      "2025-06-01 14:51:14 [INFO]: epoch 0: training loss 1.7108\n",
      "2025-06-01 14:51:14 [INFO]: epoch 1: training loss 0.7522\n",
      "2025-06-01 14:51:14 [INFO]: epoch 2: training loss 0.6578\n",
      "2025-06-01 14:51:14 [INFO]: epoch 3: training loss 0.7736\n",
      "2025-06-01 14:51:14 [INFO]: epoch 4: training loss 0.7193\n",
      "2025-06-01 14:51:14 [INFO]: epoch 5: training loss 0.6577\n",
      "2025-06-01 14:51:14 [INFO]: epoch 6: training loss 0.5775\n",
      "2025-06-01 14:51:14 [INFO]: epoch 7: training loss 0.5950\n",
      "2025-06-01 14:51:14 [INFO]: epoch 8: training loss 0.5640\n",
      "2025-06-01 14:51:14 [INFO]: epoch 9: training loss 0.5089\n",
      "2025-06-01 14:51:14 [INFO]: epoch 10: training loss 0.4919\n",
      "2025-06-01 14:51:14 [INFO]: epoch 11: training loss 0.4960\n",
      "2025-06-01 14:51:14 [INFO]: epoch 12: training loss 0.5254\n",
      "2025-06-01 14:51:14 [INFO]: epoch 13: training loss 0.5197\n",
      "2025-06-01 14:51:14 [INFO]: epoch 14: training loss 0.4712\n",
      "2025-06-01 14:51:14 [INFO]: epoch 15: training loss 0.4709\n",
      "2025-06-01 14:51:14 [INFO]: epoch 16: training loss 0.4554\n",
      "2025-06-01 14:51:14 [INFO]: epoch 17: training loss 0.4613\n",
      "2025-06-01 14:51:14 [INFO]: epoch 18: training loss 0.4032\n",
      "2025-06-01 14:51:14 [INFO]: epoch 19: training loss 0.4046\n",
      "2025-06-01 14:51:14 [INFO]: epoch 20: training loss 0.4769\n",
      "2025-06-01 14:51:14 [INFO]: epoch 21: training loss 0.3877\n",
      "2025-06-01 14:51:14 [INFO]: epoch 22: training loss 0.3972\n",
      "2025-06-01 14:51:14 [INFO]: epoch 23: training loss 0.4055\n",
      "2025-06-01 14:51:14 [INFO]: epoch 24: training loss 0.4034\n",
      "2025-06-01 14:51:14 [INFO]: epoch 25: training loss 0.3745\n",
      "2025-06-01 14:51:14 [INFO]: epoch 26: training loss 0.3915\n",
      "2025-06-01 14:51:14 [INFO]: epoch 27: training loss 0.3902\n",
      "2025-06-01 14:51:14 [INFO]: epoch 28: training loss 0.3855\n",
      "2025-06-01 14:51:14 [INFO]: epoch 29: training loss 0.3675\n",
      "2025-06-01 14:51:14 [INFO]: epoch 30: training loss 0.3683\n",
      "2025-06-01 14:51:14 [INFO]: epoch 31: training loss 0.3556\n",
      "2025-06-01 14:51:14 [INFO]: epoch 32: training loss 0.3695\n",
      "2025-06-01 14:51:14 [INFO]: epoch 33: training loss 0.3615\n",
      "2025-06-01 14:51:14 [INFO]: epoch 34: training loss 0.3767\n",
      "2025-06-01 14:51:14 [INFO]: epoch 35: training loss 0.3413\n",
      "2025-06-01 14:51:14 [INFO]: epoch 36: training loss 0.3542\n",
      "2025-06-01 14:51:14 [INFO]: epoch 37: training loss 0.3385\n",
      "2025-06-01 14:51:14 [INFO]: epoch 38: training loss 0.3311\n",
      "2025-06-01 14:51:14 [INFO]: epoch 39: training loss 0.3385\n",
      "2025-06-01 14:51:14 [INFO]: epoch 40: training loss 0.3101\n",
      "2025-06-01 14:51:14 [INFO]: epoch 41: training loss 0.2734\n",
      "2025-06-01 14:51:14 [INFO]: epoch 42: training loss 0.2967\n",
      "2025-06-01 14:51:14 [INFO]: epoch 43: training loss 0.3154\n",
      "2025-06-01 14:51:14 [INFO]: epoch 44: training loss 0.2887\n",
      "2025-06-01 14:51:14 [INFO]: epoch 45: training loss 0.2724\n",
      "2025-06-01 14:51:14 [INFO]: epoch 46: training loss 0.3029\n",
      "2025-06-01 14:51:14 [INFO]: epoch 47: training loss 0.2698\n",
      "2025-06-01 14:51:14 [INFO]: epoch 48: training loss 0.2808\n",
      "2025-06-01 14:51:14 [INFO]: epoch 49: training loss 0.2301\n",
      "2025-06-01 14:51:14 [INFO]: epoch 50: training loss 0.2951\n",
      "2025-06-01 14:51:14 [INFO]: epoch 51: training loss 0.2635\n",
      "2025-06-01 14:51:14 [INFO]: epoch 52: training loss 0.2519\n",
      "2025-06-01 14:51:14 [INFO]: epoch 53: training loss 0.2567\n",
      "2025-06-01 14:51:14 [INFO]: epoch 54: training loss 0.2592\n",
      "2025-06-01 14:51:14 [INFO]: epoch 55: training loss 0.2304\n",
      "2025-06-01 14:51:14 [INFO]: epoch 56: training loss 0.2799\n",
      "2025-06-01 14:51:14 [INFO]: epoch 57: training loss 0.2600\n",
      "2025-06-01 14:51:14 [INFO]: epoch 58: training loss 0.2442\n",
      "2025-06-01 14:51:14 [INFO]: epoch 59: training loss 0.2514\n",
      "2025-06-01 14:51:14 [INFO]: epoch 60: training loss 0.2411\n",
      "2025-06-01 14:51:14 [INFO]: epoch 61: training loss 0.2570\n",
      "2025-06-01 14:51:14 [INFO]: epoch 62: training loss 0.2195\n",
      "2025-06-01 14:51:14 [INFO]: epoch 63: training loss 0.2245\n",
      "2025-06-01 14:51:14 [INFO]: epoch 64: training loss 0.2476\n",
      "2025-06-01 14:51:14 [INFO]: epoch 65: training loss 0.2390\n",
      "2025-06-01 14:51:14 [INFO]: epoch 66: training loss 0.2083\n",
      "2025-06-01 14:51:14 [INFO]: epoch 67: training loss 0.2057\n",
      "2025-06-01 14:51:14 [INFO]: epoch 68: training loss 0.2148\n",
      "2025-06-01 14:51:14 [INFO]: epoch 69: training loss 0.2383\n",
      "2025-06-01 14:51:14 [INFO]: epoch 70: training loss 0.2477\n",
      "2025-06-01 14:51:14 [INFO]: epoch 71: training loss 0.1853\n",
      "2025-06-01 14:51:14 [INFO]: epoch 72: training loss 0.2070\n",
      "2025-06-01 14:51:14 [INFO]: epoch 73: training loss 0.2378\n",
      "2025-06-01 14:51:14 [INFO]: epoch 74: training loss 0.2364\n",
      "2025-06-01 14:51:14 [INFO]: epoch 75: training loss 0.1940\n",
      "2025-06-01 14:51:15 [INFO]: epoch 76: training loss 0.1825\n",
      "2025-06-01 14:51:15 [INFO]: epoch 77: training loss 0.2004\n",
      "2025-06-01 14:51:15 [INFO]: epoch 78: training loss 0.2170\n",
      "2025-06-01 14:51:15 [INFO]: epoch 79: training loss 0.1980\n",
      "2025-06-01 14:51:15 [INFO]: epoch 80: training loss 0.1862\n",
      "2025-06-01 14:51:15 [INFO]: epoch 81: training loss 0.2007\n",
      "2025-06-01 14:51:15 [INFO]: epoch 82: training loss 0.2014\n",
      "2025-06-01 14:51:15 [INFO]: epoch 83: training loss 0.1830\n",
      "2025-06-01 14:51:15 [INFO]: epoch 84: training loss 0.1710\n",
      "2025-06-01 14:51:15 [INFO]: epoch 85: training loss 0.1949\n",
      "2025-06-01 14:51:15 [INFO]: epoch 86: training loss 0.2034\n",
      "2025-06-01 14:51:15 [INFO]: epoch 87: training loss 0.1653\n",
      "2025-06-01 14:51:15 [INFO]: epoch 88: training loss 0.1606\n",
      "2025-06-01 14:51:15 [INFO]: epoch 89: training loss 0.1626\n",
      "2025-06-01 14:51:15 [INFO]: epoch 90: training loss 0.1554\n",
      "2025-06-01 14:51:15 [INFO]: epoch 91: training loss 0.1803\n",
      "2025-06-01 14:51:15 [INFO]: epoch 92: training loss 0.1640\n",
      "2025-06-01 14:51:15 [INFO]: epoch 93: training loss 0.1602\n",
      "2025-06-01 14:51:15 [INFO]: epoch 94: training loss 0.1672\n",
      "2025-06-01 14:51:15 [INFO]: epoch 95: training loss 0.1537\n",
      "2025-06-01 14:51:15 [INFO]: epoch 96: training loss 0.1585\n",
      "2025-06-01 14:51:15 [INFO]: epoch 97: training loss 0.1498\n",
      "2025-06-01 14:51:15 [INFO]: epoch 98: training loss 0.1482\n",
      "2025-06-01 14:51:15 [INFO]: epoch 99: training loss 0.1787\n",
      "2025-06-01 14:51:15 [INFO]: epoch 100: training loss 0.1608\n",
      "2025-06-01 14:51:15 [INFO]: epoch 101: training loss 0.1416\n",
      "2025-06-01 14:51:15 [INFO]: epoch 102: training loss 0.1624\n",
      "2025-06-01 14:51:15 [INFO]: epoch 103: training loss 0.1460\n",
      "2025-06-01 14:51:15 [INFO]: epoch 104: training loss 0.1559\n",
      "2025-06-01 14:51:15 [INFO]: epoch 105: training loss 0.1511\n",
      "2025-06-01 14:51:15 [INFO]: epoch 106: training loss 0.1364\n",
      "2025-06-01 14:51:15 [INFO]: epoch 107: training loss 0.1550\n",
      "2025-06-01 14:51:15 [INFO]: epoch 108: training loss 0.1398\n",
      "2025-06-01 14:51:15 [INFO]: epoch 109: training loss 0.1389\n",
      "2025-06-01 14:51:15 [INFO]: epoch 110: training loss 0.1491\n",
      "2025-06-01 14:51:15 [INFO]: epoch 111: training loss 0.1427\n",
      "2025-06-01 14:51:15 [INFO]: epoch 112: training loss 0.1323\n",
      "2025-06-01 14:51:15 [INFO]: epoch 113: training loss 0.1377\n",
      "2025-06-01 14:51:15 [INFO]: epoch 114: training loss 0.1317\n",
      "2025-06-01 14:51:15 [INFO]: epoch 115: training loss 0.1429\n",
      "2025-06-01 14:51:15 [INFO]: epoch 116: training loss 0.1211\n",
      "2025-06-01 14:51:15 [INFO]: epoch 117: training loss 0.1243\n",
      "2025-06-01 14:51:15 [INFO]: epoch 118: training loss 0.1267\n",
      "2025-06-01 14:51:15 [INFO]: epoch 119: training loss 0.1335\n",
      "2025-06-01 14:51:15 [INFO]: epoch 120: training loss 0.1341\n",
      "2025-06-01 14:51:15 [INFO]: epoch 121: training loss 0.1229\n",
      "2025-06-01 14:51:15 [INFO]: epoch 122: training loss 0.1218\n",
      "2025-06-01 14:51:15 [INFO]: epoch 123: training loss 0.1248\n",
      "2025-06-01 14:51:15 [INFO]: epoch 124: training loss 0.1314\n",
      "2025-06-01 14:51:15 [INFO]: epoch 125: training loss 0.1312\n",
      "2025-06-01 14:51:15 [INFO]: epoch 126: training loss 0.1104\n",
      "2025-06-01 14:51:15 [INFO]: epoch 127: training loss 0.1168\n",
      "2025-06-01 14:51:15 [INFO]: epoch 128: training loss 0.1346\n",
      "2025-06-01 14:51:15 [INFO]: epoch 129: training loss 0.1261\n",
      "2025-06-01 14:51:15 [INFO]: epoch 130: training loss 0.1314\n",
      "2025-06-01 14:51:15 [INFO]: epoch 131: training loss 0.1130\n",
      "2025-06-01 14:51:15 [INFO]: epoch 132: training loss 0.1155\n",
      "2025-06-01 14:51:15 [INFO]: epoch 133: training loss 0.1146\n",
      "2025-06-01 14:51:15 [INFO]: epoch 134: training loss 0.0971\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-01 14:51:15 [INFO]: epoch 135: training loss 0.1075\n",
      "2025-06-01 14:51:15 [INFO]: epoch 136: training loss 0.1099\n",
      "2025-06-01 14:51:15 [INFO]: epoch 137: training loss 0.0967\n",
      "2025-06-01 14:51:15 [INFO]: epoch 138: training loss 0.1104\n",
      "2025-06-01 14:51:15 [INFO]: epoch 139: training loss 0.1099\n",
      "2025-06-01 14:51:15 [INFO]: epoch 140: training loss 0.1116\n",
      "2025-06-01 14:51:15 [INFO]: epoch 141: training loss 0.0954\n",
      "2025-06-01 14:51:15 [INFO]: epoch 142: training loss 0.0889\n",
      "2025-06-01 14:51:15 [INFO]: epoch 143: training loss 0.0942\n",
      "2025-06-01 14:51:15 [INFO]: epoch 144: training loss 0.1030\n",
      "2025-06-01 14:51:15 [INFO]: epoch 145: training loss 0.0915\n",
      "2025-06-01 14:51:15 [INFO]: epoch 146: training loss 0.0939\n",
      "2025-06-01 14:51:15 [INFO]: epoch 147: training loss 0.0943\n",
      "2025-06-01 14:51:15 [INFO]: epoch 148: training loss 0.1130\n",
      "2025-06-01 14:51:15 [INFO]: epoch 149: training loss 0.0958\n",
      "2025-06-01 14:51:15 [INFO]: epoch 150: training loss 0.0942\n",
      "2025-06-01 14:51:15 [INFO]: epoch 151: training loss 0.0806\n",
      "2025-06-01 14:51:16 [INFO]: epoch 152: training loss 0.0819\n",
      "2025-06-01 14:51:16 [INFO]: epoch 153: training loss 0.0851\n",
      "2025-06-01 14:51:16 [INFO]: epoch 154: training loss 0.0804\n",
      "2025-06-01 14:51:16 [INFO]: epoch 155: training loss 0.0973\n",
      "2025-06-01 14:51:16 [INFO]: epoch 156: training loss 0.0823\n",
      "2025-06-01 14:51:16 [INFO]: epoch 157: training loss 0.1067\n",
      "2025-06-01 14:51:16 [INFO]: epoch 158: training loss 0.0859\n",
      "2025-06-01 14:51:16 [INFO]: epoch 159: training loss 0.0872\n",
      "2025-06-01 14:51:16 [INFO]: epoch 160: training loss 0.1053\n",
      "2025-06-01 14:51:16 [INFO]: epoch 161: training loss 0.0954\n",
      "2025-06-01 14:51:16 [INFO]: epoch 162: training loss 0.0874\n",
      "2025-06-01 14:51:16 [INFO]: epoch 163: training loss 0.0967\n",
      "2025-06-01 14:51:16 [INFO]: epoch 164: training loss 0.0881\n",
      "2025-06-01 14:51:16 [INFO]: epoch 165: training loss 0.0772\n",
      "2025-06-01 14:51:16 [INFO]: epoch 166: training loss 0.0670\n",
      "2025-06-01 14:51:16 [INFO]: epoch 167: training loss 0.0693\n",
      "2025-06-01 14:51:16 [INFO]: epoch 168: training loss 0.0794\n",
      "2025-06-01 14:51:16 [INFO]: epoch 169: training loss 0.0886\n",
      "2025-06-01 14:51:16 [INFO]: epoch 170: training loss 0.0883\n",
      "2025-06-01 14:51:16 [INFO]: epoch 171: training loss 0.0916\n",
      "2025-06-01 14:51:16 [INFO]: epoch 172: training loss 0.0829\n",
      "2025-06-01 14:51:16 [INFO]: epoch 173: training loss 0.0710\n",
      "2025-06-01 14:51:16 [INFO]: epoch 174: training loss 0.0756\n",
      "2025-06-01 14:51:16 [INFO]: epoch 175: training loss 0.0733\n",
      "2025-06-01 14:51:16 [INFO]: epoch 176: training loss 0.0819\n",
      "2025-06-01 14:51:16 [INFO]: epoch 177: training loss 0.0797\n",
      "2025-06-01 14:51:16 [INFO]: epoch 178: training loss 0.0684\n",
      "2025-06-01 14:51:16 [INFO]: epoch 179: training loss 0.0658\n",
      "2025-06-01 14:51:16 [INFO]: epoch 180: training loss 0.0611\n",
      "2025-06-01 14:51:16 [INFO]: epoch 181: training loss 0.0660\n",
      "2025-06-01 14:51:16 [INFO]: epoch 182: training loss 0.0678\n",
      "2025-06-01 14:51:16 [INFO]: epoch 183: training loss 0.0679\n",
      "2025-06-01 14:51:16 [INFO]: epoch 184: training loss 0.0684\n",
      "2025-06-01 14:51:16 [INFO]: epoch 185: training loss 0.0648\n",
      "2025-06-01 14:51:16 [INFO]: epoch 186: training loss 0.0530\n",
      "2025-06-01 14:51:16 [INFO]: epoch 187: training loss 0.0656\n",
      "2025-06-01 14:51:16 [INFO]: epoch 188: training loss 0.0621\n",
      "2025-06-01 14:51:16 [INFO]: epoch 189: training loss 0.0616\n",
      "2025-06-01 14:51:16 [INFO]: epoch 190: training loss 0.0654\n",
      "2025-06-01 14:51:16 [INFO]: epoch 191: training loss 0.0603\n",
      "2025-06-01 14:51:16 [INFO]: epoch 192: training loss 0.0761\n",
      "2025-06-01 14:51:16 [INFO]: epoch 193: training loss 0.0560\n",
      "2025-06-01 14:51:16 [INFO]: epoch 194: training loss 0.0507\n",
      "2025-06-01 14:51:16 [INFO]: epoch 195: training loss 0.0702\n",
      "2025-06-01 14:51:16 [INFO]: epoch 196: training loss 0.0756\n",
      "2025-06-01 14:51:16 [INFO]: epoch 197: training loss 0.0585\n",
      "2025-06-01 14:51:16 [INFO]: epoch 198: training loss 0.0625\n",
      "2025-06-01 14:51:16 [INFO]: epoch 199: training loss 0.0596\n",
      "2025-06-01 14:51:16 [INFO]: epoch 200: training loss 0.0659\n",
      "2025-06-01 14:51:16 [INFO]: epoch 201: training loss 0.0620\n",
      "2025-06-01 14:51:16 [INFO]: epoch 202: training loss 0.0728\n",
      "2025-06-01 14:51:16 [INFO]: epoch 203: training loss 0.0560\n",
      "2025-06-01 14:51:16 [INFO]: epoch 204: training loss 0.0689\n",
      "2025-06-01 14:51:16 [INFO]: epoch 205: training loss 0.0683\n",
      "2025-06-01 14:51:16 [INFO]: epoch 206: training loss 0.0594\n",
      "2025-06-01 14:51:16 [INFO]: epoch 207: training loss 0.0642\n",
      "2025-06-01 14:51:16 [INFO]: epoch 208: training loss 0.0505\n",
      "2025-06-01 14:51:16 [INFO]: epoch 209: training loss 0.0603\n",
      "2025-06-01 14:51:16 [INFO]: epoch 210: training loss 0.0570\n",
      "2025-06-01 14:51:16 [INFO]: epoch 211: training loss 0.0568\n",
      "2025-06-01 14:51:16 [INFO]: epoch 212: training loss 0.0524\n",
      "2025-06-01 14:51:16 [INFO]: epoch 213: training loss 0.0484\n",
      "2025-06-01 14:51:16 [INFO]: epoch 214: training loss 0.0542\n",
      "2025-06-01 14:51:16 [INFO]: epoch 215: training loss 0.0557\n",
      "2025-06-01 14:51:16 [INFO]: epoch 216: training loss 0.0519\n",
      "2025-06-01 14:51:16 [INFO]: epoch 217: training loss 0.0481\n",
      "2025-06-01 14:51:16 [INFO]: epoch 218: training loss 0.0556\n",
      "2025-06-01 14:51:16 [INFO]: epoch 219: training loss 0.0564\n",
      "2025-06-01 14:51:16 [INFO]: epoch 220: training loss 0.0571\n",
      "2025-06-01 14:51:16 [INFO]: epoch 221: training loss 0.0564\n",
      "2025-06-01 14:51:16 [INFO]: epoch 222: training loss 0.0575\n",
      "2025-06-01 14:51:16 [INFO]: epoch 223: training loss 0.0618\n",
      "2025-06-01 14:51:16 [INFO]: epoch 224: training loss 0.0539\n",
      "2025-06-01 14:51:16 [INFO]: epoch 225: training loss 0.0459\n",
      "2025-06-01 14:51:16 [INFO]: epoch 226: training loss 0.0527\n",
      "2025-06-01 14:51:16 [INFO]: epoch 227: training loss 0.0550\n",
      "2025-06-01 14:51:17 [INFO]: epoch 228: training loss 0.0548\n",
      "2025-06-01 14:51:17 [INFO]: epoch 229: training loss 0.0560\n",
      "2025-06-01 14:51:17 [INFO]: epoch 230: training loss 0.0515\n",
      "2025-06-01 14:51:17 [INFO]: epoch 231: training loss 0.0543\n",
      "2025-06-01 14:51:17 [INFO]: epoch 232: training loss 0.0407\n",
      "2025-06-01 14:51:17 [INFO]: epoch 233: training loss 0.0572\n",
      "2025-06-01 14:51:17 [INFO]: epoch 234: training loss 0.0485\n",
      "2025-06-01 14:51:17 [INFO]: epoch 235: training loss 0.0515\n",
      "2025-06-01 14:51:17 [INFO]: epoch 236: training loss 0.0462\n",
      "2025-06-01 14:51:17 [INFO]: epoch 237: training loss 0.0563\n",
      "2025-06-01 14:51:17 [INFO]: epoch 238: training loss 0.0473\n",
      "2025-06-01 14:51:17 [INFO]: epoch 239: training loss 0.0470\n",
      "2025-06-01 14:51:17 [INFO]: epoch 240: training loss 0.0497\n",
      "2025-06-01 14:51:17 [INFO]: epoch 241: training loss 0.0484\n",
      "2025-06-01 14:51:17 [INFO]: epoch 242: training loss 0.0525\n",
      "2025-06-01 14:51:17 [INFO]: epoch 243: training loss 0.0450\n",
      "2025-06-01 14:51:17 [INFO]: epoch 244: training loss 0.0603\n",
      "2025-06-01 14:51:17 [INFO]: epoch 245: training loss 0.0723\n",
      "2025-06-01 14:51:17 [INFO]: epoch 246: training loss 0.0584\n",
      "2025-06-01 14:51:17 [INFO]: epoch 247: training loss 0.0413\n",
      "2025-06-01 14:51:17 [INFO]: epoch 248: training loss 0.0651\n",
      "2025-06-01 14:51:17 [INFO]: epoch 249: training loss 0.0572\n",
      "2025-06-01 14:51:17 [INFO]: epoch 250: training loss 0.0459\n",
      "2025-06-01 14:51:17 [INFO]: epoch 251: training loss 0.0487\n",
      "2025-06-01 14:51:17 [INFO]: epoch 252: training loss 0.0500\n",
      "2025-06-01 14:51:17 [INFO]: epoch 253: training loss 0.0474\n",
      "2025-06-01 14:51:17 [INFO]: epoch 254: training loss 0.0493\n",
      "2025-06-01 14:51:17 [INFO]: epoch 255: training loss 0.0521\n",
      "2025-06-01 14:51:17 [INFO]: epoch 256: training loss 0.0500\n",
      "2025-06-01 14:51:17 [INFO]: epoch 257: training loss 0.0548\n",
      "2025-06-01 14:51:17 [INFO]: epoch 258: training loss 0.0439\n",
      "2025-06-01 14:51:17 [INFO]: epoch 259: training loss 0.0489\n",
      "2025-06-01 14:51:17 [INFO]: epoch 260: training loss 0.0498\n",
      "2025-06-01 14:51:17 [INFO]: epoch 261: training loss 0.0424\n",
      "2025-06-01 14:51:17 [INFO]: epoch 262: training loss 0.0560\n",
      "2025-06-01 14:51:17 [INFO]: epoch 263: training loss 0.0520\n",
      "2025-06-01 14:51:17 [INFO]: epoch 264: training loss 0.0445\n",
      "2025-06-01 14:51:17 [INFO]: epoch 265: training loss 0.0443\n",
      "2025-06-01 14:51:17 [INFO]: epoch 266: training loss 0.0452\n",
      "2025-06-01 14:51:17 [INFO]: epoch 267: training loss 0.0457\n",
      "2025-06-01 14:51:17 [INFO]: epoch 268: training loss 0.0476\n",
      "2025-06-01 14:51:17 [INFO]: epoch 269: training loss 0.0429\n",
      "2025-06-01 14:51:17 [INFO]: epoch 270: training loss 0.0467\n",
      "2025-06-01 14:51:17 [INFO]: epoch 271: training loss 0.0409\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-01 14:51:17 [INFO]: epoch 272: training loss 0.0544\n",
      "2025-06-01 14:51:17 [INFO]: epoch 273: training loss 0.0453\n",
      "2025-06-01 14:51:17 [INFO]: epoch 274: training loss 0.0460\n",
      "2025-06-01 14:51:17 [INFO]: epoch 275: training loss 0.0457\n",
      "2025-06-01 14:51:17 [INFO]: epoch 276: training loss 0.0384\n",
      "2025-06-01 14:51:17 [INFO]: epoch 277: training loss 0.0438\n",
      "2025-06-01 14:51:17 [INFO]: epoch 278: training loss 0.0333\n",
      "2025-06-01 14:51:17 [INFO]: epoch 279: training loss 0.0455\n",
      "2025-06-01 14:51:17 [INFO]: epoch 280: training loss 0.0462\n",
      "2025-06-01 14:51:17 [INFO]: epoch 281: training loss 0.0390\n",
      "2025-06-01 14:51:17 [INFO]: epoch 282: training loss 0.0426\n",
      "2025-06-01 14:51:17 [INFO]: epoch 283: training loss 0.0423\n",
      "2025-06-01 14:51:17 [INFO]: epoch 284: training loss 0.0366\n",
      "2025-06-01 14:51:17 [INFO]: epoch 285: training loss 0.0369\n",
      "2025-06-01 14:51:17 [INFO]: epoch 286: training loss 0.0492\n",
      "2025-06-01 14:51:17 [INFO]: epoch 287: training loss 0.0431\n",
      "2025-06-01 14:51:17 [INFO]: epoch 288: training loss 0.0450\n",
      "2025-06-01 14:51:17 [INFO]: epoch 289: training loss 0.0408\n",
      "2025-06-01 14:51:17 [INFO]: epoch 290: training loss 0.0354\n",
      "2025-06-01 14:51:17 [INFO]: epoch 291: training loss 0.0381\n",
      "2025-06-01 14:51:17 [INFO]: epoch 292: training loss 0.0461\n",
      "2025-06-01 14:51:17 [INFO]: epoch 293: training loss 0.0330\n",
      "2025-06-01 14:51:17 [INFO]: epoch 294: training loss 0.0351\n",
      "2025-06-01 14:51:17 [INFO]: epoch 295: training loss 0.0541\n",
      "2025-06-01 14:51:17 [INFO]: epoch 296: training loss 0.0414\n",
      "2025-06-01 14:51:17 [INFO]: epoch 297: training loss 0.0466\n",
      "2025-06-01 14:51:17 [INFO]: epoch 298: training loss 0.0451\n",
      "2025-06-01 14:51:17 [INFO]: epoch 299: training loss 0.0432\n",
      "2025-06-01 14:51:17 [INFO]: epoch 300: training loss 0.0403\n",
      "2025-06-01 14:51:17 [INFO]: epoch 301: training loss 0.0502\n",
      "2025-06-01 14:51:17 [INFO]: epoch 302: training loss 0.0574\n",
      "2025-06-01 14:51:18 [INFO]: epoch 303: training loss 0.0426\n",
      "2025-06-01 14:51:18 [INFO]: epoch 304: training loss 0.0412\n",
      "2025-06-01 14:51:18 [INFO]: epoch 305: training loss 0.0456\n",
      "2025-06-01 14:51:18 [INFO]: epoch 306: training loss 0.0312\n",
      "2025-06-01 14:51:18 [INFO]: epoch 307: training loss 0.0409\n",
      "2025-06-01 14:51:18 [INFO]: epoch 308: training loss 0.0343\n",
      "2025-06-01 14:51:18 [INFO]: epoch 309: training loss 0.0405\n",
      "2025-06-01 14:51:18 [INFO]: epoch 310: training loss 0.0430\n",
      "2025-06-01 14:51:18 [INFO]: epoch 311: training loss 0.0334\n",
      "2025-06-01 14:51:18 [INFO]: epoch 312: training loss 0.0484\n",
      "2025-06-01 14:51:18 [INFO]: epoch 313: training loss 0.0518\n",
      "2025-06-01 14:51:18 [INFO]: epoch 314: training loss 0.0356\n",
      "2025-06-01 14:51:18 [INFO]: epoch 315: training loss 0.0551\n",
      "2025-06-01 14:51:18 [INFO]: epoch 316: training loss 0.0485\n",
      "2025-06-01 14:51:18 [INFO]: epoch 317: training loss 0.0363\n",
      "2025-06-01 14:51:18 [INFO]: epoch 318: training loss 0.0499\n",
      "2025-06-01 14:51:18 [INFO]: epoch 319: training loss 0.0656\n",
      "2025-06-01 14:51:18 [INFO]: epoch 320: training loss 0.0466\n",
      "2025-06-01 14:51:18 [INFO]: epoch 321: training loss 0.0389\n",
      "2025-06-01 14:51:18 [INFO]: epoch 322: training loss 0.0334\n",
      "2025-06-01 14:51:18 [INFO]: epoch 323: training loss 0.0405\n",
      "2025-06-01 14:51:18 [INFO]: epoch 324: training loss 0.0383\n",
      "2025-06-01 14:51:18 [INFO]: epoch 325: training loss 0.0420\n",
      "2025-06-01 14:51:18 [INFO]: epoch 326: training loss 0.0388\n",
      "2025-06-01 14:51:18 [INFO]: epoch 327: training loss 0.0334\n",
      "2025-06-01 14:51:18 [INFO]: epoch 328: training loss 0.0444\n",
      "2025-06-01 14:51:18 [INFO]: epoch 329: training loss 0.0384\n",
      "2025-06-01 14:51:18 [INFO]: epoch 330: training loss 0.0428\n",
      "2025-06-01 14:51:18 [INFO]: epoch 331: training loss 0.0397\n",
      "2025-06-01 14:51:18 [INFO]: epoch 332: training loss 0.0338\n",
      "2025-06-01 14:51:18 [INFO]: epoch 333: training loss 0.0319\n",
      "2025-06-01 14:51:18 [INFO]: epoch 334: training loss 0.0318\n",
      "2025-06-01 14:51:18 [INFO]: epoch 335: training loss 0.0321\n",
      "2025-06-01 14:51:18 [INFO]: epoch 336: training loss 0.0326\n",
      "2025-06-01 14:51:18 [INFO]: epoch 337: training loss 0.0355\n",
      "2025-06-01 14:51:18 [INFO]: epoch 338: training loss 0.0317\n",
      "2025-06-01 14:51:18 [INFO]: epoch 339: training loss 0.0282\n",
      "2025-06-01 14:51:18 [INFO]: epoch 340: training loss 0.0302\n",
      "2025-06-01 14:51:18 [INFO]: epoch 341: training loss 0.0283\n",
      "2025-06-01 14:51:18 [INFO]: epoch 342: training loss 0.0315\n",
      "2025-06-01 14:51:18 [INFO]: epoch 343: training loss 0.0374\n",
      "2025-06-01 14:51:18 [INFO]: epoch 344: training loss 0.0360\n",
      "2025-06-01 14:51:18 [INFO]: epoch 345: training loss 0.0325\n",
      "2025-06-01 14:51:18 [INFO]: epoch 346: training loss 0.0295\n",
      "2025-06-01 14:51:18 [INFO]: epoch 347: training loss 0.0346\n",
      "2025-06-01 14:51:18 [INFO]: epoch 348: training loss 0.0295\n",
      "2025-06-01 14:51:18 [INFO]: epoch 349: training loss 0.0316\n",
      "2025-06-01 14:51:18 [INFO]: epoch 350: training loss 0.0336\n",
      "2025-06-01 14:51:18 [INFO]: epoch 351: training loss 0.0298\n",
      "2025-06-01 14:51:18 [INFO]: epoch 352: training loss 0.0285\n",
      "2025-06-01 14:51:18 [INFO]: epoch 353: training loss 0.0344\n",
      "2025-06-01 14:51:18 [INFO]: epoch 354: training loss 0.0280\n",
      "2025-06-01 14:51:18 [INFO]: epoch 355: training loss 0.0336\n",
      "2025-06-01 14:51:18 [INFO]: epoch 356: training loss 0.0355\n",
      "2025-06-01 14:51:18 [INFO]: epoch 357: training loss 0.0310\n",
      "2025-06-01 14:51:18 [INFO]: epoch 358: training loss 0.0282\n",
      "2025-06-01 14:51:18 [INFO]: epoch 359: training loss 0.0318\n",
      "2025-06-01 14:51:18 [INFO]: epoch 360: training loss 0.0328\n",
      "2025-06-01 14:51:18 [INFO]: epoch 361: training loss 0.0350\n",
      "2025-06-01 14:51:18 [INFO]: epoch 362: training loss 0.0287\n",
      "2025-06-01 14:51:18 [INFO]: epoch 363: training loss 0.0378\n",
      "2025-06-01 14:51:18 [INFO]: epoch 364: training loss 0.0329\n",
      "2025-06-01 14:51:18 [INFO]: epoch 365: training loss 0.0330\n",
      "2025-06-01 14:51:18 [INFO]: epoch 366: training loss 0.0330\n",
      "2025-06-01 14:51:18 [INFO]: epoch 367: training loss 0.0368\n",
      "2025-06-01 14:51:18 [INFO]: epoch 368: training loss 0.0363\n",
      "2025-06-01 14:51:18 [INFO]: epoch 369: training loss 0.0354\n",
      "2025-06-01 14:51:18 [INFO]: epoch 370: training loss 0.0304\n",
      "2025-06-01 14:51:18 [INFO]: epoch 371: training loss 0.0353\n",
      "2025-06-01 14:51:18 [INFO]: epoch 372: training loss 0.0346\n",
      "2025-06-01 14:51:18 [INFO]: epoch 373: training loss 0.0287\n",
      "2025-06-01 14:51:18 [INFO]: epoch 374: training loss 0.0337\n",
      "2025-06-01 14:51:18 [INFO]: epoch 375: training loss 0.0253\n",
      "2025-06-01 14:51:18 [INFO]: epoch 376: training loss 0.0254\n",
      "2025-06-01 14:51:18 [INFO]: epoch 377: training loss 0.0340\n",
      "2025-06-01 14:51:18 [INFO]: epoch 378: training loss 0.0271\n",
      "2025-06-01 14:51:19 [INFO]: epoch 379: training loss 0.0349\n",
      "2025-06-01 14:51:19 [INFO]: epoch 380: training loss 0.0308\n",
      "2025-06-01 14:51:19 [INFO]: epoch 381: training loss 0.0363\n",
      "2025-06-01 14:51:19 [INFO]: epoch 382: training loss 0.0325\n",
      "2025-06-01 14:51:19 [INFO]: epoch 383: training loss 0.0270\n",
      "2025-06-01 14:51:19 [INFO]: epoch 384: training loss 0.0342\n",
      "2025-06-01 14:51:19 [INFO]: epoch 385: training loss 0.0335\n",
      "2025-06-01 14:51:19 [INFO]: epoch 386: training loss 0.0281\n",
      "2025-06-01 14:51:19 [INFO]: epoch 387: training loss 0.0384\n",
      "2025-06-01 14:51:19 [INFO]: epoch 388: training loss 0.0337\n",
      "2025-06-01 14:51:19 [INFO]: epoch 389: training loss 0.0273\n",
      "2025-06-01 14:51:19 [INFO]: epoch 390: training loss 0.0264\n",
      "2025-06-01 14:51:19 [INFO]: epoch 391: training loss 0.0364\n",
      "2025-06-01 14:51:19 [INFO]: epoch 392: training loss 0.0328\n",
      "2025-06-01 14:51:19 [INFO]: epoch 393: training loss 0.0367\n",
      "2025-06-01 14:51:19 [INFO]: epoch 394: training loss 0.0328\n",
      "2025-06-01 14:51:19 [INFO]: epoch 395: training loss 0.0309\n",
      "2025-06-01 14:51:19 [INFO]: epoch 396: training loss 0.0330\n",
      "2025-06-01 14:51:19 [INFO]: epoch 397: training loss 0.0260\n",
      "2025-06-01 14:51:19 [INFO]: epoch 398: training loss 0.0259\n",
      "2025-06-01 14:51:19 [INFO]: epoch 399: training loss 0.0258\n",
      "2025-06-01 14:51:19 [INFO]: Finished training.\n",
      "2025-06-01 14:51:19 [WARNING]: 🚨DeprecationWarning: The method impute is deprecated. Please use `predict` instead.\n",
      " 33%|████████████████████████████                                                        | 2/6 [00:10<00:21,  5.37s/it]2025-06-01 14:51:19 [INFO]: No given device, using default device: cuda\n",
      "2025-06-01 14:51:19 [WARNING]: saving_path not given. Model files and tensorboard file will not be saved.\n",
      "2025-06-01 14:51:19 [INFO]: Model initialized successfully with the number of trainable parameters: 597,780\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-01 14:51:19 [INFO]: epoch 0: training loss 1.2885\n",
      "2025-06-01 14:51:19 [INFO]: epoch 1: training loss 0.6643\n",
      "2025-06-01 14:51:19 [INFO]: epoch 2: training loss 0.5848\n",
      "2025-06-01 14:51:19 [INFO]: epoch 3: training loss 0.6061\n",
      "2025-06-01 14:51:19 [INFO]: epoch 4: training loss 0.5746\n",
      "2025-06-01 14:51:19 [INFO]: epoch 5: training loss 0.5391\n",
      "2025-06-01 14:51:19 [INFO]: epoch 6: training loss 0.5495\n",
      "2025-06-01 14:51:19 [INFO]: epoch 7: training loss 0.4827\n",
      "2025-06-01 14:51:19 [INFO]: epoch 8: training loss 0.4559\n",
      "2025-06-01 14:51:19 [INFO]: epoch 9: training loss 0.4384\n",
      "2025-06-01 14:51:19 [INFO]: epoch 10: training loss 0.4606\n",
      "2025-06-01 14:51:19 [INFO]: epoch 11: training loss 0.4801\n",
      "2025-06-01 14:51:19 [INFO]: epoch 12: training loss 0.4499\n",
      "2025-06-01 14:51:19 [INFO]: epoch 13: training loss 0.4687\n",
      "2025-06-01 14:51:19 [INFO]: epoch 14: training loss 0.4609\n",
      "2025-06-01 14:51:19 [INFO]: epoch 15: training loss 0.4447\n",
      "2025-06-01 14:51:19 [INFO]: epoch 16: training loss 0.4255\n",
      "2025-06-01 14:51:19 [INFO]: epoch 17: training loss 0.4214\n",
      "2025-06-01 14:51:19 [INFO]: epoch 18: training loss 0.4412\n",
      "2025-06-01 14:51:19 [INFO]: epoch 19: training loss 0.4163\n",
      "2025-06-01 14:51:19 [INFO]: epoch 20: training loss 0.3987\n",
      "2025-06-01 14:51:19 [INFO]: epoch 21: training loss 0.4071\n",
      "2025-06-01 14:51:19 [INFO]: epoch 22: training loss 0.4115\n",
      "2025-06-01 14:51:19 [INFO]: epoch 23: training loss 0.4241\n",
      "2025-06-01 14:51:19 [INFO]: epoch 24: training loss 0.4137\n",
      "2025-06-01 14:51:19 [INFO]: epoch 25: training loss 0.4046\n",
      "2025-06-01 14:51:19 [INFO]: epoch 26: training loss 0.4120\n",
      "2025-06-01 14:51:19 [INFO]: epoch 27: training loss 0.4159\n",
      "2025-06-01 14:51:19 [INFO]: epoch 28: training loss 0.4029\n",
      "2025-06-01 14:51:19 [INFO]: epoch 29: training loss 0.4056\n",
      "2025-06-01 14:51:19 [INFO]: epoch 30: training loss 0.3871\n",
      "2025-06-01 14:51:19 [INFO]: epoch 31: training loss 0.4318\n",
      "2025-06-01 14:51:19 [INFO]: epoch 32: training loss 0.3922\n",
      "2025-06-01 14:51:19 [INFO]: epoch 33: training loss 0.3740\n",
      "2025-06-01 14:51:19 [INFO]: epoch 34: training loss 0.3862\n",
      "2025-06-01 14:51:19 [INFO]: epoch 35: training loss 0.3679\n",
      "2025-06-01 14:51:19 [INFO]: epoch 36: training loss 0.3991\n",
      "2025-06-01 14:51:19 [INFO]: epoch 37: training loss 0.3899\n",
      "2025-06-01 14:51:19 [INFO]: epoch 38: training loss 0.4081\n",
      "2025-06-01 14:51:19 [INFO]: epoch 39: training loss 0.4086\n",
      "2025-06-01 14:51:19 [INFO]: epoch 40: training loss 0.3665\n",
      "2025-06-01 14:51:19 [INFO]: epoch 41: training loss 0.3795\n",
      "2025-06-01 14:51:19 [INFO]: epoch 42: training loss 0.3910\n",
      "2025-06-01 14:51:19 [INFO]: epoch 43: training loss 0.3742\n",
      "2025-06-01 14:51:19 [INFO]: epoch 44: training loss 0.3696\n",
      "2025-06-01 14:51:19 [INFO]: epoch 45: training loss 0.3626\n",
      "2025-06-01 14:51:19 [INFO]: epoch 46: training loss 0.3596\n",
      "2025-06-01 14:51:19 [INFO]: epoch 47: training loss 0.3634\n",
      "2025-06-01 14:51:20 [INFO]: epoch 48: training loss 0.3418\n",
      "2025-06-01 14:51:20 [INFO]: epoch 49: training loss 0.3380\n",
      "2025-06-01 14:51:20 [INFO]: epoch 50: training loss 0.3690\n",
      "2025-06-01 14:51:20 [INFO]: epoch 51: training loss 0.3768\n",
      "2025-06-01 14:51:20 [INFO]: epoch 52: training loss 0.3587\n",
      "2025-06-01 14:51:20 [INFO]: epoch 53: training loss 0.3505\n",
      "2025-06-01 14:51:20 [INFO]: epoch 54: training loss 0.3626\n",
      "2025-06-01 14:51:20 [INFO]: epoch 55: training loss 0.3648\n",
      "2025-06-01 14:51:20 [INFO]: epoch 56: training loss 0.3397\n",
      "2025-06-01 14:51:20 [INFO]: epoch 57: training loss 0.3518\n",
      "2025-06-01 14:51:20 [INFO]: epoch 58: training loss 0.3883\n",
      "2025-06-01 14:51:20 [INFO]: epoch 59: training loss 0.3578\n",
      "2025-06-01 14:51:20 [INFO]: epoch 60: training loss 0.3481\n",
      "2025-06-01 14:51:20 [INFO]: epoch 61: training loss 0.3577\n",
      "2025-06-01 14:51:20 [INFO]: epoch 62: training loss 0.3839\n",
      "2025-06-01 14:51:20 [INFO]: epoch 63: training loss 0.3428\n",
      "2025-06-01 14:51:20 [INFO]: epoch 64: training loss 0.3759\n",
      "2025-06-01 14:51:20 [INFO]: epoch 65: training loss 0.3577\n",
      "2025-06-01 14:51:20 [INFO]: epoch 66: training loss 0.3249\n",
      "2025-06-01 14:51:20 [INFO]: epoch 67: training loss 0.3590\n",
      "2025-06-01 14:51:20 [INFO]: epoch 68: training loss 0.3455\n",
      "2025-06-01 14:51:20 [INFO]: epoch 69: training loss 0.3529\n",
      "2025-06-01 14:51:20 [INFO]: epoch 70: training loss 0.3341\n",
      "2025-06-01 14:51:20 [INFO]: epoch 71: training loss 0.3392\n",
      "2025-06-01 14:51:20 [INFO]: epoch 72: training loss 0.3519\n",
      "2025-06-01 14:51:20 [INFO]: epoch 73: training loss 0.3108\n",
      "2025-06-01 14:51:20 [INFO]: epoch 74: training loss 0.3268\n",
      "2025-06-01 14:51:20 [INFO]: epoch 75: training loss 0.3311\n",
      "2025-06-01 14:51:20 [INFO]: epoch 76: training loss 0.3605\n",
      "2025-06-01 14:51:20 [INFO]: epoch 77: training loss 0.3402\n",
      "2025-06-01 14:51:20 [INFO]: epoch 78: training loss 0.3276\n",
      "2025-06-01 14:51:20 [INFO]: epoch 79: training loss 0.3133\n",
      "2025-06-01 14:51:20 [INFO]: epoch 80: training loss 0.3216\n",
      "2025-06-01 14:51:20 [INFO]: epoch 81: training loss 0.3344\n",
      "2025-06-01 14:51:20 [INFO]: epoch 82: training loss 0.3405\n",
      "2025-06-01 14:51:20 [INFO]: epoch 83: training loss 0.3242\n",
      "2025-06-01 14:51:20 [INFO]: epoch 84: training loss 0.3196\n",
      "2025-06-01 14:51:20 [INFO]: epoch 85: training loss 0.3364\n",
      "2025-06-01 14:51:20 [INFO]: epoch 86: training loss 0.3246\n",
      "2025-06-01 14:51:20 [INFO]: epoch 87: training loss 0.3269\n",
      "2025-06-01 14:51:20 [INFO]: epoch 88: training loss 0.3259\n",
      "2025-06-01 14:51:20 [INFO]: epoch 89: training loss 0.3496\n",
      "2025-06-01 14:51:20 [INFO]: epoch 90: training loss 0.3193\n",
      "2025-06-01 14:51:20 [INFO]: epoch 91: training loss 0.3215\n",
      "2025-06-01 14:51:20 [INFO]: epoch 92: training loss 0.3255\n",
      "2025-06-01 14:51:20 [INFO]: epoch 93: training loss 0.3232\n",
      "2025-06-01 14:51:20 [INFO]: epoch 94: training loss 0.3219\n",
      "2025-06-01 14:51:20 [INFO]: epoch 95: training loss 0.3141\n",
      "2025-06-01 14:51:20 [INFO]: epoch 96: training loss 0.3250\n",
      "2025-06-01 14:51:20 [INFO]: epoch 97: training loss 0.2942\n",
      "2025-06-01 14:51:20 [INFO]: epoch 98: training loss 0.3245\n",
      "2025-06-01 14:51:20 [INFO]: epoch 99: training loss 0.3324\n",
      "2025-06-01 14:51:20 [INFO]: epoch 100: training loss 0.3111\n",
      "2025-06-01 14:51:20 [INFO]: epoch 101: training loss 0.3237\n",
      "2025-06-01 14:51:20 [INFO]: epoch 102: training loss 0.3231\n",
      "2025-06-01 14:51:20 [INFO]: epoch 103: training loss 0.2900\n",
      "2025-06-01 14:51:20 [INFO]: epoch 104: training loss 0.2994\n",
      "2025-06-01 14:51:20 [INFO]: epoch 105: training loss 0.2985\n",
      "2025-06-01 14:51:20 [INFO]: epoch 106: training loss 0.3053\n",
      "2025-06-01 14:51:20 [INFO]: epoch 107: training loss 0.2868\n",
      "2025-06-01 14:51:20 [INFO]: epoch 108: training loss 0.3035\n",
      "2025-06-01 14:51:20 [INFO]: epoch 109: training loss 0.2894\n",
      "2025-06-01 14:51:20 [INFO]: epoch 110: training loss 0.3112\n",
      "2025-06-01 14:51:20 [INFO]: epoch 111: training loss 0.2941\n",
      "2025-06-01 14:51:20 [INFO]: epoch 112: training loss 0.2887\n",
      "2025-06-01 14:51:20 [INFO]: epoch 113: training loss 0.2950\n",
      "2025-06-01 14:51:20 [INFO]: epoch 114: training loss 0.3029\n",
      "2025-06-01 14:51:20 [INFO]: epoch 115: training loss 0.2831\n",
      "2025-06-01 14:51:20 [INFO]: epoch 116: training loss 0.2914\n",
      "2025-06-01 14:51:20 [INFO]: epoch 117: training loss 0.3034\n",
      "2025-06-01 14:51:20 [INFO]: epoch 118: training loss 0.3018\n",
      "2025-06-01 14:51:20 [INFO]: epoch 119: training loss 0.3033\n",
      "2025-06-01 14:51:20 [INFO]: epoch 120: training loss 0.3061\n",
      "2025-06-01 14:51:20 [INFO]: epoch 121: training loss 0.2989\n",
      "2025-06-01 14:51:20 [INFO]: epoch 122: training loss 0.3054\n",
      "2025-06-01 14:51:21 [INFO]: epoch 123: training loss 0.2880\n",
      "2025-06-01 14:51:21 [INFO]: epoch 124: training loss 0.2923\n",
      "2025-06-01 14:51:21 [INFO]: epoch 125: training loss 0.2865\n",
      "2025-06-01 14:51:21 [INFO]: epoch 126: training loss 0.2789\n",
      "2025-06-01 14:51:21 [INFO]: epoch 127: training loss 0.3075\n",
      "2025-06-01 14:51:21 [INFO]: epoch 128: training loss 0.3019\n",
      "2025-06-01 14:51:21 [INFO]: epoch 129: training loss 0.3006\n",
      "2025-06-01 14:51:21 [INFO]: epoch 130: training loss 0.2826\n",
      "2025-06-01 14:51:21 [INFO]: epoch 131: training loss 0.2991\n",
      "2025-06-01 14:51:21 [INFO]: epoch 132: training loss 0.2928\n",
      "2025-06-01 14:51:21 [INFO]: epoch 133: training loss 0.2818\n",
      "2025-06-01 14:51:21 [INFO]: epoch 134: training loss 0.2783\n",
      "2025-06-01 14:51:21 [INFO]: epoch 135: training loss 0.2845\n",
      "2025-06-01 14:51:21 [INFO]: epoch 136: training loss 0.2827\n",
      "2025-06-01 14:51:21 [INFO]: epoch 137: training loss 0.3005\n",
      "2025-06-01 14:51:21 [INFO]: epoch 138: training loss 0.3189\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-01 14:51:21 [INFO]: epoch 139: training loss 0.2770\n",
      "2025-06-01 14:51:21 [INFO]: epoch 140: training loss 0.2712\n",
      "2025-06-01 14:51:21 [INFO]: epoch 141: training loss 0.2715\n",
      "2025-06-01 14:51:21 [INFO]: epoch 142: training loss 0.2727\n",
      "2025-06-01 14:51:21 [INFO]: epoch 143: training loss 0.2745\n",
      "2025-06-01 14:51:21 [INFO]: epoch 144: training loss 0.2731\n",
      "2025-06-01 14:51:21 [INFO]: epoch 145: training loss 0.2678\n",
      "2025-06-01 14:51:21 [INFO]: epoch 146: training loss 0.2659\n",
      "2025-06-01 14:51:21 [INFO]: epoch 147: training loss 0.2731\n",
      "2025-06-01 14:51:21 [INFO]: epoch 148: training loss 0.2676\n",
      "2025-06-01 14:51:21 [INFO]: epoch 149: training loss 0.2845\n",
      "2025-06-01 14:51:21 [INFO]: epoch 150: training loss 0.2709\n",
      "2025-06-01 14:51:21 [INFO]: epoch 151: training loss 0.2732\n",
      "2025-06-01 14:51:21 [INFO]: epoch 152: training loss 0.2552\n",
      "2025-06-01 14:51:21 [INFO]: epoch 153: training loss 0.2557\n",
      "2025-06-01 14:51:21 [INFO]: epoch 154: training loss 0.2674\n",
      "2025-06-01 14:51:21 [INFO]: epoch 155: training loss 0.2707\n",
      "2025-06-01 14:51:21 [INFO]: epoch 156: training loss 0.2690\n",
      "2025-06-01 14:51:21 [INFO]: epoch 157: training loss 0.2530\n",
      "2025-06-01 14:51:21 [INFO]: epoch 158: training loss 0.2559\n",
      "2025-06-01 14:51:21 [INFO]: epoch 159: training loss 0.2538\n",
      "2025-06-01 14:51:21 [INFO]: epoch 160: training loss 0.2525\n",
      "2025-06-01 14:51:21 [INFO]: epoch 161: training loss 0.2672\n",
      "2025-06-01 14:51:21 [INFO]: epoch 162: training loss 0.2528\n",
      "2025-06-01 14:51:21 [INFO]: epoch 163: training loss 0.2522\n",
      "2025-06-01 14:51:21 [INFO]: epoch 164: training loss 0.2606\n",
      "2025-06-01 14:51:21 [INFO]: epoch 165: training loss 0.2599\n",
      "2025-06-01 14:51:21 [INFO]: epoch 166: training loss 0.2532\n",
      "2025-06-01 14:51:21 [INFO]: epoch 167: training loss 0.2511\n",
      "2025-06-01 14:51:21 [INFO]: epoch 168: training loss 0.2531\n",
      "2025-06-01 14:51:21 [INFO]: epoch 169: training loss 0.2632\n",
      "2025-06-01 14:51:21 [INFO]: epoch 170: training loss 0.2505\n",
      "2025-06-01 14:51:21 [INFO]: epoch 171: training loss 0.2559\n",
      "2025-06-01 14:51:21 [INFO]: epoch 172: training loss 0.2574\n",
      "2025-06-01 14:51:21 [INFO]: epoch 173: training loss 0.2591\n",
      "2025-06-01 14:51:21 [INFO]: epoch 174: training loss 0.2376\n",
      "2025-06-01 14:51:21 [INFO]: epoch 175: training loss 0.2497\n",
      "2025-06-01 14:51:21 [INFO]: epoch 176: training loss 0.2546\n",
      "2025-06-01 14:51:21 [INFO]: epoch 177: training loss 0.2503\n",
      "2025-06-01 14:51:21 [INFO]: epoch 178: training loss 0.2521\n",
      "2025-06-01 14:51:21 [INFO]: epoch 179: training loss 0.2466\n",
      "2025-06-01 14:51:21 [INFO]: epoch 180: training loss 0.2387\n",
      "2025-06-01 14:51:21 [INFO]: epoch 181: training loss 0.2603\n",
      "2025-06-01 14:51:21 [INFO]: epoch 182: training loss 0.2326\n",
      "2025-06-01 14:51:21 [INFO]: epoch 183: training loss 0.2444\n",
      "2025-06-01 14:51:21 [INFO]: epoch 184: training loss 0.2581\n",
      "2025-06-01 14:51:21 [INFO]: epoch 185: training loss 0.2427\n",
      "2025-06-01 14:51:21 [INFO]: epoch 186: training loss 0.2448\n",
      "2025-06-01 14:51:21 [INFO]: epoch 187: training loss 0.2618\n",
      "2025-06-01 14:51:21 [INFO]: epoch 188: training loss 0.2602\n",
      "2025-06-01 14:51:21 [INFO]: epoch 189: training loss 0.2457\n",
      "2025-06-01 14:51:21 [INFO]: epoch 190: training loss 0.2468\n",
      "2025-06-01 14:51:21 [INFO]: epoch 191: training loss 0.2353\n",
      "2025-06-01 14:51:21 [INFO]: epoch 192: training loss 0.2514\n",
      "2025-06-01 14:51:21 [INFO]: epoch 193: training loss 0.2521\n",
      "2025-06-01 14:51:21 [INFO]: epoch 194: training loss 0.2539\n",
      "2025-06-01 14:51:21 [INFO]: epoch 195: training loss 0.2516\n",
      "2025-06-01 14:51:21 [INFO]: epoch 196: training loss 0.2477\n",
      "2025-06-01 14:51:21 [INFO]: epoch 197: training loss 0.2476\n",
      "2025-06-01 14:51:21 [INFO]: epoch 198: training loss 0.2595\n",
      "2025-06-01 14:51:22 [INFO]: epoch 199: training loss 0.2411\n",
      "2025-06-01 14:51:22 [INFO]: epoch 200: training loss 0.2593\n",
      "2025-06-01 14:51:22 [INFO]: epoch 201: training loss 0.2339\n",
      "2025-06-01 14:51:22 [INFO]: epoch 202: training loss 0.2338\n",
      "2025-06-01 14:51:22 [INFO]: epoch 203: training loss 0.2331\n",
      "2025-06-01 14:51:22 [INFO]: epoch 204: training loss 0.2335\n",
      "2025-06-01 14:51:22 [INFO]: epoch 205: training loss 0.2430\n",
      "2025-06-01 14:51:22 [INFO]: epoch 206: training loss 0.2437\n",
      "2025-06-01 14:51:22 [INFO]: epoch 207: training loss 0.2372\n",
      "2025-06-01 14:51:22 [INFO]: epoch 208: training loss 0.2367\n",
      "2025-06-01 14:51:22 [INFO]: epoch 209: training loss 0.2321\n",
      "2025-06-01 14:51:22 [INFO]: epoch 210: training loss 0.2343\n",
      "2025-06-01 14:51:22 [INFO]: epoch 211: training loss 0.2349\n",
      "2025-06-01 14:51:22 [INFO]: epoch 212: training loss 0.2296\n",
      "2025-06-01 14:51:22 [INFO]: epoch 213: training loss 0.2179\n",
      "2025-06-01 14:51:22 [INFO]: epoch 214: training loss 0.2267\n",
      "2025-06-01 14:51:22 [INFO]: epoch 215: training loss 0.2145\n",
      "2025-06-01 14:51:22 [INFO]: epoch 216: training loss 0.2306\n",
      "2025-06-01 14:51:22 [INFO]: epoch 217: training loss 0.2154\n",
      "2025-06-01 14:51:22 [INFO]: epoch 218: training loss 0.2128\n",
      "2025-06-01 14:51:22 [INFO]: epoch 219: training loss 0.2178\n",
      "2025-06-01 14:51:22 [INFO]: epoch 220: training loss 0.2190\n",
      "2025-06-01 14:51:22 [INFO]: epoch 221: training loss 0.2277\n",
      "2025-06-01 14:51:22 [INFO]: epoch 222: training loss 0.2215\n",
      "2025-06-01 14:51:22 [INFO]: epoch 223: training loss 0.2389\n",
      "2025-06-01 14:51:22 [INFO]: epoch 224: training loss 0.2267\n",
      "2025-06-01 14:51:22 [INFO]: epoch 225: training loss 0.2321\n",
      "2025-06-01 14:51:22 [INFO]: epoch 226: training loss 0.2144\n",
      "2025-06-01 14:51:22 [INFO]: epoch 227: training loss 0.2199\n",
      "2025-06-01 14:51:22 [INFO]: epoch 228: training loss 0.2250\n",
      "2025-06-01 14:51:22 [INFO]: epoch 229: training loss 0.2238\n",
      "2025-06-01 14:51:22 [INFO]: epoch 230: training loss 0.2165\n",
      "2025-06-01 14:51:22 [INFO]: epoch 231: training loss 0.2203\n",
      "2025-06-01 14:51:22 [INFO]: epoch 232: training loss 0.2180\n",
      "2025-06-01 14:51:22 [INFO]: epoch 233: training loss 0.2123\n",
      "2025-06-01 14:51:22 [INFO]: epoch 234: training loss 0.2038\n",
      "2025-06-01 14:51:22 [INFO]: epoch 235: training loss 0.2184\n",
      "2025-06-01 14:51:22 [INFO]: epoch 236: training loss 0.2202\n",
      "2025-06-01 14:51:22 [INFO]: epoch 237: training loss 0.2124\n",
      "2025-06-01 14:51:22 [INFO]: epoch 238: training loss 0.2134\n",
      "2025-06-01 14:51:22 [INFO]: epoch 239: training loss 0.2183\n",
      "2025-06-01 14:51:22 [INFO]: epoch 240: training loss 0.2194\n",
      "2025-06-01 14:51:22 [INFO]: epoch 241: training loss 0.2097\n",
      "2025-06-01 14:51:22 [INFO]: epoch 242: training loss 0.2046\n",
      "2025-06-01 14:51:22 [INFO]: epoch 243: training loss 0.2056\n",
      "2025-06-01 14:51:22 [INFO]: epoch 244: training loss 0.2088\n",
      "2025-06-01 14:51:22 [INFO]: epoch 245: training loss 0.2062\n",
      "2025-06-01 14:51:22 [INFO]: epoch 246: training loss 0.2205\n",
      "2025-06-01 14:51:22 [INFO]: epoch 247: training loss 0.1969\n",
      "2025-06-01 14:51:22 [INFO]: epoch 248: training loss 0.1901\n",
      "2025-06-01 14:51:22 [INFO]: epoch 249: training loss 0.2097\n",
      "2025-06-01 14:51:22 [INFO]: epoch 250: training loss 0.2167\n",
      "2025-06-01 14:51:22 [INFO]: epoch 251: training loss 0.2082\n",
      "2025-06-01 14:51:22 [INFO]: epoch 252: training loss 0.2039\n",
      "2025-06-01 14:51:22 [INFO]: epoch 253: training loss 0.2041\n",
      "2025-06-01 14:51:22 [INFO]: epoch 254: training loss 0.1995\n",
      "2025-06-01 14:51:22 [INFO]: epoch 255: training loss 0.2037\n",
      "2025-06-01 14:51:22 [INFO]: epoch 256: training loss 0.1956\n",
      "2025-06-01 14:51:22 [INFO]: epoch 257: training loss 0.2075\n",
      "2025-06-01 14:51:22 [INFO]: epoch 258: training loss 0.1999\n",
      "2025-06-01 14:51:22 [INFO]: epoch 259: training loss 0.2141\n",
      "2025-06-01 14:51:22 [INFO]: epoch 260: training loss 0.2053\n",
      "2025-06-01 14:51:22 [INFO]: epoch 261: training loss 0.1940\n",
      "2025-06-01 14:51:22 [INFO]: epoch 262: training loss 0.2001\n",
      "2025-06-01 14:51:22 [INFO]: epoch 263: training loss 0.2044\n",
      "2025-06-01 14:51:22 [INFO]: epoch 264: training loss 0.2031\n",
      "2025-06-01 14:51:22 [INFO]: epoch 265: training loss 0.2100\n",
      "2025-06-01 14:51:22 [INFO]: epoch 266: training loss 0.1890\n",
      "2025-06-01 14:51:22 [INFO]: epoch 267: training loss 0.2103\n",
      "2025-06-01 14:51:22 [INFO]: epoch 268: training loss 0.1958\n",
      "2025-06-01 14:51:22 [INFO]: epoch 269: training loss 0.1945\n",
      "2025-06-01 14:51:22 [INFO]: epoch 270: training loss 0.1951\n",
      "2025-06-01 14:51:22 [INFO]: epoch 271: training loss 0.2039\n",
      "2025-06-01 14:51:22 [INFO]: epoch 272: training loss 0.1986\n",
      "2025-06-01 14:51:22 [INFO]: epoch 273: training loss 0.2022\n",
      "2025-06-01 14:51:23 [INFO]: epoch 274: training loss 0.2011\n",
      "2025-06-01 14:51:23 [INFO]: epoch 275: training loss 0.1965\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-01 14:51:23 [INFO]: epoch 276: training loss 0.2005\n",
      "2025-06-01 14:51:23 [INFO]: epoch 277: training loss 0.1868\n",
      "2025-06-01 14:51:23 [INFO]: epoch 278: training loss 0.1915\n",
      "2025-06-01 14:51:23 [INFO]: epoch 279: training loss 0.2064\n",
      "2025-06-01 14:51:23 [INFO]: epoch 280: training loss 0.2023\n",
      "2025-06-01 14:51:23 [INFO]: epoch 281: training loss 0.1960\n",
      "2025-06-01 14:51:23 [INFO]: epoch 282: training loss 0.1884\n",
      "2025-06-01 14:51:23 [INFO]: epoch 283: training loss 0.1855\n",
      "2025-06-01 14:51:23 [INFO]: epoch 284: training loss 0.2069\n",
      "2025-06-01 14:51:23 [INFO]: epoch 285: training loss 0.2017\n",
      "2025-06-01 14:51:23 [INFO]: epoch 286: training loss 0.1889\n",
      "2025-06-01 14:51:23 [INFO]: epoch 287: training loss 0.1959\n",
      "2025-06-01 14:51:23 [INFO]: epoch 288: training loss 0.1958\n",
      "2025-06-01 14:51:23 [INFO]: epoch 289: training loss 0.1902\n",
      "2025-06-01 14:51:23 [INFO]: epoch 290: training loss 0.1910\n",
      "2025-06-01 14:51:23 [INFO]: epoch 291: training loss 0.1936\n",
      "2025-06-01 14:51:23 [INFO]: epoch 292: training loss 0.1957\n",
      "2025-06-01 14:51:23 [INFO]: epoch 293: training loss 0.1889\n",
      "2025-06-01 14:51:23 [INFO]: epoch 294: training loss 0.1792\n",
      "2025-06-01 14:51:23 [INFO]: epoch 295: training loss 0.1901\n",
      "2025-06-01 14:51:23 [INFO]: epoch 296: training loss 0.1921\n",
      "2025-06-01 14:51:23 [INFO]: epoch 297: training loss 0.1755\n",
      "2025-06-01 14:51:23 [INFO]: epoch 298: training loss 0.1755\n",
      "2025-06-01 14:51:23 [INFO]: epoch 299: training loss 0.1815\n",
      "2025-06-01 14:51:23 [INFO]: epoch 300: training loss 0.1795\n",
      "2025-06-01 14:51:23 [INFO]: epoch 301: training loss 0.1740\n",
      "2025-06-01 14:51:23 [INFO]: epoch 302: training loss 0.1793\n",
      "2025-06-01 14:51:23 [INFO]: epoch 303: training loss 0.1909\n",
      "2025-06-01 14:51:23 [INFO]: epoch 304: training loss 0.1869\n",
      "2025-06-01 14:51:23 [INFO]: epoch 305: training loss 0.1809\n",
      "2025-06-01 14:51:23 [INFO]: epoch 306: training loss 0.1958\n",
      "2025-06-01 14:51:23 [INFO]: epoch 307: training loss 0.1924\n",
      "2025-06-01 14:51:23 [INFO]: epoch 308: training loss 0.1856\n",
      "2025-06-01 14:51:23 [INFO]: epoch 309: training loss 0.1904\n",
      "2025-06-01 14:51:23 [INFO]: epoch 310: training loss 0.1803\n",
      "2025-06-01 14:51:23 [INFO]: epoch 311: training loss 0.1801\n",
      "2025-06-01 14:51:23 [INFO]: epoch 312: training loss 0.1852\n",
      "2025-06-01 14:51:23 [INFO]: epoch 313: training loss 0.1864\n",
      "2025-06-01 14:51:23 [INFO]: epoch 314: training loss 0.1746\n",
      "2025-06-01 14:51:23 [INFO]: epoch 315: training loss 0.1687\n",
      "2025-06-01 14:51:23 [INFO]: epoch 316: training loss 0.1740\n",
      "2025-06-01 14:51:23 [INFO]: epoch 317: training loss 0.1795\n",
      "2025-06-01 14:51:23 [INFO]: epoch 318: training loss 0.1675\n",
      "2025-06-01 14:51:23 [INFO]: epoch 319: training loss 0.1828\n",
      "2025-06-01 14:51:23 [INFO]: epoch 320: training loss 0.1788\n",
      "2025-06-01 14:51:23 [INFO]: epoch 321: training loss 0.1789\n",
      "2025-06-01 14:51:23 [INFO]: epoch 322: training loss 0.1743\n",
      "2025-06-01 14:51:23 [INFO]: epoch 323: training loss 0.1715\n",
      "2025-06-01 14:51:23 [INFO]: epoch 324: training loss 0.1810\n",
      "2025-06-01 14:51:23 [INFO]: epoch 325: training loss 0.1765\n",
      "2025-06-01 14:51:23 [INFO]: epoch 326: training loss 0.1700\n",
      "2025-06-01 14:51:23 [INFO]: epoch 327: training loss 0.1736\n",
      "2025-06-01 14:51:23 [INFO]: epoch 328: training loss 0.1798\n",
      "2025-06-01 14:51:23 [INFO]: epoch 329: training loss 0.1781\n",
      "2025-06-01 14:51:23 [INFO]: epoch 330: training loss 0.1585\n",
      "2025-06-01 14:51:23 [INFO]: epoch 331: training loss 0.1671\n",
      "2025-06-01 14:51:23 [INFO]: epoch 332: training loss 0.1709\n",
      "2025-06-01 14:51:23 [INFO]: epoch 333: training loss 0.1733\n",
      "2025-06-01 14:51:23 [INFO]: epoch 334: training loss 0.1723\n",
      "2025-06-01 14:51:23 [INFO]: epoch 335: training loss 0.1632\n",
      "2025-06-01 14:51:23 [INFO]: epoch 336: training loss 0.1755\n",
      "2025-06-01 14:51:23 [INFO]: epoch 337: training loss 0.1696\n",
      "2025-06-01 14:51:23 [INFO]: epoch 338: training loss 0.1668\n",
      "2025-06-01 14:51:23 [INFO]: epoch 339: training loss 0.1685\n",
      "2025-06-01 14:51:23 [INFO]: epoch 340: training loss 0.1617\n",
      "2025-06-01 14:51:23 [INFO]: epoch 341: training loss 0.1704\n",
      "2025-06-01 14:51:23 [INFO]: epoch 342: training loss 0.1669\n",
      "2025-06-01 14:51:23 [INFO]: epoch 343: training loss 0.1546\n",
      "2025-06-01 14:51:23 [INFO]: epoch 344: training loss 0.1654\n",
      "2025-06-01 14:51:23 [INFO]: epoch 345: training loss 0.1628\n",
      "2025-06-01 14:51:23 [INFO]: epoch 346: training loss 0.1855\n",
      "2025-06-01 14:51:23 [INFO]: epoch 347: training loss 0.1728\n",
      "2025-06-01 14:51:23 [INFO]: epoch 348: training loss 0.1636\n",
      "2025-06-01 14:51:23 [INFO]: epoch 349: training loss 0.1727\n",
      "2025-06-01 14:51:24 [INFO]: epoch 350: training loss 0.1781\n",
      "2025-06-01 14:51:24 [INFO]: epoch 351: training loss 0.1820\n",
      "2025-06-01 14:51:24 [INFO]: epoch 352: training loss 0.1686\n",
      "2025-06-01 14:51:24 [INFO]: epoch 353: training loss 0.1678\n",
      "2025-06-01 14:51:24 [INFO]: epoch 354: training loss 0.1633\n",
      "2025-06-01 14:51:24 [INFO]: epoch 355: training loss 0.1762\n",
      "2025-06-01 14:51:24 [INFO]: epoch 356: training loss 0.1805\n",
      "2025-06-01 14:51:24 [INFO]: epoch 357: training loss 0.1726\n",
      "2025-06-01 14:51:24 [INFO]: epoch 358: training loss 0.1495\n",
      "2025-06-01 14:51:24 [INFO]: epoch 359: training loss 0.1583\n",
      "2025-06-01 14:51:24 [INFO]: epoch 360: training loss 0.1601\n",
      "2025-06-01 14:51:24 [INFO]: epoch 361: training loss 0.1488\n",
      "2025-06-01 14:51:24 [INFO]: epoch 362: training loss 0.1509\n",
      "2025-06-01 14:51:24 [INFO]: epoch 363: training loss 0.1673\n",
      "2025-06-01 14:51:24 [INFO]: epoch 364: training loss 0.1586\n",
      "2025-06-01 14:51:24 [INFO]: epoch 365: training loss 0.1615\n",
      "2025-06-01 14:51:24 [INFO]: epoch 366: training loss 0.1503\n",
      "2025-06-01 14:51:24 [INFO]: epoch 367: training loss 0.1505\n",
      "2025-06-01 14:51:24 [INFO]: epoch 368: training loss 0.1565\n",
      "2025-06-01 14:51:24 [INFO]: epoch 369: training loss 0.1464\n",
      "2025-06-01 14:51:24 [INFO]: epoch 370: training loss 0.1532\n",
      "2025-06-01 14:51:24 [INFO]: epoch 371: training loss 0.1548\n",
      "2025-06-01 14:51:24 [INFO]: epoch 372: training loss 0.1462\n",
      "2025-06-01 14:51:24 [INFO]: epoch 373: training loss 0.1498\n",
      "2025-06-01 14:51:24 [INFO]: epoch 374: training loss 0.1469\n",
      "2025-06-01 14:51:24 [INFO]: epoch 375: training loss 0.1399\n",
      "2025-06-01 14:51:24 [INFO]: epoch 376: training loss 0.1545\n",
      "2025-06-01 14:51:24 [INFO]: epoch 377: training loss 0.1429\n",
      "2025-06-01 14:51:24 [INFO]: epoch 378: training loss 0.1421\n",
      "2025-06-01 14:51:24 [INFO]: epoch 379: training loss 0.1476\n",
      "2025-06-01 14:51:24 [INFO]: epoch 380: training loss 0.1447\n",
      "2025-06-01 14:51:24 [INFO]: epoch 381: training loss 0.1451\n",
      "2025-06-01 14:51:24 [INFO]: epoch 382: training loss 0.1381\n",
      "2025-06-01 14:51:24 [INFO]: epoch 383: training loss 0.1488\n",
      "2025-06-01 14:51:24 [INFO]: epoch 384: training loss 0.1484\n",
      "2025-06-01 14:51:24 [INFO]: epoch 385: training loss 0.1316\n",
      "2025-06-01 14:51:24 [INFO]: epoch 386: training loss 0.1413\n",
      "2025-06-01 14:51:24 [INFO]: epoch 387: training loss 0.1395\n",
      "2025-06-01 14:51:24 [INFO]: epoch 388: training loss 0.1494\n",
      "2025-06-01 14:51:24 [INFO]: epoch 389: training loss 0.1347\n",
      "2025-06-01 14:51:24 [INFO]: epoch 390: training loss 0.1312\n",
      "2025-06-01 14:51:24 [INFO]: epoch 391: training loss 0.1437\n",
      "2025-06-01 14:51:24 [INFO]: epoch 392: training loss 0.1436\n",
      "2025-06-01 14:51:24 [INFO]: epoch 393: training loss 0.1403\n",
      "2025-06-01 14:51:24 [INFO]: epoch 394: training loss 0.1481\n",
      "2025-06-01 14:51:24 [INFO]: epoch 395: training loss 0.1339\n",
      "2025-06-01 14:51:24 [INFO]: epoch 396: training loss 0.1301\n",
      "2025-06-01 14:51:24 [INFO]: epoch 397: training loss 0.1375\n",
      "2025-06-01 14:51:24 [INFO]: epoch 398: training loss 0.1446\n",
      "2025-06-01 14:51:24 [INFO]: epoch 399: training loss 0.1362\n",
      "2025-06-01 14:51:24 [INFO]: Finished training.\n",
      "2025-06-01 14:51:24 [WARNING]: 🚨DeprecationWarning: The method impute is deprecated. Please use `predict` instead.\n",
      " 50%|██████████████████████████████████████████                                          | 3/6 [00:16<00:16,  5.37s/it]2025-06-01 14:51:24 [INFO]: No given device, using default device: cuda\n",
      "2025-06-01 14:51:24 [WARNING]: saving_path not given. Model files and tensorboard file will not be saved.\n",
      "2025-06-01 14:51:24 [INFO]: Model initialized successfully with the number of trainable parameters: 597,780\n",
      "2025-06-01 14:51:24 [INFO]: epoch 0: training loss 0.8536\n",
      "2025-06-01 14:51:24 [INFO]: epoch 1: training loss 0.5180\n",
      "2025-06-01 14:51:24 [INFO]: epoch 2: training loss 0.6672\n",
      "2025-06-01 14:51:24 [INFO]: epoch 3: training loss 0.5726\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-01 14:51:24 [INFO]: epoch 4: training loss 0.5420\n",
      "2025-06-01 14:51:24 [INFO]: epoch 5: training loss 0.4611\n",
      "2025-06-01 14:51:24 [INFO]: epoch 6: training loss 0.5449\n",
      "2025-06-01 14:51:24 [INFO]: epoch 7: training loss 0.5202\n",
      "2025-06-01 14:51:24 [INFO]: epoch 8: training loss 0.3785\n",
      "2025-06-01 14:51:24 [INFO]: epoch 9: training loss 0.3330\n",
      "2025-06-01 14:51:24 [INFO]: epoch 10: training loss 0.3366\n",
      "2025-06-01 14:51:24 [INFO]: epoch 11: training loss 0.3116\n",
      "2025-06-01 14:51:24 [INFO]: epoch 12: training loss 0.3328\n",
      "2025-06-01 14:51:24 [INFO]: epoch 13: training loss 0.3248\n",
      "2025-06-01 14:51:24 [INFO]: epoch 14: training loss 0.3125\n",
      "2025-06-01 14:51:24 [INFO]: epoch 15: training loss 0.2702\n",
      "2025-06-01 14:51:24 [INFO]: epoch 16: training loss 0.2252\n",
      "2025-06-01 14:51:24 [INFO]: epoch 17: training loss 0.2461\n",
      "2025-06-01 14:51:24 [INFO]: epoch 18: training loss 0.2646\n",
      "2025-06-01 14:51:24 [INFO]: epoch 19: training loss 0.2765\n",
      "2025-06-01 14:51:25 [INFO]: epoch 20: training loss 0.2615\n",
      "2025-06-01 14:51:25 [INFO]: epoch 21: training loss 0.2096\n",
      "2025-06-01 14:51:25 [INFO]: epoch 22: training loss 0.2113\n",
      "2025-06-01 14:51:25 [INFO]: epoch 23: training loss 0.2214\n",
      "2025-06-01 14:51:25 [INFO]: epoch 24: training loss 0.2213\n",
      "2025-06-01 14:51:25 [INFO]: epoch 25: training loss 0.2378\n",
      "2025-06-01 14:51:25 [INFO]: epoch 26: training loss 0.2403\n",
      "2025-06-01 14:51:25 [INFO]: epoch 27: training loss 0.2046\n",
      "2025-06-01 14:51:25 [INFO]: epoch 28: training loss 0.1871\n",
      "2025-06-01 14:51:25 [INFO]: epoch 29: training loss 0.1908\n",
      "2025-06-01 14:51:25 [INFO]: epoch 30: training loss 0.2067\n",
      "2025-06-01 14:51:25 [INFO]: epoch 31: training loss 0.1909\n",
      "2025-06-01 14:51:25 [INFO]: epoch 32: training loss 0.1953\n",
      "2025-06-01 14:51:25 [INFO]: epoch 33: training loss 0.1746\n",
      "2025-06-01 14:51:25 [INFO]: epoch 34: training loss 0.1801\n",
      "2025-06-01 14:51:25 [INFO]: epoch 35: training loss 0.1601\n",
      "2025-06-01 14:51:25 [INFO]: epoch 36: training loss 0.1527\n",
      "2025-06-01 14:51:25 [INFO]: epoch 37: training loss 0.1775\n",
      "2025-06-01 14:51:25 [INFO]: epoch 38: training loss 0.1732\n",
      "2025-06-01 14:51:25 [INFO]: epoch 39: training loss 0.1716\n",
      "2025-06-01 14:51:25 [INFO]: epoch 40: training loss 0.1627\n",
      "2025-06-01 14:51:25 [INFO]: epoch 41: training loss 0.1525\n",
      "2025-06-01 14:51:25 [INFO]: epoch 42: training loss 0.1714\n",
      "2025-06-01 14:51:25 [INFO]: epoch 43: training loss 0.1584\n",
      "2025-06-01 14:51:25 [INFO]: epoch 44: training loss 0.1509\n",
      "2025-06-01 14:51:25 [INFO]: epoch 45: training loss 0.1684\n",
      "2025-06-01 14:51:25 [INFO]: epoch 46: training loss 0.1421\n",
      "2025-06-01 14:51:25 [INFO]: epoch 47: training loss 0.1496\n",
      "2025-06-01 14:51:25 [INFO]: epoch 48: training loss 0.1587\n",
      "2025-06-01 14:51:25 [INFO]: epoch 49: training loss 0.1352\n",
      "2025-06-01 14:51:25 [INFO]: epoch 50: training loss 0.1504\n",
      "2025-06-01 14:51:25 [INFO]: epoch 51: training loss 0.1607\n",
      "2025-06-01 14:51:25 [INFO]: epoch 52: training loss 0.1484\n",
      "2025-06-01 14:51:25 [INFO]: epoch 53: training loss 0.1471\n",
      "2025-06-01 14:51:25 [INFO]: epoch 54: training loss 0.1855\n",
      "2025-06-01 14:51:25 [INFO]: epoch 55: training loss 0.1550\n",
      "2025-06-01 14:51:25 [INFO]: epoch 56: training loss 0.1351\n",
      "2025-06-01 14:51:25 [INFO]: epoch 57: training loss 0.1360\n",
      "2025-06-01 14:51:25 [INFO]: epoch 58: training loss 0.1414\n",
      "2025-06-01 14:51:25 [INFO]: epoch 59: training loss 0.1387\n",
      "2025-06-01 14:51:25 [INFO]: epoch 60: training loss 0.1463\n",
      "2025-06-01 14:51:25 [INFO]: epoch 61: training loss 0.1404\n",
      "2025-06-01 14:51:25 [INFO]: epoch 62: training loss 0.1307\n",
      "2025-06-01 14:51:25 [INFO]: epoch 63: training loss 0.1410\n",
      "2025-06-01 14:51:25 [INFO]: epoch 64: training loss 0.1332\n",
      "2025-06-01 14:51:25 [INFO]: epoch 65: training loss 0.1191\n",
      "2025-06-01 14:51:25 [INFO]: epoch 66: training loss 0.1299\n",
      "2025-06-01 14:51:25 [INFO]: epoch 67: training loss 0.1365\n",
      "2025-06-01 14:51:25 [INFO]: epoch 68: training loss 0.1560\n",
      "2025-06-01 14:51:25 [INFO]: epoch 69: training loss 0.1454\n",
      "2025-06-01 14:51:25 [INFO]: epoch 70: training loss 0.1282\n",
      "2025-06-01 14:51:25 [INFO]: epoch 71: training loss 0.1499\n",
      "2025-06-01 14:51:25 [INFO]: epoch 72: training loss 0.1219\n",
      "2025-06-01 14:51:25 [INFO]: epoch 73: training loss 0.1330\n",
      "2025-06-01 14:51:25 [INFO]: epoch 74: training loss 0.1450\n",
      "2025-06-01 14:51:25 [INFO]: epoch 75: training loss 0.1250\n",
      "2025-06-01 14:51:25 [INFO]: epoch 76: training loss 0.1213\n",
      "2025-06-01 14:51:25 [INFO]: epoch 77: training loss 0.1335\n",
      "2025-06-01 14:51:25 [INFO]: epoch 78: training loss 0.1370\n",
      "2025-06-01 14:51:25 [INFO]: epoch 79: training loss 0.1304\n",
      "2025-06-01 14:51:25 [INFO]: epoch 80: training loss 0.1190\n",
      "2025-06-01 14:51:25 [INFO]: epoch 81: training loss 0.1243\n",
      "2025-06-01 14:51:25 [INFO]: epoch 82: training loss 0.1266\n",
      "2025-06-01 14:51:25 [INFO]: epoch 83: training loss 0.1366\n",
      "2025-06-01 14:51:25 [INFO]: epoch 84: training loss 0.1111\n",
      "2025-06-01 14:51:25 [INFO]: epoch 85: training loss 0.1224\n",
      "2025-06-01 14:51:25 [INFO]: epoch 86: training loss 0.1023\n",
      "2025-06-01 14:51:25 [INFO]: epoch 87: training loss 0.1249\n",
      "2025-06-01 14:51:25 [INFO]: epoch 88: training loss 0.1158\n",
      "2025-06-01 14:51:25 [INFO]: epoch 89: training loss 0.1236\n",
      "2025-06-01 14:51:25 [INFO]: epoch 90: training loss 0.1219\n",
      "2025-06-01 14:51:25 [INFO]: epoch 91: training loss 0.1189\n",
      "2025-06-01 14:51:25 [INFO]: epoch 92: training loss 0.1068\n",
      "2025-06-01 14:51:25 [INFO]: epoch 93: training loss 0.1123\n",
      "2025-06-01 14:51:25 [INFO]: epoch 94: training loss 0.1265\n",
      "2025-06-01 14:51:25 [INFO]: epoch 95: training loss 0.0923\n",
      "2025-06-01 14:51:26 [INFO]: epoch 96: training loss 0.1059\n",
      "2025-06-01 14:51:26 [INFO]: epoch 97: training loss 0.0996\n",
      "2025-06-01 14:51:26 [INFO]: epoch 98: training loss 0.0938\n",
      "2025-06-01 14:51:26 [INFO]: epoch 99: training loss 0.0952\n",
      "2025-06-01 14:51:26 [INFO]: epoch 100: training loss 0.1030\n",
      "2025-06-01 14:51:26 [INFO]: epoch 101: training loss 0.0952\n",
      "2025-06-01 14:51:26 [INFO]: epoch 102: training loss 0.0931\n",
      "2025-06-01 14:51:26 [INFO]: epoch 103: training loss 0.1068\n",
      "2025-06-01 14:51:26 [INFO]: epoch 104: training loss 0.0971\n",
      "2025-06-01 14:51:26 [INFO]: epoch 105: training loss 0.0863\n",
      "2025-06-01 14:51:26 [INFO]: epoch 106: training loss 0.1062\n",
      "2025-06-01 14:51:26 [INFO]: epoch 107: training loss 0.0972\n",
      "2025-06-01 14:51:26 [INFO]: epoch 108: training loss 0.0917\n",
      "2025-06-01 14:51:26 [INFO]: epoch 109: training loss 0.1088\n",
      "2025-06-01 14:51:26 [INFO]: epoch 110: training loss 0.0921\n",
      "2025-06-01 14:51:26 [INFO]: epoch 111: training loss 0.1134\n",
      "2025-06-01 14:51:26 [INFO]: epoch 112: training loss 0.1158\n",
      "2025-06-01 14:51:26 [INFO]: epoch 113: training loss 0.1122\n",
      "2025-06-01 14:51:26 [INFO]: epoch 114: training loss 0.0996\n",
      "2025-06-01 14:51:26 [INFO]: epoch 115: training loss 0.1075\n",
      "2025-06-01 14:51:26 [INFO]: epoch 116: training loss 0.0953\n",
      "2025-06-01 14:51:26 [INFO]: epoch 117: training loss 0.1094\n",
      "2025-06-01 14:51:26 [INFO]: epoch 118: training loss 0.0977\n",
      "2025-06-01 14:51:26 [INFO]: epoch 119: training loss 0.0921\n",
      "2025-06-01 14:51:26 [INFO]: epoch 120: training loss 0.0806\n",
      "2025-06-01 14:51:26 [INFO]: epoch 121: training loss 0.1005\n",
      "2025-06-01 14:51:26 [INFO]: epoch 122: training loss 0.0944\n",
      "2025-06-01 14:51:26 [INFO]: epoch 123: training loss 0.1041\n",
      "2025-06-01 14:51:26 [INFO]: epoch 124: training loss 0.0946\n",
      "2025-06-01 14:51:26 [INFO]: epoch 125: training loss 0.0828\n",
      "2025-06-01 14:51:26 [INFO]: epoch 126: training loss 0.1047\n",
      "2025-06-01 14:51:26 [INFO]: epoch 127: training loss 0.0775\n",
      "2025-06-01 14:51:26 [INFO]: epoch 128: training loss 0.0876\n",
      "2025-06-01 14:51:26 [INFO]: epoch 129: training loss 0.1001\n",
      "2025-06-01 14:51:26 [INFO]: epoch 130: training loss 0.0835\n",
      "2025-06-01 14:51:26 [INFO]: epoch 131: training loss 0.0883\n",
      "2025-06-01 14:51:26 [INFO]: epoch 132: training loss 0.0976\n",
      "2025-06-01 14:51:26 [INFO]: epoch 133: training loss 0.0927\n",
      "2025-06-01 14:51:26 [INFO]: epoch 134: training loss 0.0849\n",
      "2025-06-01 14:51:26 [INFO]: epoch 135: training loss 0.0828\n",
      "2025-06-01 14:51:26 [INFO]: epoch 136: training loss 0.0862\n",
      "2025-06-01 14:51:26 [INFO]: epoch 137: training loss 0.0731\n",
      "2025-06-01 14:51:26 [INFO]: epoch 138: training loss 0.0981\n",
      "2025-06-01 14:51:26 [INFO]: epoch 139: training loss 0.0861\n",
      "2025-06-01 14:51:26 [INFO]: epoch 140: training loss 0.0636\n",
      "2025-06-01 14:51:26 [INFO]: epoch 141: training loss 0.0802\n",
      "2025-06-01 14:51:26 [INFO]: epoch 142: training loss 0.0793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-01 14:51:26 [INFO]: epoch 143: training loss 0.0770\n",
      "2025-06-01 14:51:26 [INFO]: epoch 144: training loss 0.0961\n",
      "2025-06-01 14:51:26 [INFO]: epoch 145: training loss 0.0950\n",
      "2025-06-01 14:51:26 [INFO]: epoch 146: training loss 0.1003\n",
      "2025-06-01 14:51:26 [INFO]: epoch 147: training loss 0.0758\n",
      "2025-06-01 14:51:26 [INFO]: epoch 148: training loss 0.0813\n",
      "2025-06-01 14:51:26 [INFO]: epoch 149: training loss 0.0902\n",
      "2025-06-01 14:51:26 [INFO]: epoch 150: training loss 0.0823\n",
      "2025-06-01 14:51:26 [INFO]: epoch 151: training loss 0.0805\n",
      "2025-06-01 14:51:26 [INFO]: epoch 152: training loss 0.0921\n",
      "2025-06-01 14:51:26 [INFO]: epoch 153: training loss 0.0706\n",
      "2025-06-01 14:51:26 [INFO]: epoch 154: training loss 0.0720\n",
      "2025-06-01 14:51:26 [INFO]: epoch 155: training loss 0.0907\n",
      "2025-06-01 14:51:26 [INFO]: epoch 156: training loss 0.0891\n",
      "2025-06-01 14:51:26 [INFO]: epoch 157: training loss 0.0785\n",
      "2025-06-01 14:51:26 [INFO]: epoch 158: training loss 0.0700\n",
      "2025-06-01 14:51:26 [INFO]: epoch 159: training loss 0.0892\n",
      "2025-06-01 14:51:26 [INFO]: epoch 160: training loss 0.0730\n",
      "2025-06-01 14:51:26 [INFO]: epoch 161: training loss 0.0722\n",
      "2025-06-01 14:51:26 [INFO]: epoch 162: training loss 0.0747\n",
      "2025-06-01 14:51:26 [INFO]: epoch 163: training loss 0.1041\n",
      "2025-06-01 14:51:26 [INFO]: epoch 164: training loss 0.0840\n",
      "2025-06-01 14:51:26 [INFO]: epoch 165: training loss 0.0660\n",
      "2025-06-01 14:51:26 [INFO]: epoch 166: training loss 0.0755\n",
      "2025-06-01 14:51:26 [INFO]: epoch 167: training loss 0.0815\n",
      "2025-06-01 14:51:26 [INFO]: epoch 168: training loss 0.0983\n",
      "2025-06-01 14:51:26 [INFO]: epoch 169: training loss 0.0789\n",
      "2025-06-01 14:51:26 [INFO]: epoch 170: training loss 0.0725\n",
      "2025-06-01 14:51:26 [INFO]: epoch 171: training loss 0.0596\n",
      "2025-06-01 14:51:27 [INFO]: epoch 172: training loss 0.0805\n",
      "2025-06-01 14:51:27 [INFO]: epoch 173: training loss 0.0799\n",
      "2025-06-01 14:51:27 [INFO]: epoch 174: training loss 0.0640\n",
      "2025-06-01 14:51:27 [INFO]: epoch 175: training loss 0.0713\n",
      "2025-06-01 14:51:27 [INFO]: epoch 176: training loss 0.0720\n",
      "2025-06-01 14:51:27 [INFO]: epoch 177: training loss 0.0607\n",
      "2025-06-01 14:51:27 [INFO]: epoch 178: training loss 0.0680\n",
      "2025-06-01 14:51:27 [INFO]: epoch 179: training loss 0.0702\n",
      "2025-06-01 14:51:27 [INFO]: epoch 180: training loss 0.0650\n",
      "2025-06-01 14:51:27 [INFO]: epoch 181: training loss 0.0732\n",
      "2025-06-01 14:51:27 [INFO]: epoch 182: training loss 0.0696\n",
      "2025-06-01 14:51:27 [INFO]: epoch 183: training loss 0.0701\n",
      "2025-06-01 14:51:27 [INFO]: epoch 184: training loss 0.0594\n",
      "2025-06-01 14:51:27 [INFO]: epoch 185: training loss 0.0695\n",
      "2025-06-01 14:51:27 [INFO]: epoch 186: training loss 0.0768\n",
      "2025-06-01 14:51:27 [INFO]: epoch 187: training loss 0.0656\n",
      "2025-06-01 14:51:27 [INFO]: epoch 188: training loss 0.0601\n",
      "2025-06-01 14:51:27 [INFO]: epoch 189: training loss 0.0634\n",
      "2025-06-01 14:51:27 [INFO]: epoch 190: training loss 0.0611\n",
      "2025-06-01 14:51:27 [INFO]: epoch 191: training loss 0.0694\n",
      "2025-06-01 14:51:27 [INFO]: epoch 192: training loss 0.0557\n",
      "2025-06-01 14:51:27 [INFO]: epoch 193: training loss 0.0709\n",
      "2025-06-01 14:51:27 [INFO]: epoch 194: training loss 0.0837\n",
      "2025-06-01 14:51:27 [INFO]: epoch 195: training loss 0.0579\n",
      "2025-06-01 14:51:27 [INFO]: epoch 196: training loss 0.0559\n",
      "2025-06-01 14:51:27 [INFO]: epoch 197: training loss 0.0634\n",
      "2025-06-01 14:51:27 [INFO]: epoch 198: training loss 0.0641\n",
      "2025-06-01 14:51:27 [INFO]: epoch 199: training loss 0.0488\n",
      "2025-06-01 14:51:27 [INFO]: epoch 200: training loss 0.0565\n",
      "2025-06-01 14:51:27 [INFO]: epoch 201: training loss 0.0619\n",
      "2025-06-01 14:51:27 [INFO]: epoch 202: training loss 0.0536\n",
      "2025-06-01 14:51:27 [INFO]: epoch 203: training loss 0.0505\n",
      "2025-06-01 14:51:27 [INFO]: epoch 204: training loss 0.0531\n",
      "2025-06-01 14:51:27 [INFO]: epoch 205: training loss 0.0556\n",
      "2025-06-01 14:51:27 [INFO]: epoch 206: training loss 0.0480\n",
      "2025-06-01 14:51:27 [INFO]: epoch 207: training loss 0.0541\n",
      "2025-06-01 14:51:27 [INFO]: epoch 208: training loss 0.0518\n",
      "2025-06-01 14:51:27 [INFO]: epoch 209: training loss 0.0508\n",
      "2025-06-01 14:51:27 [INFO]: epoch 210: training loss 0.0503\n",
      "2025-06-01 14:51:27 [INFO]: epoch 211: training loss 0.0514\n",
      "2025-06-01 14:51:27 [INFO]: epoch 212: training loss 0.0561\n",
      "2025-06-01 14:51:27 [INFO]: epoch 213: training loss 0.0554\n",
      "2025-06-01 14:51:27 [INFO]: epoch 214: training loss 0.0549\n",
      "2025-06-01 14:51:27 [INFO]: epoch 215: training loss 0.0636\n",
      "2025-06-01 14:51:27 [INFO]: epoch 216: training loss 0.0477\n",
      "2025-06-01 14:51:27 [INFO]: epoch 217: training loss 0.0479\n",
      "2025-06-01 14:51:27 [INFO]: epoch 218: training loss 0.0536\n",
      "2025-06-01 14:51:27 [INFO]: epoch 219: training loss 0.0544\n",
      "2025-06-01 14:51:27 [INFO]: epoch 220: training loss 0.0612\n",
      "2025-06-01 14:51:27 [INFO]: epoch 221: training loss 0.0443\n",
      "2025-06-01 14:51:27 [INFO]: epoch 222: training loss 0.0654\n",
      "2025-06-01 14:51:27 [INFO]: epoch 223: training loss 0.0491\n",
      "2025-06-01 14:51:27 [INFO]: epoch 224: training loss 0.0488\n",
      "2025-06-01 14:51:27 [INFO]: epoch 225: training loss 0.0578\n",
      "2025-06-01 14:51:27 [INFO]: epoch 226: training loss 0.0772\n",
      "2025-06-01 14:51:27 [INFO]: epoch 227: training loss 0.0651\n",
      "2025-06-01 14:51:27 [INFO]: epoch 228: training loss 0.0445\n",
      "2025-06-01 14:51:27 [INFO]: epoch 229: training loss 0.0580\n",
      "2025-06-01 14:51:27 [INFO]: epoch 230: training loss 0.0641\n",
      "2025-06-01 14:51:27 [INFO]: epoch 231: training loss 0.0452\n",
      "2025-06-01 14:51:27 [INFO]: epoch 232: training loss 0.0525\n",
      "2025-06-01 14:51:27 [INFO]: epoch 233: training loss 0.0679\n",
      "2025-06-01 14:51:27 [INFO]: epoch 234: training loss 0.0460\n",
      "2025-06-01 14:51:27 [INFO]: epoch 235: training loss 0.0480\n",
      "2025-06-01 14:51:27 [INFO]: epoch 236: training loss 0.0680\n",
      "2025-06-01 14:51:27 [INFO]: epoch 237: training loss 0.0533\n",
      "2025-06-01 14:51:27 [INFO]: epoch 238: training loss 0.0425\n",
      "2025-06-01 14:51:27 [INFO]: epoch 239: training loss 0.0516\n",
      "2025-06-01 14:51:27 [INFO]: epoch 240: training loss 0.0474\n",
      "2025-06-01 14:51:27 [INFO]: epoch 241: training loss 0.0438\n",
      "2025-06-01 14:51:27 [INFO]: epoch 242: training loss 0.0402\n",
      "2025-06-01 14:51:27 [INFO]: epoch 243: training loss 0.0518\n",
      "2025-06-01 14:51:27 [INFO]: epoch 244: training loss 0.0533\n",
      "2025-06-01 14:51:27 [INFO]: epoch 245: training loss 0.0537\n",
      "2025-06-01 14:51:27 [INFO]: epoch 246: training loss 0.0420\n",
      "2025-06-01 14:51:27 [INFO]: epoch 247: training loss 0.0455\n",
      "2025-06-01 14:51:28 [INFO]: epoch 248: training loss 0.0513\n",
      "2025-06-01 14:51:28 [INFO]: epoch 249: training loss 0.0387\n",
      "2025-06-01 14:51:28 [INFO]: epoch 250: training loss 0.0491\n",
      "2025-06-01 14:51:28 [INFO]: epoch 251: training loss 0.0430\n",
      "2025-06-01 14:51:28 [INFO]: epoch 252: training loss 0.0447\n",
      "2025-06-01 14:51:28 [INFO]: epoch 253: training loss 0.0576\n",
      "2025-06-01 14:51:28 [INFO]: epoch 254: training loss 0.0404\n",
      "2025-06-01 14:51:28 [INFO]: epoch 255: training loss 0.0444\n",
      "2025-06-01 14:51:28 [INFO]: epoch 256: training loss 0.0411\n",
      "2025-06-01 14:51:28 [INFO]: epoch 257: training loss 0.0428\n",
      "2025-06-01 14:51:28 [INFO]: epoch 258: training loss 0.0500\n",
      "2025-06-01 14:51:28 [INFO]: epoch 259: training loss 0.0410\n",
      "2025-06-01 14:51:28 [INFO]: epoch 260: training loss 0.0308\n",
      "2025-06-01 14:51:28 [INFO]: epoch 261: training loss 0.0400\n",
      "2025-06-01 14:51:28 [INFO]: epoch 262: training loss 0.0432\n",
      "2025-06-01 14:51:28 [INFO]: epoch 263: training loss 0.0464\n",
      "2025-06-01 14:51:28 [INFO]: epoch 264: training loss 0.0362\n",
      "2025-06-01 14:51:28 [INFO]: epoch 265: training loss 0.0400\n",
      "2025-06-01 14:51:28 [INFO]: epoch 266: training loss 0.0398\n",
      "2025-06-01 14:51:28 [INFO]: epoch 267: training loss 0.0412\n",
      "2025-06-01 14:51:28 [INFO]: epoch 268: training loss 0.0421\n",
      "2025-06-01 14:51:28 [INFO]: epoch 269: training loss 0.0452\n",
      "2025-06-01 14:51:28 [INFO]: epoch 270: training loss 0.0412\n",
      "2025-06-01 14:51:28 [INFO]: epoch 271: training loss 0.0416\n",
      "2025-06-01 14:51:28 [INFO]: epoch 272: training loss 0.0459\n",
      "2025-06-01 14:51:28 [INFO]: epoch 273: training loss 0.0399\n",
      "2025-06-01 14:51:28 [INFO]: epoch 274: training loss 0.0497\n",
      "2025-06-01 14:51:28 [INFO]: epoch 275: training loss 0.0548\n",
      "2025-06-01 14:51:28 [INFO]: epoch 276: training loss 0.0324\n",
      "2025-06-01 14:51:28 [INFO]: epoch 277: training loss 0.0331\n",
      "2025-06-01 14:51:28 [INFO]: epoch 278: training loss 0.0417\n",
      "2025-06-01 14:51:28 [INFO]: epoch 279: training loss 0.0383\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-01 14:51:28 [INFO]: epoch 280: training loss 0.0398\n",
      "2025-06-01 14:51:28 [INFO]: epoch 281: training loss 0.0324\n",
      "2025-06-01 14:51:28 [INFO]: epoch 282: training loss 0.0333\n",
      "2025-06-01 14:51:28 [INFO]: epoch 283: training loss 0.0298\n",
      "2025-06-01 14:51:28 [INFO]: epoch 284: training loss 0.0325\n",
      "2025-06-01 14:51:28 [INFO]: epoch 285: training loss 0.0333\n",
      "2025-06-01 14:51:28 [INFO]: epoch 286: training loss 0.0313\n",
      "2025-06-01 14:51:28 [INFO]: epoch 287: training loss 0.0331\n",
      "2025-06-01 14:51:28 [INFO]: epoch 288: training loss 0.0336\n",
      "2025-06-01 14:51:28 [INFO]: epoch 289: training loss 0.0391\n",
      "2025-06-01 14:51:28 [INFO]: epoch 290: training loss 0.0383\n",
      "2025-06-01 14:51:28 [INFO]: epoch 291: training loss 0.0339\n",
      "2025-06-01 14:51:28 [INFO]: epoch 292: training loss 0.0450\n",
      "2025-06-01 14:51:28 [INFO]: epoch 293: training loss 0.0345\n",
      "2025-06-01 14:51:28 [INFO]: epoch 294: training loss 0.0406\n",
      "2025-06-01 14:51:28 [INFO]: epoch 295: training loss 0.0396\n",
      "2025-06-01 14:51:28 [INFO]: epoch 296: training loss 0.0383\n",
      "2025-06-01 14:51:28 [INFO]: epoch 297: training loss 0.0386\n",
      "2025-06-01 14:51:28 [INFO]: epoch 298: training loss 0.0330\n",
      "2025-06-01 14:51:28 [INFO]: epoch 299: training loss 0.0346\n",
      "2025-06-01 14:51:28 [INFO]: epoch 300: training loss 0.0362\n",
      "2025-06-01 14:51:28 [INFO]: epoch 301: training loss 0.0359\n",
      "2025-06-01 14:51:28 [INFO]: epoch 302: training loss 0.0434\n",
      "2025-06-01 14:51:28 [INFO]: epoch 303: training loss 0.0374\n",
      "2025-06-01 14:51:28 [INFO]: epoch 304: training loss 0.0361\n",
      "2025-06-01 14:51:28 [INFO]: epoch 305: training loss 0.0437\n",
      "2025-06-01 14:51:28 [INFO]: epoch 306: training loss 0.0417\n",
      "2025-06-01 14:51:28 [INFO]: epoch 307: training loss 0.0410\n",
      "2025-06-01 14:51:28 [INFO]: epoch 308: training loss 0.0366\n",
      "2025-06-01 14:51:28 [INFO]: epoch 309: training loss 0.0507\n",
      "2025-06-01 14:51:28 [INFO]: epoch 310: training loss 0.0416\n",
      "2025-06-01 14:51:28 [INFO]: epoch 311: training loss 0.0374\n",
      "2025-06-01 14:51:28 [INFO]: epoch 312: training loss 0.0377\n",
      "2025-06-01 14:51:28 [INFO]: epoch 313: training loss 0.0400\n",
      "2025-06-01 14:51:28 [INFO]: epoch 314: training loss 0.0305\n",
      "2025-06-01 14:51:28 [INFO]: epoch 315: training loss 0.0437\n",
      "2025-06-01 14:51:28 [INFO]: epoch 316: training loss 0.0458\n",
      "2025-06-01 14:51:28 [INFO]: epoch 317: training loss 0.0305\n",
      "2025-06-01 14:51:28 [INFO]: epoch 318: training loss 0.0519\n",
      "2025-06-01 14:51:28 [INFO]: epoch 319: training loss 0.0455\n",
      "2025-06-01 14:51:28 [INFO]: epoch 320: training loss 0.0394\n",
      "2025-06-01 14:51:28 [INFO]: epoch 321: training loss 0.0391\n",
      "2025-06-01 14:51:28 [INFO]: epoch 322: training loss 0.0447\n",
      "2025-06-01 14:51:28 [INFO]: epoch 323: training loss 0.0425\n",
      "2025-06-01 14:51:28 [INFO]: epoch 324: training loss 0.0411\n",
      "2025-06-01 14:51:29 [INFO]: epoch 325: training loss 0.0440\n",
      "2025-06-01 14:51:29 [INFO]: epoch 326: training loss 0.0396\n",
      "2025-06-01 14:51:29 [INFO]: epoch 327: training loss 0.0412\n",
      "2025-06-01 14:51:29 [INFO]: epoch 328: training loss 0.0432\n",
      "2025-06-01 14:51:29 [INFO]: epoch 329: training loss 0.0504\n",
      "2025-06-01 14:51:29 [INFO]: epoch 330: training loss 0.0509\n",
      "2025-06-01 14:51:29 [INFO]: epoch 331: training loss 0.0384\n",
      "2025-06-01 14:51:29 [INFO]: epoch 332: training loss 0.0308\n",
      "2025-06-01 14:51:29 [INFO]: epoch 333: training loss 0.0455\n",
      "2025-06-01 14:51:29 [INFO]: epoch 334: training loss 0.0373\n",
      "2025-06-01 14:51:29 [INFO]: epoch 335: training loss 0.0328\n",
      "2025-06-01 14:51:29 [INFO]: epoch 336: training loss 0.0434\n",
      "2025-06-01 14:51:29 [INFO]: epoch 337: training loss 0.0373\n",
      "2025-06-01 14:51:29 [INFO]: epoch 338: training loss 0.0324\n",
      "2025-06-01 14:51:29 [INFO]: epoch 339: training loss 0.0393\n",
      "2025-06-01 14:51:29 [INFO]: epoch 340: training loss 0.0326\n",
      "2025-06-01 14:51:29 [INFO]: epoch 341: training loss 0.0332\n",
      "2025-06-01 14:51:29 [INFO]: epoch 342: training loss 0.0362\n",
      "2025-06-01 14:51:29 [INFO]: epoch 343: training loss 0.0301\n",
      "2025-06-01 14:51:29 [INFO]: epoch 344: training loss 0.0327\n",
      "2025-06-01 14:51:29 [INFO]: epoch 345: training loss 0.0286\n",
      "2025-06-01 14:51:29 [INFO]: epoch 346: training loss 0.0279\n",
      "2025-06-01 14:51:29 [INFO]: epoch 347: training loss 0.0300\n",
      "2025-06-01 14:51:29 [INFO]: epoch 348: training loss 0.0313\n",
      "2025-06-01 14:51:29 [INFO]: epoch 349: training loss 0.0290\n",
      "2025-06-01 14:51:29 [INFO]: epoch 350: training loss 0.0258\n",
      "2025-06-01 14:51:29 [INFO]: epoch 351: training loss 0.0268\n",
      "2025-06-01 14:51:29 [INFO]: epoch 352: training loss 0.0300\n",
      "2025-06-01 14:51:29 [INFO]: epoch 353: training loss 0.0290\n",
      "2025-06-01 14:51:29 [INFO]: epoch 354: training loss 0.0344\n",
      "2025-06-01 14:51:29 [INFO]: epoch 355: training loss 0.0263\n",
      "2025-06-01 14:51:29 [INFO]: epoch 356: training loss 0.0316\n",
      "2025-06-01 14:51:29 [INFO]: epoch 357: training loss 0.0355\n",
      "2025-06-01 14:51:29 [INFO]: epoch 358: training loss 0.0289\n",
      "2025-06-01 14:51:29 [INFO]: epoch 359: training loss 0.0371\n",
      "2025-06-01 14:51:29 [INFO]: epoch 360: training loss 0.0329\n",
      "2025-06-01 14:51:29 [INFO]: epoch 361: training loss 0.0291\n",
      "2025-06-01 14:51:29 [INFO]: epoch 362: training loss 0.0369\n",
      "2025-06-01 14:51:29 [INFO]: epoch 363: training loss 0.0292\n",
      "2025-06-01 14:51:29 [INFO]: epoch 364: training loss 0.0322\n",
      "2025-06-01 14:51:29 [INFO]: epoch 365: training loss 0.0302\n",
      "2025-06-01 14:51:29 [INFO]: epoch 366: training loss 0.0351\n",
      "2025-06-01 14:51:29 [INFO]: epoch 367: training loss 0.0371\n",
      "2025-06-01 14:51:29 [INFO]: epoch 368: training loss 0.0359\n",
      "2025-06-01 14:51:29 [INFO]: epoch 369: training loss 0.0389\n",
      "2025-06-01 14:51:29 [INFO]: epoch 370: training loss 0.0322\n",
      "2025-06-01 14:51:29 [INFO]: epoch 371: training loss 0.0367\n",
      "2025-06-01 14:51:29 [INFO]: epoch 372: training loss 0.0287\n",
      "2025-06-01 14:51:29 [INFO]: epoch 373: training loss 0.0361\n",
      "2025-06-01 14:51:29 [INFO]: epoch 374: training loss 0.0303\n",
      "2025-06-01 14:51:29 [INFO]: epoch 375: training loss 0.0379\n",
      "2025-06-01 14:51:29 [INFO]: epoch 376: training loss 0.0304\n",
      "2025-06-01 14:51:29 [INFO]: epoch 377: training loss 0.0335\n",
      "2025-06-01 14:51:29 [INFO]: epoch 378: training loss 0.0331\n",
      "2025-06-01 14:51:29 [INFO]: epoch 379: training loss 0.0295\n",
      "2025-06-01 14:51:29 [INFO]: epoch 380: training loss 0.0285\n",
      "2025-06-01 14:51:29 [INFO]: epoch 381: training loss 0.0286\n",
      "2025-06-01 14:51:29 [INFO]: epoch 382: training loss 0.0317\n",
      "2025-06-01 14:51:29 [INFO]: epoch 383: training loss 0.0267\n",
      "2025-06-01 14:51:29 [INFO]: epoch 384: training loss 0.0268\n",
      "2025-06-01 14:51:29 [INFO]: epoch 385: training loss 0.0332\n",
      "2025-06-01 14:51:29 [INFO]: epoch 386: training loss 0.0265\n",
      "2025-06-01 14:51:29 [INFO]: epoch 387: training loss 0.0298\n",
      "2025-06-01 14:51:29 [INFO]: epoch 388: training loss 0.0314\n",
      "2025-06-01 14:51:29 [INFO]: epoch 389: training loss 0.0255\n",
      "2025-06-01 14:51:29 [INFO]: epoch 390: training loss 0.0230\n",
      "2025-06-01 14:51:29 [INFO]: epoch 391: training loss 0.0265\n",
      "2025-06-01 14:51:29 [INFO]: epoch 392: training loss 0.0311\n",
      "2025-06-01 14:51:29 [INFO]: epoch 393: training loss 0.0242\n",
      "2025-06-01 14:51:29 [INFO]: epoch 394: training loss 0.0389\n",
      "2025-06-01 14:51:29 [INFO]: epoch 395: training loss 0.0282\n",
      "2025-06-01 14:51:29 [INFO]: epoch 396: training loss 0.0287\n",
      "2025-06-01 14:51:29 [INFO]: epoch 397: training loss 0.0335\n",
      "2025-06-01 14:51:29 [INFO]: epoch 398: training loss 0.0266\n",
      "2025-06-01 14:51:29 [INFO]: epoch 399: training loss 0.0265\n",
      "2025-06-01 14:51:29 [INFO]: Finished training.\n",
      "2025-06-01 14:51:29 [WARNING]: 🚨DeprecationWarning: The method impute is deprecated. Please use `predict` instead.\n",
      " 67%|████████████████████████████████████████████████████████                            | 4/6 [00:21<00:10,  5.36s/it]2025-06-01 14:51:30 [INFO]: No given device, using default device: cuda\n",
      "2025-06-01 14:51:30 [WARNING]: saving_path not given. Model files and tensorboard file will not be saved.\n",
      "2025-06-01 14:51:30 [INFO]: Model initialized successfully with the number of trainable parameters: 597,780\n",
      "2025-06-01 14:51:30 [INFO]: epoch 0: training loss 1.1144\n",
      "2025-06-01 14:51:30 [INFO]: epoch 1: training loss 0.5356\n",
      "2025-06-01 14:51:30 [INFO]: epoch 2: training loss 0.5963\n",
      "2025-06-01 14:51:30 [INFO]: epoch 3: training loss 0.5590\n",
      "2025-06-01 14:51:30 [INFO]: epoch 4: training loss 0.4955\n",
      "2025-06-01 14:51:30 [INFO]: epoch 5: training loss 0.4326\n",
      "2025-06-01 14:51:30 [INFO]: epoch 6: training loss 0.4349\n",
      "2025-06-01 14:51:30 [INFO]: epoch 7: training loss 0.3461\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-01 14:51:30 [INFO]: epoch 8: training loss 0.3236\n",
      "2025-06-01 14:51:30 [INFO]: epoch 9: training loss 0.3666\n",
      "2025-06-01 14:51:30 [INFO]: epoch 10: training loss 0.3157\n",
      "2025-06-01 14:51:30 [INFO]: epoch 11: training loss 0.2842\n",
      "2025-06-01 14:51:30 [INFO]: epoch 12: training loss 0.2639\n",
      "2025-06-01 14:51:30 [INFO]: epoch 13: training loss 0.2828\n",
      "2025-06-01 14:51:30 [INFO]: epoch 14: training loss 0.2494\n",
      "2025-06-01 14:51:30 [INFO]: epoch 15: training loss 0.2142\n",
      "2025-06-01 14:51:30 [INFO]: epoch 16: training loss 0.2847\n",
      "2025-06-01 14:51:30 [INFO]: epoch 17: training loss 0.2863\n",
      "2025-06-01 14:51:30 [INFO]: epoch 18: training loss 0.2388\n",
      "2025-06-01 14:51:30 [INFO]: epoch 19: training loss 0.2354\n",
      "2025-06-01 14:51:30 [INFO]: epoch 20: training loss 0.2949\n",
      "2025-06-01 14:51:30 [INFO]: epoch 21: training loss 0.2492\n",
      "2025-06-01 14:51:30 [INFO]: epoch 22: training loss 0.2147\n",
      "2025-06-01 14:51:30 [INFO]: epoch 23: training loss 0.2195\n",
      "2025-06-01 14:51:30 [INFO]: epoch 24: training loss 0.2090\n",
      "2025-06-01 14:51:30 [INFO]: epoch 25: training loss 0.2106\n",
      "2025-06-01 14:51:30 [INFO]: epoch 26: training loss 0.1979\n",
      "2025-06-01 14:51:30 [INFO]: epoch 27: training loss 0.1833\n",
      "2025-06-01 14:51:30 [INFO]: epoch 28: training loss 0.1994\n",
      "2025-06-01 14:51:30 [INFO]: epoch 29: training loss 0.1979\n",
      "2025-06-01 14:51:30 [INFO]: epoch 30: training loss 0.1742\n",
      "2025-06-01 14:51:30 [INFO]: epoch 31: training loss 0.1645\n",
      "2025-06-01 14:51:30 [INFO]: epoch 32: training loss 0.1679\n",
      "2025-06-01 14:51:30 [INFO]: epoch 33: training loss 0.1822\n",
      "2025-06-01 14:51:30 [INFO]: epoch 34: training loss 0.1630\n",
      "2025-06-01 14:51:30 [INFO]: epoch 35: training loss 0.1705\n",
      "2025-06-01 14:51:30 [INFO]: epoch 36: training loss 0.1850\n",
      "2025-06-01 14:51:30 [INFO]: epoch 37: training loss 0.1590\n",
      "2025-06-01 14:51:30 [INFO]: epoch 38: training loss 0.1679\n",
      "2025-06-01 14:51:30 [INFO]: epoch 39: training loss 0.1729\n",
      "2025-06-01 14:51:30 [INFO]: epoch 40: training loss 0.1690\n",
      "2025-06-01 14:51:30 [INFO]: epoch 41: training loss 0.1647\n",
      "2025-06-01 14:51:30 [INFO]: epoch 42: training loss 0.1688\n",
      "2025-06-01 14:51:30 [INFO]: epoch 43: training loss 0.1596\n",
      "2025-06-01 14:51:30 [INFO]: epoch 44: training loss 0.1527\n",
      "2025-06-01 14:51:30 [INFO]: epoch 45: training loss 0.1564\n",
      "2025-06-01 14:51:30 [INFO]: epoch 46: training loss 0.1693\n",
      "2025-06-01 14:51:30 [INFO]: epoch 47: training loss 0.1485\n",
      "2025-06-01 14:51:30 [INFO]: epoch 48: training loss 0.1464\n",
      "2025-06-01 14:51:30 [INFO]: epoch 49: training loss 0.1472\n",
      "2025-06-01 14:51:30 [INFO]: epoch 50: training loss 0.1510\n",
      "2025-06-01 14:51:30 [INFO]: epoch 51: training loss 0.1465\n",
      "2025-06-01 14:51:30 [INFO]: epoch 52: training loss 0.1327\n",
      "2025-06-01 14:51:30 [INFO]: epoch 53: training loss 0.1266\n",
      "2025-06-01 14:51:30 [INFO]: epoch 54: training loss 0.1430\n",
      "2025-06-01 14:51:30 [INFO]: epoch 55: training loss 0.1438\n",
      "2025-06-01 14:51:30 [INFO]: epoch 56: training loss 0.1479\n",
      "2025-06-01 14:51:30 [INFO]: epoch 57: training loss 0.1568\n",
      "2025-06-01 14:51:30 [INFO]: epoch 58: training loss 0.1235\n",
      "2025-06-01 14:51:30 [INFO]: epoch 59: training loss 0.1404\n",
      "2025-06-01 14:51:30 [INFO]: epoch 60: training loss 0.1370\n",
      "2025-06-01 14:51:30 [INFO]: epoch 61: training loss 0.1386\n",
      "2025-06-01 14:51:30 [INFO]: epoch 62: training loss 0.1384\n",
      "2025-06-01 14:51:30 [INFO]: epoch 63: training loss 0.1411\n",
      "2025-06-01 14:51:30 [INFO]: epoch 64: training loss 0.1321\n",
      "2025-06-01 14:51:30 [INFO]: epoch 65: training loss 0.1403\n",
      "2025-06-01 14:51:30 [INFO]: epoch 66: training loss 0.1271\n",
      "2025-06-01 14:51:30 [INFO]: epoch 67: training loss 0.1322\n",
      "2025-06-01 14:51:30 [INFO]: epoch 68: training loss 0.1242\n",
      "2025-06-01 14:51:30 [INFO]: epoch 69: training loss 0.1316\n",
      "2025-06-01 14:51:31 [INFO]: epoch 70: training loss 0.1321\n",
      "2025-06-01 14:51:31 [INFO]: epoch 71: training loss 0.1225\n",
      "2025-06-01 14:51:31 [INFO]: epoch 72: training loss 0.1318\n",
      "2025-06-01 14:51:31 [INFO]: epoch 73: training loss 0.1384\n",
      "2025-06-01 14:51:31 [INFO]: epoch 74: training loss 0.1320\n",
      "2025-06-01 14:51:31 [INFO]: epoch 75: training loss 0.1101\n",
      "2025-06-01 14:51:31 [INFO]: epoch 76: training loss 0.1267\n",
      "2025-06-01 14:51:31 [INFO]: epoch 77: training loss 0.1260\n",
      "2025-06-01 14:51:31 [INFO]: epoch 78: training loss 0.1119\n",
      "2025-06-01 14:51:31 [INFO]: epoch 79: training loss 0.1268\n",
      "2025-06-01 14:51:31 [INFO]: epoch 80: training loss 0.1147\n",
      "2025-06-01 14:51:31 [INFO]: epoch 81: training loss 0.1103\n",
      "2025-06-01 14:51:31 [INFO]: epoch 82: training loss 0.1099\n",
      "2025-06-01 14:51:31 [INFO]: epoch 83: training loss 0.1001\n",
      "2025-06-01 14:51:31 [INFO]: epoch 84: training loss 0.1071\n",
      "2025-06-01 14:51:31 [INFO]: epoch 85: training loss 0.1144\n",
      "2025-06-01 14:51:31 [INFO]: epoch 86: training loss 0.1006\n",
      "2025-06-01 14:51:31 [INFO]: epoch 87: training loss 0.1432\n",
      "2025-06-01 14:51:31 [INFO]: epoch 88: training loss 0.1083\n",
      "2025-06-01 14:51:31 [INFO]: epoch 89: training loss 0.1055\n",
      "2025-06-01 14:51:31 [INFO]: epoch 90: training loss 0.1284\n",
      "2025-06-01 14:51:31 [INFO]: epoch 91: training loss 0.1145\n",
      "2025-06-01 14:51:31 [INFO]: epoch 92: training loss 0.1004\n",
      "2025-06-01 14:51:31 [INFO]: epoch 93: training loss 0.1325\n",
      "2025-06-01 14:51:31 [INFO]: epoch 94: training loss 0.1268\n",
      "2025-06-01 14:51:31 [INFO]: epoch 95: training loss 0.0925\n",
      "2025-06-01 14:51:31 [INFO]: epoch 96: training loss 0.1122\n",
      "2025-06-01 14:51:31 [INFO]: epoch 97: training loss 0.0944\n",
      "2025-06-01 14:51:31 [INFO]: epoch 98: training loss 0.1063\n",
      "2025-06-01 14:51:31 [INFO]: epoch 99: training loss 0.0982\n",
      "2025-06-01 14:51:31 [INFO]: epoch 100: training loss 0.0994\n",
      "2025-06-01 14:51:31 [INFO]: epoch 101: training loss 0.1070\n",
      "2025-06-01 14:51:31 [INFO]: epoch 102: training loss 0.1105\n",
      "2025-06-01 14:51:31 [INFO]: epoch 103: training loss 0.1075\n",
      "2025-06-01 14:51:31 [INFO]: epoch 104: training loss 0.1048\n",
      "2025-06-01 14:51:31 [INFO]: epoch 105: training loss 0.1012\n",
      "2025-06-01 14:51:31 [INFO]: epoch 106: training loss 0.1070\n",
      "2025-06-01 14:51:31 [INFO]: epoch 107: training loss 0.1031\n",
      "2025-06-01 14:51:31 [INFO]: epoch 108: training loss 0.1164\n",
      "2025-06-01 14:51:31 [INFO]: epoch 109: training loss 0.0841\n",
      "2025-06-01 14:51:31 [INFO]: epoch 110: training loss 0.0956\n",
      "2025-06-01 14:51:31 [INFO]: epoch 111: training loss 0.0937\n",
      "2025-06-01 14:51:31 [INFO]: epoch 112: training loss 0.0981\n",
      "2025-06-01 14:51:31 [INFO]: epoch 113: training loss 0.1110\n",
      "2025-06-01 14:51:31 [INFO]: epoch 114: training loss 0.0903\n",
      "2025-06-01 14:51:31 [INFO]: epoch 115: training loss 0.0805\n",
      "2025-06-01 14:51:31 [INFO]: epoch 116: training loss 0.0882\n",
      "2025-06-01 14:51:31 [INFO]: epoch 117: training loss 0.1136\n",
      "2025-06-01 14:51:31 [INFO]: epoch 118: training loss 0.0981\n",
      "2025-06-01 14:51:31 [INFO]: epoch 119: training loss 0.0957\n",
      "2025-06-01 14:51:31 [INFO]: epoch 120: training loss 0.0985\n",
      "2025-06-01 14:51:31 [INFO]: epoch 121: training loss 0.1003\n",
      "2025-06-01 14:51:31 [INFO]: epoch 122: training loss 0.0913\n",
      "2025-06-01 14:51:31 [INFO]: epoch 123: training loss 0.0853\n",
      "2025-06-01 14:51:31 [INFO]: epoch 124: training loss 0.0947\n",
      "2025-06-01 14:51:31 [INFO]: epoch 125: training loss 0.0972\n",
      "2025-06-01 14:51:31 [INFO]: epoch 126: training loss 0.0927\n",
      "2025-06-01 14:51:31 [INFO]: epoch 127: training loss 0.0862\n",
      "2025-06-01 14:51:31 [INFO]: epoch 128: training loss 0.0762\n",
      "2025-06-01 14:51:31 [INFO]: epoch 129: training loss 0.0939\n",
      "2025-06-01 14:51:31 [INFO]: epoch 130: training loss 0.0926\n",
      "2025-06-01 14:51:31 [INFO]: epoch 131: training loss 0.0766\n",
      "2025-06-01 14:51:31 [INFO]: epoch 132: training loss 0.0939\n",
      "2025-06-01 14:51:31 [INFO]: epoch 133: training loss 0.0828\n",
      "2025-06-01 14:51:31 [INFO]: epoch 134: training loss 0.0848\n",
      "2025-06-01 14:51:31 [INFO]: epoch 135: training loss 0.0843\n",
      "2025-06-01 14:51:31 [INFO]: epoch 136: training loss 0.0703\n",
      "2025-06-01 14:51:31 [INFO]: epoch 137: training loss 0.0922\n",
      "2025-06-01 14:51:31 [INFO]: epoch 138: training loss 0.0916\n",
      "2025-06-01 14:51:31 [INFO]: epoch 139: training loss 0.0870\n",
      "2025-06-01 14:51:31 [INFO]: epoch 140: training loss 0.0844\n",
      "2025-06-01 14:51:31 [INFO]: epoch 141: training loss 0.0794\n",
      "2025-06-01 14:51:31 [INFO]: epoch 142: training loss 0.0833\n",
      "2025-06-01 14:51:31 [INFO]: epoch 143: training loss 0.0759\n",
      "2025-06-01 14:51:31 [INFO]: epoch 144: training loss 0.0741\n",
      "2025-06-01 14:51:31 [INFO]: epoch 145: training loss 0.0717\n",
      "2025-06-01 14:51:31 [INFO]: epoch 146: training loss 0.0735\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-01 14:51:32 [INFO]: epoch 147: training loss 0.0793\n",
      "2025-06-01 14:51:32 [INFO]: epoch 148: training loss 0.0688\n",
      "2025-06-01 14:51:32 [INFO]: epoch 149: training loss 0.0927\n",
      "2025-06-01 14:51:32 [INFO]: epoch 150: training loss 0.0766\n",
      "2025-06-01 14:51:32 [INFO]: epoch 151: training loss 0.0818\n",
      "2025-06-01 14:51:32 [INFO]: epoch 152: training loss 0.0795\n",
      "2025-06-01 14:51:32 [INFO]: epoch 153: training loss 0.0895\n",
      "2025-06-01 14:51:32 [INFO]: epoch 154: training loss 0.0825\n",
      "2025-06-01 14:51:32 [INFO]: epoch 155: training loss 0.0730\n",
      "2025-06-01 14:51:32 [INFO]: epoch 156: training loss 0.0933\n",
      "2025-06-01 14:51:32 [INFO]: epoch 157: training loss 0.0873\n",
      "2025-06-01 14:51:32 [INFO]: epoch 158: training loss 0.0884\n",
      "2025-06-01 14:51:32 [INFO]: epoch 159: training loss 0.0720\n",
      "2025-06-01 14:51:32 [INFO]: epoch 160: training loss 0.0846\n",
      "2025-06-01 14:51:32 [INFO]: epoch 161: training loss 0.0833\n",
      "2025-06-01 14:51:32 [INFO]: epoch 162: training loss 0.0748\n",
      "2025-06-01 14:51:32 [INFO]: epoch 163: training loss 0.0880\n",
      "2025-06-01 14:51:32 [INFO]: epoch 164: training loss 0.0893\n",
      "2025-06-01 14:51:32 [INFO]: epoch 165: training loss 0.0667\n",
      "2025-06-01 14:51:32 [INFO]: epoch 166: training loss 0.0888\n",
      "2025-06-01 14:51:32 [INFO]: epoch 167: training loss 0.0834\n",
      "2025-06-01 14:51:32 [INFO]: epoch 168: training loss 0.0817\n",
      "2025-06-01 14:51:32 [INFO]: epoch 169: training loss 0.0662\n",
      "2025-06-01 14:51:32 [INFO]: epoch 170: training loss 0.0808\n",
      "2025-06-01 14:51:32 [INFO]: epoch 171: training loss 0.0803\n",
      "2025-06-01 14:51:32 [INFO]: epoch 172: training loss 0.0568\n",
      "2025-06-01 14:51:32 [INFO]: epoch 173: training loss 0.0698\n",
      "2025-06-01 14:51:32 [INFO]: epoch 174: training loss 0.0750\n",
      "2025-06-01 14:51:32 [INFO]: epoch 175: training loss 0.0636\n",
      "2025-06-01 14:51:32 [INFO]: epoch 176: training loss 0.0592\n",
      "2025-06-01 14:51:32 [INFO]: epoch 177: training loss 0.0604\n",
      "2025-06-01 14:51:32 [INFO]: epoch 178: training loss 0.0675\n",
      "2025-06-01 14:51:32 [INFO]: epoch 179: training loss 0.0579\n",
      "2025-06-01 14:51:32 [INFO]: epoch 180: training loss 0.0616\n",
      "2025-06-01 14:51:32 [INFO]: epoch 181: training loss 0.0558\n",
      "2025-06-01 14:51:32 [INFO]: epoch 182: training loss 0.0695\n",
      "2025-06-01 14:51:32 [INFO]: epoch 183: training loss 0.0670\n",
      "2025-06-01 14:51:32 [INFO]: epoch 184: training loss 0.0555\n",
      "2025-06-01 14:51:32 [INFO]: epoch 185: training loss 0.0700\n",
      "2025-06-01 14:51:32 [INFO]: epoch 186: training loss 0.0658\n",
      "2025-06-01 14:51:32 [INFO]: epoch 187: training loss 0.0505\n",
      "2025-06-01 14:51:32 [INFO]: epoch 188: training loss 0.0492\n",
      "2025-06-01 14:51:32 [INFO]: epoch 189: training loss 0.0560\n",
      "2025-06-01 14:51:32 [INFO]: epoch 190: training loss 0.0541\n",
      "2025-06-01 14:51:32 [INFO]: epoch 191: training loss 0.0594\n",
      "2025-06-01 14:51:32 [INFO]: epoch 192: training loss 0.0467\n",
      "2025-06-01 14:51:32 [INFO]: epoch 193: training loss 0.0592\n",
      "2025-06-01 14:51:32 [INFO]: epoch 194: training loss 0.0714\n",
      "2025-06-01 14:51:32 [INFO]: epoch 195: training loss 0.0587\n",
      "2025-06-01 14:51:32 [INFO]: epoch 196: training loss 0.0539\n",
      "2025-06-01 14:51:32 [INFO]: epoch 197: training loss 0.0614\n",
      "2025-06-01 14:51:32 [INFO]: epoch 198: training loss 0.0660\n",
      "2025-06-01 14:51:32 [INFO]: epoch 199: training loss 0.0683\n",
      "2025-06-01 14:51:32 [INFO]: epoch 200: training loss 0.0704\n",
      "2025-06-01 14:51:32 [INFO]: epoch 201: training loss 0.0673\n",
      "2025-06-01 14:51:32 [INFO]: epoch 202: training loss 0.0605\n",
      "2025-06-01 14:51:32 [INFO]: epoch 203: training loss 0.0681\n",
      "2025-06-01 14:51:32 [INFO]: epoch 204: training loss 0.0679\n",
      "2025-06-01 14:51:32 [INFO]: epoch 205: training loss 0.0523\n",
      "2025-06-01 14:51:32 [INFO]: epoch 206: training loss 0.0560\n",
      "2025-06-01 14:51:32 [INFO]: epoch 207: training loss 0.0717\n",
      "2025-06-01 14:51:32 [INFO]: epoch 208: training loss 0.0563\n",
      "2025-06-01 14:51:32 [INFO]: epoch 209: training loss 0.0482\n",
      "2025-06-01 14:51:32 [INFO]: epoch 210: training loss 0.0555\n",
      "2025-06-01 14:51:32 [INFO]: epoch 211: training loss 0.0570\n",
      "2025-06-01 14:51:32 [INFO]: epoch 212: training loss 0.0619\n",
      "2025-06-01 14:51:32 [INFO]: epoch 213: training loss 0.0582\n",
      "2025-06-01 14:51:32 [INFO]: epoch 214: training loss 0.0671\n",
      "2025-06-01 14:51:32 [INFO]: epoch 215: training loss 0.0508\n",
      "2025-06-01 14:51:32 [INFO]: epoch 216: training loss 0.0595\n",
      "2025-06-01 14:51:32 [INFO]: epoch 217: training loss 0.0802\n",
      "2025-06-01 14:51:32 [INFO]: epoch 218: training loss 0.0587\n",
      "2025-06-01 14:51:32 [INFO]: epoch 219: training loss 0.0500\n",
      "2025-06-01 14:51:32 [INFO]: epoch 220: training loss 0.0639\n",
      "2025-06-01 14:51:32 [INFO]: epoch 221: training loss 0.0583\n",
      "2025-06-01 14:51:33 [INFO]: epoch 222: training loss 0.0504\n",
      "2025-06-01 14:51:33 [INFO]: epoch 223: training loss 0.0745\n",
      "2025-06-01 14:51:33 [INFO]: epoch 224: training loss 0.0842\n",
      "2025-06-01 14:51:33 [INFO]: epoch 225: training loss 0.0536\n",
      "2025-06-01 14:51:33 [INFO]: epoch 226: training loss 0.0554\n",
      "2025-06-01 14:51:33 [INFO]: epoch 227: training loss 0.0621\n",
      "2025-06-01 14:51:33 [INFO]: epoch 228: training loss 0.0453\n",
      "2025-06-01 14:51:33 [INFO]: epoch 229: training loss 0.0602\n",
      "2025-06-01 14:51:33 [INFO]: epoch 230: training loss 0.0555\n",
      "2025-06-01 14:51:33 [INFO]: epoch 231: training loss 0.0502\n",
      "2025-06-01 14:51:33 [INFO]: epoch 232: training loss 0.0537\n",
      "2025-06-01 14:51:33 [INFO]: epoch 233: training loss 0.0564\n",
      "2025-06-01 14:51:33 [INFO]: epoch 234: training loss 0.0570\n",
      "2025-06-01 14:51:33 [INFO]: epoch 235: training loss 0.0585\n",
      "2025-06-01 14:51:33 [INFO]: epoch 236: training loss 0.0573\n",
      "2025-06-01 14:51:33 [INFO]: epoch 237: training loss 0.0456\n",
      "2025-06-01 14:51:33 [INFO]: epoch 238: training loss 0.0607\n",
      "2025-06-01 14:51:33 [INFO]: epoch 239: training loss 0.0694\n",
      "2025-06-01 14:51:33 [INFO]: epoch 240: training loss 0.0587\n",
      "2025-06-01 14:51:33 [INFO]: epoch 241: training loss 0.0560\n",
      "2025-06-01 14:51:33 [INFO]: epoch 242: training loss 0.0565\n",
      "2025-06-01 14:51:33 [INFO]: epoch 243: training loss 0.0542\n",
      "2025-06-01 14:51:33 [INFO]: epoch 244: training loss 0.0576\n",
      "2025-06-01 14:51:33 [INFO]: epoch 245: training loss 0.0679\n",
      "2025-06-01 14:51:33 [INFO]: epoch 246: training loss 0.0535\n",
      "2025-06-01 14:51:33 [INFO]: epoch 247: training loss 0.0609\n",
      "2025-06-01 14:51:33 [INFO]: epoch 248: training loss 0.0761\n",
      "2025-06-01 14:51:33 [INFO]: epoch 249: training loss 0.0589\n",
      "2025-06-01 14:51:33 [INFO]: epoch 250: training loss 0.0817\n",
      "2025-06-01 14:51:33 [INFO]: epoch 251: training loss 0.0623\n",
      "2025-06-01 14:51:33 [INFO]: epoch 252: training loss 0.0631\n",
      "2025-06-01 14:51:33 [INFO]: epoch 253: training loss 0.0673\n",
      "2025-06-01 14:51:33 [INFO]: epoch 254: training loss 0.0663\n",
      "2025-06-01 14:51:33 [INFO]: epoch 255: training loss 0.0602\n",
      "2025-06-01 14:51:33 [INFO]: epoch 256: training loss 0.0572\n",
      "2025-06-01 14:51:33 [INFO]: epoch 257: training loss 0.0578\n",
      "2025-06-01 14:51:33 [INFO]: epoch 258: training loss 0.0557\n",
      "2025-06-01 14:51:33 [INFO]: epoch 259: training loss 0.0521\n",
      "2025-06-01 14:51:33 [INFO]: epoch 260: training loss 0.0509\n",
      "2025-06-01 14:51:33 [INFO]: epoch 261: training loss 0.0486\n",
      "2025-06-01 14:51:33 [INFO]: epoch 262: training loss 0.0478\n",
      "2025-06-01 14:51:33 [INFO]: epoch 263: training loss 0.0378\n",
      "2025-06-01 14:51:33 [INFO]: epoch 264: training loss 0.0431\n",
      "2025-06-01 14:51:33 [INFO]: epoch 265: training loss 0.0431\n",
      "2025-06-01 14:51:33 [INFO]: epoch 266: training loss 0.0429\n",
      "2025-06-01 14:51:33 [INFO]: epoch 267: training loss 0.0563\n",
      "2025-06-01 14:51:33 [INFO]: epoch 268: training loss 0.0423\n",
      "2025-06-01 14:51:33 [INFO]: epoch 269: training loss 0.0430\n",
      "2025-06-01 14:51:33 [INFO]: epoch 270: training loss 0.0451\n",
      "2025-06-01 14:51:33 [INFO]: epoch 271: training loss 0.0494\n",
      "2025-06-01 14:51:33 [INFO]: epoch 272: training loss 0.0416\n",
      "2025-06-01 14:51:33 [INFO]: epoch 273: training loss 0.0537\n",
      "2025-06-01 14:51:33 [INFO]: epoch 274: training loss 0.0493\n",
      "2025-06-01 14:51:33 [INFO]: epoch 275: training loss 0.0399\n",
      "2025-06-01 14:51:33 [INFO]: epoch 276: training loss 0.0442\n",
      "2025-06-01 14:51:33 [INFO]: epoch 277: training loss 0.0394\n",
      "2025-06-01 14:51:33 [INFO]: epoch 278: training loss 0.0436\n",
      "2025-06-01 14:51:33 [INFO]: epoch 279: training loss 0.0373\n",
      "2025-06-01 14:51:33 [INFO]: epoch 280: training loss 0.0384\n",
      "2025-06-01 14:51:33 [INFO]: epoch 281: training loss 0.0440\n",
      "2025-06-01 14:51:33 [INFO]: epoch 282: training loss 0.0499\n",
      "2025-06-01 14:51:33 [INFO]: epoch 283: training loss 0.0370\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-01 14:51:33 [INFO]: epoch 284: training loss 0.0417\n",
      "2025-06-01 14:51:33 [INFO]: epoch 285: training loss 0.0463\n",
      "2025-06-01 14:51:33 [INFO]: epoch 286: training loss 0.0382\n",
      "2025-06-01 14:51:33 [INFO]: epoch 287: training loss 0.0397\n",
      "2025-06-01 14:51:33 [INFO]: epoch 288: training loss 0.0502\n",
      "2025-06-01 14:51:33 [INFO]: epoch 289: training loss 0.0478\n",
      "2025-06-01 14:51:33 [INFO]: epoch 290: training loss 0.0591\n",
      "2025-06-01 14:51:33 [INFO]: epoch 291: training loss 0.0469\n",
      "2025-06-01 14:51:33 [INFO]: epoch 292: training loss 0.0407\n",
      "2025-06-01 14:51:33 [INFO]: epoch 293: training loss 0.0515\n",
      "2025-06-01 14:51:33 [INFO]: epoch 294: training loss 0.0593\n",
      "2025-06-01 14:51:33 [INFO]: epoch 295: training loss 0.0508\n",
      "2025-06-01 14:51:33 [INFO]: epoch 296: training loss 0.0383\n",
      "2025-06-01 14:51:33 [INFO]: epoch 297: training loss 0.0449\n",
      "2025-06-01 14:51:33 [INFO]: epoch 298: training loss 0.0537\n",
      "2025-06-01 14:51:34 [INFO]: epoch 299: training loss 0.0424\n",
      "2025-06-01 14:51:34 [INFO]: epoch 300: training loss 0.0393\n",
      "2025-06-01 14:51:34 [INFO]: epoch 301: training loss 0.0513\n",
      "2025-06-01 14:51:34 [INFO]: epoch 302: training loss 0.0458\n",
      "2025-06-01 14:51:34 [INFO]: epoch 303: training loss 0.0363\n",
      "2025-06-01 14:51:34 [INFO]: epoch 304: training loss 0.0393\n",
      "2025-06-01 14:51:34 [INFO]: epoch 305: training loss 0.0341\n",
      "2025-06-01 14:51:34 [INFO]: epoch 306: training loss 0.0396\n",
      "2025-06-01 14:51:34 [INFO]: epoch 307: training loss 0.0358\n",
      "2025-06-01 14:51:34 [INFO]: epoch 308: training loss 0.0438\n",
      "2025-06-01 14:51:34 [INFO]: epoch 309: training loss 0.0341\n",
      "2025-06-01 14:51:34 [INFO]: epoch 310: training loss 0.0390\n",
      "2025-06-01 14:51:34 [INFO]: epoch 311: training loss 0.0355\n",
      "2025-06-01 14:51:34 [INFO]: epoch 312: training loss 0.0333\n",
      "2025-06-01 14:51:34 [INFO]: epoch 313: training loss 0.0376\n",
      "2025-06-01 14:51:34 [INFO]: epoch 314: training loss 0.0414\n",
      "2025-06-01 14:51:34 [INFO]: epoch 315: training loss 0.0400\n",
      "2025-06-01 14:51:34 [INFO]: epoch 316: training loss 0.0338\n",
      "2025-06-01 14:51:34 [INFO]: epoch 317: training loss 0.0412\n",
      "2025-06-01 14:51:34 [INFO]: epoch 318: training loss 0.0395\n",
      "2025-06-01 14:51:34 [INFO]: epoch 319: training loss 0.0432\n",
      "2025-06-01 14:51:34 [INFO]: epoch 320: training loss 0.0399\n",
      "2025-06-01 14:51:34 [INFO]: epoch 321: training loss 0.0322\n",
      "2025-06-01 14:51:34 [INFO]: epoch 322: training loss 0.0476\n",
      "2025-06-01 14:51:34 [INFO]: epoch 323: training loss 0.0360\n",
      "2025-06-01 14:51:34 [INFO]: epoch 324: training loss 0.0348\n",
      "2025-06-01 14:51:34 [INFO]: epoch 325: training loss 0.0397\n",
      "2025-06-01 14:51:34 [INFO]: epoch 326: training loss 0.0433\n",
      "2025-06-01 14:51:34 [INFO]: epoch 327: training loss 0.0412\n",
      "2025-06-01 14:51:34 [INFO]: epoch 328: training loss 0.0412\n",
      "2025-06-01 14:51:34 [INFO]: epoch 329: training loss 0.0376\n",
      "2025-06-01 14:51:34 [INFO]: epoch 330: training loss 0.0414\n",
      "2025-06-01 14:51:34 [INFO]: epoch 331: training loss 0.0474\n",
      "2025-06-01 14:51:34 [INFO]: epoch 332: training loss 0.0423\n",
      "2025-06-01 14:51:34 [INFO]: epoch 333: training loss 0.0414\n",
      "2025-06-01 14:51:34 [INFO]: epoch 334: training loss 0.0378\n",
      "2025-06-01 14:51:34 [INFO]: epoch 335: training loss 0.0430\n",
      "2025-06-01 14:51:34 [INFO]: epoch 336: training loss 0.0388\n",
      "2025-06-01 14:51:34 [INFO]: epoch 337: training loss 0.0318\n",
      "2025-06-01 14:51:34 [INFO]: epoch 338: training loss 0.0388\n",
      "2025-06-01 14:51:34 [INFO]: epoch 339: training loss 0.0394\n",
      "2025-06-01 14:51:34 [INFO]: epoch 340: training loss 0.0354\n",
      "2025-06-01 14:51:34 [INFO]: epoch 341: training loss 0.0323\n",
      "2025-06-01 14:51:34 [INFO]: epoch 342: training loss 0.0323\n",
      "2025-06-01 14:51:34 [INFO]: epoch 343: training loss 0.0408\n",
      "2025-06-01 14:51:34 [INFO]: epoch 344: training loss 0.0310\n",
      "2025-06-01 14:51:34 [INFO]: epoch 345: training loss 0.0293\n",
      "2025-06-01 14:51:34 [INFO]: epoch 346: training loss 0.0361\n",
      "2025-06-01 14:51:34 [INFO]: epoch 347: training loss 0.0364\n",
      "2025-06-01 14:51:34 [INFO]: epoch 348: training loss 0.0360\n",
      "2025-06-01 14:51:34 [INFO]: epoch 349: training loss 0.0329\n",
      "2025-06-01 14:51:34 [INFO]: epoch 350: training loss 0.0333\n",
      "2025-06-01 14:51:34 [INFO]: epoch 351: training loss 0.0369\n",
      "2025-06-01 14:51:34 [INFO]: epoch 352: training loss 0.0318\n",
      "2025-06-01 14:51:34 [INFO]: epoch 353: training loss 0.0344\n",
      "2025-06-01 14:51:34 [INFO]: epoch 354: training loss 0.0310\n",
      "2025-06-01 14:51:34 [INFO]: epoch 355: training loss 0.0349\n",
      "2025-06-01 14:51:34 [INFO]: epoch 356: training loss 0.0316\n",
      "2025-06-01 14:51:34 [INFO]: epoch 357: training loss 0.0433\n",
      "2025-06-01 14:51:34 [INFO]: epoch 358: training loss 0.0378\n",
      "2025-06-01 14:51:34 [INFO]: epoch 359: training loss 0.0379\n",
      "2025-06-01 14:51:34 [INFO]: epoch 360: training loss 0.0361\n",
      "2025-06-01 14:51:34 [INFO]: epoch 361: training loss 0.0290\n",
      "2025-06-01 14:51:34 [INFO]: epoch 362: training loss 0.0348\n",
      "2025-06-01 14:51:34 [INFO]: epoch 363: training loss 0.0407\n",
      "2025-06-01 14:51:34 [INFO]: epoch 364: training loss 0.0340\n",
      "2025-06-01 14:51:34 [INFO]: epoch 365: training loss 0.0423\n",
      "2025-06-01 14:51:34 [INFO]: epoch 366: training loss 0.0391\n",
      "2025-06-01 14:51:34 [INFO]: epoch 367: training loss 0.0345\n",
      "2025-06-01 14:51:34 [INFO]: epoch 368: training loss 0.0344\n",
      "2025-06-01 14:51:34 [INFO]: epoch 369: training loss 0.0346\n",
      "2025-06-01 14:51:34 [INFO]: epoch 370: training loss 0.0330\n",
      "2025-06-01 14:51:34 [INFO]: epoch 371: training loss 0.0376\n",
      "2025-06-01 14:51:34 [INFO]: epoch 372: training loss 0.0378\n",
      "2025-06-01 14:51:34 [INFO]: epoch 373: training loss 0.0340\n",
      "2025-06-01 14:51:34 [INFO]: epoch 374: training loss 0.0290\n",
      "2025-06-01 14:51:34 [INFO]: epoch 375: training loss 0.0353\n",
      "2025-06-01 14:51:34 [INFO]: epoch 376: training loss 0.0336\n",
      "2025-06-01 14:51:35 [INFO]: epoch 377: training loss 0.0364\n",
      "2025-06-01 14:51:35 [INFO]: epoch 378: training loss 0.0331\n",
      "2025-06-01 14:51:35 [INFO]: epoch 379: training loss 0.0312\n",
      "2025-06-01 14:51:35 [INFO]: epoch 380: training loss 0.0308\n",
      "2025-06-01 14:51:35 [INFO]: epoch 381: training loss 0.0319\n",
      "2025-06-01 14:51:35 [INFO]: epoch 382: training loss 0.0311\n",
      "2025-06-01 14:51:35 [INFO]: epoch 383: training loss 0.0301\n",
      "2025-06-01 14:51:35 [INFO]: epoch 384: training loss 0.0343\n",
      "2025-06-01 14:51:35 [INFO]: epoch 385: training loss 0.0306\n",
      "2025-06-01 14:51:35 [INFO]: epoch 386: training loss 0.0313\n",
      "2025-06-01 14:51:35 [INFO]: epoch 387: training loss 0.0305\n",
      "2025-06-01 14:51:35 [INFO]: epoch 388: training loss 0.0319\n",
      "2025-06-01 14:51:35 [INFO]: epoch 389: training loss 0.0339\n",
      "2025-06-01 14:51:35 [INFO]: epoch 390: training loss 0.0302\n",
      "2025-06-01 14:51:35 [INFO]: epoch 391: training loss 0.0287\n",
      "2025-06-01 14:51:35 [INFO]: epoch 392: training loss 0.0406\n",
      "2025-06-01 14:51:35 [INFO]: epoch 393: training loss 0.0331\n",
      "2025-06-01 14:51:35 [INFO]: epoch 394: training loss 0.0287\n",
      "2025-06-01 14:51:35 [INFO]: epoch 395: training loss 0.0331\n",
      "2025-06-01 14:51:35 [INFO]: epoch 396: training loss 0.0370\n",
      "2025-06-01 14:51:35 [INFO]: epoch 397: training loss 0.0309\n",
      "2025-06-01 14:51:35 [INFO]: epoch 398: training loss 0.0301\n",
      "2025-06-01 14:51:35 [INFO]: epoch 399: training loss 0.0343\n",
      "2025-06-01 14:51:35 [INFO]: Finished training.\n",
      "2025-06-01 14:51:35 [WARNING]: 🚨DeprecationWarning: The method impute is deprecated. Please use `predict` instead.\n",
      " 83%|██████████████████████████████████████████████████████████████████████              | 5/6 [00:26<00:05,  5.35s/it]2025-06-01 14:51:35 [INFO]: No given device, using default device: cuda\n",
      "2025-06-01 14:51:35 [WARNING]: saving_path not given. Model files and tensorboard file will not be saved.\n",
      "2025-06-01 14:51:35 [INFO]: Model initialized successfully with the number of trainable parameters: 597,780\n",
      "2025-06-01 14:51:35 [INFO]: epoch 0: training loss 1.1496\n",
      "2025-06-01 14:51:35 [INFO]: epoch 1: training loss 0.6500\n",
      "2025-06-01 14:51:35 [INFO]: epoch 2: training loss 0.6153\n",
      "2025-06-01 14:51:35 [INFO]: epoch 3: training loss 0.5511\n",
      "2025-06-01 14:51:35 [INFO]: epoch 4: training loss 0.4894\n",
      "2025-06-01 14:51:35 [INFO]: epoch 5: training loss 0.4405\n",
      "2025-06-01 14:51:35 [INFO]: epoch 6: training loss 0.4213\n",
      "2025-06-01 14:51:35 [INFO]: epoch 7: training loss 0.3947\n",
      "2025-06-01 14:51:35 [INFO]: epoch 8: training loss 0.3801\n",
      "2025-06-01 14:51:35 [INFO]: epoch 9: training loss 0.3648\n",
      "2025-06-01 14:51:35 [INFO]: epoch 10: training loss 0.4091\n",
      "2025-06-01 14:51:35 [INFO]: epoch 11: training loss 0.3896\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-01 14:51:35 [INFO]: epoch 12: training loss 0.3836\n",
      "2025-06-01 14:51:35 [INFO]: epoch 13: training loss 0.3461\n",
      "2025-06-01 14:51:35 [INFO]: epoch 14: training loss 0.3277\n",
      "2025-06-01 14:51:35 [INFO]: epoch 15: training loss 0.3277\n",
      "2025-06-01 14:51:35 [INFO]: epoch 16: training loss 0.3012\n",
      "2025-06-01 14:51:35 [INFO]: epoch 17: training loss 0.3333\n",
      "2025-06-01 14:51:35 [INFO]: epoch 18: training loss 0.3213\n",
      "2025-06-01 14:51:35 [INFO]: epoch 19: training loss 0.3658\n",
      "2025-06-01 14:51:35 [INFO]: epoch 20: training loss 0.3186\n",
      "2025-06-01 14:51:35 [INFO]: epoch 21: training loss 0.3096\n",
      "2025-06-01 14:51:35 [INFO]: epoch 22: training loss 0.2936\n",
      "2025-06-01 14:51:35 [INFO]: epoch 23: training loss 0.2974\n",
      "2025-06-01 14:51:35 [INFO]: epoch 24: training loss 0.3212\n",
      "2025-06-01 14:51:35 [INFO]: epoch 25: training loss 0.3105\n",
      "2025-06-01 14:51:35 [INFO]: epoch 26: training loss 0.3322\n",
      "2025-06-01 14:51:35 [INFO]: epoch 27: training loss 0.2915\n",
      "2025-06-01 14:51:35 [INFO]: epoch 28: training loss 0.2886\n",
      "2025-06-01 14:51:35 [INFO]: epoch 29: training loss 0.2922\n",
      "2025-06-01 14:51:35 [INFO]: epoch 30: training loss 0.3073\n",
      "2025-06-01 14:51:35 [INFO]: epoch 31: training loss 0.3093\n",
      "2025-06-01 14:51:35 [INFO]: epoch 32: training loss 0.2778\n",
      "2025-06-01 14:51:35 [INFO]: epoch 33: training loss 0.2713\n",
      "2025-06-01 14:51:35 [INFO]: epoch 34: training loss 0.2819\n",
      "2025-06-01 14:51:35 [INFO]: epoch 35: training loss 0.2880\n",
      "2025-06-01 14:51:35 [INFO]: epoch 36: training loss 0.2605\n",
      "2025-06-01 14:51:35 [INFO]: epoch 37: training loss 0.2781\n",
      "2025-06-01 14:51:35 [INFO]: epoch 38: training loss 0.2776\n",
      "2025-06-01 14:51:35 [INFO]: epoch 39: training loss 0.2916\n",
      "2025-06-01 14:51:35 [INFO]: epoch 40: training loss 0.2618\n",
      "2025-06-01 14:51:35 [INFO]: epoch 41: training loss 0.2684\n",
      "2025-06-01 14:51:35 [INFO]: epoch 42: training loss 0.2645\n",
      "2025-06-01 14:51:35 [INFO]: epoch 43: training loss 0.2759\n",
      "2025-06-01 14:51:36 [INFO]: epoch 44: training loss 0.2662\n",
      "2025-06-01 14:51:36 [INFO]: epoch 45: training loss 0.2672\n",
      "2025-06-01 14:51:36 [INFO]: epoch 46: training loss 0.2584\n",
      "2025-06-01 14:51:36 [INFO]: epoch 47: training loss 0.2553\n",
      "2025-06-01 14:51:36 [INFO]: epoch 48: training loss 0.2473\n",
      "2025-06-01 14:51:36 [INFO]: epoch 49: training loss 0.2530\n",
      "2025-06-01 14:51:36 [INFO]: epoch 50: training loss 0.2356\n",
      "2025-06-01 14:51:36 [INFO]: epoch 51: training loss 0.2536\n",
      "2025-06-01 14:51:36 [INFO]: epoch 52: training loss 0.2384\n",
      "2025-06-01 14:51:36 [INFO]: epoch 53: training loss 0.2355\n",
      "2025-06-01 14:51:36 [INFO]: epoch 54: training loss 0.2262\n",
      "2025-06-01 14:51:36 [INFO]: epoch 55: training loss 0.2544\n",
      "2025-06-01 14:51:36 [INFO]: epoch 56: training loss 0.2048\n",
      "2025-06-01 14:51:36 [INFO]: epoch 57: training loss 0.2192\n",
      "2025-06-01 14:51:36 [INFO]: epoch 58: training loss 0.2259\n",
      "2025-06-01 14:51:36 [INFO]: epoch 59: training loss 0.2228\n",
      "2025-06-01 14:51:36 [INFO]: epoch 60: training loss 0.2507\n",
      "2025-06-01 14:51:36 [INFO]: epoch 61: training loss 0.2316\n",
      "2025-06-01 14:51:36 [INFO]: epoch 62: training loss 0.2386\n",
      "2025-06-01 14:51:36 [INFO]: epoch 63: training loss 0.2406\n",
      "2025-06-01 14:51:36 [INFO]: epoch 64: training loss 0.2115\n",
      "2025-06-01 14:51:36 [INFO]: epoch 65: training loss 0.2252\n",
      "2025-06-01 14:51:36 [INFO]: epoch 66: training loss 0.2148\n",
      "2025-06-01 14:51:36 [INFO]: epoch 67: training loss 0.2389\n",
      "2025-06-01 14:51:36 [INFO]: epoch 68: training loss 0.2276\n",
      "2025-06-01 14:51:36 [INFO]: epoch 69: training loss 0.2146\n",
      "2025-06-01 14:51:36 [INFO]: epoch 70: training loss 0.2187\n",
      "2025-06-01 14:51:36 [INFO]: epoch 71: training loss 0.2147\n",
      "2025-06-01 14:51:36 [INFO]: epoch 72: training loss 0.2188\n",
      "2025-06-01 14:51:36 [INFO]: epoch 73: training loss 0.2079\n",
      "2025-06-01 14:51:36 [INFO]: epoch 74: training loss 0.2018\n",
      "2025-06-01 14:51:36 [INFO]: epoch 75: training loss 0.2133\n",
      "2025-06-01 14:51:36 [INFO]: epoch 76: training loss 0.1932\n",
      "2025-06-01 14:51:36 [INFO]: epoch 77: training loss 0.1993\n",
      "2025-06-01 14:51:36 [INFO]: epoch 78: training loss 0.2124\n",
      "2025-06-01 14:51:36 [INFO]: epoch 79: training loss 0.2079\n",
      "2025-06-01 14:51:36 [INFO]: epoch 80: training loss 0.1961\n",
      "2025-06-01 14:51:36 [INFO]: epoch 81: training loss 0.2028\n",
      "2025-06-01 14:51:36 [INFO]: epoch 82: training loss 0.2004\n",
      "2025-06-01 14:51:36 [INFO]: epoch 83: training loss 0.1986\n",
      "2025-06-01 14:51:36 [INFO]: epoch 84: training loss 0.2029\n",
      "2025-06-01 14:51:36 [INFO]: epoch 85: training loss 0.2006\n",
      "2025-06-01 14:51:36 [INFO]: epoch 86: training loss 0.1887\n",
      "2025-06-01 14:51:36 [INFO]: epoch 87: training loss 0.1937\n",
      "2025-06-01 14:51:36 [INFO]: epoch 88: training loss 0.2068\n",
      "2025-06-01 14:51:36 [INFO]: epoch 89: training loss 0.2114\n",
      "2025-06-01 14:51:36 [INFO]: epoch 90: training loss 0.2069\n",
      "2025-06-01 14:51:36 [INFO]: epoch 91: training loss 0.2034\n",
      "2025-06-01 14:51:36 [INFO]: epoch 92: training loss 0.1840\n",
      "2025-06-01 14:51:36 [INFO]: epoch 93: training loss 0.2037\n",
      "2025-06-01 14:51:36 [INFO]: epoch 94: training loss 0.1866\n",
      "2025-06-01 14:51:36 [INFO]: epoch 95: training loss 0.1995\n",
      "2025-06-01 14:51:36 [INFO]: epoch 96: training loss 0.1900\n",
      "2025-06-01 14:51:36 [INFO]: epoch 97: training loss 0.1937\n",
      "2025-06-01 14:51:36 [INFO]: epoch 98: training loss 0.1791\n",
      "2025-06-01 14:51:36 [INFO]: epoch 99: training loss 0.1739\n",
      "2025-06-01 14:51:36 [INFO]: epoch 100: training loss 0.1828\n",
      "2025-06-01 14:51:36 [INFO]: epoch 101: training loss 0.1932\n",
      "2025-06-01 14:51:36 [INFO]: epoch 102: training loss 0.1877\n",
      "2025-06-01 14:51:36 [INFO]: epoch 103: training loss 0.1870\n",
      "2025-06-01 14:51:36 [INFO]: epoch 104: training loss 0.1824\n",
      "2025-06-01 14:51:36 [INFO]: epoch 105: training loss 0.1961\n",
      "2025-06-01 14:51:36 [INFO]: epoch 106: training loss 0.1742\n",
      "2025-06-01 14:51:36 [INFO]: epoch 107: training loss 0.1710\n",
      "2025-06-01 14:51:36 [INFO]: epoch 108: training loss 0.2017\n",
      "2025-06-01 14:51:36 [INFO]: epoch 109: training loss 0.1801\n",
      "2025-06-01 14:51:36 [INFO]: epoch 110: training loss 0.1746\n",
      "2025-06-01 14:51:36 [INFO]: epoch 111: training loss 0.1776\n",
      "2025-06-01 14:51:36 [INFO]: epoch 112: training loss 0.1792\n",
      "2025-06-01 14:51:36 [INFO]: epoch 113: training loss 0.1867\n",
      "2025-06-01 14:51:36 [INFO]: epoch 114: training loss 0.1823\n",
      "2025-06-01 14:51:36 [INFO]: epoch 115: training loss 0.1862\n",
      "2025-06-01 14:51:36 [INFO]: epoch 116: training loss 0.1705\n",
      "2025-06-01 14:51:36 [INFO]: epoch 117: training loss 0.1717\n",
      "2025-06-01 14:51:36 [INFO]: epoch 118: training loss 0.1917\n",
      "2025-06-01 14:51:36 [INFO]: epoch 119: training loss 0.1757\n",
      "2025-06-01 14:51:36 [INFO]: epoch 120: training loss 0.1770\n",
      "2025-06-01 14:51:37 [INFO]: epoch 121: training loss 0.1654\n",
      "2025-06-01 14:51:37 [INFO]: epoch 122: training loss 0.1786\n",
      "2025-06-01 14:51:37 [INFO]: epoch 123: training loss 0.1681\n",
      "2025-06-01 14:51:37 [INFO]: epoch 124: training loss 0.1670\n",
      "2025-06-01 14:51:37 [INFO]: epoch 125: training loss 0.1784\n",
      "2025-06-01 14:51:37 [INFO]: epoch 126: training loss 0.1845\n",
      "2025-06-01 14:51:37 [INFO]: epoch 127: training loss 0.1662\n",
      "2025-06-01 14:51:37 [INFO]: epoch 128: training loss 0.1644\n",
      "2025-06-01 14:51:37 [INFO]: epoch 129: training loss 0.1682\n",
      "2025-06-01 14:51:37 [INFO]: epoch 130: training loss 0.1553\n",
      "2025-06-01 14:51:37 [INFO]: epoch 131: training loss 0.1702\n",
      "2025-06-01 14:51:37 [INFO]: epoch 132: training loss 0.1741\n",
      "2025-06-01 14:51:37 [INFO]: epoch 133: training loss 0.1815\n",
      "2025-06-01 14:51:37 [INFO]: epoch 134: training loss 0.1619\n",
      "2025-06-01 14:51:37 [INFO]: epoch 135: training loss 0.1614\n",
      "2025-06-01 14:51:37 [INFO]: epoch 136: training loss 0.1802\n",
      "2025-06-01 14:51:37 [INFO]: epoch 137: training loss 0.1731\n",
      "2025-06-01 14:51:37 [INFO]: epoch 138: training loss 0.1721\n",
      "2025-06-01 14:51:37 [INFO]: epoch 139: training loss 0.1595\n",
      "2025-06-01 14:51:37 [INFO]: epoch 140: training loss 0.1622\n",
      "2025-06-01 14:51:37 [INFO]: epoch 141: training loss 0.1768\n",
      "2025-06-01 14:51:37 [INFO]: epoch 142: training loss 0.1538\n",
      "2025-06-01 14:51:37 [INFO]: epoch 143: training loss 0.1547\n",
      "2025-06-01 14:51:37 [INFO]: epoch 144: training loss 0.1609\n",
      "2025-06-01 14:51:37 [INFO]: epoch 145: training loss 0.1438\n",
      "2025-06-01 14:51:37 [INFO]: epoch 146: training loss 0.1671\n",
      "2025-06-01 14:51:37 [INFO]: epoch 147: training loss 0.1469\n",
      "2025-06-01 14:51:37 [INFO]: epoch 148: training loss 0.1639\n",
      "2025-06-01 14:51:37 [INFO]: epoch 149: training loss 0.1519\n",
      "2025-06-01 14:51:37 [INFO]: epoch 150: training loss 0.1553\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-01 14:51:37 [INFO]: epoch 151: training loss 0.1546\n",
      "2025-06-01 14:51:37 [INFO]: epoch 152: training loss 0.1607\n",
      "2025-06-01 14:51:37 [INFO]: epoch 153: training loss 0.1525\n",
      "2025-06-01 14:51:37 [INFO]: epoch 154: training loss 0.1634\n",
      "2025-06-01 14:51:37 [INFO]: epoch 155: training loss 0.1463\n",
      "2025-06-01 14:51:37 [INFO]: epoch 156: training loss 0.1391\n",
      "2025-06-01 14:51:37 [INFO]: epoch 157: training loss 0.1511\n",
      "2025-06-01 14:51:37 [INFO]: epoch 158: training loss 0.1363\n",
      "2025-06-01 14:51:37 [INFO]: epoch 159: training loss 0.1441\n",
      "2025-06-01 14:51:37 [INFO]: epoch 160: training loss 0.1533\n",
      "2025-06-01 14:51:37 [INFO]: epoch 161: training loss 0.1455\n",
      "2025-06-01 14:51:37 [INFO]: epoch 162: training loss 0.1580\n",
      "2025-06-01 14:51:37 [INFO]: epoch 163: training loss 0.1467\n",
      "2025-06-01 14:51:37 [INFO]: epoch 164: training loss 0.1437\n",
      "2025-06-01 14:51:37 [INFO]: epoch 165: training loss 0.1513\n",
      "2025-06-01 14:51:37 [INFO]: epoch 166: training loss 0.1485\n",
      "2025-06-01 14:51:37 [INFO]: epoch 167: training loss 0.1654\n",
      "2025-06-01 14:51:37 [INFO]: epoch 168: training loss 0.1597\n",
      "2025-06-01 14:51:37 [INFO]: epoch 169: training loss 0.1484\n",
      "2025-06-01 14:51:37 [INFO]: epoch 170: training loss 0.1401\n",
      "2025-06-01 14:51:37 [INFO]: epoch 171: training loss 0.1374\n",
      "2025-06-01 14:51:37 [INFO]: epoch 172: training loss 0.1618\n",
      "2025-06-01 14:51:37 [INFO]: epoch 173: training loss 0.1657\n",
      "2025-06-01 14:51:37 [INFO]: epoch 174: training loss 0.1412\n",
      "2025-06-01 14:51:37 [INFO]: epoch 175: training loss 0.1624\n",
      "2025-06-01 14:51:37 [INFO]: epoch 176: training loss 0.1399\n",
      "2025-06-01 14:51:37 [INFO]: epoch 177: training loss 0.1395\n",
      "2025-06-01 14:51:37 [INFO]: epoch 178: training loss 0.1423\n",
      "2025-06-01 14:51:37 [INFO]: epoch 179: training loss 0.1441\n",
      "2025-06-01 14:51:37 [INFO]: epoch 180: training loss 0.1452\n",
      "2025-06-01 14:51:37 [INFO]: epoch 181: training loss 0.1352\n",
      "2025-06-01 14:51:37 [INFO]: epoch 182: training loss 0.1282\n",
      "2025-06-01 14:51:37 [INFO]: epoch 183: training loss 0.1320\n",
      "2025-06-01 14:51:37 [INFO]: epoch 184: training loss 0.1388\n",
      "2025-06-01 14:51:37 [INFO]: epoch 185: training loss 0.1309\n",
      "2025-06-01 14:51:37 [INFO]: epoch 186: training loss 0.1367\n",
      "2025-06-01 14:51:37 [INFO]: epoch 187: training loss 0.1421\n",
      "2025-06-01 14:51:37 [INFO]: epoch 188: training loss 0.1400\n",
      "2025-06-01 14:51:37 [INFO]: epoch 189: training loss 0.1425\n",
      "2025-06-01 14:51:37 [INFO]: epoch 190: training loss 0.1314\n",
      "2025-06-01 14:51:37 [INFO]: epoch 191: training loss 0.1367\n",
      "2025-06-01 14:51:37 [INFO]: epoch 192: training loss 0.1317\n",
      "2025-06-01 14:51:37 [INFO]: epoch 193: training loss 0.1309\n",
      "2025-06-01 14:51:37 [INFO]: epoch 194: training loss 0.1324\n",
      "2025-06-01 14:51:37 [INFO]: epoch 195: training loss 0.1346\n",
      "2025-06-01 14:51:37 [INFO]: epoch 196: training loss 0.1581\n",
      "2025-06-01 14:51:37 [INFO]: epoch 197: training loss 0.1349\n",
      "2025-06-01 14:51:38 [INFO]: epoch 198: training loss 0.1360\n",
      "2025-06-01 14:51:38 [INFO]: epoch 199: training loss 0.1261\n",
      "2025-06-01 14:51:38 [INFO]: epoch 200: training loss 0.1368\n",
      "2025-06-01 14:51:38 [INFO]: epoch 201: training loss 0.1429\n",
      "2025-06-01 14:51:38 [INFO]: epoch 202: training loss 0.1400\n",
      "2025-06-01 14:51:38 [INFO]: epoch 203: training loss 0.1210\n",
      "2025-06-01 14:51:38 [INFO]: epoch 204: training loss 0.1279\n",
      "2025-06-01 14:51:38 [INFO]: epoch 205: training loss 0.1312\n",
      "2025-06-01 14:51:38 [INFO]: epoch 206: training loss 0.1266\n",
      "2025-06-01 14:51:38 [INFO]: epoch 207: training loss 0.1323\n",
      "2025-06-01 14:51:38 [INFO]: epoch 208: training loss 0.1239\n",
      "2025-06-01 14:51:38 [INFO]: epoch 209: training loss 0.1189\n",
      "2025-06-01 14:51:38 [INFO]: epoch 210: training loss 0.1277\n",
      "2025-06-01 14:51:38 [INFO]: epoch 211: training loss 0.1086\n",
      "2025-06-01 14:51:38 [INFO]: epoch 212: training loss 0.1261\n",
      "2025-06-01 14:51:38 [INFO]: epoch 213: training loss 0.1393\n",
      "2025-06-01 14:51:38 [INFO]: epoch 214: training loss 0.1312\n",
      "2025-06-01 14:51:38 [INFO]: epoch 215: training loss 0.1035\n",
      "2025-06-01 14:51:38 [INFO]: epoch 216: training loss 0.1094\n",
      "2025-06-01 14:51:38 [INFO]: epoch 217: training loss 0.1220\n",
      "2025-06-01 14:51:38 [INFO]: epoch 218: training loss 0.1188\n",
      "2025-06-01 14:51:38 [INFO]: epoch 219: training loss 0.1145\n",
      "2025-06-01 14:51:38 [INFO]: epoch 220: training loss 0.1150\n",
      "2025-06-01 14:51:38 [INFO]: epoch 221: training loss 0.1126\n",
      "2025-06-01 14:51:38 [INFO]: epoch 222: training loss 0.1252\n",
      "2025-06-01 14:51:38 [INFO]: epoch 223: training loss 0.1190\n",
      "2025-06-01 14:51:38 [INFO]: epoch 224: training loss 0.1067\n",
      "2025-06-01 14:51:38 [INFO]: epoch 225: training loss 0.1258\n",
      "2025-06-01 14:51:38 [INFO]: epoch 226: training loss 0.1154\n",
      "2025-06-01 14:51:38 [INFO]: epoch 227: training loss 0.1174\n",
      "2025-06-01 14:51:38 [INFO]: epoch 228: training loss 0.1346\n",
      "2025-06-01 14:51:38 [INFO]: epoch 229: training loss 0.1166\n",
      "2025-06-01 14:51:38 [INFO]: epoch 230: training loss 0.1026\n",
      "2025-06-01 14:51:38 [INFO]: epoch 231: training loss 0.1171\n",
      "2025-06-01 14:51:38 [INFO]: epoch 232: training loss 0.1373\n",
      "2025-06-01 14:51:38 [INFO]: epoch 233: training loss 0.0994\n",
      "2025-06-01 14:51:38 [INFO]: epoch 234: training loss 0.1014\n",
      "2025-06-01 14:51:38 [INFO]: epoch 235: training loss 0.1193\n",
      "2025-06-01 14:51:38 [INFO]: epoch 236: training loss 0.1131\n",
      "2025-06-01 14:51:38 [INFO]: epoch 237: training loss 0.0972\n",
      "2025-06-01 14:51:38 [INFO]: epoch 238: training loss 0.1031\n",
      "2025-06-01 14:51:38 [INFO]: epoch 239: training loss 0.1171\n",
      "2025-06-01 14:51:38 [INFO]: epoch 240: training loss 0.1026\n",
      "2025-06-01 14:51:38 [INFO]: epoch 241: training loss 0.1093\n",
      "2025-06-01 14:51:38 [INFO]: epoch 242: training loss 0.1061\n",
      "2025-06-01 14:51:38 [INFO]: epoch 243: training loss 0.0980\n",
      "2025-06-01 14:51:38 [INFO]: epoch 244: training loss 0.0912\n",
      "2025-06-01 14:51:38 [INFO]: epoch 245: training loss 0.0852\n",
      "2025-06-01 14:51:38 [INFO]: epoch 246: training loss 0.0945\n",
      "2025-06-01 14:51:38 [INFO]: epoch 247: training loss 0.1069\n",
      "2025-06-01 14:51:38 [INFO]: epoch 248: training loss 0.1012\n",
      "2025-06-01 14:51:38 [INFO]: epoch 249: training loss 0.0912\n",
      "2025-06-01 14:51:38 [INFO]: epoch 250: training loss 0.1002\n",
      "2025-06-01 14:51:38 [INFO]: epoch 251: training loss 0.1066\n",
      "2025-06-01 14:51:38 [INFO]: epoch 252: training loss 0.1127\n",
      "2025-06-01 14:51:38 [INFO]: epoch 253: training loss 0.0912\n",
      "2025-06-01 14:51:38 [INFO]: epoch 254: training loss 0.0919\n",
      "2025-06-01 14:51:38 [INFO]: epoch 255: training loss 0.1008\n",
      "2025-06-01 14:51:38 [INFO]: epoch 256: training loss 0.1057\n",
      "2025-06-01 14:51:38 [INFO]: epoch 257: training loss 0.1022\n",
      "2025-06-01 14:51:38 [INFO]: epoch 258: training loss 0.0933\n",
      "2025-06-01 14:51:38 [INFO]: epoch 259: training loss 0.1016\n",
      "2025-06-01 14:51:38 [INFO]: epoch 260: training loss 0.1027\n",
      "2025-06-01 14:51:38 [INFO]: epoch 261: training loss 0.0980\n",
      "2025-06-01 14:51:38 [INFO]: epoch 262: training loss 0.0979\n",
      "2025-06-01 14:51:38 [INFO]: epoch 263: training loss 0.0879\n",
      "2025-06-01 14:51:38 [INFO]: epoch 264: training loss 0.0974\n",
      "2025-06-01 14:51:38 [INFO]: epoch 265: training loss 0.0934\n",
      "2025-06-01 14:51:38 [INFO]: epoch 266: training loss 0.0814\n",
      "2025-06-01 14:51:38 [INFO]: epoch 267: training loss 0.0911\n",
      "2025-06-01 14:51:38 [INFO]: epoch 268: training loss 0.1056\n",
      "2025-06-01 14:51:38 [INFO]: epoch 269: training loss 0.0892\n",
      "2025-06-01 14:51:38 [INFO]: epoch 270: training loss 0.0899\n",
      "2025-06-01 14:51:38 [INFO]: epoch 271: training loss 0.0934\n",
      "2025-06-01 14:51:38 [INFO]: epoch 272: training loss 0.0770\n",
      "2025-06-01 14:51:38 [INFO]: epoch 273: training loss 0.0986\n",
      "2025-06-01 14:51:39 [INFO]: epoch 274: training loss 0.1000\n",
      "2025-06-01 14:51:39 [INFO]: epoch 275: training loss 0.0838\n",
      "2025-06-01 14:51:39 [INFO]: epoch 276: training loss 0.0865\n",
      "2025-06-01 14:51:39 [INFO]: epoch 277: training loss 0.0762\n",
      "2025-06-01 14:51:39 [INFO]: epoch 278: training loss 0.0999\n",
      "2025-06-01 14:51:39 [INFO]: epoch 279: training loss 0.1049\n",
      "2025-06-01 14:51:39 [INFO]: epoch 280: training loss 0.0840\n",
      "2025-06-01 14:51:39 [INFO]: epoch 281: training loss 0.0740\n",
      "2025-06-01 14:51:39 [INFO]: epoch 282: training loss 0.0973\n",
      "2025-06-01 14:51:39 [INFO]: epoch 283: training loss 0.0859\n",
      "2025-06-01 14:51:39 [INFO]: epoch 284: training loss 0.0915\n",
      "2025-06-01 14:51:39 [INFO]: epoch 285: training loss 0.0855\n",
      "2025-06-01 14:51:39 [INFO]: epoch 286: training loss 0.0856\n",
      "2025-06-01 14:51:39 [INFO]: epoch 287: training loss 0.0782\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-01 14:51:39 [INFO]: epoch 288: training loss 0.0929\n",
      "2025-06-01 14:51:39 [INFO]: epoch 289: training loss 0.0886\n",
      "2025-06-01 14:51:39 [INFO]: epoch 290: training loss 0.0956\n",
      "2025-06-01 14:51:39 [INFO]: epoch 291: training loss 0.0939\n",
      "2025-06-01 14:51:39 [INFO]: epoch 292: training loss 0.0873\n",
      "2025-06-01 14:51:39 [INFO]: epoch 293: training loss 0.0896\n",
      "2025-06-01 14:51:39 [INFO]: epoch 294: training loss 0.0847\n",
      "2025-06-01 14:51:39 [INFO]: epoch 295: training loss 0.0880\n",
      "2025-06-01 14:51:39 [INFO]: epoch 296: training loss 0.0811\n",
      "2025-06-01 14:51:39 [INFO]: epoch 297: training loss 0.0694\n",
      "2025-06-01 14:51:39 [INFO]: epoch 298: training loss 0.0844\n",
      "2025-06-01 14:51:39 [INFO]: epoch 299: training loss 0.0803\n",
      "2025-06-01 14:51:39 [INFO]: epoch 300: training loss 0.0894\n",
      "2025-06-01 14:51:39 [INFO]: epoch 301: training loss 0.0805\n",
      "2025-06-01 14:51:39 [INFO]: epoch 302: training loss 0.0828\n",
      "2025-06-01 14:51:39 [INFO]: epoch 303: training loss 0.0773\n",
      "2025-06-01 14:51:39 [INFO]: epoch 304: training loss 0.0817\n",
      "2025-06-01 14:51:39 [INFO]: epoch 305: training loss 0.0746\n",
      "2025-06-01 14:51:39 [INFO]: epoch 306: training loss 0.0803\n",
      "2025-06-01 14:51:39 [INFO]: epoch 307: training loss 0.0779\n",
      "2025-06-01 14:51:39 [INFO]: epoch 308: training loss 0.0830\n",
      "2025-06-01 14:51:39 [INFO]: epoch 309: training loss 0.0717\n",
      "2025-06-01 14:51:39 [INFO]: epoch 310: training loss 0.0743\n",
      "2025-06-01 14:51:39 [INFO]: epoch 311: training loss 0.0776\n",
      "2025-06-01 14:51:39 [INFO]: epoch 312: training loss 0.0805\n",
      "2025-06-01 14:51:39 [INFO]: epoch 313: training loss 0.0784\n",
      "2025-06-01 14:51:39 [INFO]: epoch 314: training loss 0.0936\n",
      "2025-06-01 14:51:39 [INFO]: epoch 315: training loss 0.0713\n",
      "2025-06-01 14:51:39 [INFO]: epoch 316: training loss 0.0743\n",
      "2025-06-01 14:51:39 [INFO]: epoch 317: training loss 0.0753\n",
      "2025-06-01 14:51:39 [INFO]: epoch 318: training loss 0.0860\n",
      "2025-06-01 14:51:39 [INFO]: epoch 319: training loss 0.0866\n",
      "2025-06-01 14:51:39 [INFO]: epoch 320: training loss 0.0694\n",
      "2025-06-01 14:51:39 [INFO]: epoch 321: training loss 0.0786\n",
      "2025-06-01 14:51:39 [INFO]: epoch 322: training loss 0.0758\n",
      "2025-06-01 14:51:39 [INFO]: epoch 323: training loss 0.0746\n",
      "2025-06-01 14:51:39 [INFO]: epoch 324: training loss 0.0791\n",
      "2025-06-01 14:51:39 [INFO]: epoch 325: training loss 0.0748\n",
      "2025-06-01 14:51:39 [INFO]: epoch 326: training loss 0.0734\n",
      "2025-06-01 14:51:39 [INFO]: epoch 327: training loss 0.0744\n",
      "2025-06-01 14:51:39 [INFO]: epoch 328: training loss 0.0685\n",
      "2025-06-01 14:51:39 [INFO]: epoch 329: training loss 0.0649\n",
      "2025-06-01 14:51:39 [INFO]: epoch 330: training loss 0.0755\n",
      "2025-06-01 14:51:39 [INFO]: epoch 331: training loss 0.0679\n",
      "2025-06-01 14:51:39 [INFO]: epoch 332: training loss 0.0707\n",
      "2025-06-01 14:51:39 [INFO]: epoch 333: training loss 0.0727\n",
      "2025-06-01 14:51:39 [INFO]: epoch 334: training loss 0.0667\n",
      "2025-06-01 14:51:39 [INFO]: epoch 335: training loss 0.0741\n",
      "2025-06-01 14:51:39 [INFO]: epoch 336: training loss 0.0790\n",
      "2025-06-01 14:51:39 [INFO]: epoch 337: training loss 0.0676\n",
      "2025-06-01 14:51:39 [INFO]: epoch 338: training loss 0.0574\n",
      "2025-06-01 14:51:39 [INFO]: epoch 339: training loss 0.0641\n",
      "2025-06-01 14:51:39 [INFO]: epoch 340: training loss 0.0750\n",
      "2025-06-01 14:51:39 [INFO]: epoch 341: training loss 0.0689\n",
      "2025-06-01 14:51:39 [INFO]: epoch 342: training loss 0.0682\n",
      "2025-06-01 14:51:39 [INFO]: epoch 343: training loss 0.0673\n",
      "2025-06-01 14:51:39 [INFO]: epoch 344: training loss 0.0645\n",
      "2025-06-01 14:51:39 [INFO]: epoch 345: training loss 0.0697\n",
      "2025-06-01 14:51:39 [INFO]: epoch 346: training loss 0.0690\n",
      "2025-06-01 14:51:39 [INFO]: epoch 347: training loss 0.0726\n",
      "2025-06-01 14:51:39 [INFO]: epoch 348: training loss 0.0614\n",
      "2025-06-01 14:51:39 [INFO]: epoch 349: training loss 0.0648\n",
      "2025-06-01 14:51:40 [INFO]: epoch 350: training loss 0.0657\n",
      "2025-06-01 14:51:40 [INFO]: epoch 351: training loss 0.0696\n",
      "2025-06-01 14:51:40 [INFO]: epoch 352: training loss 0.0664\n",
      "2025-06-01 14:51:40 [INFO]: epoch 353: training loss 0.0683\n",
      "2025-06-01 14:51:40 [INFO]: epoch 354: training loss 0.0705\n",
      "2025-06-01 14:51:40 [INFO]: epoch 355: training loss 0.0597\n",
      "2025-06-01 14:51:40 [INFO]: epoch 356: training loss 0.0685\n",
      "2025-06-01 14:51:40 [INFO]: epoch 357: training loss 0.0679\n",
      "2025-06-01 14:51:40 [INFO]: epoch 358: training loss 0.0623\n",
      "2025-06-01 14:51:40 [INFO]: epoch 359: training loss 0.0615\n",
      "2025-06-01 14:51:40 [INFO]: epoch 360: training loss 0.0652\n",
      "2025-06-01 14:51:40 [INFO]: epoch 361: training loss 0.0630\n",
      "2025-06-01 14:51:40 [INFO]: epoch 362: training loss 0.0596\n",
      "2025-06-01 14:51:40 [INFO]: epoch 363: training loss 0.0613\n",
      "2025-06-01 14:51:40 [INFO]: epoch 364: training loss 0.0664\n",
      "2025-06-01 14:51:40 [INFO]: epoch 365: training loss 0.0617\n",
      "2025-06-01 14:51:40 [INFO]: epoch 366: training loss 0.0663\n",
      "2025-06-01 14:51:40 [INFO]: epoch 367: training loss 0.0551\n",
      "2025-06-01 14:51:40 [INFO]: epoch 368: training loss 0.0600\n",
      "2025-06-01 14:51:40 [INFO]: epoch 369: training loss 0.0623\n",
      "2025-06-01 14:51:40 [INFO]: epoch 370: training loss 0.0615\n",
      "2025-06-01 14:51:40 [INFO]: epoch 371: training loss 0.0643\n",
      "2025-06-01 14:51:40 [INFO]: epoch 372: training loss 0.0540\n",
      "2025-06-01 14:51:40 [INFO]: epoch 373: training loss 0.0581\n",
      "2025-06-01 14:51:40 [INFO]: epoch 374: training loss 0.0616\n",
      "2025-06-01 14:51:40 [INFO]: epoch 375: training loss 0.0513\n",
      "2025-06-01 14:51:40 [INFO]: epoch 376: training loss 0.0616\n",
      "2025-06-01 14:51:40 [INFO]: epoch 377: training loss 0.0520\n",
      "2025-06-01 14:51:40 [INFO]: epoch 378: training loss 0.0681\n",
      "2025-06-01 14:51:40 [INFO]: epoch 379: training loss 0.0540\n",
      "2025-06-01 14:51:40 [INFO]: epoch 380: training loss 0.0569\n",
      "2025-06-01 14:51:40 [INFO]: epoch 381: training loss 0.0567\n",
      "2025-06-01 14:51:40 [INFO]: epoch 382: training loss 0.0545\n",
      "2025-06-01 14:51:40 [INFO]: epoch 383: training loss 0.0576\n",
      "2025-06-01 14:51:40 [INFO]: epoch 384: training loss 0.0532\n",
      "2025-06-01 14:51:40 [INFO]: epoch 385: training loss 0.0543\n",
      "2025-06-01 14:51:40 [INFO]: epoch 386: training loss 0.0544\n",
      "2025-06-01 14:51:40 [INFO]: epoch 387: training loss 0.0515\n",
      "2025-06-01 14:51:40 [INFO]: epoch 388: training loss 0.0567\n",
      "2025-06-01 14:51:40 [INFO]: epoch 389: training loss 0.0515\n",
      "2025-06-01 14:51:40 [INFO]: epoch 390: training loss 0.0592\n",
      "2025-06-01 14:51:40 [INFO]: epoch 391: training loss 0.0652\n",
      "2025-06-01 14:51:40 [INFO]: epoch 392: training loss 0.0574\n",
      "2025-06-01 14:51:40 [INFO]: epoch 393: training loss 0.0495\n",
      "2025-06-01 14:51:40 [INFO]: epoch 394: training loss 0.0577\n",
      "2025-06-01 14:51:40 [INFO]: epoch 395: training loss 0.0544\n",
      "2025-06-01 14:51:40 [INFO]: epoch 396: training loss 0.0639\n",
      "2025-06-01 14:51:40 [INFO]: epoch 397: training loss 0.0556\n",
      "2025-06-01 14:51:40 [INFO]: epoch 398: training loss 0.0588\n",
      "2025-06-01 14:51:40 [INFO]: epoch 399: training loss 0.0471\n",
      "2025-06-01 14:51:40 [INFO]: Finished training.\n",
      "2025-06-01 14:51:40 [WARNING]: 🚨DeprecationWarning: The method impute is deprecated. Please use `predict` instead.\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 6/6 [00:32<00:00,  5.35s/it]\n",
      "  0%|                                                                                            | 0/6 [00:00<?, ?it/s]2025-06-01 14:51:40 [INFO]: No given device, using default device: cuda\n",
      "2025-06-01 14:51:40 [WARNING]: saving_path not given. Model files and tensorboard file will not be saved.\n",
      "2025-06-01 14:51:40 [INFO]: Model initialized successfully with the number of trainable parameters: 597,780\n",
      "2025-06-01 14:51:40 [INFO]: epoch 0: training loss 0.7660\n",
      "2025-06-01 14:51:40 [INFO]: epoch 1: training loss 0.8672\n",
      "2025-06-01 14:51:40 [INFO]: epoch 2: training loss 0.5241\n",
      "2025-06-01 14:51:40 [INFO]: epoch 3: training loss 0.4706\n",
      "2025-06-01 14:51:40 [INFO]: epoch 4: training loss 0.4926\n",
      "2025-06-01 14:51:40 [INFO]: epoch 5: training loss 0.5352\n",
      "2025-06-01 14:51:40 [INFO]: epoch 6: training loss 0.4052\n",
      "2025-06-01 14:51:40 [INFO]: epoch 7: training loss 0.3559\n",
      "2025-06-01 14:51:40 [INFO]: epoch 8: training loss 0.3889\n",
      "2025-06-01 14:51:40 [INFO]: epoch 9: training loss 0.3671\n",
      "2025-06-01 14:51:40 [INFO]: epoch 10: training loss 0.3809\n",
      "2025-06-01 14:51:40 [INFO]: epoch 11: training loss 0.3351\n",
      "2025-06-01 14:51:40 [INFO]: epoch 12: training loss 0.3767\n",
      "2025-06-01 14:51:40 [INFO]: epoch 13: training loss 0.3414\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-01 14:51:40 [INFO]: epoch 14: training loss 0.3441\n",
      "2025-06-01 14:51:40 [INFO]: epoch 15: training loss 0.3519\n",
      "2025-06-01 14:51:40 [INFO]: epoch 16: training loss 0.3216\n",
      "2025-06-01 14:51:40 [INFO]: epoch 17: training loss 0.3083\n",
      "2025-06-01 14:51:40 [INFO]: epoch 18: training loss 0.3076\n",
      "2025-06-01 14:51:40 [INFO]: epoch 19: training loss 0.3076\n",
      "2025-06-01 14:51:41 [INFO]: epoch 20: training loss 0.3265\n",
      "2025-06-01 14:51:41 [INFO]: epoch 21: training loss 0.3368\n",
      "2025-06-01 14:51:41 [INFO]: epoch 22: training loss 0.3398\n",
      "2025-06-01 14:51:41 [INFO]: epoch 23: training loss 0.3080\n",
      "2025-06-01 14:51:41 [INFO]: epoch 24: training loss 0.2927\n",
      "2025-06-01 14:51:41 [INFO]: epoch 25: training loss 0.3055\n",
      "2025-06-01 14:51:41 [INFO]: epoch 26: training loss 0.2877\n",
      "2025-06-01 14:51:41 [INFO]: epoch 27: training loss 0.3235\n",
      "2025-06-01 14:51:41 [INFO]: epoch 28: training loss 0.3158\n",
      "2025-06-01 14:51:41 [INFO]: epoch 29: training loss 0.2905\n",
      "2025-06-01 14:51:41 [INFO]: epoch 30: training loss 0.3064\n",
      "2025-06-01 14:51:41 [INFO]: epoch 31: training loss 0.2905\n",
      "2025-06-01 14:51:41 [INFO]: epoch 32: training loss 0.2772\n",
      "2025-06-01 14:51:41 [INFO]: epoch 33: training loss 0.2821\n",
      "2025-06-01 14:51:41 [INFO]: epoch 34: training loss 0.2848\n",
      "2025-06-01 14:51:41 [INFO]: epoch 35: training loss 0.2860\n",
      "2025-06-01 14:51:41 [INFO]: epoch 36: training loss 0.2637\n",
      "2025-06-01 14:51:41 [INFO]: epoch 37: training loss 0.2547\n",
      "2025-06-01 14:51:41 [INFO]: epoch 38: training loss 0.2776\n",
      "2025-06-01 14:51:41 [INFO]: epoch 39: training loss 0.2637\n",
      "2025-06-01 14:51:41 [INFO]: epoch 40: training loss 0.2679\n",
      "2025-06-01 14:51:41 [INFO]: epoch 41: training loss 0.2627\n",
      "2025-06-01 14:51:41 [INFO]: epoch 42: training loss 0.2558\n",
      "2025-06-01 14:51:41 [INFO]: epoch 43: training loss 0.2490\n",
      "2025-06-01 14:51:41 [INFO]: epoch 44: training loss 0.2723\n",
      "2025-06-01 14:51:41 [INFO]: epoch 45: training loss 0.2607\n",
      "2025-06-01 14:51:41 [INFO]: epoch 46: training loss 0.2609\n",
      "2025-06-01 14:51:41 [INFO]: epoch 47: training loss 0.2769\n",
      "2025-06-01 14:51:41 [INFO]: epoch 48: training loss 0.2680\n",
      "2025-06-01 14:51:41 [INFO]: epoch 49: training loss 0.2500\n",
      "2025-06-01 14:51:41 [INFO]: epoch 50: training loss 0.2697\n",
      "2025-06-01 14:51:41 [INFO]: epoch 51: training loss 0.2468\n",
      "2025-06-01 14:51:41 [INFO]: epoch 52: training loss 0.2599\n",
      "2025-06-01 14:51:41 [INFO]: epoch 53: training loss 0.2705\n",
      "2025-06-01 14:51:41 [INFO]: epoch 54: training loss 0.2531\n",
      "2025-06-01 14:51:41 [INFO]: epoch 55: training loss 0.2497\n",
      "2025-06-01 14:51:41 [INFO]: epoch 56: training loss 0.2464\n",
      "2025-06-01 14:51:41 [INFO]: epoch 57: training loss 0.2450\n",
      "2025-06-01 14:51:41 [INFO]: epoch 58: training loss 0.2543\n",
      "2025-06-01 14:51:41 [INFO]: epoch 59: training loss 0.2394\n",
      "2025-06-01 14:51:41 [INFO]: epoch 60: training loss 0.2421\n",
      "2025-06-01 14:51:41 [INFO]: epoch 61: training loss 0.2425\n",
      "2025-06-01 14:51:41 [INFO]: epoch 62: training loss 0.2327\n",
      "2025-06-01 14:51:41 [INFO]: epoch 63: training loss 0.2317\n",
      "2025-06-01 14:51:41 [INFO]: epoch 64: training loss 0.2352\n",
      "2025-06-01 14:51:41 [INFO]: epoch 65: training loss 0.2377\n",
      "2025-06-01 14:51:41 [INFO]: epoch 66: training loss 0.2332\n",
      "2025-06-01 14:51:41 [INFO]: epoch 67: training loss 0.2405\n",
      "2025-06-01 14:51:41 [INFO]: epoch 68: training loss 0.2350\n",
      "2025-06-01 14:51:41 [INFO]: epoch 69: training loss 0.2298\n",
      "2025-06-01 14:51:41 [INFO]: epoch 70: training loss 0.2299\n",
      "2025-06-01 14:51:41 [INFO]: epoch 71: training loss 0.2458\n",
      "2025-06-01 14:51:41 [INFO]: epoch 72: training loss 0.2201\n",
      "2025-06-01 14:51:41 [INFO]: epoch 73: training loss 0.2332\n",
      "2025-06-01 14:51:41 [INFO]: epoch 74: training loss 0.2299\n",
      "2025-06-01 14:51:41 [INFO]: epoch 75: training loss 0.2326\n",
      "2025-06-01 14:51:41 [INFO]: epoch 76: training loss 0.2477\n",
      "2025-06-01 14:51:41 [INFO]: epoch 77: training loss 0.2136\n",
      "2025-06-01 14:51:41 [INFO]: epoch 78: training loss 0.2562\n",
      "2025-06-01 14:51:41 [INFO]: epoch 79: training loss 0.2523\n",
      "2025-06-01 14:51:41 [INFO]: epoch 80: training loss 0.2130\n",
      "2025-06-01 14:51:41 [INFO]: epoch 81: training loss 0.2267\n",
      "2025-06-01 14:51:41 [INFO]: epoch 82: training loss 0.2166\n",
      "2025-06-01 14:51:41 [INFO]: epoch 83: training loss 0.2359\n",
      "2025-06-01 14:51:41 [INFO]: epoch 84: training loss 0.2248\n",
      "2025-06-01 14:51:41 [INFO]: epoch 85: training loss 0.2223\n",
      "2025-06-01 14:51:41 [INFO]: epoch 86: training loss 0.2106\n",
      "2025-06-01 14:51:41 [INFO]: epoch 87: training loss 0.2214\n",
      "2025-06-01 14:51:41 [INFO]: epoch 88: training loss 0.2217\n",
      "2025-06-01 14:51:41 [INFO]: epoch 89: training loss 0.2253\n",
      "2025-06-01 14:51:41 [INFO]: epoch 90: training loss 0.1869\n",
      "2025-06-01 14:51:41 [INFO]: epoch 91: training loss 0.2113\n",
      "2025-06-01 14:51:41 [INFO]: epoch 92: training loss 0.2251\n",
      "2025-06-01 14:51:41 [INFO]: epoch 93: training loss 0.2101\n",
      "2025-06-01 14:51:42 [INFO]: epoch 94: training loss 0.2157\n",
      "2025-06-01 14:51:42 [INFO]: epoch 95: training loss 0.2011\n",
      "2025-06-01 14:51:42 [INFO]: epoch 96: training loss 0.1945\n",
      "2025-06-01 14:51:42 [INFO]: epoch 97: training loss 0.2110\n",
      "2025-06-01 14:51:42 [INFO]: epoch 98: training loss 0.2266\n",
      "2025-06-01 14:51:42 [INFO]: epoch 99: training loss 0.2121\n",
      "2025-06-01 14:51:42 [INFO]: epoch 100: training loss 0.1897\n",
      "2025-06-01 14:51:42 [INFO]: epoch 101: training loss 0.2211\n",
      "2025-06-01 14:51:42 [INFO]: epoch 102: training loss 0.2219\n",
      "2025-06-01 14:51:42 [INFO]: epoch 103: training loss 0.2090\n",
      "2025-06-01 14:51:42 [INFO]: epoch 104: training loss 0.2094\n",
      "2025-06-01 14:51:42 [INFO]: epoch 105: training loss 0.2024\n",
      "2025-06-01 14:51:42 [INFO]: epoch 106: training loss 0.2142\n",
      "2025-06-01 14:51:42 [INFO]: epoch 107: training loss 0.1963\n",
      "2025-06-01 14:51:42 [INFO]: epoch 108: training loss 0.2089\n",
      "2025-06-01 14:51:42 [INFO]: epoch 109: training loss 0.2132\n",
      "2025-06-01 14:51:42 [INFO]: epoch 110: training loss 0.2089\n",
      "2025-06-01 14:51:42 [INFO]: epoch 111: training loss 0.2040\n",
      "2025-06-01 14:51:42 [INFO]: epoch 112: training loss 0.2100\n",
      "2025-06-01 14:51:42 [INFO]: epoch 113: training loss 0.2036\n",
      "2025-06-01 14:51:42 [INFO]: epoch 114: training loss 0.2028\n",
      "2025-06-01 14:51:42 [INFO]: epoch 115: training loss 0.2096\n",
      "2025-06-01 14:51:42 [INFO]: epoch 116: training loss 0.1900\n",
      "2025-06-01 14:51:42 [INFO]: epoch 117: training loss 0.1934\n",
      "2025-06-01 14:51:42 [INFO]: epoch 118: training loss 0.2084\n",
      "2025-06-01 14:51:42 [INFO]: epoch 119: training loss 0.2104\n",
      "2025-06-01 14:51:42 [INFO]: epoch 120: training loss 0.1985\n",
      "2025-06-01 14:51:42 [INFO]: epoch 121: training loss 0.2028\n",
      "2025-06-01 14:51:42 [INFO]: epoch 122: training loss 0.2214\n",
      "2025-06-01 14:51:42 [INFO]: epoch 123: training loss 0.2059\n",
      "2025-06-01 14:51:42 [INFO]: epoch 124: training loss 0.1964\n",
      "2025-06-01 14:51:42 [INFO]: epoch 125: training loss 0.1765\n",
      "2025-06-01 14:51:42 [INFO]: epoch 126: training loss 0.2197\n",
      "2025-06-01 14:51:42 [INFO]: epoch 127: training loss 0.1971\n",
      "2025-06-01 14:51:42 [INFO]: epoch 128: training loss 0.1818\n",
      "2025-06-01 14:51:42 [INFO]: epoch 129: training loss 0.1861\n",
      "2025-06-01 14:51:42 [INFO]: epoch 130: training loss 0.1946\n",
      "2025-06-01 14:51:42 [INFO]: epoch 131: training loss 0.1978\n",
      "2025-06-01 14:51:42 [INFO]: epoch 132: training loss 0.1999\n",
      "2025-06-01 14:51:42 [INFO]: epoch 133: training loss 0.1904\n",
      "2025-06-01 14:51:42 [INFO]: epoch 134: training loss 0.1732\n",
      "2025-06-01 14:51:42 [INFO]: epoch 135: training loss 0.1861\n",
      "2025-06-01 14:51:42 [INFO]: epoch 136: training loss 0.1990\n",
      "2025-06-01 14:51:42 [INFO]: epoch 137: training loss 0.1944\n",
      "2025-06-01 14:51:42 [INFO]: epoch 138: training loss 0.1813\n",
      "2025-06-01 14:51:42 [INFO]: epoch 139: training loss 0.1785\n",
      "2025-06-01 14:51:42 [INFO]: epoch 140: training loss 0.1802\n",
      "2025-06-01 14:51:42 [INFO]: epoch 141: training loss 0.1773\n",
      "2025-06-01 14:51:42 [INFO]: epoch 142: training loss 0.1875\n",
      "2025-06-01 14:51:42 [INFO]: epoch 143: training loss 0.1845\n",
      "2025-06-01 14:51:42 [INFO]: epoch 144: training loss 0.1922\n",
      "2025-06-01 14:51:42 [INFO]: epoch 145: training loss 0.1855\n",
      "2025-06-01 14:51:42 [INFO]: epoch 146: training loss 0.1777\n",
      "2025-06-01 14:51:42 [INFO]: epoch 147: training loss 0.1883\n",
      "2025-06-01 14:51:42 [INFO]: epoch 148: training loss 0.2011\n",
      "2025-06-01 14:51:42 [INFO]: epoch 149: training loss 0.1952\n",
      "2025-06-01 14:51:42 [INFO]: epoch 150: training loss 0.1890\n",
      "2025-06-01 14:51:42 [INFO]: epoch 151: training loss 0.1825\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-01 14:51:42 [INFO]: epoch 152: training loss 0.1794\n",
      "2025-06-01 14:51:42 [INFO]: epoch 153: training loss 0.1860\n",
      "2025-06-01 14:51:42 [INFO]: epoch 154: training loss 0.1930\n",
      "2025-06-01 14:51:42 [INFO]: epoch 155: training loss 0.1911\n",
      "2025-06-01 14:51:42 [INFO]: epoch 156: training loss 0.1780\n",
      "2025-06-01 14:51:42 [INFO]: epoch 157: training loss 0.1746\n",
      "2025-06-01 14:51:42 [INFO]: epoch 158: training loss 0.1664\n",
      "2025-06-01 14:51:42 [INFO]: epoch 159: training loss 0.1918\n",
      "2025-06-01 14:51:42 [INFO]: epoch 160: training loss 0.1787\n",
      "2025-06-01 14:51:42 [INFO]: epoch 161: training loss 0.1580\n",
      "2025-06-01 14:51:42 [INFO]: epoch 162: training loss 0.1508\n",
      "2025-06-01 14:51:42 [INFO]: epoch 163: training loss 0.1675\n",
      "2025-06-01 14:51:42 [INFO]: epoch 164: training loss 0.1614\n",
      "2025-06-01 14:51:42 [INFO]: epoch 165: training loss 0.1688\n",
      "2025-06-01 14:51:42 [INFO]: epoch 166: training loss 0.1602\n",
      "2025-06-01 14:51:42 [INFO]: epoch 167: training loss 0.1691\n",
      "2025-06-01 14:51:42 [INFO]: epoch 168: training loss 0.1950\n",
      "2025-06-01 14:51:42 [INFO]: epoch 169: training loss 0.1792\n",
      "2025-06-01 14:51:42 [INFO]: epoch 170: training loss 0.1724\n",
      "2025-06-01 14:51:42 [INFO]: epoch 171: training loss 0.1808\n",
      "2025-06-01 14:51:43 [INFO]: epoch 172: training loss 0.1578\n",
      "2025-06-01 14:51:43 [INFO]: epoch 173: training loss 0.1734\n",
      "2025-06-01 14:51:43 [INFO]: epoch 174: training loss 0.1763\n",
      "2025-06-01 14:51:43 [INFO]: epoch 175: training loss 0.1684\n",
      "2025-06-01 14:51:43 [INFO]: epoch 176: training loss 0.1771\n",
      "2025-06-01 14:51:43 [INFO]: epoch 177: training loss 0.1487\n",
      "2025-06-01 14:51:43 [INFO]: epoch 178: training loss 0.1643\n",
      "2025-06-01 14:51:43 [INFO]: epoch 179: training loss 0.1850\n",
      "2025-06-01 14:51:43 [INFO]: epoch 180: training loss 0.1660\n",
      "2025-06-01 14:51:43 [INFO]: epoch 181: training loss 0.1743\n",
      "2025-06-01 14:51:43 [INFO]: epoch 182: training loss 0.1732\n",
      "2025-06-01 14:51:43 [INFO]: epoch 183: training loss 0.1645\n",
      "2025-06-01 14:51:43 [INFO]: epoch 184: training loss 0.1636\n",
      "2025-06-01 14:51:43 [INFO]: epoch 185: training loss 0.1664\n",
      "2025-06-01 14:51:43 [INFO]: epoch 186: training loss 0.1558\n",
      "2025-06-01 14:51:43 [INFO]: epoch 187: training loss 0.1554\n",
      "2025-06-01 14:51:43 [INFO]: epoch 188: training loss 0.1660\n",
      "2025-06-01 14:51:43 [INFO]: epoch 189: training loss 0.1586\n",
      "2025-06-01 14:51:43 [INFO]: epoch 190: training loss 0.1398\n",
      "2025-06-01 14:51:43 [INFO]: epoch 191: training loss 0.1551\n",
      "2025-06-01 14:51:43 [INFO]: epoch 192: training loss 0.1703\n",
      "2025-06-01 14:51:43 [INFO]: epoch 193: training loss 0.1541\n",
      "2025-06-01 14:51:43 [INFO]: epoch 194: training loss 0.1509\n",
      "2025-06-01 14:51:43 [INFO]: epoch 195: training loss 0.1453\n",
      "2025-06-01 14:51:43 [INFO]: epoch 196: training loss 0.1505\n",
      "2025-06-01 14:51:43 [INFO]: epoch 197: training loss 0.1563\n",
      "2025-06-01 14:51:43 [INFO]: epoch 198: training loss 0.1580\n",
      "2025-06-01 14:51:43 [INFO]: epoch 199: training loss 0.1428\n",
      "2025-06-01 14:51:43 [INFO]: epoch 200: training loss 0.1529\n",
      "2025-06-01 14:51:43 [INFO]: epoch 201: training loss 0.1491\n",
      "2025-06-01 14:51:43 [INFO]: epoch 202: training loss 0.1617\n",
      "2025-06-01 14:51:43 [INFO]: epoch 203: training loss 0.1656\n",
      "2025-06-01 14:51:43 [INFO]: epoch 204: training loss 0.1634\n",
      "2025-06-01 14:51:43 [INFO]: epoch 205: training loss 0.1635\n",
      "2025-06-01 14:51:43 [INFO]: epoch 206: training loss 0.1486\n",
      "2025-06-01 14:51:43 [INFO]: epoch 207: training loss 0.1560\n",
      "2025-06-01 14:51:43 [INFO]: epoch 208: training loss 0.1539\n",
      "2025-06-01 14:51:43 [INFO]: epoch 209: training loss 0.1638\n",
      "2025-06-01 14:51:43 [INFO]: epoch 210: training loss 0.1607\n",
      "2025-06-01 14:51:43 [INFO]: epoch 211: training loss 0.1558\n",
      "2025-06-01 14:51:43 [INFO]: epoch 212: training loss 0.1471\n",
      "2025-06-01 14:51:43 [INFO]: epoch 213: training loss 0.1492\n",
      "2025-06-01 14:51:43 [INFO]: epoch 214: training loss 0.1444\n",
      "2025-06-01 14:51:43 [INFO]: epoch 215: training loss 0.1415\n",
      "2025-06-01 14:51:43 [INFO]: epoch 216: training loss 0.1434\n",
      "2025-06-01 14:51:43 [INFO]: epoch 217: training loss 0.1434\n",
      "2025-06-01 14:51:43 [INFO]: epoch 218: training loss 0.1289\n",
      "2025-06-01 14:51:43 [INFO]: epoch 219: training loss 0.1419\n",
      "2025-06-01 14:51:43 [INFO]: epoch 220: training loss 0.1484\n",
      "2025-06-01 14:51:43 [INFO]: epoch 221: training loss 0.1448\n",
      "2025-06-01 14:51:43 [INFO]: epoch 222: training loss 0.1376\n",
      "2025-06-01 14:51:43 [INFO]: epoch 223: training loss 0.1352\n",
      "2025-06-01 14:51:43 [INFO]: epoch 224: training loss 0.1321\n",
      "2025-06-01 14:51:43 [INFO]: epoch 225: training loss 0.1458\n",
      "2025-06-01 14:51:43 [INFO]: epoch 226: training loss 0.1277\n",
      "2025-06-01 14:51:43 [INFO]: epoch 227: training loss 0.1352\n",
      "2025-06-01 14:51:43 [INFO]: epoch 228: training loss 0.1403\n",
      "2025-06-01 14:51:43 [INFO]: epoch 229: training loss 0.1396\n",
      "2025-06-01 14:51:43 [INFO]: epoch 230: training loss 0.1387\n",
      "2025-06-01 14:51:43 [INFO]: epoch 231: training loss 0.1393\n",
      "2025-06-01 14:51:43 [INFO]: epoch 232: training loss 0.1466\n",
      "2025-06-01 14:51:43 [INFO]: epoch 233: training loss 0.1444\n",
      "2025-06-01 14:51:43 [INFO]: epoch 234: training loss 0.1345\n",
      "2025-06-01 14:51:43 [INFO]: epoch 235: training loss 0.1271\n",
      "2025-06-01 14:51:43 [INFO]: epoch 236: training loss 0.1319\n",
      "2025-06-01 14:51:43 [INFO]: epoch 237: training loss 0.1454\n",
      "2025-06-01 14:51:43 [INFO]: epoch 238: training loss 0.1391\n",
      "2025-06-01 14:51:43 [INFO]: epoch 239: training loss 0.1341\n",
      "2025-06-01 14:51:43 [INFO]: epoch 240: training loss 0.1438\n",
      "2025-06-01 14:51:43 [INFO]: epoch 241: training loss 0.1345\n",
      "2025-06-01 14:51:43 [INFO]: epoch 242: training loss 0.1277\n",
      "2025-06-01 14:51:43 [INFO]: epoch 243: training loss 0.1394\n",
      "2025-06-01 14:51:43 [INFO]: epoch 244: training loss 0.1490\n",
      "2025-06-01 14:51:43 [INFO]: epoch 245: training loss 0.1566\n",
      "2025-06-01 14:51:43 [INFO]: epoch 246: training loss 0.1448\n",
      "2025-06-01 14:51:43 [INFO]: epoch 247: training loss 0.1321\n",
      "2025-06-01 14:51:43 [INFO]: epoch 248: training loss 0.1272\n",
      "2025-06-01 14:51:44 [INFO]: epoch 249: training loss 0.1382\n",
      "2025-06-01 14:51:44 [INFO]: epoch 250: training loss 0.1459\n",
      "2025-06-01 14:51:44 [INFO]: epoch 251: training loss 0.1374\n",
      "2025-06-01 14:51:44 [INFO]: epoch 252: training loss 0.1348\n",
      "2025-06-01 14:51:44 [INFO]: epoch 253: training loss 0.1379\n",
      "2025-06-01 14:51:44 [INFO]: epoch 254: training loss 0.1320\n",
      "2025-06-01 14:51:44 [INFO]: epoch 255: training loss 0.1428\n",
      "2025-06-01 14:51:44 [INFO]: epoch 256: training loss 0.1310\n",
      "2025-06-01 14:51:44 [INFO]: epoch 257: training loss 0.1290\n",
      "2025-06-01 14:51:44 [INFO]: epoch 258: training loss 0.1250\n",
      "2025-06-01 14:51:44 [INFO]: epoch 259: training loss 0.1392\n",
      "2025-06-01 14:51:44 [INFO]: epoch 260: training loss 0.1393\n",
      "2025-06-01 14:51:44 [INFO]: epoch 261: training loss 0.1203\n",
      "2025-06-01 14:51:44 [INFO]: epoch 262: training loss 0.1380\n",
      "2025-06-01 14:51:44 [INFO]: epoch 263: training loss 0.1384\n",
      "2025-06-01 14:51:44 [INFO]: epoch 264: training loss 0.1246\n",
      "2025-06-01 14:51:44 [INFO]: epoch 265: training loss 0.1205\n",
      "2025-06-01 14:51:44 [INFO]: epoch 266: training loss 0.1245\n",
      "2025-06-01 14:51:44 [INFO]: epoch 267: training loss 0.1422\n",
      "2025-06-01 14:51:44 [INFO]: epoch 268: training loss 0.1349\n",
      "2025-06-01 14:51:44 [INFO]: epoch 269: training loss 0.1388\n",
      "2025-06-01 14:51:44 [INFO]: epoch 270: training loss 0.1294\n",
      "2025-06-01 14:51:44 [INFO]: epoch 271: training loss 0.1183\n",
      "2025-06-01 14:51:44 [INFO]: epoch 272: training loss 0.1256\n",
      "2025-06-01 14:51:44 [INFO]: epoch 273: training loss 0.1273\n",
      "2025-06-01 14:51:44 [INFO]: epoch 274: training loss 0.1293\n",
      "2025-06-01 14:51:44 [INFO]: epoch 275: training loss 0.1234\n",
      "2025-06-01 14:51:44 [INFO]: epoch 276: training loss 0.1123\n",
      "2025-06-01 14:51:44 [INFO]: epoch 277: training loss 0.1278\n",
      "2025-06-01 14:51:44 [INFO]: epoch 278: training loss 0.1288\n",
      "2025-06-01 14:51:44 [INFO]: epoch 279: training loss 0.1204\n",
      "2025-06-01 14:51:44 [INFO]: epoch 280: training loss 0.1160\n",
      "2025-06-01 14:51:44 [INFO]: epoch 281: training loss 0.1286\n",
      "2025-06-01 14:51:44 [INFO]: epoch 282: training loss 0.1229\n",
      "2025-06-01 14:51:44 [INFO]: epoch 283: training loss 0.1296\n",
      "2025-06-01 14:51:44 [INFO]: epoch 284: training loss 0.1142\n",
      "2025-06-01 14:51:44 [INFO]: epoch 285: training loss 0.1138\n",
      "2025-06-01 14:51:44 [INFO]: epoch 286: training loss 0.1196\n",
      "2025-06-01 14:51:44 [INFO]: epoch 287: training loss 0.1110\n",
      "2025-06-01 14:51:44 [INFO]: epoch 288: training loss 0.1358\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-01 14:51:44 [INFO]: epoch 289: training loss 0.1290\n",
      "2025-06-01 14:51:44 [INFO]: epoch 290: training loss 0.1153\n",
      "2025-06-01 14:51:44 [INFO]: epoch 291: training loss 0.1254\n",
      "2025-06-01 14:51:44 [INFO]: epoch 292: training loss 0.1175\n",
      "2025-06-01 14:51:44 [INFO]: epoch 293: training loss 0.1157\n",
      "2025-06-01 14:51:44 [INFO]: epoch 294: training loss 0.1207\n",
      "2025-06-01 14:51:44 [INFO]: epoch 295: training loss 0.1346\n",
      "2025-06-01 14:51:44 [INFO]: epoch 296: training loss 0.1148\n",
      "2025-06-01 14:51:44 [INFO]: epoch 297: training loss 0.1188\n",
      "2025-06-01 14:51:44 [INFO]: epoch 298: training loss 0.1106\n",
      "2025-06-01 14:51:44 [INFO]: epoch 299: training loss 0.1091\n",
      "2025-06-01 14:51:44 [INFO]: epoch 300: training loss 0.1154\n",
      "2025-06-01 14:51:44 [INFO]: epoch 301: training loss 0.1185\n",
      "2025-06-01 14:51:44 [INFO]: epoch 302: training loss 0.1100\n",
      "2025-06-01 14:51:44 [INFO]: epoch 303: training loss 0.1147\n",
      "2025-06-01 14:51:44 [INFO]: epoch 304: training loss 0.1169\n",
      "2025-06-01 14:51:44 [INFO]: epoch 305: training loss 0.1010\n",
      "2025-06-01 14:51:44 [INFO]: epoch 306: training loss 0.1104\n",
      "2025-06-01 14:51:44 [INFO]: epoch 307: training loss 0.1152\n",
      "2025-06-01 14:51:44 [INFO]: epoch 308: training loss 0.1084\n",
      "2025-06-01 14:51:44 [INFO]: epoch 309: training loss 0.1060\n",
      "2025-06-01 14:51:44 [INFO]: epoch 310: training loss 0.1280\n",
      "2025-06-01 14:51:44 [INFO]: epoch 311: training loss 0.1235\n",
      "2025-06-01 14:51:44 [INFO]: epoch 312: training loss 0.1163\n",
      "2025-06-01 14:51:44 [INFO]: epoch 313: training loss 0.1134\n",
      "2025-06-01 14:51:44 [INFO]: epoch 314: training loss 0.1132\n",
      "2025-06-01 14:51:44 [INFO]: epoch 315: training loss 0.1095\n",
      "2025-06-01 14:51:44 [INFO]: epoch 316: training loss 0.1159\n",
      "2025-06-01 14:51:44 [INFO]: epoch 317: training loss 0.1001\n",
      "2025-06-01 14:51:44 [INFO]: epoch 318: training loss 0.1142\n",
      "2025-06-01 14:51:44 [INFO]: epoch 319: training loss 0.1083\n",
      "2025-06-01 14:51:44 [INFO]: epoch 320: training loss 0.1187\n",
      "2025-06-01 14:51:44 [INFO]: epoch 321: training loss 0.1165\n",
      "2025-06-01 14:51:44 [INFO]: epoch 322: training loss 0.1049\n",
      "2025-06-01 14:51:44 [INFO]: epoch 323: training loss 0.1036\n",
      "2025-06-01 14:51:44 [INFO]: epoch 324: training loss 0.1086\n",
      "2025-06-01 14:51:45 [INFO]: epoch 325: training loss 0.1002\n",
      "2025-06-01 14:51:45 [INFO]: epoch 326: training loss 0.1051\n",
      "2025-06-01 14:51:45 [INFO]: epoch 327: training loss 0.1024\n",
      "2025-06-01 14:51:45 [INFO]: epoch 328: training loss 0.1061\n",
      "2025-06-01 14:51:45 [INFO]: epoch 329: training loss 0.1113\n",
      "2025-06-01 14:51:45 [INFO]: epoch 330: training loss 0.1015\n",
      "2025-06-01 14:51:45 [INFO]: epoch 331: training loss 0.1045\n",
      "2025-06-01 14:51:45 [INFO]: epoch 332: training loss 0.1068\n",
      "2025-06-01 14:51:45 [INFO]: epoch 333: training loss 0.1055\n",
      "2025-06-01 14:51:45 [INFO]: epoch 334: training loss 0.1083\n",
      "2025-06-01 14:51:45 [INFO]: epoch 335: training loss 0.1019\n",
      "2025-06-01 14:51:45 [INFO]: epoch 336: training loss 0.1082\n",
      "2025-06-01 14:51:45 [INFO]: epoch 337: training loss 0.0924\n",
      "2025-06-01 14:51:45 [INFO]: epoch 338: training loss 0.0992\n",
      "2025-06-01 14:51:45 [INFO]: epoch 339: training loss 0.0994\n",
      "2025-06-01 14:51:45 [INFO]: epoch 340: training loss 0.1102\n",
      "2025-06-01 14:51:45 [INFO]: epoch 341: training loss 0.1015\n",
      "2025-06-01 14:51:45 [INFO]: epoch 342: training loss 0.0974\n",
      "2025-06-01 14:51:45 [INFO]: epoch 343: training loss 0.0979\n",
      "2025-06-01 14:51:45 [INFO]: epoch 344: training loss 0.1074\n",
      "2025-06-01 14:51:45 [INFO]: epoch 345: training loss 0.1033\n",
      "2025-06-01 14:51:45 [INFO]: epoch 346: training loss 0.0999\n",
      "2025-06-01 14:51:45 [INFO]: epoch 347: training loss 0.0938\n",
      "2025-06-01 14:51:45 [INFO]: epoch 348: training loss 0.0979\n",
      "2025-06-01 14:51:45 [INFO]: epoch 349: training loss 0.0976\n",
      "2025-06-01 14:51:45 [INFO]: epoch 350: training loss 0.1111\n",
      "2025-06-01 14:51:45 [INFO]: epoch 351: training loss 0.0951\n",
      "2025-06-01 14:51:45 [INFO]: epoch 352: training loss 0.0951\n",
      "2025-06-01 14:51:45 [INFO]: epoch 353: training loss 0.0966\n",
      "2025-06-01 14:51:45 [INFO]: epoch 354: training loss 0.1004\n",
      "2025-06-01 14:51:45 [INFO]: epoch 355: training loss 0.0852\n",
      "2025-06-01 14:51:45 [INFO]: epoch 356: training loss 0.0940\n",
      "2025-06-01 14:51:45 [INFO]: epoch 357: training loss 0.0997\n",
      "2025-06-01 14:51:45 [INFO]: epoch 358: training loss 0.1030\n",
      "2025-06-01 14:51:45 [INFO]: epoch 359: training loss 0.0884\n",
      "2025-06-01 14:51:45 [INFO]: epoch 360: training loss 0.1015\n",
      "2025-06-01 14:51:45 [INFO]: epoch 361: training loss 0.0984\n",
      "2025-06-01 14:51:45 [INFO]: epoch 362: training loss 0.0963\n",
      "2025-06-01 14:51:45 [INFO]: epoch 363: training loss 0.1102\n",
      "2025-06-01 14:51:45 [INFO]: epoch 364: training loss 0.1080\n",
      "2025-06-01 14:51:45 [INFO]: epoch 365: training loss 0.1024\n",
      "2025-06-01 14:51:45 [INFO]: epoch 366: training loss 0.0978\n",
      "2025-06-01 14:51:45 [INFO]: epoch 367: training loss 0.1041\n",
      "2025-06-01 14:51:45 [INFO]: epoch 368: training loss 0.0977\n",
      "2025-06-01 14:51:45 [INFO]: epoch 369: training loss 0.0958\n",
      "2025-06-01 14:51:45 [INFO]: epoch 370: training loss 0.0884\n",
      "2025-06-01 14:51:45 [INFO]: epoch 371: training loss 0.0917\n",
      "2025-06-01 14:51:45 [INFO]: epoch 372: training loss 0.0884\n",
      "2025-06-01 14:51:45 [INFO]: epoch 373: training loss 0.0943\n",
      "2025-06-01 14:51:45 [INFO]: epoch 374: training loss 0.0978\n",
      "2025-06-01 14:51:45 [INFO]: epoch 375: training loss 0.1023\n",
      "2025-06-01 14:51:45 [INFO]: epoch 376: training loss 0.0969\n",
      "2025-06-01 14:51:45 [INFO]: epoch 377: training loss 0.0916\n",
      "2025-06-01 14:51:45 [INFO]: epoch 378: training loss 0.0944\n",
      "2025-06-01 14:51:45 [INFO]: epoch 379: training loss 0.0948\n",
      "2025-06-01 14:51:45 [INFO]: epoch 380: training loss 0.0885\n",
      "2025-06-01 14:51:45 [INFO]: epoch 381: training loss 0.0886\n",
      "2025-06-01 14:51:45 [INFO]: epoch 382: training loss 0.0971\n",
      "2025-06-01 14:51:45 [INFO]: epoch 383: training loss 0.0931\n",
      "2025-06-01 14:51:45 [INFO]: epoch 384: training loss 0.0862\n",
      "2025-06-01 14:51:45 [INFO]: epoch 385: training loss 0.0922\n",
      "2025-06-01 14:51:45 [INFO]: epoch 386: training loss 0.0806\n",
      "2025-06-01 14:51:45 [INFO]: epoch 387: training loss 0.0895\n",
      "2025-06-01 14:51:45 [INFO]: epoch 388: training loss 0.0891\n",
      "2025-06-01 14:51:45 [INFO]: epoch 389: training loss 0.0778\n",
      "2025-06-01 14:51:45 [INFO]: epoch 390: training loss 0.0804\n",
      "2025-06-01 14:51:45 [INFO]: epoch 391: training loss 0.0859\n",
      "2025-06-01 14:51:45 [INFO]: epoch 392: training loss 0.0934\n",
      "2025-06-01 14:51:45 [INFO]: epoch 393: training loss 0.0875\n",
      "2025-06-01 14:51:45 [INFO]: epoch 394: training loss 0.0908\n",
      "2025-06-01 14:51:45 [INFO]: epoch 395: training loss 0.0819\n",
      "2025-06-01 14:51:45 [INFO]: epoch 396: training loss 0.0921\n",
      "2025-06-01 14:51:45 [INFO]: epoch 397: training loss 0.0933\n",
      "2025-06-01 14:51:45 [INFO]: epoch 398: training loss 0.0894\n",
      "2025-06-01 14:51:45 [INFO]: epoch 399: training loss 0.0892\n",
      "2025-06-01 14:51:45 [INFO]: epoch 400: training loss 0.0878\n",
      "2025-06-01 14:51:46 [INFO]: epoch 401: training loss 0.0861\n",
      "2025-06-01 14:51:46 [INFO]: epoch 402: training loss 0.0867\n",
      "2025-06-01 14:51:46 [INFO]: epoch 403: training loss 0.0897\n",
      "2025-06-01 14:51:46 [INFO]: epoch 404: training loss 0.0912\n",
      "2025-06-01 14:51:46 [INFO]: epoch 405: training loss 0.0881\n",
      "2025-06-01 14:51:46 [INFO]: epoch 406: training loss 0.0862\n",
      "2025-06-01 14:51:46 [INFO]: epoch 407: training loss 0.0859\n",
      "2025-06-01 14:51:46 [INFO]: epoch 408: training loss 0.0927\n",
      "2025-06-01 14:51:46 [INFO]: epoch 409: training loss 0.0816\n",
      "2025-06-01 14:51:46 [INFO]: epoch 410: training loss 0.0960\n",
      "2025-06-01 14:51:46 [INFO]: epoch 411: training loss 0.0976\n",
      "2025-06-01 14:51:46 [INFO]: epoch 412: training loss 0.0868\n",
      "2025-06-01 14:51:46 [INFO]: epoch 413: training loss 0.0817\n",
      "2025-06-01 14:51:46 [INFO]: epoch 414: training loss 0.0939\n",
      "2025-06-01 14:51:46 [INFO]: epoch 415: training loss 0.0804\n",
      "2025-06-01 14:51:46 [INFO]: epoch 416: training loss 0.0788\n",
      "2025-06-01 14:51:46 [INFO]: epoch 417: training loss 0.0826\n",
      "2025-06-01 14:51:46 [INFO]: epoch 418: training loss 0.0906\n",
      "2025-06-01 14:51:46 [INFO]: epoch 419: training loss 0.0817\n",
      "2025-06-01 14:51:46 [INFO]: epoch 420: training loss 0.0769\n",
      "2025-06-01 14:51:46 [INFO]: epoch 421: training loss 0.0835\n",
      "2025-06-01 14:51:46 [INFO]: epoch 422: training loss 0.0848\n",
      "2025-06-01 14:51:46 [INFO]: epoch 423: training loss 0.0817\n",
      "2025-06-01 14:51:46 [INFO]: epoch 424: training loss 0.0717\n",
      "2025-06-01 14:51:46 [INFO]: epoch 425: training loss 0.0792\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-01 14:51:46 [INFO]: epoch 426: training loss 0.0841\n",
      "2025-06-01 14:51:46 [INFO]: epoch 427: training loss 0.0822\n",
      "2025-06-01 14:51:46 [INFO]: epoch 428: training loss 0.0734\n",
      "2025-06-01 14:51:46 [INFO]: epoch 429: training loss 0.0753\n",
      "2025-06-01 14:51:46 [INFO]: epoch 430: training loss 0.0802\n",
      "2025-06-01 14:51:46 [INFO]: epoch 431: training loss 0.0797\n",
      "2025-06-01 14:51:46 [INFO]: epoch 432: training loss 0.0709\n",
      "2025-06-01 14:51:46 [INFO]: epoch 433: training loss 0.0825\n",
      "2025-06-01 14:51:46 [INFO]: epoch 434: training loss 0.0799\n",
      "2025-06-01 14:51:46 [INFO]: epoch 435: training loss 0.0789\n",
      "2025-06-01 14:51:46 [INFO]: epoch 436: training loss 0.0824\n",
      "2025-06-01 14:51:46 [INFO]: epoch 437: training loss 0.0825\n",
      "2025-06-01 14:51:46 [INFO]: epoch 438: training loss 0.0855\n",
      "2025-06-01 14:51:46 [INFO]: epoch 439: training loss 0.0724\n",
      "2025-06-01 14:51:46 [INFO]: epoch 440: training loss 0.0827\n",
      "2025-06-01 14:51:46 [INFO]: epoch 441: training loss 0.0823\n",
      "2025-06-01 14:51:46 [INFO]: epoch 442: training loss 0.0793\n",
      "2025-06-01 14:51:46 [INFO]: epoch 443: training loss 0.0744\n",
      "2025-06-01 14:51:46 [INFO]: epoch 444: training loss 0.0724\n",
      "2025-06-01 14:51:46 [INFO]: epoch 445: training loss 0.0787\n",
      "2025-06-01 14:51:46 [INFO]: epoch 446: training loss 0.0782\n",
      "2025-06-01 14:51:46 [INFO]: epoch 447: training loss 0.0730\n",
      "2025-06-01 14:51:46 [INFO]: epoch 448: training loss 0.0768\n",
      "2025-06-01 14:51:46 [INFO]: epoch 449: training loss 0.0751\n",
      "2025-06-01 14:51:46 [INFO]: epoch 450: training loss 0.0739\n",
      "2025-06-01 14:51:46 [INFO]: epoch 451: training loss 0.0716\n",
      "2025-06-01 14:51:46 [INFO]: epoch 452: training loss 0.0755\n",
      "2025-06-01 14:51:46 [INFO]: epoch 453: training loss 0.0715\n",
      "2025-06-01 14:51:46 [INFO]: epoch 454: training loss 0.0722\n",
      "2025-06-01 14:51:46 [INFO]: epoch 455: training loss 0.0668\n",
      "2025-06-01 14:51:46 [INFO]: epoch 456: training loss 0.0647\n",
      "2025-06-01 14:51:46 [INFO]: epoch 457: training loss 0.0690\n",
      "2025-06-01 14:51:46 [INFO]: epoch 458: training loss 0.0783\n",
      "2025-06-01 14:51:46 [INFO]: epoch 459: training loss 0.0691\n",
      "2025-06-01 14:51:46 [INFO]: epoch 460: training loss 0.0697\n",
      "2025-06-01 14:51:46 [INFO]: epoch 461: training loss 0.0733\n",
      "2025-06-01 14:51:46 [INFO]: epoch 462: training loss 0.0746\n",
      "2025-06-01 14:51:46 [INFO]: epoch 463: training loss 0.0719\n",
      "2025-06-01 14:51:46 [INFO]: epoch 464: training loss 0.0833\n",
      "2025-06-01 14:51:46 [INFO]: epoch 465: training loss 0.0671\n",
      "2025-06-01 14:51:46 [INFO]: epoch 466: training loss 0.0674\n",
      "2025-06-01 14:51:46 [INFO]: epoch 467: training loss 0.0766\n",
      "2025-06-01 14:51:46 [INFO]: epoch 468: training loss 0.0703\n",
      "2025-06-01 14:51:46 [INFO]: epoch 469: training loss 0.0678\n",
      "2025-06-01 14:51:46 [INFO]: epoch 470: training loss 0.0704\n",
      "2025-06-01 14:51:46 [INFO]: epoch 471: training loss 0.0692\n",
      "2025-06-01 14:51:46 [INFO]: epoch 472: training loss 0.0682\n",
      "2025-06-01 14:51:46 [INFO]: epoch 473: training loss 0.0714\n",
      "2025-06-01 14:51:46 [INFO]: epoch 474: training loss 0.0723\n",
      "2025-06-01 14:51:46 [INFO]: epoch 475: training loss 0.0729\n",
      "2025-06-01 14:51:46 [INFO]: epoch 476: training loss 0.0721\n",
      "2025-06-01 14:51:47 [INFO]: epoch 477: training loss 0.0655\n",
      "2025-06-01 14:51:47 [INFO]: epoch 478: training loss 0.0733\n",
      "2025-06-01 14:51:47 [INFO]: epoch 479: training loss 0.0678\n",
      "2025-06-01 14:51:47 [INFO]: epoch 480: training loss 0.0667\n",
      "2025-06-01 14:51:47 [INFO]: epoch 481: training loss 0.0639\n",
      "2025-06-01 14:51:47 [INFO]: epoch 482: training loss 0.0653\n",
      "2025-06-01 14:51:47 [INFO]: epoch 483: training loss 0.0623\n",
      "2025-06-01 14:51:47 [INFO]: epoch 484: training loss 0.0611\n",
      "2025-06-01 14:51:47 [INFO]: epoch 485: training loss 0.0642\n",
      "2025-06-01 14:51:47 [INFO]: epoch 486: training loss 0.0687\n",
      "2025-06-01 14:51:47 [INFO]: epoch 487: training loss 0.0667\n",
      "2025-06-01 14:51:47 [INFO]: epoch 488: training loss 0.0659\n",
      "2025-06-01 14:51:47 [INFO]: epoch 489: training loss 0.0679\n",
      "2025-06-01 14:51:47 [INFO]: epoch 490: training loss 0.0717\n",
      "2025-06-01 14:51:47 [INFO]: epoch 491: training loss 0.0729\n",
      "2025-06-01 14:51:47 [INFO]: epoch 492: training loss 0.0612\n",
      "2025-06-01 14:51:47 [INFO]: epoch 493: training loss 0.0707\n",
      "2025-06-01 14:51:47 [INFO]: epoch 494: training loss 0.0651\n",
      "2025-06-01 14:51:47 [INFO]: epoch 495: training loss 0.0698\n",
      "2025-06-01 14:51:47 [INFO]: epoch 496: training loss 0.0639\n",
      "2025-06-01 14:51:47 [INFO]: epoch 497: training loss 0.0648\n",
      "2025-06-01 14:51:47 [INFO]: epoch 498: training loss 0.0645\n",
      "2025-06-01 14:51:47 [INFO]: epoch 499: training loss 0.0755\n",
      "2025-06-01 14:51:47 [INFO]: epoch 500: training loss 0.0650\n",
      "2025-06-01 14:51:47 [INFO]: epoch 501: training loss 0.0736\n",
      "2025-06-01 14:51:47 [INFO]: epoch 502: training loss 0.0673\n",
      "2025-06-01 14:51:47 [INFO]: epoch 503: training loss 0.0669\n",
      "2025-06-01 14:51:47 [INFO]: epoch 504: training loss 0.0643\n",
      "2025-06-01 14:51:47 [INFO]: epoch 505: training loss 0.0696\n",
      "2025-06-01 14:51:47 [INFO]: epoch 506: training loss 0.0673\n",
      "2025-06-01 14:51:47 [INFO]: epoch 507: training loss 0.0757\n",
      "2025-06-01 14:51:47 [INFO]: epoch 508: training loss 0.0593\n",
      "2025-06-01 14:51:47 [INFO]: epoch 509: training loss 0.0679\n",
      "2025-06-01 14:51:47 [INFO]: epoch 510: training loss 0.0698\n",
      "2025-06-01 14:51:47 [INFO]: epoch 511: training loss 0.0670\n",
      "2025-06-01 14:51:47 [INFO]: epoch 512: training loss 0.0752\n",
      "2025-06-01 14:51:47 [INFO]: epoch 513: training loss 0.0682\n",
      "2025-06-01 14:51:47 [INFO]: epoch 514: training loss 0.0615\n",
      "2025-06-01 14:51:47 [INFO]: epoch 515: training loss 0.0694\n",
      "2025-06-01 14:51:47 [INFO]: epoch 516: training loss 0.0674\n",
      "2025-06-01 14:51:47 [INFO]: epoch 517: training loss 0.0590\n",
      "2025-06-01 14:51:47 [INFO]: epoch 518: training loss 0.0589\n",
      "2025-06-01 14:51:47 [INFO]: epoch 519: training loss 0.0699\n",
      "2025-06-01 14:51:47 [INFO]: epoch 520: training loss 0.0580\n",
      "2025-06-01 14:51:47 [INFO]: epoch 521: training loss 0.0621\n",
      "2025-06-01 14:51:47 [INFO]: epoch 522: training loss 0.0636\n",
      "2025-06-01 14:51:47 [INFO]: epoch 523: training loss 0.0648\n",
      "2025-06-01 14:51:47 [INFO]: epoch 524: training loss 0.0613\n",
      "2025-06-01 14:51:47 [INFO]: epoch 525: training loss 0.0638\n",
      "2025-06-01 14:51:47 [INFO]: epoch 526: training loss 0.0618\n",
      "2025-06-01 14:51:47 [INFO]: epoch 527: training loss 0.0722\n",
      "2025-06-01 14:51:47 [INFO]: epoch 528: training loss 0.0637\n",
      "2025-06-01 14:51:47 [INFO]: epoch 529: training loss 0.0623\n",
      "2025-06-01 14:51:47 [INFO]: epoch 530: training loss 0.0683\n",
      "2025-06-01 14:51:47 [INFO]: epoch 531: training loss 0.0712\n",
      "2025-06-01 14:51:47 [INFO]: epoch 532: training loss 0.0640\n",
      "2025-06-01 14:51:47 [INFO]: epoch 533: training loss 0.0571\n",
      "2025-06-01 14:51:47 [INFO]: epoch 534: training loss 0.0579\n",
      "2025-06-01 14:51:47 [INFO]: epoch 535: training loss 0.0739\n",
      "2025-06-01 14:51:47 [INFO]: epoch 536: training loss 0.0627\n",
      "2025-06-01 14:51:47 [INFO]: epoch 537: training loss 0.0634\n",
      "2025-06-01 14:51:47 [INFO]: epoch 538: training loss 0.0638\n",
      "2025-06-01 14:51:47 [INFO]: epoch 539: training loss 0.0632\n",
      "2025-06-01 14:51:47 [INFO]: epoch 540: training loss 0.0672\n",
      "2025-06-01 14:51:47 [INFO]: epoch 541: training loss 0.0644\n",
      "2025-06-01 14:51:47 [INFO]: epoch 542: training loss 0.0657\n",
      "2025-06-01 14:51:47 [INFO]: epoch 543: training loss 0.0624\n",
      "2025-06-01 14:51:47 [INFO]: epoch 544: training loss 0.0606\n",
      "2025-06-01 14:51:47 [INFO]: epoch 545: training loss 0.0644\n",
      "2025-06-01 14:51:47 [INFO]: epoch 546: training loss 0.0715\n",
      "2025-06-01 14:51:47 [INFO]: epoch 547: training loss 0.0642\n",
      "2025-06-01 14:51:47 [INFO]: epoch 548: training loss 0.0631\n",
      "2025-06-01 14:51:47 [INFO]: epoch 549: training loss 0.0569\n",
      "2025-06-01 14:51:47 [INFO]: epoch 550: training loss 0.0633\n",
      "2025-06-01 14:51:47 [INFO]: epoch 551: training loss 0.0536\n",
      "2025-06-01 14:51:47 [INFO]: epoch 552: training loss 0.0649\n",
      "2025-06-01 14:51:48 [INFO]: epoch 553: training loss 0.0659\n",
      "2025-06-01 14:51:48 [INFO]: epoch 554: training loss 0.0564\n",
      "2025-06-01 14:51:48 [INFO]: epoch 555: training loss 0.0593\n",
      "2025-06-01 14:51:48 [INFO]: epoch 556: training loss 0.0594\n",
      "2025-06-01 14:51:48 [INFO]: epoch 557: training loss 0.0547\n",
      "2025-06-01 14:51:48 [INFO]: epoch 558: training loss 0.0557\n",
      "2025-06-01 14:51:48 [INFO]: epoch 559: training loss 0.0604\n",
      "2025-06-01 14:51:48 [INFO]: epoch 560: training loss 0.0639\n",
      "2025-06-01 14:51:48 [INFO]: epoch 561: training loss 0.0562\n",
      "2025-06-01 14:51:48 [INFO]: epoch 562: training loss 0.0540\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-01 14:51:48 [INFO]: epoch 563: training loss 0.0624\n",
      "2025-06-01 14:51:48 [INFO]: epoch 564: training loss 0.0565\n",
      "2025-06-01 14:51:48 [INFO]: epoch 565: training loss 0.0617\n",
      "2025-06-01 14:51:48 [INFO]: epoch 566: training loss 0.0529\n",
      "2025-06-01 14:51:48 [INFO]: epoch 567: training loss 0.0606\n",
      "2025-06-01 14:51:48 [INFO]: epoch 568: training loss 0.0607\n",
      "2025-06-01 14:51:48 [INFO]: epoch 569: training loss 0.0608\n",
      "2025-06-01 14:51:48 [INFO]: epoch 570: training loss 0.0640\n",
      "2025-06-01 14:51:48 [INFO]: epoch 571: training loss 0.0585\n",
      "2025-06-01 14:51:48 [INFO]: epoch 572: training loss 0.0566\n",
      "2025-06-01 14:51:48 [INFO]: epoch 573: training loss 0.0639\n",
      "2025-06-01 14:51:48 [INFO]: epoch 574: training loss 0.0635\n",
      "2025-06-01 14:51:48 [INFO]: epoch 575: training loss 0.0606\n",
      "2025-06-01 14:51:48 [INFO]: epoch 576: training loss 0.0498\n",
      "2025-06-01 14:51:48 [INFO]: epoch 577: training loss 0.0543\n",
      "2025-06-01 14:51:48 [INFO]: epoch 578: training loss 0.0557\n",
      "2025-06-01 14:51:48 [INFO]: epoch 579: training loss 0.0632\n",
      "2025-06-01 14:51:48 [INFO]: epoch 580: training loss 0.0683\n",
      "2025-06-01 14:51:48 [INFO]: epoch 581: training loss 0.0609\n",
      "2025-06-01 14:51:48 [INFO]: epoch 582: training loss 0.0576\n",
      "2025-06-01 14:51:48 [INFO]: epoch 583: training loss 0.0586\n",
      "2025-06-01 14:51:48 [INFO]: epoch 584: training loss 0.0577\n",
      "2025-06-01 14:51:48 [INFO]: epoch 585: training loss 0.0546\n",
      "2025-06-01 14:51:48 [INFO]: epoch 586: training loss 0.0623\n",
      "2025-06-01 14:51:48 [INFO]: epoch 587: training loss 0.0526\n",
      "2025-06-01 14:51:48 [INFO]: epoch 588: training loss 0.0525\n",
      "2025-06-01 14:51:48 [INFO]: epoch 589: training loss 0.0559\n",
      "2025-06-01 14:51:48 [INFO]: epoch 590: training loss 0.0577\n",
      "2025-06-01 14:51:48 [INFO]: epoch 591: training loss 0.0592\n",
      "2025-06-01 14:51:48 [INFO]: epoch 592: training loss 0.0551\n",
      "2025-06-01 14:51:48 [INFO]: epoch 593: training loss 0.0567\n",
      "2025-06-01 14:51:48 [INFO]: epoch 594: training loss 0.0505\n",
      "2025-06-01 14:51:48 [INFO]: epoch 595: training loss 0.0550\n",
      "2025-06-01 14:51:48 [INFO]: epoch 596: training loss 0.0542\n",
      "2025-06-01 14:51:48 [INFO]: epoch 597: training loss 0.0538\n",
      "2025-06-01 14:51:48 [INFO]: epoch 598: training loss 0.0559\n",
      "2025-06-01 14:51:48 [INFO]: epoch 599: training loss 0.0543\n",
      "2025-06-01 14:51:48 [INFO]: epoch 600: training loss 0.0531\n",
      "2025-06-01 14:51:48 [INFO]: epoch 601: training loss 0.0522\n",
      "2025-06-01 14:51:48 [INFO]: epoch 602: training loss 0.0583\n",
      "2025-06-01 14:51:48 [INFO]: epoch 603: training loss 0.0532\n",
      "2025-06-01 14:51:48 [INFO]: epoch 604: training loss 0.0573\n",
      "2025-06-01 14:51:48 [INFO]: epoch 605: training loss 0.0587\n",
      "2025-06-01 14:51:48 [INFO]: epoch 606: training loss 0.0569\n",
      "2025-06-01 14:51:48 [INFO]: epoch 607: training loss 0.0526\n",
      "2025-06-01 14:51:48 [INFO]: epoch 608: training loss 0.0565\n",
      "2025-06-01 14:51:48 [INFO]: epoch 609: training loss 0.0522\n",
      "2025-06-01 14:51:48 [INFO]: epoch 610: training loss 0.0559\n",
      "2025-06-01 14:51:48 [INFO]: epoch 611: training loss 0.0511\n",
      "2025-06-01 14:51:48 [INFO]: epoch 612: training loss 0.0481\n",
      "2025-06-01 14:51:48 [INFO]: epoch 613: training loss 0.0536\n",
      "2025-06-01 14:51:48 [INFO]: epoch 614: training loss 0.0579\n",
      "2025-06-01 14:51:48 [INFO]: epoch 615: training loss 0.0494\n",
      "2025-06-01 14:51:48 [INFO]: epoch 616: training loss 0.0538\n",
      "2025-06-01 14:51:48 [INFO]: epoch 617: training loss 0.0521\n",
      "2025-06-01 14:51:48 [INFO]: epoch 618: training loss 0.0547\n",
      "2025-06-01 14:51:48 [INFO]: epoch 619: training loss 0.0562\n",
      "2025-06-01 14:51:48 [INFO]: epoch 620: training loss 0.0545\n",
      "2025-06-01 14:51:48 [INFO]: epoch 621: training loss 0.0546\n",
      "2025-06-01 14:51:48 [INFO]: epoch 622: training loss 0.0527\n",
      "2025-06-01 14:51:48 [INFO]: epoch 623: training loss 0.0561\n",
      "2025-06-01 14:51:48 [INFO]: epoch 624: training loss 0.0546\n",
      "2025-06-01 14:51:48 [INFO]: epoch 625: training loss 0.0493\n",
      "2025-06-01 14:51:48 [INFO]: epoch 626: training loss 0.0467\n",
      "2025-06-01 14:51:48 [INFO]: epoch 627: training loss 0.0526\n",
      "2025-06-01 14:51:48 [INFO]: epoch 628: training loss 0.0489\n",
      "2025-06-01 14:51:49 [INFO]: epoch 629: training loss 0.0540\n",
      "2025-06-01 14:51:49 [INFO]: epoch 630: training loss 0.0546\n",
      "2025-06-01 14:51:49 [INFO]: epoch 631: training loss 0.0512\n",
      "2025-06-01 14:51:49 [INFO]: epoch 632: training loss 0.0523\n",
      "2025-06-01 14:51:49 [INFO]: epoch 633: training loss 0.0486\n",
      "2025-06-01 14:51:49 [INFO]: epoch 634: training loss 0.0546\n",
      "2025-06-01 14:51:49 [INFO]: epoch 635: training loss 0.0525\n",
      "2025-06-01 14:51:49 [INFO]: epoch 636: training loss 0.0459\n",
      "2025-06-01 14:51:49 [INFO]: epoch 637: training loss 0.0523\n",
      "2025-06-01 14:51:49 [INFO]: epoch 638: training loss 0.0527\n",
      "2025-06-01 14:51:49 [INFO]: epoch 639: training loss 0.0552\n",
      "2025-06-01 14:51:49 [INFO]: epoch 640: training loss 0.0509\n",
      "2025-06-01 14:51:49 [INFO]: epoch 641: training loss 0.0480\n",
      "2025-06-01 14:51:49 [INFO]: epoch 642: training loss 0.0465\n",
      "2025-06-01 14:51:49 [INFO]: epoch 643: training loss 0.0535\n",
      "2025-06-01 14:51:49 [INFO]: epoch 644: training loss 0.0505\n",
      "2025-06-01 14:51:49 [INFO]: epoch 645: training loss 0.0502\n",
      "2025-06-01 14:51:49 [INFO]: epoch 646: training loss 0.0485\n",
      "2025-06-01 14:51:49 [INFO]: epoch 647: training loss 0.0510\n",
      "2025-06-01 14:51:49 [INFO]: epoch 648: training loss 0.0495\n",
      "2025-06-01 14:51:49 [INFO]: epoch 649: training loss 0.0518\n",
      "2025-06-01 14:51:49 [INFO]: epoch 650: training loss 0.0498\n",
      "2025-06-01 14:51:49 [INFO]: epoch 651: training loss 0.0493\n",
      "2025-06-01 14:51:49 [INFO]: epoch 652: training loss 0.0516\n",
      "2025-06-01 14:51:49 [INFO]: epoch 653: training loss 0.0457\n",
      "2025-06-01 14:51:49 [INFO]: epoch 654: training loss 0.0511\n",
      "2025-06-01 14:51:49 [INFO]: epoch 655: training loss 0.0467\n",
      "2025-06-01 14:51:49 [INFO]: epoch 656: training loss 0.0541\n",
      "2025-06-01 14:51:49 [INFO]: epoch 657: training loss 0.0486\n",
      "2025-06-01 14:51:49 [INFO]: epoch 658: training loss 0.0508\n",
      "2025-06-01 14:51:49 [INFO]: epoch 659: training loss 0.0498\n",
      "2025-06-01 14:51:49 [INFO]: epoch 660: training loss 0.0499\n",
      "2025-06-01 14:51:49 [INFO]: epoch 661: training loss 0.0438\n",
      "2025-06-01 14:51:49 [INFO]: epoch 662: training loss 0.0488\n",
      "2025-06-01 14:51:49 [INFO]: epoch 663: training loss 0.0570\n",
      "2025-06-01 14:51:49 [INFO]: epoch 664: training loss 0.0517\n",
      "2025-06-01 14:51:49 [INFO]: epoch 665: training loss 0.0548\n",
      "2025-06-01 14:51:49 [INFO]: epoch 666: training loss 0.0573\n",
      "2025-06-01 14:51:49 [INFO]: epoch 667: training loss 0.0526\n",
      "2025-06-01 14:51:49 [INFO]: epoch 668: training loss 0.0494\n",
      "2025-06-01 14:51:49 [INFO]: epoch 669: training loss 0.0464\n",
      "2025-06-01 14:51:49 [INFO]: epoch 670: training loss 0.0463\n",
      "2025-06-01 14:51:49 [INFO]: epoch 671: training loss 0.0493\n",
      "2025-06-01 14:51:49 [INFO]: epoch 672: training loss 0.0539\n",
      "2025-06-01 14:51:49 [INFO]: epoch 673: training loss 0.0482\n",
      "2025-06-01 14:51:49 [INFO]: epoch 674: training loss 0.0475\n",
      "2025-06-01 14:51:49 [INFO]: epoch 675: training loss 0.0509\n",
      "2025-06-01 14:51:49 [INFO]: epoch 676: training loss 0.0550\n",
      "2025-06-01 14:51:49 [INFO]: epoch 677: training loss 0.0482\n",
      "2025-06-01 14:51:49 [INFO]: epoch 678: training loss 0.0533\n",
      "2025-06-01 14:51:49 [INFO]: epoch 679: training loss 0.0502\n",
      "2025-06-01 14:51:49 [INFO]: epoch 680: training loss 0.0516\n",
      "2025-06-01 14:51:49 [INFO]: epoch 681: training loss 0.0527\n",
      "2025-06-01 14:51:49 [INFO]: epoch 682: training loss 0.0555\n",
      "2025-06-01 14:51:49 [INFO]: epoch 683: training loss 0.0551\n",
      "2025-06-01 14:51:49 [INFO]: epoch 684: training loss 0.0547\n",
      "2025-06-01 14:51:49 [INFO]: epoch 685: training loss 0.0516\n",
      "2025-06-01 14:51:49 [INFO]: epoch 686: training loss 0.0503\n",
      "2025-06-01 14:51:49 [INFO]: epoch 687: training loss 0.0486\n",
      "2025-06-01 14:51:49 [INFO]: epoch 688: training loss 0.0493\n",
      "2025-06-01 14:51:49 [INFO]: epoch 689: training loss 0.0493\n",
      "2025-06-01 14:51:49 [INFO]: epoch 690: training loss 0.0492\n",
      "2025-06-01 14:51:49 [INFO]: epoch 691: training loss 0.0442\n",
      "2025-06-01 14:51:49 [INFO]: epoch 692: training loss 0.0466\n",
      "2025-06-01 14:51:49 [INFO]: epoch 693: training loss 0.0486\n",
      "2025-06-01 14:51:49 [INFO]: epoch 694: training loss 0.0469\n",
      "2025-06-01 14:51:49 [INFO]: epoch 695: training loss 0.0481\n",
      "2025-06-01 14:51:49 [INFO]: epoch 696: training loss 0.0444\n",
      "2025-06-01 14:51:49 [INFO]: epoch 697: training loss 0.0494\n",
      "2025-06-01 14:51:49 [INFO]: epoch 698: training loss 0.0511\n",
      "2025-06-01 14:51:49 [INFO]: epoch 699: training loss 0.0457\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-01 14:51:49 [INFO]: epoch 700: training loss 0.0489\n",
      "2025-06-01 14:51:49 [INFO]: epoch 701: training loss 0.0484\n",
      "2025-06-01 14:51:49 [INFO]: epoch 702: training loss 0.0479\n",
      "2025-06-01 14:51:49 [INFO]: epoch 703: training loss 0.0418\n",
      "2025-06-01 14:51:49 [INFO]: epoch 704: training loss 0.0463\n",
      "2025-06-01 14:51:50 [INFO]: epoch 705: training loss 0.0443\n",
      "2025-06-01 14:51:50 [INFO]: epoch 706: training loss 0.0502\n",
      "2025-06-01 14:51:50 [INFO]: epoch 707: training loss 0.0463\n",
      "2025-06-01 14:51:50 [INFO]: epoch 708: training loss 0.0457\n",
      "2025-06-01 14:51:50 [INFO]: epoch 709: training loss 0.0475\n",
      "2025-06-01 14:51:50 [INFO]: epoch 710: training loss 0.0474\n",
      "2025-06-01 14:51:50 [INFO]: epoch 711: training loss 0.0465\n",
      "2025-06-01 14:51:50 [INFO]: epoch 712: training loss 0.0458\n",
      "2025-06-01 14:51:50 [INFO]: epoch 713: training loss 0.0469\n",
      "2025-06-01 14:51:50 [INFO]: epoch 714: training loss 0.0481\n",
      "2025-06-01 14:51:50 [INFO]: epoch 715: training loss 0.0477\n",
      "2025-06-01 14:51:50 [INFO]: epoch 716: training loss 0.0457\n",
      "2025-06-01 14:51:50 [INFO]: epoch 717: training loss 0.0471\n",
      "2025-06-01 14:51:50 [INFO]: epoch 718: training loss 0.0496\n",
      "2025-06-01 14:51:50 [INFO]: epoch 719: training loss 0.0439\n",
      "2025-06-01 14:51:50 [INFO]: epoch 720: training loss 0.0414\n",
      "2025-06-01 14:51:50 [INFO]: epoch 721: training loss 0.0458\n",
      "2025-06-01 14:51:50 [INFO]: epoch 722: training loss 0.0509\n",
      "2025-06-01 14:51:50 [INFO]: epoch 723: training loss 0.0523\n",
      "2025-06-01 14:51:50 [INFO]: epoch 724: training loss 0.0428\n",
      "2025-06-01 14:51:50 [INFO]: epoch 725: training loss 0.0506\n",
      "2025-06-01 14:51:50 [INFO]: epoch 726: training loss 0.0488\n",
      "2025-06-01 14:51:50 [INFO]: epoch 727: training loss 0.0457\n",
      "2025-06-01 14:51:50 [INFO]: epoch 728: training loss 0.0460\n",
      "2025-06-01 14:51:50 [INFO]: epoch 729: training loss 0.0483\n",
      "2025-06-01 14:51:50 [INFO]: epoch 730: training loss 0.0465\n",
      "2025-06-01 14:51:50 [INFO]: epoch 731: training loss 0.0464\n",
      "2025-06-01 14:51:50 [INFO]: epoch 732: training loss 0.0484\n",
      "2025-06-01 14:51:50 [INFO]: epoch 733: training loss 0.0480\n",
      "2025-06-01 14:51:50 [INFO]: epoch 734: training loss 0.0461\n",
      "2025-06-01 14:51:50 [INFO]: epoch 735: training loss 0.0496\n",
      "2025-06-01 14:51:50 [INFO]: epoch 736: training loss 0.0460\n",
      "2025-06-01 14:51:50 [INFO]: epoch 737: training loss 0.0463\n",
      "2025-06-01 14:51:50 [INFO]: epoch 738: training loss 0.0473\n",
      "2025-06-01 14:51:50 [INFO]: epoch 739: training loss 0.0437\n",
      "2025-06-01 14:51:50 [INFO]: epoch 740: training loss 0.0427\n",
      "2025-06-01 14:51:50 [INFO]: epoch 741: training loss 0.0467\n",
      "2025-06-01 14:51:50 [INFO]: epoch 742: training loss 0.0496\n",
      "2025-06-01 14:51:50 [INFO]: epoch 743: training loss 0.0468\n",
      "2025-06-01 14:51:50 [INFO]: epoch 744: training loss 0.0465\n",
      "2025-06-01 14:51:50 [INFO]: epoch 745: training loss 0.0471\n",
      "2025-06-01 14:51:50 [INFO]: epoch 746: training loss 0.0479\n",
      "2025-06-01 14:51:50 [INFO]: epoch 747: training loss 0.0478\n",
      "2025-06-01 14:51:50 [INFO]: epoch 748: training loss 0.0428\n",
      "2025-06-01 14:51:50 [INFO]: epoch 749: training loss 0.0410\n",
      "2025-06-01 14:51:50 [INFO]: epoch 750: training loss 0.0460\n",
      "2025-06-01 14:51:50 [INFO]: epoch 751: training loss 0.0494\n",
      "2025-06-01 14:51:50 [INFO]: epoch 752: training loss 0.0423\n",
      "2025-06-01 14:51:50 [INFO]: epoch 753: training loss 0.0450\n",
      "2025-06-01 14:51:50 [INFO]: epoch 754: training loss 0.0412\n",
      "2025-06-01 14:51:50 [INFO]: epoch 755: training loss 0.0446\n",
      "2025-06-01 14:51:50 [INFO]: epoch 756: training loss 0.0437\n",
      "2025-06-01 14:51:50 [INFO]: epoch 757: training loss 0.0427\n",
      "2025-06-01 14:51:50 [INFO]: epoch 758: training loss 0.0413\n",
      "2025-06-01 14:51:50 [INFO]: epoch 759: training loss 0.0426\n",
      "2025-06-01 14:51:50 [INFO]: epoch 760: training loss 0.0453\n",
      "2025-06-01 14:51:50 [INFO]: epoch 761: training loss 0.0438\n",
      "2025-06-01 14:51:50 [INFO]: epoch 762: training loss 0.0397\n",
      "2025-06-01 14:51:50 [INFO]: epoch 763: training loss 0.0421\n",
      "2025-06-01 14:51:50 [INFO]: epoch 764: training loss 0.0400\n",
      "2025-06-01 14:51:50 [INFO]: epoch 765: training loss 0.0431\n",
      "2025-06-01 14:51:50 [INFO]: epoch 766: training loss 0.0406\n",
      "2025-06-01 14:51:50 [INFO]: epoch 767: training loss 0.0401\n",
      "2025-06-01 14:51:50 [INFO]: epoch 768: training loss 0.0426\n",
      "2025-06-01 14:51:50 [INFO]: epoch 769: training loss 0.0431\n",
      "2025-06-01 14:51:50 [INFO]: epoch 770: training loss 0.0413\n",
      "2025-06-01 14:51:50 [INFO]: epoch 771: training loss 0.0427\n",
      "2025-06-01 14:51:50 [INFO]: epoch 772: training loss 0.0362\n",
      "2025-06-01 14:51:50 [INFO]: epoch 773: training loss 0.0441\n",
      "2025-06-01 14:51:50 [INFO]: epoch 774: training loss 0.0468\n",
      "2025-06-01 14:51:50 [INFO]: epoch 775: training loss 0.0400\n",
      "2025-06-01 14:51:50 [INFO]: epoch 776: training loss 0.0391\n",
      "2025-06-01 14:51:50 [INFO]: epoch 777: training loss 0.0391\n",
      "2025-06-01 14:51:50 [INFO]: epoch 778: training loss 0.0439\n",
      "2025-06-01 14:51:50 [INFO]: epoch 779: training loss 0.0403\n",
      "2025-06-01 14:51:50 [INFO]: epoch 780: training loss 0.0416\n",
      "2025-06-01 14:51:51 [INFO]: epoch 781: training loss 0.0423\n",
      "2025-06-01 14:51:51 [INFO]: epoch 782: training loss 0.0418\n",
      "2025-06-01 14:51:51 [INFO]: epoch 783: training loss 0.0405\n",
      "2025-06-01 14:51:51 [INFO]: epoch 784: training loss 0.0422\n",
      "2025-06-01 14:51:51 [INFO]: epoch 785: training loss 0.0420\n",
      "2025-06-01 14:51:51 [INFO]: epoch 786: training loss 0.0407\n",
      "2025-06-01 14:51:51 [INFO]: epoch 787: training loss 0.0391\n",
      "2025-06-01 14:51:51 [INFO]: epoch 788: training loss 0.0428\n",
      "2025-06-01 14:51:51 [INFO]: epoch 789: training loss 0.0476\n",
      "2025-06-01 14:51:51 [INFO]: epoch 790: training loss 0.0375\n",
      "2025-06-01 14:51:51 [INFO]: epoch 791: training loss 0.0410\n",
      "2025-06-01 14:51:51 [INFO]: epoch 792: training loss 0.0445\n",
      "2025-06-01 14:51:51 [INFO]: epoch 793: training loss 0.0441\n",
      "2025-06-01 14:51:51 [INFO]: epoch 794: training loss 0.0397\n",
      "2025-06-01 14:51:51 [INFO]: epoch 795: training loss 0.0391\n",
      "2025-06-01 14:51:51 [INFO]: epoch 796: training loss 0.0443\n",
      "2025-06-01 14:51:51 [INFO]: epoch 797: training loss 0.0396\n",
      "2025-06-01 14:51:51 [INFO]: epoch 798: training loss 0.0402\n",
      "2025-06-01 14:51:51 [INFO]: epoch 799: training loss 0.0412\n",
      "2025-06-01 14:51:51 [INFO]: Finished training.\n",
      "2025-06-01 14:51:51 [WARNING]: 🚨DeprecationWarning: The method impute is deprecated. Please use `predict` instead.\n",
      " 17%|██████████████                                                                      | 1/6 [00:10<00:53, 10.61s/it]2025-06-01 14:51:51 [INFO]: No given device, using default device: cuda\n",
      "2025-06-01 14:51:51 [WARNING]: saving_path not given. Model files and tensorboard file will not be saved.\n",
      "2025-06-01 14:51:51 [INFO]: Model initialized successfully with the number of trainable parameters: 597,780\n",
      "2025-06-01 14:51:51 [INFO]: epoch 0: training loss 0.8553\n",
      "2025-06-01 14:51:51 [INFO]: epoch 1: training loss 0.8133\n",
      "2025-06-01 14:51:51 [INFO]: epoch 2: training loss 0.6138\n",
      "2025-06-01 14:51:51 [INFO]: epoch 3: training loss 0.5140\n",
      "2025-06-01 14:51:51 [INFO]: epoch 4: training loss 0.5385\n",
      "2025-06-01 14:51:51 [INFO]: epoch 5: training loss 0.4204\n",
      "2025-06-01 14:51:51 [INFO]: epoch 6: training loss 0.4465\n",
      "2025-06-01 14:51:51 [INFO]: epoch 7: training loss 0.4622\n",
      "2025-06-01 14:51:51 [INFO]: epoch 8: training loss 0.4944\n",
      "2025-06-01 14:51:51 [INFO]: epoch 9: training loss 0.4253\n",
      "2025-06-01 14:51:51 [INFO]: epoch 10: training loss 0.3698\n",
      "2025-06-01 14:51:51 [INFO]: epoch 11: training loss 0.3575\n",
      "2025-06-01 14:51:51 [INFO]: epoch 12: training loss 0.3642\n",
      "2025-06-01 14:51:51 [INFO]: epoch 13: training loss 0.3333\n",
      "2025-06-01 14:51:51 [INFO]: epoch 14: training loss 0.3546\n",
      "2025-06-01 14:51:51 [INFO]: epoch 15: training loss 0.3124\n",
      "2025-06-01 14:51:51 [INFO]: epoch 16: training loss 0.3047\n",
      "2025-06-01 14:51:51 [INFO]: epoch 17: training loss 0.3041\n",
      "2025-06-01 14:51:51 [INFO]: epoch 18: training loss 0.3346\n",
      "2025-06-01 14:51:51 [INFO]: epoch 19: training loss 0.3232\n",
      "2025-06-01 14:51:51 [INFO]: epoch 20: training loss 0.3029\n",
      "2025-06-01 14:51:51 [INFO]: epoch 21: training loss 0.2978\n",
      "2025-06-01 14:51:51 [INFO]: epoch 22: training loss 0.2955\n",
      "2025-06-01 14:51:51 [INFO]: epoch 23: training loss 0.2925\n",
      "2025-06-01 14:51:51 [INFO]: epoch 24: training loss 0.2964\n",
      "2025-06-01 14:51:51 [INFO]: epoch 25: training loss 0.2687\n",
      "2025-06-01 14:51:51 [INFO]: epoch 26: training loss 0.2867\n",
      "2025-06-01 14:51:51 [INFO]: epoch 27: training loss 0.2605\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-01 14:51:51 [INFO]: epoch 28: training loss 0.2667\n",
      "2025-06-01 14:51:51 [INFO]: epoch 29: training loss 0.2899\n",
      "2025-06-01 14:51:51 [INFO]: epoch 30: training loss 0.2910\n",
      "2025-06-01 14:51:51 [INFO]: epoch 31: training loss 0.2776\n",
      "2025-06-01 14:51:51 [INFO]: epoch 32: training loss 0.2400\n",
      "2025-06-01 14:51:51 [INFO]: epoch 33: training loss 0.2481\n",
      "2025-06-01 14:51:51 [INFO]: epoch 34: training loss 0.2393\n",
      "2025-06-01 14:51:51 [INFO]: epoch 35: training loss 0.2392\n",
      "2025-06-01 14:51:51 [INFO]: epoch 36: training loss 0.2549\n",
      "2025-06-01 14:51:51 [INFO]: epoch 37: training loss 0.2567\n",
      "2025-06-01 14:51:51 [INFO]: epoch 38: training loss 0.2349\n",
      "2025-06-01 14:51:51 [INFO]: epoch 39: training loss 0.2100\n",
      "2025-06-01 14:51:51 [INFO]: epoch 40: training loss 0.2448\n",
      "2025-06-01 14:51:51 [INFO]: epoch 41: training loss 0.2055\n",
      "2025-06-01 14:51:51 [INFO]: epoch 42: training loss 0.2186\n",
      "2025-06-01 14:51:51 [INFO]: epoch 43: training loss 0.2446\n",
      "2025-06-01 14:51:51 [INFO]: epoch 44: training loss 0.2220\n",
      "2025-06-01 14:51:51 [INFO]: epoch 45: training loss 0.1970\n",
      "2025-06-01 14:51:51 [INFO]: epoch 46: training loss 0.2148\n",
      "2025-06-01 14:51:51 [INFO]: epoch 47: training loss 0.2214\n",
      "2025-06-01 14:51:52 [INFO]: epoch 48: training loss 0.2123\n",
      "2025-06-01 14:51:52 [INFO]: epoch 49: training loss 0.2115\n",
      "2025-06-01 14:51:52 [INFO]: epoch 50: training loss 0.2034\n",
      "2025-06-01 14:51:52 [INFO]: epoch 51: training loss 0.2129\n",
      "2025-06-01 14:51:52 [INFO]: epoch 52: training loss 0.2099\n",
      "2025-06-01 14:51:52 [INFO]: epoch 53: training loss 0.1939\n",
      "2025-06-01 14:51:52 [INFO]: epoch 54: training loss 0.2068\n",
      "2025-06-01 14:51:52 [INFO]: epoch 55: training loss 0.1980\n",
      "2025-06-01 14:51:52 [INFO]: epoch 56: training loss 0.1843\n",
      "2025-06-01 14:51:52 [INFO]: epoch 57: training loss 0.1970\n",
      "2025-06-01 14:51:52 [INFO]: epoch 58: training loss 0.2141\n",
      "2025-06-01 14:51:52 [INFO]: epoch 59: training loss 0.2138\n",
      "2025-06-01 14:51:52 [INFO]: epoch 60: training loss 0.2080\n",
      "2025-06-01 14:51:52 [INFO]: epoch 61: training loss 0.1910\n",
      "2025-06-01 14:51:52 [INFO]: epoch 62: training loss 0.1857\n",
      "2025-06-01 14:51:52 [INFO]: epoch 63: training loss 0.1823\n",
      "2025-06-01 14:51:52 [INFO]: epoch 64: training loss 0.1842\n",
      "2025-06-01 14:51:52 [INFO]: epoch 65: training loss 0.1853\n",
      "2025-06-01 14:51:52 [INFO]: epoch 66: training loss 0.1839\n",
      "2025-06-01 14:51:52 [INFO]: epoch 67: training loss 0.1857\n",
      "2025-06-01 14:51:52 [INFO]: epoch 68: training loss 0.1687\n",
      "2025-06-01 14:51:52 [INFO]: epoch 69: training loss 0.1882\n",
      "2025-06-01 14:51:52 [INFO]: epoch 70: training loss 0.1784\n",
      "2025-06-01 14:51:52 [INFO]: epoch 71: training loss 0.1990\n",
      "2025-06-01 14:51:52 [INFO]: epoch 72: training loss 0.1557\n",
      "2025-06-01 14:51:52 [INFO]: epoch 73: training loss 0.1826\n",
      "2025-06-01 14:51:52 [INFO]: epoch 74: training loss 0.1965\n",
      "2025-06-01 14:51:52 [INFO]: epoch 75: training loss 0.1692\n",
      "2025-06-01 14:51:52 [INFO]: epoch 76: training loss 0.1608\n",
      "2025-06-01 14:51:52 [INFO]: epoch 77: training loss 0.1761\n",
      "2025-06-01 14:51:52 [INFO]: epoch 78: training loss 0.1753\n",
      "2025-06-01 14:51:52 [INFO]: epoch 79: training loss 0.1828\n",
      "2025-06-01 14:51:52 [INFO]: epoch 80: training loss 0.1699\n",
      "2025-06-01 14:51:52 [INFO]: epoch 81: training loss 0.1554\n",
      "2025-06-01 14:51:52 [INFO]: epoch 82: training loss 0.1761\n",
      "2025-06-01 14:51:52 [INFO]: epoch 83: training loss 0.1858\n",
      "2025-06-01 14:51:52 [INFO]: epoch 84: training loss 0.1654\n",
      "2025-06-01 14:51:52 [INFO]: epoch 85: training loss 0.1529\n",
      "2025-06-01 14:51:52 [INFO]: epoch 86: training loss 0.1545\n",
      "2025-06-01 14:51:52 [INFO]: epoch 87: training loss 0.1578\n",
      "2025-06-01 14:51:52 [INFO]: epoch 88: training loss 0.1512\n",
      "2025-06-01 14:51:52 [INFO]: epoch 89: training loss 0.1499\n",
      "2025-06-01 14:51:52 [INFO]: epoch 90: training loss 0.1563\n",
      "2025-06-01 14:51:52 [INFO]: epoch 91: training loss 0.1596\n",
      "2025-06-01 14:51:52 [INFO]: epoch 92: training loss 0.1695\n",
      "2025-06-01 14:51:52 [INFO]: epoch 93: training loss 0.1490\n",
      "2025-06-01 14:51:52 [INFO]: epoch 94: training loss 0.1457\n",
      "2025-06-01 14:51:52 [INFO]: epoch 95: training loss 0.1440\n",
      "2025-06-01 14:51:52 [INFO]: epoch 96: training loss 0.1513\n",
      "2025-06-01 14:51:52 [INFO]: epoch 97: training loss 0.1498\n",
      "2025-06-01 14:51:52 [INFO]: epoch 98: training loss 0.1384\n",
      "2025-06-01 14:51:52 [INFO]: epoch 99: training loss 0.1397\n",
      "2025-06-01 14:51:52 [INFO]: epoch 100: training loss 0.1402\n",
      "2025-06-01 14:51:52 [INFO]: epoch 101: training loss 0.1355\n",
      "2025-06-01 14:51:52 [INFO]: epoch 102: training loss 0.1366\n",
      "2025-06-01 14:51:52 [INFO]: epoch 103: training loss 0.1332\n",
      "2025-06-01 14:51:52 [INFO]: epoch 104: training loss 0.1479\n",
      "2025-06-01 14:51:52 [INFO]: epoch 105: training loss 0.1278\n",
      "2025-06-01 14:51:52 [INFO]: epoch 106: training loss 0.1299\n",
      "2025-06-01 14:51:52 [INFO]: epoch 107: training loss 0.1255\n",
      "2025-06-01 14:51:52 [INFO]: epoch 108: training loss 0.1326\n",
      "2025-06-01 14:51:52 [INFO]: epoch 109: training loss 0.1325\n",
      "2025-06-01 14:51:52 [INFO]: epoch 110: training loss 0.1332\n",
      "2025-06-01 14:51:52 [INFO]: epoch 111: training loss 0.1392\n",
      "2025-06-01 14:51:52 [INFO]: epoch 112: training loss 0.1383\n",
      "2025-06-01 14:51:52 [INFO]: epoch 113: training loss 0.1355\n",
      "2025-06-01 14:51:52 [INFO]: epoch 114: training loss 0.1317\n",
      "2025-06-01 14:51:52 [INFO]: epoch 115: training loss 0.1347\n",
      "2025-06-01 14:51:52 [INFO]: epoch 116: training loss 0.1286\n",
      "2025-06-01 14:51:52 [INFO]: epoch 117: training loss 0.1381\n",
      "2025-06-01 14:51:52 [INFO]: epoch 118: training loss 0.1410\n",
      "2025-06-01 14:51:52 [INFO]: epoch 119: training loss 0.1325\n",
      "2025-06-01 14:51:52 [INFO]: epoch 120: training loss 0.1267\n",
      "2025-06-01 14:51:52 [INFO]: epoch 121: training loss 0.1448\n",
      "2025-06-01 14:51:52 [INFO]: epoch 122: training loss 0.1560\n",
      "2025-06-01 14:51:52 [INFO]: epoch 123: training loss 0.1315\n",
      "2025-06-01 14:51:53 [INFO]: epoch 124: training loss 0.1305\n",
      "2025-06-01 14:51:53 [INFO]: epoch 125: training loss 0.1224\n",
      "2025-06-01 14:51:53 [INFO]: epoch 126: training loss 0.1442\n",
      "2025-06-01 14:51:53 [INFO]: epoch 127: training loss 0.1166\n",
      "2025-06-01 14:51:53 [INFO]: epoch 128: training loss 0.1131\n",
      "2025-06-01 14:51:53 [INFO]: epoch 129: training loss 0.1259\n",
      "2025-06-01 14:51:53 [INFO]: epoch 130: training loss 0.1304\n",
      "2025-06-01 14:51:53 [INFO]: epoch 131: training loss 0.1260\n",
      "2025-06-01 14:51:53 [INFO]: epoch 132: training loss 0.1023\n",
      "2025-06-01 14:51:53 [INFO]: epoch 133: training loss 0.1029\n",
      "2025-06-01 14:51:53 [INFO]: epoch 134: training loss 0.1216\n",
      "2025-06-01 14:51:53 [INFO]: epoch 135: training loss 0.1091\n",
      "2025-06-01 14:51:53 [INFO]: epoch 136: training loss 0.0965\n",
      "2025-06-01 14:51:53 [INFO]: epoch 137: training loss 0.1092\n",
      "2025-06-01 14:51:53 [INFO]: epoch 138: training loss 0.1203\n",
      "2025-06-01 14:51:53 [INFO]: epoch 139: training loss 0.1078\n",
      "2025-06-01 14:51:53 [INFO]: epoch 140: training loss 0.1063\n",
      "2025-06-01 14:51:53 [INFO]: epoch 141: training loss 0.1034\n",
      "2025-06-01 14:51:53 [INFO]: epoch 142: training loss 0.1158\n",
      "2025-06-01 14:51:53 [INFO]: epoch 143: training loss 0.1046\n",
      "2025-06-01 14:51:53 [INFO]: epoch 144: training loss 0.1077\n",
      "2025-06-01 14:51:53 [INFO]: epoch 145: training loss 0.1032\n",
      "2025-06-01 14:51:53 [INFO]: epoch 146: training loss 0.1024\n",
      "2025-06-01 14:51:53 [INFO]: epoch 147: training loss 0.1007\n",
      "2025-06-01 14:51:53 [INFO]: epoch 148: training loss 0.1031\n",
      "2025-06-01 14:51:53 [INFO]: epoch 149: training loss 0.0936\n",
      "2025-06-01 14:51:53 [INFO]: epoch 150: training loss 0.1056\n",
      "2025-06-01 14:51:53 [INFO]: epoch 151: training loss 0.0978\n",
      "2025-06-01 14:51:53 [INFO]: epoch 152: training loss 0.0983\n",
      "2025-06-01 14:51:53 [INFO]: epoch 153: training loss 0.0942\n",
      "2025-06-01 14:51:53 [INFO]: epoch 154: training loss 0.0944\n",
      "2025-06-01 14:51:53 [INFO]: epoch 155: training loss 0.0927\n",
      "2025-06-01 14:51:53 [INFO]: epoch 156: training loss 0.1006\n",
      "2025-06-01 14:51:53 [INFO]: epoch 157: training loss 0.0988\n",
      "2025-06-01 14:51:53 [INFO]: epoch 158: training loss 0.0989\n",
      "2025-06-01 14:51:53 [INFO]: epoch 159: training loss 0.0934\n",
      "2025-06-01 14:51:53 [INFO]: epoch 160: training loss 0.0901\n",
      "2025-06-01 14:51:53 [INFO]: epoch 161: training loss 0.0941\n",
      "2025-06-01 14:51:53 [INFO]: epoch 162: training loss 0.0915\n",
      "2025-06-01 14:51:53 [INFO]: epoch 163: training loss 0.0907\n",
      "2025-06-01 14:51:53 [INFO]: epoch 164: training loss 0.0999\n",
      "2025-06-01 14:51:53 [INFO]: epoch 165: training loss 0.0977\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-01 14:51:53 [INFO]: epoch 166: training loss 0.0925\n",
      "2025-06-01 14:51:53 [INFO]: epoch 167: training loss 0.0995\n",
      "2025-06-01 14:51:53 [INFO]: epoch 168: training loss 0.0946\n",
      "2025-06-01 14:51:53 [INFO]: epoch 169: training loss 0.0915\n",
      "2025-06-01 14:51:53 [INFO]: epoch 170: training loss 0.0936\n",
      "2025-06-01 14:51:53 [INFO]: epoch 171: training loss 0.0950\n",
      "2025-06-01 14:51:53 [INFO]: epoch 172: training loss 0.0950\n",
      "2025-06-01 14:51:53 [INFO]: epoch 173: training loss 0.0906\n",
      "2025-06-01 14:51:53 [INFO]: epoch 174: training loss 0.0845\n",
      "2025-06-01 14:51:53 [INFO]: epoch 175: training loss 0.0849\n",
      "2025-06-01 14:51:53 [INFO]: epoch 176: training loss 0.0882\n",
      "2025-06-01 14:51:53 [INFO]: epoch 177: training loss 0.0778\n",
      "2025-06-01 14:51:53 [INFO]: epoch 178: training loss 0.0891\n",
      "2025-06-01 14:51:53 [INFO]: epoch 179: training loss 0.0830\n",
      "2025-06-01 14:51:53 [INFO]: epoch 180: training loss 0.0913\n",
      "2025-06-01 14:51:53 [INFO]: epoch 181: training loss 0.0941\n",
      "2025-06-01 14:51:53 [INFO]: epoch 182: training loss 0.0796\n",
      "2025-06-01 14:51:53 [INFO]: epoch 183: training loss 0.0871\n",
      "2025-06-01 14:51:53 [INFO]: epoch 184: training loss 0.0836\n",
      "2025-06-01 14:51:53 [INFO]: epoch 185: training loss 0.0801\n",
      "2025-06-01 14:51:53 [INFO]: epoch 186: training loss 0.0929\n",
      "2025-06-01 14:51:53 [INFO]: epoch 187: training loss 0.0794\n",
      "2025-06-01 14:51:53 [INFO]: epoch 188: training loss 0.0838\n",
      "2025-06-01 14:51:53 [INFO]: epoch 189: training loss 0.0862\n",
      "2025-06-01 14:51:53 [INFO]: epoch 190: training loss 0.0741\n",
      "2025-06-01 14:51:53 [INFO]: epoch 191: training loss 0.0893\n",
      "2025-06-01 14:51:53 [INFO]: epoch 192: training loss 0.0814\n",
      "2025-06-01 14:51:53 [INFO]: epoch 193: training loss 0.0834\n",
      "2025-06-01 14:51:53 [INFO]: epoch 194: training loss 0.0812\n",
      "2025-06-01 14:51:53 [INFO]: epoch 195: training loss 0.0758\n",
      "2025-06-01 14:51:53 [INFO]: epoch 196: training loss 0.0754\n",
      "2025-06-01 14:51:53 [INFO]: epoch 197: training loss 0.0797\n",
      "2025-06-01 14:51:53 [INFO]: epoch 198: training loss 0.0669\n",
      "2025-06-01 14:51:53 [INFO]: epoch 199: training loss 0.0692\n",
      "2025-06-01 14:51:54 [INFO]: epoch 200: training loss 0.0879\n",
      "2025-06-01 14:51:54 [INFO]: epoch 201: training loss 0.0821\n",
      "2025-06-01 14:51:54 [INFO]: epoch 202: training loss 0.0735\n",
      "2025-06-01 14:51:54 [INFO]: epoch 203: training loss 0.0693\n",
      "2025-06-01 14:51:54 [INFO]: epoch 204: training loss 0.0880\n",
      "2025-06-01 14:51:54 [INFO]: epoch 205: training loss 0.0760\n",
      "2025-06-01 14:51:54 [INFO]: epoch 206: training loss 0.0764\n",
      "2025-06-01 14:51:54 [INFO]: epoch 207: training loss 0.0782\n",
      "2025-06-01 14:51:54 [INFO]: epoch 208: training loss 0.0709\n",
      "2025-06-01 14:51:54 [INFO]: epoch 209: training loss 0.0684\n",
      "2025-06-01 14:51:54 [INFO]: epoch 210: training loss 0.0785\n",
      "2025-06-01 14:51:54 [INFO]: epoch 211: training loss 0.0787\n",
      "2025-06-01 14:51:54 [INFO]: epoch 212: training loss 0.0706\n",
      "2025-06-01 14:51:54 [INFO]: epoch 213: training loss 0.0774\n",
      "2025-06-01 14:51:54 [INFO]: epoch 214: training loss 0.0701\n",
      "2025-06-01 14:51:54 [INFO]: epoch 215: training loss 0.0701\n",
      "2025-06-01 14:51:54 [INFO]: epoch 216: training loss 0.0777\n",
      "2025-06-01 14:51:54 [INFO]: epoch 217: training loss 0.0730\n",
      "2025-06-01 14:51:54 [INFO]: epoch 218: training loss 0.0686\n",
      "2025-06-01 14:51:54 [INFO]: epoch 219: training loss 0.0751\n",
      "2025-06-01 14:51:54 [INFO]: epoch 220: training loss 0.0774\n",
      "2025-06-01 14:51:54 [INFO]: epoch 221: training loss 0.0720\n",
      "2025-06-01 14:51:54 [INFO]: epoch 222: training loss 0.0627\n",
      "2025-06-01 14:51:54 [INFO]: epoch 223: training loss 0.0677\n",
      "2025-06-01 14:51:54 [INFO]: epoch 224: training loss 0.0836\n",
      "2025-06-01 14:51:54 [INFO]: epoch 225: training loss 0.0758\n",
      "2025-06-01 14:51:54 [INFO]: epoch 226: training loss 0.0636\n",
      "2025-06-01 14:51:54 [INFO]: epoch 227: training loss 0.0614\n",
      "2025-06-01 14:51:54 [INFO]: epoch 228: training loss 0.0647\n",
      "2025-06-01 14:51:54 [INFO]: epoch 229: training loss 0.0580\n",
      "2025-06-01 14:51:54 [INFO]: epoch 230: training loss 0.0593\n",
      "2025-06-01 14:51:54 [INFO]: epoch 231: training loss 0.0712\n",
      "2025-06-01 14:51:54 [INFO]: epoch 232: training loss 0.0768\n",
      "2025-06-01 14:51:54 [INFO]: epoch 233: training loss 0.0710\n",
      "2025-06-01 14:51:54 [INFO]: epoch 234: training loss 0.0604\n",
      "2025-06-01 14:51:54 [INFO]: epoch 235: training loss 0.0581\n",
      "2025-06-01 14:51:54 [INFO]: epoch 236: training loss 0.0623\n",
      "2025-06-01 14:51:54 [INFO]: epoch 237: training loss 0.0636\n",
      "2025-06-01 14:51:54 [INFO]: epoch 238: training loss 0.0640\n",
      "2025-06-01 14:51:54 [INFO]: epoch 239: training loss 0.0730\n",
      "2025-06-01 14:51:54 [INFO]: epoch 240: training loss 0.0656\n",
      "2025-06-01 14:51:54 [INFO]: epoch 241: training loss 0.0632\n",
      "2025-06-01 14:51:54 [INFO]: epoch 242: training loss 0.0730\n",
      "2025-06-01 14:51:54 [INFO]: epoch 243: training loss 0.0694\n",
      "2025-06-01 14:51:54 [INFO]: epoch 244: training loss 0.0562\n",
      "2025-06-01 14:51:54 [INFO]: epoch 245: training loss 0.0635\n",
      "2025-06-01 14:51:54 [INFO]: epoch 246: training loss 0.0677\n",
      "2025-06-01 14:51:54 [INFO]: epoch 247: training loss 0.0581\n",
      "2025-06-01 14:51:54 [INFO]: epoch 248: training loss 0.0671\n",
      "2025-06-01 14:51:54 [INFO]: epoch 249: training loss 0.0716\n",
      "2025-06-01 14:51:54 [INFO]: epoch 250: training loss 0.0627\n",
      "2025-06-01 14:51:54 [INFO]: epoch 251: training loss 0.0544\n",
      "2025-06-01 14:51:54 [INFO]: epoch 252: training loss 0.0840\n",
      "2025-06-01 14:51:54 [INFO]: epoch 253: training loss 0.0611\n",
      "2025-06-01 14:51:54 [INFO]: epoch 254: training loss 0.0513\n",
      "2025-06-01 14:51:54 [INFO]: epoch 255: training loss 0.0461\n",
      "2025-06-01 14:51:54 [INFO]: epoch 256: training loss 0.0610\n",
      "2025-06-01 14:51:54 [INFO]: epoch 257: training loss 0.0564\n",
      "2025-06-01 14:51:54 [INFO]: epoch 258: training loss 0.0532\n",
      "2025-06-01 14:51:54 [INFO]: epoch 259: training loss 0.0626\n",
      "2025-06-01 14:51:54 [INFO]: epoch 260: training loss 0.0567\n",
      "2025-06-01 14:51:54 [INFO]: epoch 261: training loss 0.0683\n",
      "2025-06-01 14:51:54 [INFO]: epoch 262: training loss 0.0649\n",
      "2025-06-01 14:51:54 [INFO]: epoch 263: training loss 0.0532\n",
      "2025-06-01 14:51:54 [INFO]: epoch 264: training loss 0.0490\n",
      "2025-06-01 14:51:54 [INFO]: epoch 265: training loss 0.0621\n",
      "2025-06-01 14:51:54 [INFO]: epoch 266: training loss 0.0782\n",
      "2025-06-01 14:51:54 [INFO]: epoch 267: training loss 0.0489\n",
      "2025-06-01 14:51:54 [INFO]: epoch 268: training loss 0.0534\n",
      "2025-06-01 14:51:54 [INFO]: epoch 269: training loss 0.0614\n",
      "2025-06-01 14:51:54 [INFO]: epoch 270: training loss 0.0604\n",
      "2025-06-01 14:51:54 [INFO]: epoch 271: training loss 0.0496\n",
      "2025-06-01 14:51:54 [INFO]: epoch 272: training loss 0.0533\n",
      "2025-06-01 14:51:54 [INFO]: epoch 273: training loss 0.0508\n",
      "2025-06-01 14:51:54 [INFO]: epoch 274: training loss 0.0518\n",
      "2025-06-01 14:51:54 [INFO]: epoch 275: training loss 0.0492\n",
      "2025-06-01 14:51:54 [INFO]: epoch 276: training loss 0.0517\n",
      "2025-06-01 14:51:55 [INFO]: epoch 277: training loss 0.0556\n",
      "2025-06-01 14:51:55 [INFO]: epoch 278: training loss 0.0457\n",
      "2025-06-01 14:51:55 [INFO]: epoch 279: training loss 0.0540\n",
      "2025-06-01 14:51:55 [INFO]: epoch 280: training loss 0.0506\n",
      "2025-06-01 14:51:55 [INFO]: epoch 281: training loss 0.0495\n",
      "2025-06-01 14:51:55 [INFO]: epoch 282: training loss 0.0436\n",
      "2025-06-01 14:51:55 [INFO]: epoch 283: training loss 0.0427\n",
      "2025-06-01 14:51:55 [INFO]: epoch 284: training loss 0.0526\n",
      "2025-06-01 14:51:55 [INFO]: epoch 285: training loss 0.0484\n",
      "2025-06-01 14:51:55 [INFO]: epoch 286: training loss 0.0501\n",
      "2025-06-01 14:51:55 [INFO]: epoch 287: training loss 0.0435\n",
      "2025-06-01 14:51:55 [INFO]: epoch 288: training loss 0.0534\n",
      "2025-06-01 14:51:55 [INFO]: epoch 289: training loss 0.0517\n",
      "2025-06-01 14:51:55 [INFO]: epoch 290: training loss 0.0381\n",
      "2025-06-01 14:51:55 [INFO]: epoch 291: training loss 0.0445\n",
      "2025-06-01 14:51:55 [INFO]: epoch 292: training loss 0.0410\n",
      "2025-06-01 14:51:55 [INFO]: epoch 293: training loss 0.0431\n",
      "2025-06-01 14:51:55 [INFO]: epoch 294: training loss 0.0379\n",
      "2025-06-01 14:51:55 [INFO]: epoch 295: training loss 0.0371\n",
      "2025-06-01 14:51:55 [INFO]: epoch 296: training loss 0.0383\n",
      "2025-06-01 14:51:55 [INFO]: epoch 297: training loss 0.0340\n",
      "2025-06-01 14:51:55 [INFO]: epoch 298: training loss 0.0385\n",
      "2025-06-01 14:51:55 [INFO]: epoch 299: training loss 0.0407\n",
      "2025-06-01 14:51:55 [INFO]: epoch 300: training loss 0.0349\n",
      "2025-06-01 14:51:55 [INFO]: epoch 301: training loss 0.0377\n",
      "2025-06-01 14:51:55 [INFO]: epoch 302: training loss 0.0424\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-01 14:51:55 [INFO]: epoch 303: training loss 0.0390\n",
      "2025-06-01 14:51:55 [INFO]: epoch 304: training loss 0.0458\n",
      "2025-06-01 14:51:55 [INFO]: epoch 305: training loss 0.0471\n",
      "2025-06-01 14:51:55 [INFO]: epoch 306: training loss 0.0415\n",
      "2025-06-01 14:51:55 [INFO]: epoch 307: training loss 0.0461\n",
      "2025-06-01 14:51:55 [INFO]: epoch 308: training loss 0.0449\n",
      "2025-06-01 14:51:55 [INFO]: epoch 309: training loss 0.0425\n",
      "2025-06-01 14:51:55 [INFO]: epoch 310: training loss 0.0391\n",
      "2025-06-01 14:51:55 [INFO]: epoch 311: training loss 0.0375\n",
      "2025-06-01 14:51:55 [INFO]: epoch 312: training loss 0.0368\n",
      "2025-06-01 14:51:55 [INFO]: epoch 313: training loss 0.0315\n",
      "2025-06-01 14:51:55 [INFO]: epoch 314: training loss 0.0383\n",
      "2025-06-01 14:51:55 [INFO]: epoch 315: training loss 0.0405\n",
      "2025-06-01 14:51:55 [INFO]: epoch 316: training loss 0.0458\n",
      "2025-06-01 14:51:55 [INFO]: epoch 317: training loss 0.0437\n",
      "2025-06-01 14:51:55 [INFO]: epoch 318: training loss 0.0427\n",
      "2025-06-01 14:51:55 [INFO]: epoch 319: training loss 0.0360\n",
      "2025-06-01 14:51:55 [INFO]: epoch 320: training loss 0.0441\n",
      "2025-06-01 14:51:55 [INFO]: epoch 321: training loss 0.0454\n",
      "2025-06-01 14:51:55 [INFO]: epoch 322: training loss 0.0387\n",
      "2025-06-01 14:51:55 [INFO]: epoch 323: training loss 0.0318\n",
      "2025-06-01 14:51:55 [INFO]: epoch 324: training loss 0.0399\n",
      "2025-06-01 14:51:55 [INFO]: epoch 325: training loss 0.0404\n",
      "2025-06-01 14:51:55 [INFO]: epoch 326: training loss 0.0391\n",
      "2025-06-01 14:51:55 [INFO]: epoch 327: training loss 0.0421\n",
      "2025-06-01 14:51:55 [INFO]: epoch 328: training loss 0.0411\n",
      "2025-06-01 14:51:55 [INFO]: epoch 329: training loss 0.0398\n",
      "2025-06-01 14:51:55 [INFO]: epoch 330: training loss 0.0390\n",
      "2025-06-01 14:51:55 [INFO]: epoch 331: training loss 0.0398\n",
      "2025-06-01 14:51:55 [INFO]: epoch 332: training loss 0.0444\n",
      "2025-06-01 14:51:55 [INFO]: epoch 333: training loss 0.0362\n",
      "2025-06-01 14:51:55 [INFO]: epoch 334: training loss 0.0432\n",
      "2025-06-01 14:51:55 [INFO]: epoch 335: training loss 0.0414\n",
      "2025-06-01 14:51:55 [INFO]: epoch 336: training loss 0.0395\n",
      "2025-06-01 14:51:55 [INFO]: epoch 337: training loss 0.0359\n",
      "2025-06-01 14:51:55 [INFO]: epoch 338: training loss 0.0441\n",
      "2025-06-01 14:51:55 [INFO]: epoch 339: training loss 0.0360\n",
      "2025-06-01 14:51:55 [INFO]: epoch 340: training loss 0.0353\n",
      "2025-06-01 14:51:55 [INFO]: epoch 341: training loss 0.0394\n",
      "2025-06-01 14:51:55 [INFO]: epoch 342: training loss 0.0379\n",
      "2025-06-01 14:51:55 [INFO]: epoch 343: training loss 0.0393\n",
      "2025-06-01 14:51:55 [INFO]: epoch 344: training loss 0.0394\n",
      "2025-06-01 14:51:55 [INFO]: epoch 345: training loss 0.0310\n",
      "2025-06-01 14:51:55 [INFO]: epoch 346: training loss 0.0498\n",
      "2025-06-01 14:51:55 [INFO]: epoch 347: training loss 0.0475\n",
      "2025-06-01 14:51:55 [INFO]: epoch 348: training loss 0.0381\n",
      "2025-06-01 14:51:55 [INFO]: epoch 349: training loss 0.0308\n",
      "2025-06-01 14:51:55 [INFO]: epoch 350: training loss 0.0437\n",
      "2025-06-01 14:51:55 [INFO]: epoch 351: training loss 0.0343\n",
      "2025-06-01 14:51:55 [INFO]: epoch 352: training loss 0.0374\n",
      "2025-06-01 14:51:56 [INFO]: epoch 353: training loss 0.0438\n",
      "2025-06-01 14:51:56 [INFO]: epoch 354: training loss 0.0300\n",
      "2025-06-01 14:51:56 [INFO]: epoch 355: training loss 0.0368\n",
      "2025-06-01 14:51:56 [INFO]: epoch 356: training loss 0.0425\n",
      "2025-06-01 14:51:56 [INFO]: epoch 357: training loss 0.0401\n",
      "2025-06-01 14:51:56 [INFO]: epoch 358: training loss 0.0361\n",
      "2025-06-01 14:51:56 [INFO]: epoch 359: training loss 0.0342\n",
      "2025-06-01 14:51:56 [INFO]: epoch 360: training loss 0.0333\n",
      "2025-06-01 14:51:56 [INFO]: epoch 361: training loss 0.0382\n",
      "2025-06-01 14:51:56 [INFO]: epoch 362: training loss 0.0373\n",
      "2025-06-01 14:51:56 [INFO]: epoch 363: training loss 0.0350\n",
      "2025-06-01 14:51:56 [INFO]: epoch 364: training loss 0.0348\n",
      "2025-06-01 14:51:56 [INFO]: epoch 365: training loss 0.0351\n",
      "2025-06-01 14:51:56 [INFO]: epoch 366: training loss 0.0352\n",
      "2025-06-01 14:51:56 [INFO]: epoch 367: training loss 0.0321\n",
      "2025-06-01 14:51:56 [INFO]: epoch 368: training loss 0.0311\n",
      "2025-06-01 14:51:56 [INFO]: epoch 369: training loss 0.0360\n",
      "2025-06-01 14:51:56 [INFO]: epoch 370: training loss 0.0373\n",
      "2025-06-01 14:51:56 [INFO]: epoch 371: training loss 0.0400\n",
      "2025-06-01 14:51:56 [INFO]: epoch 372: training loss 0.0397\n",
      "2025-06-01 14:51:56 [INFO]: epoch 373: training loss 0.0356\n",
      "2025-06-01 14:51:56 [INFO]: epoch 374: training loss 0.0364\n",
      "2025-06-01 14:51:56 [INFO]: epoch 375: training loss 0.0404\n",
      "2025-06-01 14:51:56 [INFO]: epoch 376: training loss 0.0396\n",
      "2025-06-01 14:51:56 [INFO]: epoch 377: training loss 0.0345\n",
      "2025-06-01 14:51:56 [INFO]: epoch 378: training loss 0.0361\n",
      "2025-06-01 14:51:56 [INFO]: epoch 379: training loss 0.0399\n",
      "2025-06-01 14:51:56 [INFO]: epoch 380: training loss 0.0300\n",
      "2025-06-01 14:51:56 [INFO]: epoch 381: training loss 0.0335\n",
      "2025-06-01 14:51:56 [INFO]: epoch 382: training loss 0.0298\n",
      "2025-06-01 14:51:56 [INFO]: epoch 383: training loss 0.0319\n",
      "2025-06-01 14:51:56 [INFO]: epoch 384: training loss 0.0312\n",
      "2025-06-01 14:51:56 [INFO]: epoch 385: training loss 0.0310\n",
      "2025-06-01 14:51:56 [INFO]: epoch 386: training loss 0.0352\n",
      "2025-06-01 14:51:56 [INFO]: epoch 387: training loss 0.0311\n",
      "2025-06-01 14:51:56 [INFO]: epoch 388: training loss 0.0276\n",
      "2025-06-01 14:51:56 [INFO]: epoch 389: training loss 0.0294\n",
      "2025-06-01 14:51:56 [INFO]: epoch 390: training loss 0.0348\n",
      "2025-06-01 14:51:56 [INFO]: epoch 391: training loss 0.0311\n",
      "2025-06-01 14:51:56 [INFO]: epoch 392: training loss 0.0381\n",
      "2025-06-01 14:51:56 [INFO]: epoch 393: training loss 0.0285\n",
      "2025-06-01 14:51:56 [INFO]: epoch 394: training loss 0.0332\n",
      "2025-06-01 14:51:56 [INFO]: epoch 395: training loss 0.0317\n",
      "2025-06-01 14:51:56 [INFO]: epoch 396: training loss 0.0331\n",
      "2025-06-01 14:51:56 [INFO]: epoch 397: training loss 0.0269\n",
      "2025-06-01 14:51:56 [INFO]: epoch 398: training loss 0.0339\n",
      "2025-06-01 14:51:56 [INFO]: epoch 399: training loss 0.0289\n",
      "2025-06-01 14:51:56 [INFO]: epoch 400: training loss 0.0263\n",
      "2025-06-01 14:51:56 [INFO]: epoch 401: training loss 0.0359\n",
      "2025-06-01 14:51:56 [INFO]: epoch 402: training loss 0.0334\n",
      "2025-06-01 14:51:56 [INFO]: epoch 403: training loss 0.0272\n",
      "2025-06-01 14:51:56 [INFO]: epoch 404: training loss 0.0313\n",
      "2025-06-01 14:51:56 [INFO]: epoch 405: training loss 0.0365\n",
      "2025-06-01 14:51:56 [INFO]: epoch 406: training loss 0.0299\n",
      "2025-06-01 14:51:56 [INFO]: epoch 407: training loss 0.0335\n",
      "2025-06-01 14:51:56 [INFO]: epoch 408: training loss 0.0392\n",
      "2025-06-01 14:51:56 [INFO]: epoch 409: training loss 0.0289\n",
      "2025-06-01 14:51:56 [INFO]: epoch 410: training loss 0.0257\n",
      "2025-06-01 14:51:56 [INFO]: epoch 411: training loss 0.0335\n",
      "2025-06-01 14:51:56 [INFO]: epoch 412: training loss 0.0362\n",
      "2025-06-01 14:51:56 [INFO]: epoch 413: training loss 0.0279\n",
      "2025-06-01 14:51:56 [INFO]: epoch 414: training loss 0.0289\n",
      "2025-06-01 14:51:56 [INFO]: epoch 415: training loss 0.0401\n",
      "2025-06-01 14:51:56 [INFO]: epoch 416: training loss 0.0347\n",
      "2025-06-01 14:51:56 [INFO]: epoch 417: training loss 0.0309\n",
      "2025-06-01 14:51:56 [INFO]: epoch 418: training loss 0.0359\n",
      "2025-06-01 14:51:56 [INFO]: epoch 419: training loss 0.0319\n",
      "2025-06-01 14:51:56 [INFO]: epoch 420: training loss 0.0319\n",
      "2025-06-01 14:51:56 [INFO]: epoch 421: training loss 0.0345\n",
      "2025-06-01 14:51:56 [INFO]: epoch 422: training loss 0.0260\n",
      "2025-06-01 14:51:56 [INFO]: epoch 423: training loss 0.0326\n",
      "2025-06-01 14:51:56 [INFO]: epoch 424: training loss 0.0276\n",
      "2025-06-01 14:51:56 [INFO]: epoch 425: training loss 0.0287\n",
      "2025-06-01 14:51:56 [INFO]: epoch 426: training loss 0.0312\n",
      "2025-06-01 14:51:56 [INFO]: epoch 427: training loss 0.0309\n",
      "2025-06-01 14:51:56 [INFO]: epoch 428: training loss 0.0292\n",
      "2025-06-01 14:51:56 [INFO]: epoch 429: training loss 0.0352\n",
      "2025-06-01 14:51:57 [INFO]: epoch 430: training loss 0.0285\n",
      "2025-06-01 14:51:57 [INFO]: epoch 431: training loss 0.0271\n",
      "2025-06-01 14:51:57 [INFO]: epoch 432: training loss 0.0316\n",
      "2025-06-01 14:51:57 [INFO]: epoch 433: training loss 0.0307\n",
      "2025-06-01 14:51:57 [INFO]: epoch 434: training loss 0.0252\n",
      "2025-06-01 14:51:57 [INFO]: epoch 435: training loss 0.0293\n",
      "2025-06-01 14:51:57 [INFO]: epoch 436: training loss 0.0373\n",
      "2025-06-01 14:51:57 [INFO]: epoch 437: training loss 0.0298\n",
      "2025-06-01 14:51:57 [INFO]: epoch 438: training loss 0.0273\n",
      "2025-06-01 14:51:57 [INFO]: epoch 439: training loss 0.0374\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-01 14:51:57 [INFO]: epoch 440: training loss 0.0437\n",
      "2025-06-01 14:51:57 [INFO]: epoch 441: training loss 0.0349\n",
      "2025-06-01 14:51:57 [INFO]: epoch 442: training loss 0.0259\n",
      "2025-06-01 14:51:57 [INFO]: epoch 443: training loss 0.0316\n",
      "2025-06-01 14:51:57 [INFO]: epoch 444: training loss 0.0323\n",
      "2025-06-01 14:51:57 [INFO]: epoch 445: training loss 0.0395\n",
      "2025-06-01 14:51:57 [INFO]: epoch 446: training loss 0.0322\n",
      "2025-06-01 14:51:57 [INFO]: epoch 447: training loss 0.0269\n",
      "2025-06-01 14:51:57 [INFO]: epoch 448: training loss 0.0347\n",
      "2025-06-01 14:51:57 [INFO]: epoch 449: training loss 0.0260\n",
      "2025-06-01 14:51:57 [INFO]: epoch 450: training loss 0.0244\n",
      "2025-06-01 14:51:57 [INFO]: epoch 451: training loss 0.0297\n",
      "2025-06-01 14:51:57 [INFO]: epoch 452: training loss 0.0273\n",
      "2025-06-01 14:51:57 [INFO]: epoch 453: training loss 0.0237\n",
      "2025-06-01 14:51:57 [INFO]: epoch 454: training loss 0.0293\n",
      "2025-06-01 14:51:57 [INFO]: epoch 455: training loss 0.0365\n",
      "2025-06-01 14:51:57 [INFO]: epoch 456: training loss 0.0269\n",
      "2025-06-01 14:51:57 [INFO]: epoch 457: training loss 0.0371\n",
      "2025-06-01 14:51:57 [INFO]: epoch 458: training loss 0.0320\n",
      "2025-06-01 14:51:57 [INFO]: epoch 459: training loss 0.0253\n",
      "2025-06-01 14:51:57 [INFO]: epoch 460: training loss 0.0285\n",
      "2025-06-01 14:51:57 [INFO]: epoch 461: training loss 0.0314\n",
      "2025-06-01 14:51:57 [INFO]: epoch 462: training loss 0.0293\n",
      "2025-06-01 14:51:57 [INFO]: epoch 463: training loss 0.0262\n",
      "2025-06-01 14:51:57 [INFO]: epoch 464: training loss 0.0287\n",
      "2025-06-01 14:51:57 [INFO]: epoch 465: training loss 0.0299\n",
      "2025-06-01 14:51:57 [INFO]: epoch 466: training loss 0.0345\n",
      "2025-06-01 14:51:57 [INFO]: epoch 467: training loss 0.0288\n",
      "2025-06-01 14:51:57 [INFO]: epoch 468: training loss 0.0289\n",
      "2025-06-01 14:51:57 [INFO]: epoch 469: training loss 0.0334\n",
      "2025-06-01 14:51:57 [INFO]: epoch 470: training loss 0.0349\n",
      "2025-06-01 14:51:57 [INFO]: epoch 471: training loss 0.0255\n",
      "2025-06-01 14:51:57 [INFO]: epoch 472: training loss 0.0291\n",
      "2025-06-01 14:51:57 [INFO]: epoch 473: training loss 0.0360\n",
      "2025-06-01 14:51:57 [INFO]: epoch 474: training loss 0.0304\n",
      "2025-06-01 14:51:57 [INFO]: epoch 475: training loss 0.0222\n",
      "2025-06-01 14:51:57 [INFO]: epoch 476: training loss 0.0255\n",
      "2025-06-01 14:51:57 [INFO]: epoch 477: training loss 0.0309\n",
      "2025-06-01 14:51:57 [INFO]: epoch 478: training loss 0.0333\n",
      "2025-06-01 14:51:57 [INFO]: epoch 479: training loss 0.0234\n",
      "2025-06-01 14:51:57 [INFO]: epoch 480: training loss 0.0258\n",
      "2025-06-01 14:51:57 [INFO]: epoch 481: training loss 0.0264\n",
      "2025-06-01 14:51:57 [INFO]: epoch 482: training loss 0.0254\n",
      "2025-06-01 14:51:57 [INFO]: epoch 483: training loss 0.0238\n",
      "2025-06-01 14:51:57 [INFO]: epoch 484: training loss 0.0235\n",
      "2025-06-01 14:51:57 [INFO]: epoch 485: training loss 0.0267\n",
      "2025-06-01 14:51:57 [INFO]: epoch 486: training loss 0.0265\n",
      "2025-06-01 14:51:57 [INFO]: epoch 487: training loss 0.0322\n",
      "2025-06-01 14:51:57 [INFO]: epoch 488: training loss 0.0275\n",
      "2025-06-01 14:51:57 [INFO]: epoch 489: training loss 0.0291\n",
      "2025-06-01 14:51:57 [INFO]: epoch 490: training loss 0.0242\n",
      "2025-06-01 14:51:57 [INFO]: epoch 491: training loss 0.0300\n",
      "2025-06-01 14:51:57 [INFO]: epoch 492: training loss 0.0338\n",
      "2025-06-01 14:51:57 [INFO]: epoch 493: training loss 0.0227\n",
      "2025-06-01 14:51:57 [INFO]: epoch 494: training loss 0.0261\n",
      "2025-06-01 14:51:57 [INFO]: epoch 495: training loss 0.0294\n",
      "2025-06-01 14:51:57 [INFO]: epoch 496: training loss 0.0249\n",
      "2025-06-01 14:51:57 [INFO]: epoch 497: training loss 0.0218\n",
      "2025-06-01 14:51:57 [INFO]: epoch 498: training loss 0.0241\n",
      "2025-06-01 14:51:57 [INFO]: epoch 499: training loss 0.0249\n",
      "2025-06-01 14:51:57 [INFO]: epoch 500: training loss 0.0218\n",
      "2025-06-01 14:51:57 [INFO]: epoch 501: training loss 0.0234\n",
      "2025-06-01 14:51:57 [INFO]: epoch 502: training loss 0.0229\n",
      "2025-06-01 14:51:57 [INFO]: epoch 503: training loss 0.0204\n",
      "2025-06-01 14:51:58 [INFO]: epoch 504: training loss 0.0222\n",
      "2025-06-01 14:51:58 [INFO]: epoch 505: training loss 0.0248\n",
      "2025-06-01 14:51:58 [INFO]: epoch 506: training loss 0.0268\n",
      "2025-06-01 14:51:58 [INFO]: epoch 507: training loss 0.0218\n",
      "2025-06-01 14:51:58 [INFO]: epoch 508: training loss 0.0226\n",
      "2025-06-01 14:51:58 [INFO]: epoch 509: training loss 0.0258\n",
      "2025-06-01 14:51:58 [INFO]: epoch 510: training loss 0.0228\n",
      "2025-06-01 14:51:58 [INFO]: epoch 511: training loss 0.0237\n",
      "2025-06-01 14:51:58 [INFO]: epoch 512: training loss 0.0273\n",
      "2025-06-01 14:51:58 [INFO]: epoch 513: training loss 0.0264\n",
      "2025-06-01 14:51:58 [INFO]: epoch 514: training loss 0.0241\n",
      "2025-06-01 14:51:58 [INFO]: epoch 515: training loss 0.0249\n",
      "2025-06-01 14:51:58 [INFO]: epoch 516: training loss 0.0246\n",
      "2025-06-01 14:51:58 [INFO]: epoch 517: training loss 0.0277\n",
      "2025-06-01 14:51:58 [INFO]: epoch 518: training loss 0.0233\n",
      "2025-06-01 14:51:58 [INFO]: epoch 519: training loss 0.0223\n",
      "2025-06-01 14:51:58 [INFO]: epoch 520: training loss 0.0242\n",
      "2025-06-01 14:51:58 [INFO]: epoch 521: training loss 0.0219\n",
      "2025-06-01 14:51:58 [INFO]: epoch 522: training loss 0.0301\n",
      "2025-06-01 14:51:58 [INFO]: epoch 523: training loss 0.0264\n",
      "2025-06-01 14:51:58 [INFO]: epoch 524: training loss 0.0279\n",
      "2025-06-01 14:51:58 [INFO]: epoch 525: training loss 0.0292\n",
      "2025-06-01 14:51:58 [INFO]: epoch 526: training loss 0.0294\n",
      "2025-06-01 14:51:58 [INFO]: epoch 527: training loss 0.0276\n",
      "2025-06-01 14:51:58 [INFO]: epoch 528: training loss 0.0235\n",
      "2025-06-01 14:51:58 [INFO]: epoch 529: training loss 0.0257\n",
      "2025-06-01 14:51:58 [INFO]: epoch 530: training loss 0.0322\n",
      "2025-06-01 14:51:58 [INFO]: epoch 531: training loss 0.0233\n",
      "2025-06-01 14:51:58 [INFO]: epoch 532: training loss 0.0271\n",
      "2025-06-01 14:51:58 [INFO]: epoch 533: training loss 0.0270\n",
      "2025-06-01 14:51:58 [INFO]: epoch 534: training loss 0.0239\n",
      "2025-06-01 14:51:58 [INFO]: epoch 535: training loss 0.0312\n",
      "2025-06-01 14:51:58 [INFO]: epoch 536: training loss 0.0219\n",
      "2025-06-01 14:51:58 [INFO]: epoch 537: training loss 0.0227\n",
      "2025-06-01 14:51:58 [INFO]: epoch 538: training loss 0.0288\n",
      "2025-06-01 14:51:58 [INFO]: epoch 539: training loss 0.0207\n",
      "2025-06-01 14:51:58 [INFO]: epoch 540: training loss 0.0267\n",
      "2025-06-01 14:51:58 [INFO]: epoch 541: training loss 0.0260\n",
      "2025-06-01 14:51:58 [INFO]: epoch 542: training loss 0.0250\n",
      "2025-06-01 14:51:58 [INFO]: epoch 543: training loss 0.0228\n",
      "2025-06-01 14:51:58 [INFO]: epoch 544: training loss 0.0328\n",
      "2025-06-01 14:51:58 [INFO]: epoch 545: training loss 0.0194\n",
      "2025-06-01 14:51:58 [INFO]: epoch 546: training loss 0.0207\n",
      "2025-06-01 14:51:58 [INFO]: epoch 547: training loss 0.0305\n",
      "2025-06-01 14:51:58 [INFO]: epoch 548: training loss 0.0212\n",
      "2025-06-01 14:51:58 [INFO]: epoch 549: training loss 0.0248\n",
      "2025-06-01 14:51:58 [INFO]: epoch 550: training loss 0.0265\n",
      "2025-06-01 14:51:58 [INFO]: epoch 551: training loss 0.0246\n",
      "2025-06-01 14:51:58 [INFO]: epoch 552: training loss 0.0226\n",
      "2025-06-01 14:51:58 [INFO]: epoch 553: training loss 0.0253\n",
      "2025-06-01 14:51:58 [INFO]: epoch 554: training loss 0.0221\n",
      "2025-06-01 14:51:58 [INFO]: epoch 555: training loss 0.0223\n",
      "2025-06-01 14:51:58 [INFO]: epoch 556: training loss 0.0205\n",
      "2025-06-01 14:51:58 [INFO]: epoch 557: training loss 0.0216\n",
      "2025-06-01 14:51:58 [INFO]: epoch 558: training loss 0.0250\n",
      "2025-06-01 14:51:58 [INFO]: epoch 559: training loss 0.0257\n",
      "2025-06-01 14:51:58 [INFO]: epoch 560: training loss 0.0215\n",
      "2025-06-01 14:51:58 [INFO]: epoch 561: training loss 0.0270\n",
      "2025-06-01 14:51:58 [INFO]: epoch 562: training loss 0.0243\n",
      "2025-06-01 14:51:58 [INFO]: epoch 563: training loss 0.0234\n",
      "2025-06-01 14:51:58 [INFO]: epoch 564: training loss 0.0220\n",
      "2025-06-01 14:51:58 [INFO]: epoch 565: training loss 0.0222\n",
      "2025-06-01 14:51:58 [INFO]: epoch 566: training loss 0.0217\n",
      "2025-06-01 14:51:58 [INFO]: epoch 567: training loss 0.0244\n",
      "2025-06-01 14:51:58 [INFO]: epoch 568: training loss 0.0291\n",
      "2025-06-01 14:51:58 [INFO]: epoch 569: training loss 0.0194\n",
      "2025-06-01 14:51:58 [INFO]: epoch 570: training loss 0.0235\n",
      "2025-06-01 14:51:58 [INFO]: epoch 571: training loss 0.0277\n",
      "2025-06-01 14:51:58 [INFO]: epoch 572: training loss 0.0235\n",
      "2025-06-01 14:51:58 [INFO]: epoch 573: training loss 0.0191\n",
      "2025-06-01 14:51:58 [INFO]: epoch 574: training loss 0.0256\n",
      "2025-06-01 14:51:58 [INFO]: epoch 575: training loss 0.0259\n",
      "2025-06-01 14:51:58 [INFO]: epoch 576: training loss 0.0222\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-01 14:51:58 [INFO]: epoch 577: training loss 0.0261\n",
      "2025-06-01 14:51:58 [INFO]: epoch 578: training loss 0.0275\n",
      "2025-06-01 14:51:58 [INFO]: epoch 579: training loss 0.0230\n",
      "2025-06-01 14:51:58 [INFO]: epoch 580: training loss 0.0220\n",
      "2025-06-01 14:51:59 [INFO]: epoch 581: training loss 0.0213\n",
      "2025-06-01 14:51:59 [INFO]: epoch 582: training loss 0.0193\n",
      "2025-06-01 14:51:59 [INFO]: epoch 583: training loss 0.0237\n",
      "2025-06-01 14:51:59 [INFO]: epoch 584: training loss 0.0213\n",
      "2025-06-01 14:51:59 [INFO]: epoch 585: training loss 0.0204\n",
      "2025-06-01 14:51:59 [INFO]: epoch 586: training loss 0.0213\n",
      "2025-06-01 14:51:59 [INFO]: epoch 587: training loss 0.0217\n",
      "2025-06-01 14:51:59 [INFO]: epoch 588: training loss 0.0187\n",
      "2025-06-01 14:51:59 [INFO]: epoch 589: training loss 0.0216\n",
      "2025-06-01 14:51:59 [INFO]: epoch 590: training loss 0.0252\n",
      "2025-06-01 14:51:59 [INFO]: epoch 591: training loss 0.0206\n",
      "2025-06-01 14:51:59 [INFO]: epoch 592: training loss 0.0208\n",
      "2025-06-01 14:51:59 [INFO]: epoch 593: training loss 0.0242\n",
      "2025-06-01 14:51:59 [INFO]: epoch 594: training loss 0.0229\n",
      "2025-06-01 14:51:59 [INFO]: epoch 595: training loss 0.0225\n",
      "2025-06-01 14:51:59 [INFO]: epoch 596: training loss 0.0217\n",
      "2025-06-01 14:51:59 [INFO]: epoch 597: training loss 0.0210\n",
      "2025-06-01 14:51:59 [INFO]: epoch 598: training loss 0.0224\n",
      "2025-06-01 14:51:59 [INFO]: epoch 599: training loss 0.0237\n",
      "2025-06-01 14:51:59 [INFO]: epoch 600: training loss 0.0240\n",
      "2025-06-01 14:51:59 [INFO]: epoch 601: training loss 0.0198\n",
      "2025-06-01 14:51:59 [INFO]: epoch 602: training loss 0.0188\n",
      "2025-06-01 14:51:59 [INFO]: epoch 603: training loss 0.0221\n",
      "2025-06-01 14:51:59 [INFO]: epoch 604: training loss 0.0229\n",
      "2025-06-01 14:51:59 [INFO]: epoch 605: training loss 0.0202\n",
      "2025-06-01 14:51:59 [INFO]: epoch 606: training loss 0.0190\n",
      "2025-06-01 14:51:59 [INFO]: epoch 607: training loss 0.0190\n",
      "2025-06-01 14:51:59 [INFO]: epoch 608: training loss 0.0209\n",
      "2025-06-01 14:51:59 [INFO]: epoch 609: training loss 0.0223\n",
      "2025-06-01 14:51:59 [INFO]: epoch 610: training loss 0.0230\n",
      "2025-06-01 14:51:59 [INFO]: epoch 611: training loss 0.0204\n",
      "2025-06-01 14:51:59 [INFO]: epoch 612: training loss 0.0213\n",
      "2025-06-01 14:51:59 [INFO]: epoch 613: training loss 0.0213\n",
      "2025-06-01 14:51:59 [INFO]: epoch 614: training loss 0.0187\n",
      "2025-06-01 14:51:59 [INFO]: epoch 615: training loss 0.0179\n",
      "2025-06-01 14:51:59 [INFO]: epoch 616: training loss 0.0211\n",
      "2025-06-01 14:51:59 [INFO]: epoch 617: training loss 0.0245\n",
      "2025-06-01 14:51:59 [INFO]: epoch 618: training loss 0.0229\n",
      "2025-06-01 14:51:59 [INFO]: epoch 619: training loss 0.0222\n",
      "2025-06-01 14:51:59 [INFO]: epoch 620: training loss 0.0256\n",
      "2025-06-01 14:51:59 [INFO]: epoch 621: training loss 0.0270\n",
      "2025-06-01 14:51:59 [INFO]: epoch 622: training loss 0.0218\n",
      "2025-06-01 14:51:59 [INFO]: epoch 623: training loss 0.0215\n",
      "2025-06-01 14:51:59 [INFO]: epoch 624: training loss 0.0246\n",
      "2025-06-01 14:51:59 [INFO]: epoch 625: training loss 0.0226\n",
      "2025-06-01 14:51:59 [INFO]: epoch 626: training loss 0.0215\n",
      "2025-06-01 14:51:59 [INFO]: epoch 627: training loss 0.0229\n",
      "2025-06-01 14:51:59 [INFO]: epoch 628: training loss 0.0251\n",
      "2025-06-01 14:51:59 [INFO]: epoch 629: training loss 0.0204\n",
      "2025-06-01 14:51:59 [INFO]: epoch 630: training loss 0.0222\n",
      "2025-06-01 14:51:59 [INFO]: epoch 631: training loss 0.0209\n",
      "2025-06-01 14:51:59 [INFO]: epoch 632: training loss 0.0210\n",
      "2025-06-01 14:51:59 [INFO]: epoch 633: training loss 0.0254\n",
      "2025-06-01 14:51:59 [INFO]: epoch 634: training loss 0.0215\n",
      "2025-06-01 14:51:59 [INFO]: epoch 635: training loss 0.0217\n",
      "2025-06-01 14:51:59 [INFO]: epoch 636: training loss 0.0248\n",
      "2025-06-01 14:51:59 [INFO]: epoch 637: training loss 0.0234\n",
      "2025-06-01 14:51:59 [INFO]: epoch 638: training loss 0.0207\n",
      "2025-06-01 14:51:59 [INFO]: epoch 639: training loss 0.0260\n",
      "2025-06-01 14:51:59 [INFO]: epoch 640: training loss 0.0264\n",
      "2025-06-01 14:51:59 [INFO]: epoch 641: training loss 0.0203\n",
      "2025-06-01 14:51:59 [INFO]: epoch 642: training loss 0.0276\n",
      "2025-06-01 14:51:59 [INFO]: epoch 643: training loss 0.0304\n",
      "2025-06-01 14:51:59 [INFO]: epoch 644: training loss 0.0255\n",
      "2025-06-01 14:51:59 [INFO]: epoch 645: training loss 0.0301\n",
      "2025-06-01 14:51:59 [INFO]: epoch 646: training loss 0.0270\n",
      "2025-06-01 14:51:59 [INFO]: epoch 647: training loss 0.0195\n",
      "2025-06-01 14:51:59 [INFO]: epoch 648: training loss 0.0212\n",
      "2025-06-01 14:51:59 [INFO]: epoch 649: training loss 0.0229\n",
      "2025-06-01 14:51:59 [INFO]: epoch 650: training loss 0.0183\n",
      "2025-06-01 14:51:59 [INFO]: epoch 651: training loss 0.0182\n",
      "2025-06-01 14:51:59 [INFO]: epoch 652: training loss 0.0232\n",
      "2025-06-01 14:51:59 [INFO]: epoch 653: training loss 0.0231\n",
      "2025-06-01 14:51:59 [INFO]: epoch 654: training loss 0.0226\n",
      "2025-06-01 14:51:59 [INFO]: epoch 655: training loss 0.0219\n",
      "2025-06-01 14:52:00 [INFO]: epoch 656: training loss 0.0245\n",
      "2025-06-01 14:52:00 [INFO]: epoch 657: training loss 0.0248\n",
      "2025-06-01 14:52:00 [INFO]: epoch 658: training loss 0.0220\n",
      "2025-06-01 14:52:00 [INFO]: epoch 659: training loss 0.0239\n",
      "2025-06-01 14:52:00 [INFO]: epoch 660: training loss 0.0199\n",
      "2025-06-01 14:52:00 [INFO]: epoch 661: training loss 0.0244\n",
      "2025-06-01 14:52:00 [INFO]: epoch 662: training loss 0.0200\n",
      "2025-06-01 14:52:00 [INFO]: epoch 663: training loss 0.0185\n",
      "2025-06-01 14:52:00 [INFO]: epoch 664: training loss 0.0201\n",
      "2025-06-01 14:52:00 [INFO]: epoch 665: training loss 0.0212\n",
      "2025-06-01 14:52:00 [INFO]: epoch 666: training loss 0.0195\n",
      "2025-06-01 14:52:00 [INFO]: epoch 667: training loss 0.0183\n",
      "2025-06-01 14:52:00 [INFO]: epoch 668: training loss 0.0173\n",
      "2025-06-01 14:52:00 [INFO]: epoch 669: training loss 0.0171\n",
      "2025-06-01 14:52:00 [INFO]: epoch 670: training loss 0.0197\n",
      "2025-06-01 14:52:00 [INFO]: epoch 671: training loss 0.0192\n",
      "2025-06-01 14:52:00 [INFO]: epoch 672: training loss 0.0239\n",
      "2025-06-01 14:52:00 [INFO]: epoch 673: training loss 0.0233\n",
      "2025-06-01 14:52:00 [INFO]: epoch 674: training loss 0.0181\n",
      "2025-06-01 14:52:00 [INFO]: epoch 675: training loss 0.0240\n",
      "2025-06-01 14:52:00 [INFO]: epoch 676: training loss 0.0211\n",
      "2025-06-01 14:52:00 [INFO]: epoch 677: training loss 0.0211\n",
      "2025-06-01 14:52:00 [INFO]: epoch 678: training loss 0.0206\n",
      "2025-06-01 14:52:00 [INFO]: epoch 679: training loss 0.0187\n",
      "2025-06-01 14:52:00 [INFO]: epoch 680: training loss 0.0191\n",
      "2025-06-01 14:52:00 [INFO]: epoch 681: training loss 0.0233\n",
      "2025-06-01 14:52:00 [INFO]: epoch 682: training loss 0.0218\n",
      "2025-06-01 14:52:00 [INFO]: epoch 683: training loss 0.0171\n",
      "2025-06-01 14:52:00 [INFO]: epoch 684: training loss 0.0243\n",
      "2025-06-01 14:52:00 [INFO]: epoch 685: training loss 0.0202\n",
      "2025-06-01 14:52:00 [INFO]: epoch 686: training loss 0.0185\n",
      "2025-06-01 14:52:00 [INFO]: epoch 687: training loss 0.0188\n",
      "2025-06-01 14:52:00 [INFO]: epoch 688: training loss 0.0231\n",
      "2025-06-01 14:52:00 [INFO]: epoch 689: training loss 0.0202\n",
      "2025-06-01 14:52:00 [INFO]: epoch 690: training loss 0.0195\n",
      "2025-06-01 14:52:00 [INFO]: epoch 691: training loss 0.0202\n",
      "2025-06-01 14:52:00 [INFO]: epoch 692: training loss 0.0212\n",
      "2025-06-01 14:52:00 [INFO]: epoch 693: training loss 0.0196\n",
      "2025-06-01 14:52:00 [INFO]: epoch 694: training loss 0.0190\n",
      "2025-06-01 14:52:00 [INFO]: epoch 695: training loss 0.0225\n",
      "2025-06-01 14:52:00 [INFO]: epoch 696: training loss 0.0184\n",
      "2025-06-01 14:52:00 [INFO]: epoch 697: training loss 0.0173\n",
      "2025-06-01 14:52:00 [INFO]: epoch 698: training loss 0.0212\n",
      "2025-06-01 14:52:00 [INFO]: epoch 699: training loss 0.0220\n",
      "2025-06-01 14:52:00 [INFO]: epoch 700: training loss 0.0213\n",
      "2025-06-01 14:52:00 [INFO]: epoch 701: training loss 0.0218\n",
      "2025-06-01 14:52:00 [INFO]: epoch 702: training loss 0.0209\n",
      "2025-06-01 14:52:00 [INFO]: epoch 703: training loss 0.0216\n",
      "2025-06-01 14:52:00 [INFO]: epoch 704: training loss 0.0251\n",
      "2025-06-01 14:52:00 [INFO]: epoch 705: training loss 0.0200\n",
      "2025-06-01 14:52:00 [INFO]: epoch 706: training loss 0.0281\n",
      "2025-06-01 14:52:00 [INFO]: epoch 707: training loss 0.0195\n",
      "2025-06-01 14:52:00 [INFO]: epoch 708: training loss 0.0177\n",
      "2025-06-01 14:52:00 [INFO]: epoch 709: training loss 0.0209\n",
      "2025-06-01 14:52:00 [INFO]: epoch 710: training loss 0.0219\n",
      "2025-06-01 14:52:00 [INFO]: epoch 711: training loss 0.0201\n",
      "2025-06-01 14:52:00 [INFO]: epoch 712: training loss 0.0216\n",
      "2025-06-01 14:52:00 [INFO]: epoch 713: training loss 0.0185\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-01 14:52:00 [INFO]: epoch 714: training loss 0.0174\n",
      "2025-06-01 14:52:00 [INFO]: epoch 715: training loss 0.0173\n",
      "2025-06-01 14:52:00 [INFO]: epoch 716: training loss 0.0170\n",
      "2025-06-01 14:52:00 [INFO]: epoch 717: training loss 0.0191\n",
      "2025-06-01 14:52:00 [INFO]: epoch 718: training loss 0.0224\n",
      "2025-06-01 14:52:00 [INFO]: epoch 719: training loss 0.0199\n",
      "2025-06-01 14:52:00 [INFO]: epoch 720: training loss 0.0172\n",
      "2025-06-01 14:52:00 [INFO]: epoch 721: training loss 0.0209\n",
      "2025-06-01 14:52:00 [INFO]: epoch 722: training loss 0.0186\n",
      "2025-06-01 14:52:00 [INFO]: epoch 723: training loss 0.0188\n",
      "2025-06-01 14:52:00 [INFO]: epoch 724: training loss 0.0183\n",
      "2025-06-01 14:52:00 [INFO]: epoch 725: training loss 0.0179\n",
      "2025-06-01 14:52:00 [INFO]: epoch 726: training loss 0.0190\n",
      "2025-06-01 14:52:00 [INFO]: epoch 727: training loss 0.0197\n",
      "2025-06-01 14:52:00 [INFO]: epoch 728: training loss 0.0185\n",
      "2025-06-01 14:52:00 [INFO]: epoch 729: training loss 0.0187\n",
      "2025-06-01 14:52:00 [INFO]: epoch 730: training loss 0.0211\n",
      "2025-06-01 14:52:00 [INFO]: epoch 731: training loss 0.0210\n",
      "2025-06-01 14:52:01 [INFO]: epoch 732: training loss 0.0193\n",
      "2025-06-01 14:52:01 [INFO]: epoch 733: training loss 0.0206\n",
      "2025-06-01 14:52:01 [INFO]: epoch 734: training loss 0.0194\n",
      "2025-06-01 14:52:01 [INFO]: epoch 735: training loss 0.0195\n",
      "2025-06-01 14:52:01 [INFO]: epoch 736: training loss 0.0197\n",
      "2025-06-01 14:52:01 [INFO]: epoch 737: training loss 0.0220\n",
      "2025-06-01 14:52:01 [INFO]: epoch 738: training loss 0.0186\n",
      "2025-06-01 14:52:01 [INFO]: epoch 739: training loss 0.0182\n",
      "2025-06-01 14:52:01 [INFO]: epoch 740: training loss 0.0191\n",
      "2025-06-01 14:52:01 [INFO]: epoch 741: training loss 0.0176\n",
      "2025-06-01 14:52:01 [INFO]: epoch 742: training loss 0.0185\n",
      "2025-06-01 14:52:01 [INFO]: epoch 743: training loss 0.0192\n",
      "2025-06-01 14:52:01 [INFO]: epoch 744: training loss 0.0204\n",
      "2025-06-01 14:52:01 [INFO]: epoch 745: training loss 0.0186\n",
      "2025-06-01 14:52:01 [INFO]: epoch 746: training loss 0.0218\n",
      "2025-06-01 14:52:01 [INFO]: epoch 747: training loss 0.0188\n",
      "2025-06-01 14:52:01 [INFO]: epoch 748: training loss 0.0163\n",
      "2025-06-01 14:52:01 [INFO]: epoch 749: training loss 0.0257\n",
      "2025-06-01 14:52:01 [INFO]: epoch 750: training loss 0.0206\n",
      "2025-06-01 14:52:01 [INFO]: epoch 751: training loss 0.0182\n",
      "2025-06-01 14:52:01 [INFO]: epoch 752: training loss 0.0226\n",
      "2025-06-01 14:52:01 [INFO]: epoch 753: training loss 0.0200\n",
      "2025-06-01 14:52:01 [INFO]: epoch 754: training loss 0.0169\n",
      "2025-06-01 14:52:01 [INFO]: epoch 755: training loss 0.0191\n",
      "2025-06-01 14:52:01 [INFO]: epoch 756: training loss 0.0260\n",
      "2025-06-01 14:52:01 [INFO]: epoch 757: training loss 0.0211\n",
      "2025-06-01 14:52:01 [INFO]: epoch 758: training loss 0.0197\n",
      "2025-06-01 14:52:01 [INFO]: epoch 759: training loss 0.0178\n",
      "2025-06-01 14:52:01 [INFO]: epoch 760: training loss 0.0170\n",
      "2025-06-01 14:52:01 [INFO]: epoch 761: training loss 0.0208\n",
      "2025-06-01 14:52:01 [INFO]: epoch 762: training loss 0.0208\n",
      "2025-06-01 14:52:01 [INFO]: epoch 763: training loss 0.0184\n",
      "2025-06-01 14:52:01 [INFO]: epoch 764: training loss 0.0204\n",
      "2025-06-01 14:52:01 [INFO]: epoch 765: training loss 0.0175\n",
      "2025-06-01 14:52:01 [INFO]: epoch 766: training loss 0.0203\n",
      "2025-06-01 14:52:01 [INFO]: epoch 767: training loss 0.0215\n",
      "2025-06-01 14:52:01 [INFO]: epoch 768: training loss 0.0199\n",
      "2025-06-01 14:52:01 [INFO]: epoch 769: training loss 0.0198\n",
      "2025-06-01 14:52:01 [INFO]: epoch 770: training loss 0.0183\n",
      "2025-06-01 14:52:01 [INFO]: epoch 771: training loss 0.0226\n",
      "2025-06-01 14:52:01 [INFO]: epoch 772: training loss 0.0230\n",
      "2025-06-01 14:52:01 [INFO]: epoch 773: training loss 0.0198\n",
      "2025-06-01 14:52:01 [INFO]: epoch 774: training loss 0.0181\n",
      "2025-06-01 14:52:01 [INFO]: epoch 775: training loss 0.0212\n",
      "2025-06-01 14:52:01 [INFO]: epoch 776: training loss 0.0216\n",
      "2025-06-01 14:52:01 [INFO]: epoch 777: training loss 0.0181\n",
      "2025-06-01 14:52:01 [INFO]: epoch 778: training loss 0.0183\n",
      "2025-06-01 14:52:01 [INFO]: epoch 779: training loss 0.0191\n",
      "2025-06-01 14:52:01 [INFO]: epoch 780: training loss 0.0189\n",
      "2025-06-01 14:52:01 [INFO]: epoch 781: training loss 0.0202\n",
      "2025-06-01 14:52:01 [INFO]: epoch 782: training loss 0.0169\n",
      "2025-06-01 14:52:01 [INFO]: epoch 783: training loss 0.0181\n",
      "2025-06-01 14:52:01 [INFO]: epoch 784: training loss 0.0222\n",
      "2025-06-01 14:52:01 [INFO]: epoch 785: training loss 0.0177\n",
      "2025-06-01 14:52:01 [INFO]: epoch 786: training loss 0.0190\n",
      "2025-06-01 14:52:01 [INFO]: epoch 787: training loss 0.0184\n",
      "2025-06-01 14:52:01 [INFO]: epoch 788: training loss 0.0195\n",
      "2025-06-01 14:52:01 [INFO]: epoch 789: training loss 0.0165\n",
      "2025-06-01 14:52:01 [INFO]: epoch 790: training loss 0.0161\n",
      "2025-06-01 14:52:01 [INFO]: epoch 791: training loss 0.0196\n",
      "2025-06-01 14:52:01 [INFO]: epoch 792: training loss 0.0196\n",
      "2025-06-01 14:52:01 [INFO]: epoch 793: training loss 0.0160\n",
      "2025-06-01 14:52:01 [INFO]: epoch 794: training loss 0.0159\n",
      "2025-06-01 14:52:01 [INFO]: epoch 795: training loss 0.0174\n",
      "2025-06-01 14:52:01 [INFO]: epoch 796: training loss 0.0180\n",
      "2025-06-01 14:52:01 [INFO]: epoch 797: training loss 0.0153\n",
      "2025-06-01 14:52:01 [INFO]: epoch 798: training loss 0.0154\n",
      "2025-06-01 14:52:01 [INFO]: epoch 799: training loss 0.0155\n",
      "2025-06-01 14:52:01 [INFO]: Finished training.\n",
      "2025-06-01 14:52:01 [WARNING]: 🚨DeprecationWarning: The method impute is deprecated. Please use `predict` instead.\n",
      " 33%|████████████████████████████                                                        | 2/6 [00:21<00:42, 10.63s/it]2025-06-01 14:52:01 [INFO]: No given device, using default device: cuda\n",
      "2025-06-01 14:52:01 [WARNING]: saving_path not given. Model files and tensorboard file will not be saved.\n",
      "2025-06-01 14:52:01 [INFO]: Model initialized successfully with the number of trainable parameters: 597,780\n",
      "2025-06-01 14:52:02 [INFO]: epoch 0: training loss 0.7482\n",
      "2025-06-01 14:52:02 [INFO]: epoch 1: training loss 0.7214\n",
      "2025-06-01 14:52:02 [INFO]: epoch 2: training loss 0.6942\n",
      "2025-06-01 14:52:02 [INFO]: epoch 3: training loss 0.5769\n",
      "2025-06-01 14:52:02 [INFO]: epoch 4: training loss 0.5767\n",
      "2025-06-01 14:52:02 [INFO]: epoch 5: training loss 0.4788\n",
      "2025-06-01 14:52:02 [INFO]: epoch 6: training loss 0.4257\n",
      "2025-06-01 14:52:02 [INFO]: epoch 7: training loss 0.4187\n",
      "2025-06-01 14:52:02 [INFO]: epoch 8: training loss 0.4414\n",
      "2025-06-01 14:52:02 [INFO]: epoch 9: training loss 0.4741\n",
      "2025-06-01 14:52:02 [INFO]: epoch 10: training loss 0.3794\n",
      "2025-06-01 14:52:02 [INFO]: epoch 11: training loss 0.4156\n",
      "2025-06-01 14:52:02 [INFO]: epoch 12: training loss 0.4164\n",
      "2025-06-01 14:52:02 [INFO]: epoch 13: training loss 0.3688\n",
      "2025-06-01 14:52:02 [INFO]: epoch 14: training loss 0.3468\n",
      "2025-06-01 14:52:02 [INFO]: epoch 15: training loss 0.3392\n",
      "2025-06-01 14:52:02 [INFO]: epoch 16: training loss 0.3747\n",
      "2025-06-01 14:52:02 [INFO]: epoch 17: training loss 0.3646\n",
      "2025-06-01 14:52:02 [INFO]: epoch 18: training loss 0.3282\n",
      "2025-06-01 14:52:02 [INFO]: epoch 19: training loss 0.3200\n",
      "2025-06-01 14:52:02 [INFO]: epoch 20: training loss 0.3310\n",
      "2025-06-01 14:52:02 [INFO]: epoch 21: training loss 0.3141\n",
      "2025-06-01 14:52:02 [INFO]: epoch 22: training loss 0.2979\n",
      "2025-06-01 14:52:02 [INFO]: epoch 23: training loss 0.3030\n",
      "2025-06-01 14:52:02 [INFO]: epoch 24: training loss 0.3376\n",
      "2025-06-01 14:52:02 [INFO]: epoch 25: training loss 0.3014\n",
      "2025-06-01 14:52:02 [INFO]: epoch 26: training loss 0.3180\n",
      "2025-06-01 14:52:02 [INFO]: epoch 27: training loss 0.2931\n",
      "2025-06-01 14:52:02 [INFO]: epoch 28: training loss 0.2681\n",
      "2025-06-01 14:52:02 [INFO]: epoch 29: training loss 0.3016\n",
      "2025-06-01 14:52:02 [INFO]: epoch 30: training loss 0.2999\n",
      "2025-06-01 14:52:02 [INFO]: epoch 31: training loss 0.2762\n",
      "2025-06-01 14:52:02 [INFO]: epoch 32: training loss 0.2780\n",
      "2025-06-01 14:52:02 [INFO]: epoch 33: training loss 0.2809\n",
      "2025-06-01 14:52:02 [INFO]: epoch 34: training loss 0.3033\n",
      "2025-06-01 14:52:02 [INFO]: epoch 35: training loss 0.2739\n",
      "2025-06-01 14:52:02 [INFO]: epoch 36: training loss 0.2730\n",
      "2025-06-01 14:52:02 [INFO]: epoch 37: training loss 0.2514\n",
      "2025-06-01 14:52:02 [INFO]: epoch 38: training loss 0.2827\n",
      "2025-06-01 14:52:02 [INFO]: epoch 39: training loss 0.2758\n",
      "2025-06-01 14:52:02 [INFO]: epoch 40: training loss 0.2651\n",
      "2025-06-01 14:52:02 [INFO]: epoch 41: training loss 0.2808\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-01 14:52:02 [INFO]: epoch 42: training loss 0.2626\n",
      "2025-06-01 14:52:02 [INFO]: epoch 43: training loss 0.2415\n",
      "2025-06-01 14:52:02 [INFO]: epoch 44: training loss 0.2794\n",
      "2025-06-01 14:52:02 [INFO]: epoch 45: training loss 0.2362\n",
      "2025-06-01 14:52:02 [INFO]: epoch 46: training loss 0.2451\n",
      "2025-06-01 14:52:02 [INFO]: epoch 47: training loss 0.2207\n",
      "2025-06-01 14:52:02 [INFO]: epoch 48: training loss 0.2423\n",
      "2025-06-01 14:52:02 [INFO]: epoch 49: training loss 0.2402\n",
      "2025-06-01 14:52:02 [INFO]: epoch 50: training loss 0.2350\n",
      "2025-06-01 14:52:02 [INFO]: epoch 51: training loss 0.2228\n",
      "2025-06-01 14:52:02 [INFO]: epoch 52: training loss 0.2000\n",
      "2025-06-01 14:52:02 [INFO]: epoch 53: training loss 0.2083\n",
      "2025-06-01 14:52:02 [INFO]: epoch 54: training loss 0.2196\n",
      "2025-06-01 14:52:02 [INFO]: epoch 55: training loss 0.2152\n",
      "2025-06-01 14:52:02 [INFO]: epoch 56: training loss 0.2254\n",
      "2025-06-01 14:52:02 [INFO]: epoch 57: training loss 0.2087\n",
      "2025-06-01 14:52:02 [INFO]: epoch 58: training loss 0.2094\n",
      "2025-06-01 14:52:02 [INFO]: epoch 59: training loss 0.1998\n",
      "2025-06-01 14:52:02 [INFO]: epoch 60: training loss 0.1879\n",
      "2025-06-01 14:52:02 [INFO]: epoch 61: training loss 0.2070\n",
      "2025-06-01 14:52:02 [INFO]: epoch 62: training loss 0.1910\n",
      "2025-06-01 14:52:02 [INFO]: epoch 63: training loss 0.1881\n",
      "2025-06-01 14:52:02 [INFO]: epoch 64: training loss 0.2096\n",
      "2025-06-01 14:52:02 [INFO]: epoch 65: training loss 0.1935\n",
      "2025-06-01 14:52:02 [INFO]: epoch 66: training loss 0.1782\n",
      "2025-06-01 14:52:02 [INFO]: epoch 67: training loss 0.1739\n",
      "2025-06-01 14:52:02 [INFO]: epoch 68: training loss 0.1802\n",
      "2025-06-01 14:52:02 [INFO]: epoch 69: training loss 0.1729\n",
      "2025-06-01 14:52:02 [INFO]: epoch 70: training loss 0.1701\n",
      "2025-06-01 14:52:02 [INFO]: epoch 71: training loss 0.1900\n",
      "2025-06-01 14:52:02 [INFO]: epoch 72: training loss 0.1853\n",
      "2025-06-01 14:52:02 [INFO]: epoch 73: training loss 0.1689\n",
      "2025-06-01 14:52:02 [INFO]: epoch 74: training loss 0.1529\n",
      "2025-06-01 14:52:02 [INFO]: epoch 75: training loss 0.1515\n",
      "2025-06-01 14:52:03 [INFO]: epoch 76: training loss 0.1574\n",
      "2025-06-01 14:52:03 [INFO]: epoch 77: training loss 0.1514\n",
      "2025-06-01 14:52:03 [INFO]: epoch 78: training loss 0.1483\n",
      "2025-06-01 14:52:03 [INFO]: epoch 79: training loss 0.1370\n",
      "2025-06-01 14:52:03 [INFO]: epoch 80: training loss 0.1358\n",
      "2025-06-01 14:52:03 [INFO]: epoch 81: training loss 0.1671\n",
      "2025-06-01 14:52:03 [INFO]: epoch 82: training loss 0.1465\n",
      "2025-06-01 14:52:03 [INFO]: epoch 83: training loss 0.1179\n",
      "2025-06-01 14:52:03 [INFO]: epoch 84: training loss 0.1215\n",
      "2025-06-01 14:52:03 [INFO]: epoch 85: training loss 0.1506\n",
      "2025-06-01 14:52:03 [INFO]: epoch 86: training loss 0.1505\n",
      "2025-06-01 14:52:03 [INFO]: epoch 87: training loss 0.1403\n",
      "2025-06-01 14:52:03 [INFO]: epoch 88: training loss 0.1375\n",
      "2025-06-01 14:52:03 [INFO]: epoch 89: training loss 0.1422\n",
      "2025-06-01 14:52:03 [INFO]: epoch 90: training loss 0.1350\n",
      "2025-06-01 14:52:03 [INFO]: epoch 91: training loss 0.1567\n",
      "2025-06-01 14:52:03 [INFO]: epoch 92: training loss 0.1334\n",
      "2025-06-01 14:52:03 [INFO]: epoch 93: training loss 0.1289\n",
      "2025-06-01 14:52:03 [INFO]: epoch 94: training loss 0.1336\n",
      "2025-06-01 14:52:03 [INFO]: epoch 95: training loss 0.1553\n",
      "2025-06-01 14:52:03 [INFO]: epoch 96: training loss 0.1426\n",
      "2025-06-01 14:52:03 [INFO]: epoch 97: training loss 0.1360\n",
      "2025-06-01 14:52:03 [INFO]: epoch 98: training loss 0.1315\n",
      "2025-06-01 14:52:03 [INFO]: epoch 99: training loss 0.1407\n",
      "2025-06-01 14:52:03 [INFO]: epoch 100: training loss 0.1208\n",
      "2025-06-01 14:52:03 [INFO]: epoch 101: training loss 0.1082\n",
      "2025-06-01 14:52:03 [INFO]: epoch 102: training loss 0.1176\n",
      "2025-06-01 14:52:03 [INFO]: epoch 103: training loss 0.1258\n",
      "2025-06-01 14:52:03 [INFO]: epoch 104: training loss 0.1199\n",
      "2025-06-01 14:52:03 [INFO]: epoch 105: training loss 0.1100\n",
      "2025-06-01 14:52:03 [INFO]: epoch 106: training loss 0.1067\n",
      "2025-06-01 14:52:03 [INFO]: epoch 107: training loss 0.0876\n",
      "2025-06-01 14:52:03 [INFO]: epoch 108: training loss 0.1130\n",
      "2025-06-01 14:52:03 [INFO]: epoch 109: training loss 0.1147\n",
      "2025-06-01 14:52:03 [INFO]: epoch 110: training loss 0.0991\n",
      "2025-06-01 14:52:03 [INFO]: epoch 111: training loss 0.0978\n",
      "2025-06-01 14:52:03 [INFO]: epoch 112: training loss 0.1045\n",
      "2025-06-01 14:52:03 [INFO]: epoch 113: training loss 0.1276\n",
      "2025-06-01 14:52:03 [INFO]: epoch 114: training loss 0.0986\n",
      "2025-06-01 14:52:03 [INFO]: epoch 115: training loss 0.1028\n",
      "2025-06-01 14:52:03 [INFO]: epoch 116: training loss 0.0978\n",
      "2025-06-01 14:52:03 [INFO]: epoch 117: training loss 0.1109\n",
      "2025-06-01 14:52:03 [INFO]: epoch 118: training loss 0.1047\n",
      "2025-06-01 14:52:03 [INFO]: epoch 119: training loss 0.0918\n",
      "2025-06-01 14:52:03 [INFO]: epoch 120: training loss 0.0926\n",
      "2025-06-01 14:52:03 [INFO]: epoch 121: training loss 0.1010\n",
      "2025-06-01 14:52:03 [INFO]: epoch 122: training loss 0.1025\n",
      "2025-06-01 14:52:03 [INFO]: epoch 123: training loss 0.0909\n",
      "2025-06-01 14:52:03 [INFO]: epoch 124: training loss 0.0893\n",
      "2025-06-01 14:52:03 [INFO]: epoch 125: training loss 0.0867\n",
      "2025-06-01 14:52:03 [INFO]: epoch 126: training loss 0.0875\n",
      "2025-06-01 14:52:03 [INFO]: epoch 127: training loss 0.0955\n",
      "2025-06-01 14:52:03 [INFO]: epoch 128: training loss 0.0874\n",
      "2025-06-01 14:52:03 [INFO]: epoch 129: training loss 0.0813\n",
      "2025-06-01 14:52:03 [INFO]: epoch 130: training loss 0.0828\n",
      "2025-06-01 14:52:03 [INFO]: epoch 131: training loss 0.0766\n",
      "2025-06-01 14:52:03 [INFO]: epoch 132: training loss 0.0801\n",
      "2025-06-01 14:52:03 [INFO]: epoch 133: training loss 0.0764\n",
      "2025-06-01 14:52:03 [INFO]: epoch 134: training loss 0.0621\n",
      "2025-06-01 14:52:03 [INFO]: epoch 135: training loss 0.0809\n",
      "2025-06-01 14:52:03 [INFO]: epoch 136: training loss 0.0857\n",
      "2025-06-01 14:52:03 [INFO]: epoch 137: training loss 0.0832\n",
      "2025-06-01 14:52:03 [INFO]: epoch 138: training loss 0.0782\n",
      "2025-06-01 14:52:03 [INFO]: epoch 139: training loss 0.0713\n",
      "2025-06-01 14:52:03 [INFO]: epoch 140: training loss 0.0735\n",
      "2025-06-01 14:52:03 [INFO]: epoch 141: training loss 0.0810\n",
      "2025-06-01 14:52:03 [INFO]: epoch 142: training loss 0.0812\n",
      "2025-06-01 14:52:03 [INFO]: epoch 143: training loss 0.0737\n",
      "2025-06-01 14:52:03 [INFO]: epoch 144: training loss 0.0771\n",
      "2025-06-01 14:52:03 [INFO]: epoch 145: training loss 0.0764\n",
      "2025-06-01 14:52:03 [INFO]: epoch 146: training loss 0.0743\n",
      "2025-06-01 14:52:03 [INFO]: epoch 147: training loss 0.0758\n",
      "2025-06-01 14:52:03 [INFO]: epoch 148: training loss 0.0801\n",
      "2025-06-01 14:52:03 [INFO]: epoch 149: training loss 0.0653\n",
      "2025-06-01 14:52:03 [INFO]: epoch 150: training loss 0.0938\n",
      "2025-06-01 14:52:03 [INFO]: epoch 151: training loss 0.0581\n",
      "2025-06-01 14:52:04 [INFO]: epoch 152: training loss 0.0643\n",
      "2025-06-01 14:52:04 [INFO]: epoch 153: training loss 0.0685\n",
      "2025-06-01 14:52:04 [INFO]: epoch 154: training loss 0.0821\n",
      "2025-06-01 14:52:04 [INFO]: epoch 155: training loss 0.0657\n",
      "2025-06-01 14:52:04 [INFO]: epoch 156: training loss 0.0705\n",
      "2025-06-01 14:52:04 [INFO]: epoch 157: training loss 0.0636\n",
      "2025-06-01 14:52:04 [INFO]: epoch 158: training loss 0.0774\n",
      "2025-06-01 14:52:04 [INFO]: epoch 159: training loss 0.0642\n",
      "2025-06-01 14:52:04 [INFO]: epoch 160: training loss 0.0668\n",
      "2025-06-01 14:52:04 [INFO]: epoch 161: training loss 0.0762\n",
      "2025-06-01 14:52:04 [INFO]: epoch 162: training loss 0.0754\n",
      "2025-06-01 14:52:04 [INFO]: epoch 163: training loss 0.0670\n",
      "2025-06-01 14:52:04 [INFO]: epoch 164: training loss 0.0781\n",
      "2025-06-01 14:52:04 [INFO]: epoch 165: training loss 0.0690\n",
      "2025-06-01 14:52:04 [INFO]: epoch 166: training loss 0.0653\n",
      "2025-06-01 14:52:04 [INFO]: epoch 167: training loss 0.0697\n",
      "2025-06-01 14:52:04 [INFO]: epoch 168: training loss 0.0717\n",
      "2025-06-01 14:52:04 [INFO]: epoch 169: training loss 0.0672\n",
      "2025-06-01 14:52:04 [INFO]: epoch 170: training loss 0.0778\n",
      "2025-06-01 14:52:04 [INFO]: epoch 171: training loss 0.0684\n",
      "2025-06-01 14:52:04 [INFO]: epoch 172: training loss 0.0680\n",
      "2025-06-01 14:52:04 [INFO]: epoch 173: training loss 0.0713\n",
      "2025-06-01 14:52:04 [INFO]: epoch 174: training loss 0.0796\n",
      "2025-06-01 14:52:04 [INFO]: epoch 175: training loss 0.0599\n",
      "2025-06-01 14:52:04 [INFO]: epoch 176: training loss 0.0688\n",
      "2025-06-01 14:52:04 [INFO]: epoch 177: training loss 0.0666\n",
      "2025-06-01 14:52:04 [INFO]: epoch 178: training loss 0.0655\n",
      "2025-06-01 14:52:04 [INFO]: epoch 179: training loss 0.0594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-01 14:52:04 [INFO]: epoch 180: training loss 0.0760\n",
      "2025-06-01 14:52:04 [INFO]: epoch 181: training loss 0.0633\n",
      "2025-06-01 14:52:04 [INFO]: epoch 182: training loss 0.0641\n",
      "2025-06-01 14:52:04 [INFO]: epoch 183: training loss 0.0621\n",
      "2025-06-01 14:52:04 [INFO]: epoch 184: training loss 0.0598\n",
      "2025-06-01 14:52:04 [INFO]: epoch 185: training loss 0.0607\n",
      "2025-06-01 14:52:04 [INFO]: epoch 186: training loss 0.0572\n",
      "2025-06-01 14:52:04 [INFO]: epoch 187: training loss 0.0521\n",
      "2025-06-01 14:52:04 [INFO]: epoch 188: training loss 0.0511\n",
      "2025-06-01 14:52:04 [INFO]: epoch 189: training loss 0.0566\n",
      "2025-06-01 14:52:04 [INFO]: epoch 190: training loss 0.0675\n",
      "2025-06-01 14:52:04 [INFO]: epoch 191: training loss 0.0541\n",
      "2025-06-01 14:52:04 [INFO]: epoch 192: training loss 0.0633\n",
      "2025-06-01 14:52:04 [INFO]: epoch 193: training loss 0.0542\n",
      "2025-06-01 14:52:04 [INFO]: epoch 194: training loss 0.0529\n",
      "2025-06-01 14:52:04 [INFO]: epoch 195: training loss 0.0474\n",
      "2025-06-01 14:52:04 [INFO]: epoch 196: training loss 0.0570\n",
      "2025-06-01 14:52:04 [INFO]: epoch 197: training loss 0.0586\n",
      "2025-06-01 14:52:04 [INFO]: epoch 198: training loss 0.0577\n",
      "2025-06-01 14:52:04 [INFO]: epoch 199: training loss 0.0618\n",
      "2025-06-01 14:52:04 [INFO]: epoch 200: training loss 0.0748\n",
      "2025-06-01 14:52:04 [INFO]: epoch 201: training loss 0.0721\n",
      "2025-06-01 14:52:04 [INFO]: epoch 202: training loss 0.0604\n",
      "2025-06-01 14:52:04 [INFO]: epoch 203: training loss 0.0551\n",
      "2025-06-01 14:52:04 [INFO]: epoch 204: training loss 0.0646\n",
      "2025-06-01 14:52:04 [INFO]: epoch 205: training loss 0.0628\n",
      "2025-06-01 14:52:04 [INFO]: epoch 206: training loss 0.0471\n",
      "2025-06-01 14:52:04 [INFO]: epoch 207: training loss 0.0592\n",
      "2025-06-01 14:52:04 [INFO]: epoch 208: training loss 0.0604\n",
      "2025-06-01 14:52:04 [INFO]: epoch 209: training loss 0.0583\n",
      "2025-06-01 14:52:04 [INFO]: epoch 210: training loss 0.0471\n",
      "2025-06-01 14:52:04 [INFO]: epoch 211: training loss 0.0648\n",
      "2025-06-01 14:52:04 [INFO]: epoch 212: training loss 0.0671\n",
      "2025-06-01 14:52:04 [INFO]: epoch 213: training loss 0.0491\n",
      "2025-06-01 14:52:04 [INFO]: epoch 214: training loss 0.0547\n",
      "2025-06-01 14:52:04 [INFO]: epoch 215: training loss 0.0668\n",
      "2025-06-01 14:52:04 [INFO]: epoch 216: training loss 0.0545\n",
      "2025-06-01 14:52:04 [INFO]: epoch 217: training loss 0.0501\n",
      "2025-06-01 14:52:04 [INFO]: epoch 218: training loss 0.0536\n",
      "2025-06-01 14:52:04 [INFO]: epoch 219: training loss 0.0562\n",
      "2025-06-01 14:52:04 [INFO]: epoch 220: training loss 0.0561\n",
      "2025-06-01 14:52:04 [INFO]: epoch 221: training loss 0.0577\n",
      "2025-06-01 14:52:04 [INFO]: epoch 222: training loss 0.0545\n",
      "2025-06-01 14:52:04 [INFO]: epoch 223: training loss 0.0609\n",
      "2025-06-01 14:52:04 [INFO]: epoch 224: training loss 0.0559\n",
      "2025-06-01 14:52:04 [INFO]: epoch 225: training loss 0.0515\n",
      "2025-06-01 14:52:04 [INFO]: epoch 226: training loss 0.0483\n",
      "2025-06-01 14:52:04 [INFO]: epoch 227: training loss 0.0510\n",
      "2025-06-01 14:52:05 [INFO]: epoch 228: training loss 0.0502\n",
      "2025-06-01 14:52:05 [INFO]: epoch 229: training loss 0.0513\n",
      "2025-06-01 14:52:05 [INFO]: epoch 230: training loss 0.0564\n",
      "2025-06-01 14:52:05 [INFO]: epoch 231: training loss 0.0414\n",
      "2025-06-01 14:52:05 [INFO]: epoch 232: training loss 0.0415\n",
      "2025-06-01 14:52:05 [INFO]: epoch 233: training loss 0.0489\n",
      "2025-06-01 14:52:05 [INFO]: epoch 234: training loss 0.0514\n",
      "2025-06-01 14:52:05 [INFO]: epoch 235: training loss 0.0441\n",
      "2025-06-01 14:52:05 [INFO]: epoch 236: training loss 0.0511\n",
      "2025-06-01 14:52:05 [INFO]: epoch 237: training loss 0.0601\n",
      "2025-06-01 14:52:05 [INFO]: epoch 238: training loss 0.0391\n",
      "2025-06-01 14:52:05 [INFO]: epoch 239: training loss 0.0379\n",
      "2025-06-01 14:52:05 [INFO]: epoch 240: training loss 0.0406\n",
      "2025-06-01 14:52:05 [INFO]: epoch 241: training loss 0.0462\n",
      "2025-06-01 14:52:05 [INFO]: epoch 242: training loss 0.0338\n",
      "2025-06-01 14:52:05 [INFO]: epoch 243: training loss 0.0423\n",
      "2025-06-01 14:52:05 [INFO]: epoch 244: training loss 0.0457\n",
      "2025-06-01 14:52:05 [INFO]: epoch 245: training loss 0.0555\n",
      "2025-06-01 14:52:05 [INFO]: epoch 246: training loss 0.0396\n",
      "2025-06-01 14:52:05 [INFO]: epoch 247: training loss 0.0391\n",
      "2025-06-01 14:52:05 [INFO]: epoch 248: training loss 0.0529\n",
      "2025-06-01 14:52:05 [INFO]: epoch 249: training loss 0.0426\n",
      "2025-06-01 14:52:05 [INFO]: epoch 250: training loss 0.0385\n",
      "2025-06-01 14:52:05 [INFO]: epoch 251: training loss 0.0385\n",
      "2025-06-01 14:52:05 [INFO]: epoch 252: training loss 0.0460\n",
      "2025-06-01 14:52:05 [INFO]: epoch 253: training loss 0.0454\n",
      "2025-06-01 14:52:05 [INFO]: epoch 254: training loss 0.0410\n",
      "2025-06-01 14:52:05 [INFO]: epoch 255: training loss 0.0407\n",
      "2025-06-01 14:52:05 [INFO]: epoch 256: training loss 0.0372\n",
      "2025-06-01 14:52:05 [INFO]: epoch 257: training loss 0.0432\n",
      "2025-06-01 14:52:05 [INFO]: epoch 258: training loss 0.0472\n",
      "2025-06-01 14:52:05 [INFO]: epoch 259: training loss 0.0414\n",
      "2025-06-01 14:52:05 [INFO]: epoch 260: training loss 0.0571\n",
      "2025-06-01 14:52:05 [INFO]: epoch 261: training loss 0.0577\n",
      "2025-06-01 14:52:05 [INFO]: epoch 262: training loss 0.0436\n",
      "2025-06-01 14:52:05 [INFO]: epoch 263: training loss 0.0493\n",
      "2025-06-01 14:52:05 [INFO]: epoch 264: training loss 0.0669\n",
      "2025-06-01 14:52:05 [INFO]: epoch 265: training loss 0.0559\n",
      "2025-06-01 14:52:05 [INFO]: epoch 266: training loss 0.0539\n",
      "2025-06-01 14:52:05 [INFO]: epoch 267: training loss 0.0554\n",
      "2025-06-01 14:52:05 [INFO]: epoch 268: training loss 0.0664\n",
      "2025-06-01 14:52:05 [INFO]: epoch 269: training loss 0.0588\n",
      "2025-06-01 14:52:05 [INFO]: epoch 270: training loss 0.0411\n",
      "2025-06-01 14:52:05 [INFO]: epoch 271: training loss 0.0501\n",
      "2025-06-01 14:52:05 [INFO]: epoch 272: training loss 0.0616\n",
      "2025-06-01 14:52:05 [INFO]: epoch 273: training loss 0.0384\n",
      "2025-06-01 14:52:05 [INFO]: epoch 274: training loss 0.0358\n",
      "2025-06-01 14:52:05 [INFO]: epoch 275: training loss 0.0437\n",
      "2025-06-01 14:52:05 [INFO]: epoch 276: training loss 0.0459\n",
      "2025-06-01 14:52:05 [INFO]: epoch 277: training loss 0.0413\n",
      "2025-06-01 14:52:05 [INFO]: epoch 278: training loss 0.0414\n",
      "2025-06-01 14:52:05 [INFO]: epoch 279: training loss 0.0462\n",
      "2025-06-01 14:52:05 [INFO]: epoch 280: training loss 0.0398\n",
      "2025-06-01 14:52:05 [INFO]: epoch 281: training loss 0.0463\n",
      "2025-06-01 14:52:05 [INFO]: epoch 282: training loss 0.0456\n",
      "2025-06-01 14:52:05 [INFO]: epoch 283: training loss 0.0308\n",
      "2025-06-01 14:52:05 [INFO]: epoch 284: training loss 0.0423\n",
      "2025-06-01 14:52:05 [INFO]: epoch 285: training loss 0.0420\n",
      "2025-06-01 14:52:05 [INFO]: epoch 286: training loss 0.0485\n",
      "2025-06-01 14:52:05 [INFO]: epoch 287: training loss 0.0396\n",
      "2025-06-01 14:52:05 [INFO]: epoch 288: training loss 0.0384\n",
      "2025-06-01 14:52:05 [INFO]: epoch 289: training loss 0.0415\n",
      "2025-06-01 14:52:05 [INFO]: epoch 290: training loss 0.0436\n",
      "2025-06-01 14:52:05 [INFO]: epoch 291: training loss 0.0424\n",
      "2025-06-01 14:52:05 [INFO]: epoch 292: training loss 0.0411\n",
      "2025-06-01 14:52:05 [INFO]: epoch 293: training loss 0.0370\n",
      "2025-06-01 14:52:05 [INFO]: epoch 294: training loss 0.0509\n",
      "2025-06-01 14:52:05 [INFO]: epoch 295: training loss 0.0485\n",
      "2025-06-01 14:52:05 [INFO]: epoch 296: training loss 0.0395\n",
      "2025-06-01 14:52:05 [INFO]: epoch 297: training loss 0.0330\n",
      "2025-06-01 14:52:05 [INFO]: epoch 298: training loss 0.0381\n",
      "2025-06-01 14:52:05 [INFO]: epoch 299: training loss 0.0389\n",
      "2025-06-01 14:52:05 [INFO]: epoch 300: training loss 0.0360\n",
      "2025-06-01 14:52:05 [INFO]: epoch 301: training loss 0.0316\n",
      "2025-06-01 14:52:05 [INFO]: epoch 302: training loss 0.0400\n",
      "2025-06-01 14:52:05 [INFO]: epoch 303: training loss 0.0394\n",
      "2025-06-01 14:52:06 [INFO]: epoch 304: training loss 0.0350\n",
      "2025-06-01 14:52:06 [INFO]: epoch 305: training loss 0.0375\n",
      "2025-06-01 14:52:06 [INFO]: epoch 306: training loss 0.0305\n",
      "2025-06-01 14:52:06 [INFO]: epoch 307: training loss 0.0370\n",
      "2025-06-01 14:52:06 [INFO]: epoch 308: training loss 0.0314\n",
      "2025-06-01 14:52:06 [INFO]: epoch 309: training loss 0.0316\n",
      "2025-06-01 14:52:06 [INFO]: epoch 310: training loss 0.0299\n",
      "2025-06-01 14:52:06 [INFO]: epoch 311: training loss 0.0321\n",
      "2025-06-01 14:52:06 [INFO]: epoch 312: training loss 0.0313\n",
      "2025-06-01 14:52:06 [INFO]: epoch 313: training loss 0.0336\n",
      "2025-06-01 14:52:06 [INFO]: epoch 314: training loss 0.0306\n",
      "2025-06-01 14:52:06 [INFO]: epoch 315: training loss 0.0327\n",
      "2025-06-01 14:52:06 [INFO]: epoch 316: training loss 0.0352\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-01 14:52:06 [INFO]: epoch 317: training loss 0.0328\n",
      "2025-06-01 14:52:06 [INFO]: epoch 318: training loss 0.0405\n",
      "2025-06-01 14:52:06 [INFO]: epoch 319: training loss 0.0334\n",
      "2025-06-01 14:52:06 [INFO]: epoch 320: training loss 0.0280\n",
      "2025-06-01 14:52:06 [INFO]: epoch 321: training loss 0.0350\n",
      "2025-06-01 14:52:06 [INFO]: epoch 322: training loss 0.0433\n",
      "2025-06-01 14:52:06 [INFO]: epoch 323: training loss 0.0437\n",
      "2025-06-01 14:52:06 [INFO]: epoch 324: training loss 0.0412\n",
      "2025-06-01 14:52:06 [INFO]: epoch 325: training loss 0.0379\n",
      "2025-06-01 14:52:06 [INFO]: epoch 326: training loss 0.0350\n",
      "2025-06-01 14:52:06 [INFO]: epoch 327: training loss 0.0337\n",
      "2025-06-01 14:52:06 [INFO]: epoch 328: training loss 0.0369\n",
      "2025-06-01 14:52:06 [INFO]: epoch 329: training loss 0.0272\n",
      "2025-06-01 14:52:06 [INFO]: epoch 330: training loss 0.0326\n",
      "2025-06-01 14:52:06 [INFO]: epoch 331: training loss 0.0298\n",
      "2025-06-01 14:52:06 [INFO]: epoch 332: training loss 0.0360\n",
      "2025-06-01 14:52:06 [INFO]: epoch 333: training loss 0.0315\n",
      "2025-06-01 14:52:06 [INFO]: epoch 334: training loss 0.0305\n",
      "2025-06-01 14:52:06 [INFO]: epoch 335: training loss 0.0274\n",
      "2025-06-01 14:52:06 [INFO]: epoch 336: training loss 0.0299\n",
      "2025-06-01 14:52:06 [INFO]: epoch 337: training loss 0.0365\n",
      "2025-06-01 14:52:06 [INFO]: epoch 338: training loss 0.0389\n",
      "2025-06-01 14:52:06 [INFO]: epoch 339: training loss 0.0294\n",
      "2025-06-01 14:52:06 [INFO]: epoch 340: training loss 0.0286\n",
      "2025-06-01 14:52:06 [INFO]: epoch 341: training loss 0.0298\n",
      "2025-06-01 14:52:06 [INFO]: epoch 342: training loss 0.0282\n",
      "2025-06-01 14:52:06 [INFO]: epoch 343: training loss 0.0360\n",
      "2025-06-01 14:52:06 [INFO]: epoch 344: training loss 0.0267\n",
      "2025-06-01 14:52:06 [INFO]: epoch 345: training loss 0.0327\n",
      "2025-06-01 14:52:06 [INFO]: epoch 346: training loss 0.0321\n",
      "2025-06-01 14:52:06 [INFO]: epoch 347: training loss 0.0275\n",
      "2025-06-01 14:52:06 [INFO]: epoch 348: training loss 0.0298\n",
      "2025-06-01 14:52:06 [INFO]: epoch 349: training loss 0.0320\n",
      "2025-06-01 14:52:06 [INFO]: epoch 350: training loss 0.0266\n",
      "2025-06-01 14:52:06 [INFO]: epoch 351: training loss 0.0252\n",
      "2025-06-01 14:52:06 [INFO]: epoch 352: training loss 0.0268\n",
      "2025-06-01 14:52:06 [INFO]: epoch 353: training loss 0.0317\n",
      "2025-06-01 14:52:06 [INFO]: epoch 354: training loss 0.0329\n",
      "2025-06-01 14:52:06 [INFO]: epoch 355: training loss 0.0251\n",
      "2025-06-01 14:52:06 [INFO]: epoch 356: training loss 0.0303\n",
      "2025-06-01 14:52:06 [INFO]: epoch 357: training loss 0.0307\n",
      "2025-06-01 14:52:06 [INFO]: epoch 358: training loss 0.0275\n",
      "2025-06-01 14:52:06 [INFO]: epoch 359: training loss 0.0266\n",
      "2025-06-01 14:52:06 [INFO]: epoch 360: training loss 0.0354\n",
      "2025-06-01 14:52:06 [INFO]: epoch 361: training loss 0.0296\n",
      "2025-06-01 14:52:06 [INFO]: epoch 362: training loss 0.0302\n",
      "2025-06-01 14:52:06 [INFO]: epoch 363: training loss 0.0332\n",
      "2025-06-01 14:52:06 [INFO]: epoch 364: training loss 0.0411\n",
      "2025-06-01 14:52:06 [INFO]: epoch 365: training loss 0.0257\n",
      "2025-06-01 14:52:06 [INFO]: epoch 366: training loss 0.0293\n",
      "2025-06-01 14:52:06 [INFO]: epoch 367: training loss 0.0387\n",
      "2025-06-01 14:52:06 [INFO]: epoch 368: training loss 0.0332\n",
      "2025-06-01 14:52:06 [INFO]: epoch 369: training loss 0.0310\n",
      "2025-06-01 14:52:06 [INFO]: epoch 370: training loss 0.0359\n",
      "2025-06-01 14:52:06 [INFO]: epoch 371: training loss 0.0306\n",
      "2025-06-01 14:52:06 [INFO]: epoch 372: training loss 0.0288\n",
      "2025-06-01 14:52:06 [INFO]: epoch 373: training loss 0.0309\n",
      "2025-06-01 14:52:06 [INFO]: epoch 374: training loss 0.0305\n",
      "2025-06-01 14:52:06 [INFO]: epoch 375: training loss 0.0290\n",
      "2025-06-01 14:52:06 [INFO]: epoch 376: training loss 0.0282\n",
      "2025-06-01 14:52:06 [INFO]: epoch 377: training loss 0.0291\n",
      "2025-06-01 14:52:06 [INFO]: epoch 378: training loss 0.0327\n",
      "2025-06-01 14:52:06 [INFO]: epoch 379: training loss 0.0310\n",
      "2025-06-01 14:52:06 [INFO]: epoch 380: training loss 0.0293\n",
      "2025-06-01 14:52:07 [INFO]: epoch 381: training loss 0.0269\n",
      "2025-06-01 14:52:07 [INFO]: epoch 382: training loss 0.0274\n",
      "2025-06-01 14:52:07 [INFO]: epoch 383: training loss 0.0366\n",
      "2025-06-01 14:52:07 [INFO]: epoch 384: training loss 0.0340\n",
      "2025-06-01 14:52:07 [INFO]: epoch 385: training loss 0.0261\n",
      "2025-06-01 14:52:07 [INFO]: epoch 386: training loss 0.0271\n",
      "2025-06-01 14:52:07 [INFO]: epoch 387: training loss 0.0287\n",
      "2025-06-01 14:52:07 [INFO]: epoch 388: training loss 0.0257\n",
      "2025-06-01 14:52:07 [INFO]: epoch 389: training loss 0.0288\n",
      "2025-06-01 14:52:07 [INFO]: epoch 390: training loss 0.0267\n",
      "2025-06-01 14:52:07 [INFO]: epoch 391: training loss 0.0313\n",
      "2025-06-01 14:52:07 [INFO]: epoch 392: training loss 0.0255\n",
      "2025-06-01 14:52:07 [INFO]: epoch 393: training loss 0.0237\n",
      "2025-06-01 14:52:07 [INFO]: epoch 394: training loss 0.0299\n",
      "2025-06-01 14:52:07 [INFO]: epoch 395: training loss 0.0283\n",
      "2025-06-01 14:52:07 [INFO]: epoch 396: training loss 0.0339\n",
      "2025-06-01 14:52:07 [INFO]: epoch 397: training loss 0.0242\n",
      "2025-06-01 14:52:07 [INFO]: epoch 398: training loss 0.0240\n",
      "2025-06-01 14:52:07 [INFO]: epoch 399: training loss 0.0262\n",
      "2025-06-01 14:52:07 [INFO]: epoch 400: training loss 0.0211\n",
      "2025-06-01 14:52:07 [INFO]: epoch 401: training loss 0.0247\n",
      "2025-06-01 14:52:07 [INFO]: epoch 402: training loss 0.0249\n",
      "2025-06-01 14:52:07 [INFO]: epoch 403: training loss 0.0283\n",
      "2025-06-01 14:52:07 [INFO]: epoch 404: training loss 0.0239\n",
      "2025-06-01 14:52:07 [INFO]: epoch 405: training loss 0.0220\n",
      "2025-06-01 14:52:07 [INFO]: epoch 406: training loss 0.0294\n",
      "2025-06-01 14:52:07 [INFO]: epoch 407: training loss 0.0268\n",
      "2025-06-01 14:52:07 [INFO]: epoch 408: training loss 0.0285\n",
      "2025-06-01 14:52:07 [INFO]: epoch 409: training loss 0.0323\n",
      "2025-06-01 14:52:07 [INFO]: epoch 410: training loss 0.0267\n",
      "2025-06-01 14:52:07 [INFO]: epoch 411: training loss 0.0262\n",
      "2025-06-01 14:52:07 [INFO]: epoch 412: training loss 0.0319\n",
      "2025-06-01 14:52:07 [INFO]: epoch 413: training loss 0.0272\n",
      "2025-06-01 14:52:07 [INFO]: epoch 414: training loss 0.0226\n",
      "2025-06-01 14:52:07 [INFO]: epoch 415: training loss 0.0287\n",
      "2025-06-01 14:52:07 [INFO]: epoch 416: training loss 0.0277\n",
      "2025-06-01 14:52:07 [INFO]: epoch 417: training loss 0.0268\n",
      "2025-06-01 14:52:07 [INFO]: epoch 418: training loss 0.0285\n",
      "2025-06-01 14:52:07 [INFO]: epoch 419: training loss 0.0296\n",
      "2025-06-01 14:52:07 [INFO]: epoch 420: training loss 0.0338\n",
      "2025-06-01 14:52:07 [INFO]: epoch 421: training loss 0.0271\n",
      "2025-06-01 14:52:07 [INFO]: epoch 422: training loss 0.0219\n",
      "2025-06-01 14:52:07 [INFO]: epoch 423: training loss 0.0234\n",
      "2025-06-01 14:52:07 [INFO]: epoch 424: training loss 0.0239\n",
      "2025-06-01 14:52:07 [INFO]: epoch 425: training loss 0.0238\n",
      "2025-06-01 14:52:07 [INFO]: epoch 426: training loss 0.0216\n",
      "2025-06-01 14:52:07 [INFO]: epoch 427: training loss 0.0204\n",
      "2025-06-01 14:52:07 [INFO]: epoch 428: training loss 0.0227\n",
      "2025-06-01 14:52:07 [INFO]: epoch 429: training loss 0.0273\n",
      "2025-06-01 14:52:07 [INFO]: epoch 430: training loss 0.0290\n",
      "2025-06-01 14:52:07 [INFO]: epoch 431: training loss 0.0248\n",
      "2025-06-01 14:52:07 [INFO]: epoch 432: training loss 0.0209\n",
      "2025-06-01 14:52:07 [INFO]: epoch 433: training loss 0.0214\n",
      "2025-06-01 14:52:07 [INFO]: epoch 434: training loss 0.0237\n",
      "2025-06-01 14:52:07 [INFO]: epoch 435: training loss 0.0272\n",
      "2025-06-01 14:52:07 [INFO]: epoch 436: training loss 0.0226\n",
      "2025-06-01 14:52:07 [INFO]: epoch 437: training loss 0.0207\n",
      "2025-06-01 14:52:07 [INFO]: epoch 438: training loss 0.0234\n",
      "2025-06-01 14:52:07 [INFO]: epoch 439: training loss 0.0241\n",
      "2025-06-01 14:52:07 [INFO]: epoch 440: training loss 0.0279\n",
      "2025-06-01 14:52:07 [INFO]: epoch 441: training loss 0.0227\n",
      "2025-06-01 14:52:07 [INFO]: epoch 442: training loss 0.0274\n",
      "2025-06-01 14:52:07 [INFO]: epoch 443: training loss 0.0292\n",
      "2025-06-01 14:52:07 [INFO]: epoch 444: training loss 0.0264\n",
      "2025-06-01 14:52:07 [INFO]: epoch 445: training loss 0.0211\n",
      "2025-06-01 14:52:07 [INFO]: epoch 446: training loss 0.0282\n",
      "2025-06-01 14:52:07 [INFO]: epoch 447: training loss 0.0243\n",
      "2025-06-01 14:52:07 [INFO]: epoch 448: training loss 0.0227\n",
      "2025-06-01 14:52:07 [INFO]: epoch 449: training loss 0.0271\n",
      "2025-06-01 14:52:07 [INFO]: epoch 450: training loss 0.0232\n",
      "2025-06-01 14:52:07 [INFO]: epoch 451: training loss 0.0258\n",
      "2025-06-01 14:52:07 [INFO]: epoch 452: training loss 0.0247\n",
      "2025-06-01 14:52:07 [INFO]: epoch 453: training loss 0.0206\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-01 14:52:07 [INFO]: epoch 454: training loss 0.0217\n",
      "2025-06-01 14:52:07 [INFO]: epoch 455: training loss 0.0239\n",
      "2025-06-01 14:52:08 [INFO]: epoch 456: training loss 0.0270\n",
      "2025-06-01 14:52:08 [INFO]: epoch 457: training loss 0.0217\n",
      "2025-06-01 14:52:08 [INFO]: epoch 458: training loss 0.0202\n",
      "2025-06-01 14:52:08 [INFO]: epoch 459: training loss 0.0210\n",
      "2025-06-01 14:52:08 [INFO]: epoch 460: training loss 0.0224\n",
      "2025-06-01 14:52:08 [INFO]: epoch 461: training loss 0.0243\n",
      "2025-06-01 14:52:08 [INFO]: epoch 462: training loss 0.0283\n",
      "2025-06-01 14:52:08 [INFO]: epoch 463: training loss 0.0249\n",
      "2025-06-01 14:52:08 [INFO]: epoch 464: training loss 0.0225\n",
      "2025-06-01 14:52:08 [INFO]: epoch 465: training loss 0.0264\n",
      "2025-06-01 14:52:08 [INFO]: epoch 466: training loss 0.0265\n",
      "2025-06-01 14:52:08 [INFO]: epoch 467: training loss 0.0258\n",
      "2025-06-01 14:52:08 [INFO]: epoch 468: training loss 0.0204\n",
      "2025-06-01 14:52:08 [INFO]: epoch 469: training loss 0.0268\n",
      "2025-06-01 14:52:08 [INFO]: epoch 470: training loss 0.0290\n",
      "2025-06-01 14:52:08 [INFO]: epoch 471: training loss 0.0244\n",
      "2025-06-01 14:52:08 [INFO]: epoch 472: training loss 0.0233\n",
      "2025-06-01 14:52:08 [INFO]: epoch 473: training loss 0.0269\n",
      "2025-06-01 14:52:08 [INFO]: epoch 474: training loss 0.0276\n",
      "2025-06-01 14:52:08 [INFO]: epoch 475: training loss 0.0266\n",
      "2025-06-01 14:52:08 [INFO]: epoch 476: training loss 0.0261\n",
      "2025-06-01 14:52:08 [INFO]: epoch 477: training loss 0.0287\n",
      "2025-06-01 14:52:08 [INFO]: epoch 478: training loss 0.0280\n",
      "2025-06-01 14:52:08 [INFO]: epoch 479: training loss 0.0214\n",
      "2025-06-01 14:52:08 [INFO]: epoch 480: training loss 0.0243\n",
      "2025-06-01 14:52:08 [INFO]: epoch 481: training loss 0.0237\n",
      "2025-06-01 14:52:08 [INFO]: epoch 482: training loss 0.0263\n",
      "2025-06-01 14:52:08 [INFO]: epoch 483: training loss 0.0245\n",
      "2025-06-01 14:52:08 [INFO]: epoch 484: training loss 0.0194\n",
      "2025-06-01 14:52:08 [INFO]: epoch 485: training loss 0.0252\n",
      "2025-06-01 14:52:08 [INFO]: epoch 486: training loss 0.0228\n",
      "2025-06-01 14:52:08 [INFO]: epoch 487: training loss 0.0212\n",
      "2025-06-01 14:52:08 [INFO]: epoch 488: training loss 0.0248\n",
      "2025-06-01 14:52:08 [INFO]: epoch 489: training loss 0.0216\n",
      "2025-06-01 14:52:08 [INFO]: epoch 490: training loss 0.0204\n",
      "2025-06-01 14:52:08 [INFO]: epoch 491: training loss 0.0222\n",
      "2025-06-01 14:52:08 [INFO]: epoch 492: training loss 0.0220\n",
      "2025-06-01 14:52:08 [INFO]: epoch 493: training loss 0.0222\n",
      "2025-06-01 14:52:08 [INFO]: epoch 494: training loss 0.0226\n",
      "2025-06-01 14:52:08 [INFO]: epoch 495: training loss 0.0205\n",
      "2025-06-01 14:52:08 [INFO]: epoch 496: training loss 0.0221\n",
      "2025-06-01 14:52:08 [INFO]: epoch 497: training loss 0.0235\n",
      "2025-06-01 14:52:08 [INFO]: epoch 498: training loss 0.0233\n",
      "2025-06-01 14:52:08 [INFO]: epoch 499: training loss 0.0238\n",
      "2025-06-01 14:52:08 [INFO]: epoch 500: training loss 0.0286\n",
      "2025-06-01 14:52:08 [INFO]: epoch 501: training loss 0.0240\n",
      "2025-06-01 14:52:08 [INFO]: epoch 502: training loss 0.0213\n",
      "2025-06-01 14:52:08 [INFO]: epoch 503: training loss 0.0229\n",
      "2025-06-01 14:52:08 [INFO]: epoch 504: training loss 0.0201\n",
      "2025-06-01 14:52:08 [INFO]: epoch 505: training loss 0.0228\n",
      "2025-06-01 14:52:08 [INFO]: epoch 506: training loss 0.0194\n",
      "2025-06-01 14:52:08 [INFO]: epoch 507: training loss 0.0179\n",
      "2025-06-01 14:52:08 [INFO]: epoch 508: training loss 0.0191\n",
      "2025-06-01 14:52:08 [INFO]: epoch 509: training loss 0.0208\n",
      "2025-06-01 14:52:08 [INFO]: epoch 510: training loss 0.0235\n",
      "2025-06-01 14:52:08 [INFO]: epoch 511: training loss 0.0221\n",
      "2025-06-01 14:52:08 [INFO]: epoch 512: training loss 0.0243\n",
      "2025-06-01 14:52:08 [INFO]: epoch 513: training loss 0.0185\n",
      "2025-06-01 14:52:08 [INFO]: epoch 514: training loss 0.0226\n",
      "2025-06-01 14:52:08 [INFO]: epoch 515: training loss 0.0231\n",
      "2025-06-01 14:52:08 [INFO]: epoch 516: training loss 0.0209\n",
      "2025-06-01 14:52:08 [INFO]: epoch 517: training loss 0.0247\n",
      "2025-06-01 14:52:08 [INFO]: epoch 518: training loss 0.0255\n",
      "2025-06-01 14:52:08 [INFO]: epoch 519: training loss 0.0219\n",
      "2025-06-01 14:52:08 [INFO]: epoch 520: training loss 0.0257\n",
      "2025-06-01 14:52:08 [INFO]: epoch 521: training loss 0.0220\n",
      "2025-06-01 14:52:08 [INFO]: epoch 522: training loss 0.0188\n",
      "2025-06-01 14:52:08 [INFO]: epoch 523: training loss 0.0206\n",
      "2025-06-01 14:52:08 [INFO]: epoch 524: training loss 0.0219\n",
      "2025-06-01 14:52:08 [INFO]: epoch 525: training loss 0.0232\n",
      "2025-06-01 14:52:08 [INFO]: epoch 526: training loss 0.0198\n",
      "2025-06-01 14:52:08 [INFO]: epoch 527: training loss 0.0168\n",
      "2025-06-01 14:52:08 [INFO]: epoch 528: training loss 0.0187\n",
      "2025-06-01 14:52:08 [INFO]: epoch 529: training loss 0.0170\n",
      "2025-06-01 14:52:08 [INFO]: epoch 530: training loss 0.0192\n",
      "2025-06-01 14:52:08 [INFO]: epoch 531: training loss 0.0205\n",
      "2025-06-01 14:52:08 [INFO]: epoch 532: training loss 0.0204\n",
      "2025-06-01 14:52:08 [INFO]: epoch 533: training loss 0.0195\n",
      "2025-06-01 14:52:09 [INFO]: epoch 534: training loss 0.0212\n",
      "2025-06-01 14:52:09 [INFO]: epoch 535: training loss 0.0258\n",
      "2025-06-01 14:52:09 [INFO]: epoch 536: training loss 0.0216\n",
      "2025-06-01 14:52:09 [INFO]: epoch 537: training loss 0.0188\n",
      "2025-06-01 14:52:09 [INFO]: epoch 538: training loss 0.0212\n",
      "2025-06-01 14:52:09 [INFO]: epoch 539: training loss 0.0205\n",
      "2025-06-01 14:52:09 [INFO]: epoch 540: training loss 0.0211\n",
      "2025-06-01 14:52:09 [INFO]: epoch 541: training loss 0.0202\n",
      "2025-06-01 14:52:09 [INFO]: epoch 542: training loss 0.0233\n",
      "2025-06-01 14:52:09 [INFO]: epoch 543: training loss 0.0197\n",
      "2025-06-01 14:52:09 [INFO]: epoch 544: training loss 0.0184\n",
      "2025-06-01 14:52:09 [INFO]: epoch 545: training loss 0.0193\n",
      "2025-06-01 14:52:09 [INFO]: epoch 546: training loss 0.0193\n",
      "2025-06-01 14:52:09 [INFO]: epoch 547: training loss 0.0213\n",
      "2025-06-01 14:52:09 [INFO]: epoch 548: training loss 0.0195\n",
      "2025-06-01 14:52:09 [INFO]: epoch 549: training loss 0.0197\n",
      "2025-06-01 14:52:09 [INFO]: epoch 550: training loss 0.0198\n",
      "2025-06-01 14:52:09 [INFO]: epoch 551: training loss 0.0208\n",
      "2025-06-01 14:52:09 [INFO]: epoch 552: training loss 0.0193\n",
      "2025-06-01 14:52:09 [INFO]: epoch 553: training loss 0.0189\n",
      "2025-06-01 14:52:09 [INFO]: epoch 554: training loss 0.0197\n",
      "2025-06-01 14:52:09 [INFO]: epoch 555: training loss 0.0198\n",
      "2025-06-01 14:52:09 [INFO]: epoch 556: training loss 0.0196\n",
      "2025-06-01 14:52:09 [INFO]: epoch 557: training loss 0.0187\n",
      "2025-06-01 14:52:09 [INFO]: epoch 558: training loss 0.0176\n",
      "2025-06-01 14:52:09 [INFO]: epoch 559: training loss 0.0236\n",
      "2025-06-01 14:52:09 [INFO]: epoch 560: training loss 0.0220\n",
      "2025-06-01 14:52:09 [INFO]: epoch 561: training loss 0.0207\n",
      "2025-06-01 14:52:09 [INFO]: epoch 562: training loss 0.0237\n",
      "2025-06-01 14:52:09 [INFO]: epoch 563: training loss 0.0229\n",
      "2025-06-01 14:52:09 [INFO]: epoch 564: training loss 0.0237\n",
      "2025-06-01 14:52:09 [INFO]: epoch 565: training loss 0.0243\n",
      "2025-06-01 14:52:09 [INFO]: epoch 566: training loss 0.0222\n",
      "2025-06-01 14:52:09 [INFO]: epoch 567: training loss 0.0237\n",
      "2025-06-01 14:52:09 [INFO]: epoch 568: training loss 0.0228\n",
      "2025-06-01 14:52:09 [INFO]: epoch 569: training loss 0.0180\n",
      "2025-06-01 14:52:09 [INFO]: epoch 570: training loss 0.0200\n",
      "2025-06-01 14:52:09 [INFO]: epoch 571: training loss 0.0236\n",
      "2025-06-01 14:52:09 [INFO]: epoch 572: training loss 0.0190\n",
      "2025-06-01 14:52:09 [INFO]: epoch 573: training loss 0.0202\n",
      "2025-06-01 14:52:09 [INFO]: epoch 574: training loss 0.0214\n",
      "2025-06-01 14:52:09 [INFO]: epoch 575: training loss 0.0192\n",
      "2025-06-01 14:52:09 [INFO]: epoch 576: training loss 0.0185\n",
      "2025-06-01 14:52:09 [INFO]: epoch 577: training loss 0.0253\n",
      "2025-06-01 14:52:09 [INFO]: epoch 578: training loss 0.0225\n",
      "2025-06-01 14:52:09 [INFO]: epoch 579: training loss 0.0201\n",
      "2025-06-01 14:52:09 [INFO]: epoch 580: training loss 0.0213\n",
      "2025-06-01 14:52:09 [INFO]: epoch 581: training loss 0.0190\n",
      "2025-06-01 14:52:09 [INFO]: epoch 582: training loss 0.0198\n",
      "2025-06-01 14:52:09 [INFO]: epoch 583: training loss 0.0217\n",
      "2025-06-01 14:52:09 [INFO]: epoch 584: training loss 0.0195\n",
      "2025-06-01 14:52:09 [INFO]: epoch 585: training loss 0.0186\n",
      "2025-06-01 14:52:09 [INFO]: epoch 586: training loss 0.0208\n",
      "2025-06-01 14:52:09 [INFO]: epoch 587: training loss 0.0232\n",
      "2025-06-01 14:52:09 [INFO]: epoch 588: training loss 0.0195\n",
      "2025-06-01 14:52:09 [INFO]: epoch 589: training loss 0.0175\n",
      "2025-06-01 14:52:09 [INFO]: epoch 590: training loss 0.0202\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-01 14:52:09 [INFO]: epoch 591: training loss 0.0212\n",
      "2025-06-01 14:52:09 [INFO]: epoch 592: training loss 0.0208\n",
      "2025-06-01 14:52:09 [INFO]: epoch 593: training loss 0.0210\n",
      "2025-06-01 14:52:09 [INFO]: epoch 594: training loss 0.0216\n",
      "2025-06-01 14:52:09 [INFO]: epoch 595: training loss 0.0253\n",
      "2025-06-01 14:52:09 [INFO]: epoch 596: training loss 0.0169\n",
      "2025-06-01 14:52:09 [INFO]: epoch 597: training loss 0.0190\n",
      "2025-06-01 14:52:09 [INFO]: epoch 598: training loss 0.0182\n",
      "2025-06-01 14:52:09 [INFO]: epoch 599: training loss 0.0180\n",
      "2025-06-01 14:52:09 [INFO]: epoch 600: training loss 0.0218\n",
      "2025-06-01 14:52:09 [INFO]: epoch 601: training loss 0.0171\n",
      "2025-06-01 14:52:09 [INFO]: epoch 602: training loss 0.0225\n",
      "2025-06-01 14:52:09 [INFO]: epoch 603: training loss 0.0193\n",
      "2025-06-01 14:52:09 [INFO]: epoch 604: training loss 0.0183\n",
      "2025-06-01 14:52:09 [INFO]: epoch 605: training loss 0.0201\n",
      "2025-06-01 14:52:09 [INFO]: epoch 606: training loss 0.0217\n",
      "2025-06-01 14:52:09 [INFO]: epoch 607: training loss 0.0209\n",
      "2025-06-01 14:52:09 [INFO]: epoch 608: training loss 0.0173\n",
      "2025-06-01 14:52:09 [INFO]: epoch 609: training loss 0.0182\n",
      "2025-06-01 14:52:09 [INFO]: epoch 610: training loss 0.0182\n",
      "2025-06-01 14:52:10 [INFO]: epoch 611: training loss 0.0209\n",
      "2025-06-01 14:52:10 [INFO]: epoch 612: training loss 0.0228\n",
      "2025-06-01 14:52:10 [INFO]: epoch 613: training loss 0.0159\n",
      "2025-06-01 14:52:10 [INFO]: epoch 614: training loss 0.0185\n",
      "2025-06-01 14:52:10 [INFO]: epoch 615: training loss 0.0186\n",
      "2025-06-01 14:52:10 [INFO]: epoch 616: training loss 0.0165\n",
      "2025-06-01 14:52:10 [INFO]: epoch 617: training loss 0.0161\n",
      "2025-06-01 14:52:10 [INFO]: epoch 618: training loss 0.0197\n",
      "2025-06-01 14:52:10 [INFO]: epoch 619: training loss 0.0174\n",
      "2025-06-01 14:52:10 [INFO]: epoch 620: training loss 0.0167\n",
      "2025-06-01 14:52:10 [INFO]: epoch 621: training loss 0.0157\n",
      "2025-06-01 14:52:10 [INFO]: epoch 622: training loss 0.0195\n",
      "2025-06-01 14:52:10 [INFO]: epoch 623: training loss 0.0183\n",
      "2025-06-01 14:52:10 [INFO]: epoch 624: training loss 0.0191\n",
      "2025-06-01 14:52:10 [INFO]: epoch 625: training loss 0.0170\n",
      "2025-06-01 14:52:10 [INFO]: epoch 626: training loss 0.0181\n",
      "2025-06-01 14:52:10 [INFO]: epoch 627: training loss 0.0196\n",
      "2025-06-01 14:52:10 [INFO]: epoch 628: training loss 0.0189\n",
      "2025-06-01 14:52:10 [INFO]: epoch 629: training loss 0.0172\n",
      "2025-06-01 14:52:10 [INFO]: epoch 630: training loss 0.0175\n",
      "2025-06-01 14:52:10 [INFO]: epoch 631: training loss 0.0176\n",
      "2025-06-01 14:52:10 [INFO]: epoch 632: training loss 0.0175\n",
      "2025-06-01 14:52:10 [INFO]: epoch 633: training loss 0.0192\n",
      "2025-06-01 14:52:10 [INFO]: epoch 634: training loss 0.0173\n",
      "2025-06-01 14:52:10 [INFO]: epoch 635: training loss 0.0176\n",
      "2025-06-01 14:52:10 [INFO]: epoch 636: training loss 0.0202\n",
      "2025-06-01 14:52:10 [INFO]: epoch 637: training loss 0.0171\n",
      "2025-06-01 14:52:10 [INFO]: epoch 638: training loss 0.0182\n",
      "2025-06-01 14:52:10 [INFO]: epoch 639: training loss 0.0170\n",
      "2025-06-01 14:52:10 [INFO]: epoch 640: training loss 0.0187\n",
      "2025-06-01 14:52:10 [INFO]: epoch 641: training loss 0.0160\n",
      "2025-06-01 14:52:10 [INFO]: epoch 642: training loss 0.0211\n",
      "2025-06-01 14:52:10 [INFO]: epoch 643: training loss 0.0241\n",
      "2025-06-01 14:52:10 [INFO]: epoch 644: training loss 0.0199\n",
      "2025-06-01 14:52:10 [INFO]: epoch 645: training loss 0.0239\n",
      "2025-06-01 14:52:10 [INFO]: epoch 646: training loss 0.0219\n",
      "2025-06-01 14:52:10 [INFO]: epoch 647: training loss 0.0180\n",
      "2025-06-01 14:52:10 [INFO]: epoch 648: training loss 0.0212\n",
      "2025-06-01 14:52:10 [INFO]: epoch 649: training loss 0.0196\n",
      "2025-06-01 14:52:10 [INFO]: epoch 650: training loss 0.0194\n",
      "2025-06-01 14:52:10 [INFO]: epoch 651: training loss 0.0199\n",
      "2025-06-01 14:52:10 [INFO]: epoch 652: training loss 0.0159\n",
      "2025-06-01 14:52:10 [INFO]: epoch 653: training loss 0.0182\n",
      "2025-06-01 14:52:10 [INFO]: epoch 654: training loss 0.0193\n",
      "2025-06-01 14:52:10 [INFO]: epoch 655: training loss 0.0185\n",
      "2025-06-01 14:52:10 [INFO]: epoch 656: training loss 0.0150\n",
      "2025-06-01 14:52:10 [INFO]: epoch 657: training loss 0.0189\n",
      "2025-06-01 14:52:10 [INFO]: epoch 658: training loss 0.0160\n",
      "2025-06-01 14:52:10 [INFO]: epoch 659: training loss 0.0197\n",
      "2025-06-01 14:52:10 [INFO]: epoch 660: training loss 0.0250\n",
      "2025-06-01 14:52:10 [INFO]: epoch 661: training loss 0.0204\n",
      "2025-06-01 14:52:10 [INFO]: epoch 662: training loss 0.0191\n",
      "2025-06-01 14:52:10 [INFO]: epoch 663: training loss 0.0206\n",
      "2025-06-01 14:52:10 [INFO]: epoch 664: training loss 0.0192\n",
      "2025-06-01 14:52:10 [INFO]: epoch 665: training loss 0.0173\n",
      "2025-06-01 14:52:10 [INFO]: epoch 666: training loss 0.0241\n",
      "2025-06-01 14:52:10 [INFO]: epoch 667: training loss 0.0181\n",
      "2025-06-01 14:52:10 [INFO]: epoch 668: training loss 0.0171\n",
      "2025-06-01 14:52:10 [INFO]: epoch 669: training loss 0.0166\n",
      "2025-06-01 14:52:10 [INFO]: epoch 670: training loss 0.0171\n",
      "2025-06-01 14:52:10 [INFO]: epoch 671: training loss 0.0164\n",
      "2025-06-01 14:52:10 [INFO]: epoch 672: training loss 0.0172\n",
      "2025-06-01 14:52:10 [INFO]: epoch 673: training loss 0.0215\n",
      "2025-06-01 14:52:10 [INFO]: epoch 674: training loss 0.0187\n",
      "2025-06-01 14:52:10 [INFO]: epoch 675: training loss 0.0213\n",
      "2025-06-01 14:52:10 [INFO]: epoch 676: training loss 0.0216\n",
      "2025-06-01 14:52:10 [INFO]: epoch 677: training loss 0.0167\n",
      "2025-06-01 14:52:10 [INFO]: epoch 678: training loss 0.0180\n",
      "2025-06-01 14:52:10 [INFO]: epoch 679: training loss 0.0206\n",
      "2025-06-01 14:52:10 [INFO]: epoch 680: training loss 0.0192\n",
      "2025-06-01 14:52:10 [INFO]: epoch 681: training loss 0.0190\n",
      "2025-06-01 14:52:10 [INFO]: epoch 682: training loss 0.0188\n",
      "2025-06-01 14:52:10 [INFO]: epoch 683: training loss 0.0149\n",
      "2025-06-01 14:52:10 [INFO]: epoch 684: training loss 0.0183\n",
      "2025-06-01 14:52:10 [INFO]: epoch 685: training loss 0.0175\n",
      "2025-06-01 14:52:10 [INFO]: epoch 686: training loss 0.0178\n",
      "2025-06-01 14:52:10 [INFO]: epoch 687: training loss 0.0190\n",
      "2025-06-01 14:52:11 [INFO]: epoch 688: training loss 0.0159\n",
      "2025-06-01 14:52:11 [INFO]: epoch 689: training loss 0.0169\n",
      "2025-06-01 14:52:11 [INFO]: epoch 690: training loss 0.0149\n",
      "2025-06-01 14:52:11 [INFO]: epoch 691: training loss 0.0191\n",
      "2025-06-01 14:52:11 [INFO]: epoch 692: training loss 0.0165\n",
      "2025-06-01 14:52:11 [INFO]: epoch 693: training loss 0.0172\n",
      "2025-06-01 14:52:11 [INFO]: epoch 694: training loss 0.0184\n",
      "2025-06-01 14:52:11 [INFO]: epoch 695: training loss 0.0172\n",
      "2025-06-01 14:52:11 [INFO]: epoch 696: training loss 0.0180\n",
      "2025-06-01 14:52:11 [INFO]: epoch 697: training loss 0.0184\n",
      "2025-06-01 14:52:11 [INFO]: epoch 698: training loss 0.0170\n",
      "2025-06-01 14:52:11 [INFO]: epoch 699: training loss 0.0210\n",
      "2025-06-01 14:52:11 [INFO]: epoch 700: training loss 0.0163\n",
      "2025-06-01 14:52:11 [INFO]: epoch 701: training loss 0.0175\n",
      "2025-06-01 14:52:11 [INFO]: epoch 702: training loss 0.0171\n",
      "2025-06-01 14:52:11 [INFO]: epoch 703: training loss 0.0141\n",
      "2025-06-01 14:52:11 [INFO]: epoch 704: training loss 0.0159\n",
      "2025-06-01 14:52:11 [INFO]: epoch 705: training loss 0.0162\n",
      "2025-06-01 14:52:11 [INFO]: epoch 706: training loss 0.0196\n",
      "2025-06-01 14:52:11 [INFO]: epoch 707: training loss 0.0161\n",
      "2025-06-01 14:52:11 [INFO]: epoch 708: training loss 0.0176\n",
      "2025-06-01 14:52:11 [INFO]: epoch 709: training loss 0.0218\n",
      "2025-06-01 14:52:11 [INFO]: epoch 710: training loss 0.0193\n",
      "2025-06-01 14:52:11 [INFO]: epoch 711: training loss 0.0167\n",
      "2025-06-01 14:52:11 [INFO]: epoch 712: training loss 0.0183\n",
      "2025-06-01 14:52:11 [INFO]: epoch 713: training loss 0.0180\n",
      "2025-06-01 14:52:11 [INFO]: epoch 714: training loss 0.0233\n",
      "2025-06-01 14:52:11 [INFO]: epoch 715: training loss 0.0175\n",
      "2025-06-01 14:52:11 [INFO]: epoch 716: training loss 0.0195\n",
      "2025-06-01 14:52:11 [INFO]: epoch 717: training loss 0.0220\n",
      "2025-06-01 14:52:11 [INFO]: epoch 718: training loss 0.0164\n",
      "2025-06-01 14:52:11 [INFO]: epoch 719: training loss 0.0206\n",
      "2025-06-01 14:52:11 [INFO]: epoch 720: training loss 0.0178\n",
      "2025-06-01 14:52:11 [INFO]: epoch 721: training loss 0.0187\n",
      "2025-06-01 14:52:11 [INFO]: epoch 722: training loss 0.0204\n",
      "2025-06-01 14:52:11 [INFO]: epoch 723: training loss 0.0174\n",
      "2025-06-01 14:52:11 [INFO]: epoch 724: training loss 0.0167\n",
      "2025-06-01 14:52:11 [INFO]: epoch 725: training loss 0.0211\n",
      "2025-06-01 14:52:11 [INFO]: epoch 726: training loss 0.0206\n",
      "2025-06-01 14:52:11 [INFO]: epoch 727: training loss 0.0215\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-01 14:52:11 [INFO]: epoch 728: training loss 0.0240\n",
      "2025-06-01 14:52:11 [INFO]: epoch 729: training loss 0.0277\n",
      "2025-06-01 14:52:11 [INFO]: epoch 730: training loss 0.0214\n",
      "2025-06-01 14:52:11 [INFO]: epoch 731: training loss 0.0248\n",
      "2025-06-01 14:52:11 [INFO]: epoch 732: training loss 0.0299\n",
      "2025-06-01 14:52:11 [INFO]: epoch 733: training loss 0.0210\n",
      "2025-06-01 14:52:11 [INFO]: epoch 734: training loss 0.0172\n",
      "2025-06-01 14:52:11 [INFO]: epoch 735: training loss 0.0216\n",
      "2025-06-01 14:52:11 [INFO]: epoch 736: training loss 0.0188\n",
      "2025-06-01 14:52:11 [INFO]: epoch 737: training loss 0.0167\n",
      "2025-06-01 14:52:11 [INFO]: epoch 738: training loss 0.0261\n",
      "2025-06-01 14:52:11 [INFO]: epoch 739: training loss 0.0190\n",
      "2025-06-01 14:52:11 [INFO]: epoch 740: training loss 0.0171\n",
      "2025-06-01 14:52:11 [INFO]: epoch 741: training loss 0.0191\n",
      "2025-06-01 14:52:11 [INFO]: epoch 742: training loss 0.0183\n",
      "2025-06-01 14:52:11 [INFO]: epoch 743: training loss 0.0167\n",
      "2025-06-01 14:52:11 [INFO]: epoch 744: training loss 0.0208\n",
      "2025-06-01 14:52:11 [INFO]: epoch 745: training loss 0.0174\n",
      "2025-06-01 14:52:11 [INFO]: epoch 746: training loss 0.0181\n",
      "2025-06-01 14:52:11 [INFO]: epoch 747: training loss 0.0172\n",
      "2025-06-01 14:52:11 [INFO]: epoch 748: training loss 0.0143\n",
      "2025-06-01 14:52:11 [INFO]: epoch 749: training loss 0.0167\n",
      "2025-06-01 14:52:11 [INFO]: epoch 750: training loss 0.0160\n",
      "2025-06-01 14:52:11 [INFO]: epoch 751: training loss 0.0201\n",
      "2025-06-01 14:52:11 [INFO]: epoch 752: training loss 0.0167\n",
      "2025-06-01 14:52:11 [INFO]: epoch 753: training loss 0.0146\n",
      "2025-06-01 14:52:11 [INFO]: epoch 754: training loss 0.0199\n",
      "2025-06-01 14:52:11 [INFO]: epoch 755: training loss 0.0183\n",
      "2025-06-01 14:52:11 [INFO]: epoch 756: training loss 0.0163\n",
      "2025-06-01 14:52:11 [INFO]: epoch 757: training loss 0.0188\n",
      "2025-06-01 14:52:11 [INFO]: epoch 758: training loss 0.0186\n",
      "2025-06-01 14:52:11 [INFO]: epoch 759: training loss 0.0157\n",
      "2025-06-01 14:52:11 [INFO]: epoch 760: training loss 0.0167\n",
      "2025-06-01 14:52:11 [INFO]: epoch 761: training loss 0.0176\n",
      "2025-06-01 14:52:11 [INFO]: epoch 762: training loss 0.0172\n",
      "2025-06-01 14:52:11 [INFO]: epoch 763: training loss 0.0179\n",
      "2025-06-01 14:52:11 [INFO]: epoch 764: training loss 0.0180\n",
      "2025-06-01 14:52:12 [INFO]: epoch 765: training loss 0.0191\n",
      "2025-06-01 14:52:12 [INFO]: epoch 766: training loss 0.0140\n",
      "2025-06-01 14:52:12 [INFO]: epoch 767: training loss 0.0159\n",
      "2025-06-01 14:52:12 [INFO]: epoch 768: training loss 0.0147\n",
      "2025-06-01 14:52:12 [INFO]: epoch 769: training loss 0.0195\n",
      "2025-06-01 14:52:12 [INFO]: epoch 770: training loss 0.0176\n",
      "2025-06-01 14:52:12 [INFO]: epoch 771: training loss 0.0149\n",
      "2025-06-01 14:52:12 [INFO]: epoch 772: training loss 0.0152\n",
      "2025-06-01 14:52:12 [INFO]: epoch 773: training loss 0.0170\n",
      "2025-06-01 14:52:12 [INFO]: epoch 774: training loss 0.0159\n",
      "2025-06-01 14:52:12 [INFO]: epoch 775: training loss 0.0179\n",
      "2025-06-01 14:52:12 [INFO]: epoch 776: training loss 0.0162\n",
      "2025-06-01 14:52:12 [INFO]: epoch 777: training loss 0.0180\n",
      "2025-06-01 14:52:12 [INFO]: epoch 778: training loss 0.0139\n",
      "2025-06-01 14:52:12 [INFO]: epoch 779: training loss 0.0148\n",
      "2025-06-01 14:52:12 [INFO]: epoch 780: training loss 0.0163\n",
      "2025-06-01 14:52:12 [INFO]: epoch 781: training loss 0.0152\n",
      "2025-06-01 14:52:12 [INFO]: epoch 782: training loss 0.0158\n",
      "2025-06-01 14:52:12 [INFO]: epoch 783: training loss 0.0154\n",
      "2025-06-01 14:52:12 [INFO]: epoch 784: training loss 0.0160\n",
      "2025-06-01 14:52:12 [INFO]: epoch 785: training loss 0.0176\n",
      "2025-06-01 14:52:12 [INFO]: epoch 786: training loss 0.0195\n",
      "2025-06-01 14:52:12 [INFO]: epoch 787: training loss 0.0204\n",
      "2025-06-01 14:52:12 [INFO]: epoch 788: training loss 0.0134\n",
      "2025-06-01 14:52:12 [INFO]: epoch 789: training loss 0.0237\n",
      "2025-06-01 14:52:12 [INFO]: epoch 790: training loss 0.0198\n",
      "2025-06-01 14:52:12 [INFO]: epoch 791: training loss 0.0187\n",
      "2025-06-01 14:52:12 [INFO]: epoch 792: training loss 0.0165\n",
      "2025-06-01 14:52:12 [INFO]: epoch 793: training loss 0.0169\n",
      "2025-06-01 14:52:12 [INFO]: epoch 794: training loss 0.0162\n",
      "2025-06-01 14:52:12 [INFO]: epoch 795: training loss 0.0171\n",
      "2025-06-01 14:52:12 [INFO]: epoch 796: training loss 0.0168\n",
      "2025-06-01 14:52:12 [INFO]: epoch 797: training loss 0.0156\n",
      "2025-06-01 14:52:12 [INFO]: epoch 798: training loss 0.0167\n",
      "2025-06-01 14:52:12 [INFO]: epoch 799: training loss 0.0172\n",
      "2025-06-01 14:52:12 [INFO]: Finished training.\n",
      "2025-06-01 14:52:12 [WARNING]: 🚨DeprecationWarning: The method impute is deprecated. Please use `predict` instead.\n",
      " 50%|██████████████████████████████████████████                                          | 3/6 [00:31<00:31, 10.60s/it]2025-06-01 14:52:12 [INFO]: No given device, using default device: cuda\n",
      "2025-06-01 14:52:12 [WARNING]: saving_path not given. Model files and tensorboard file will not be saved.\n",
      "2025-06-01 14:52:12 [INFO]: Model initialized successfully with the number of trainable parameters: 597,780\n",
      "2025-06-01 14:52:12 [INFO]: epoch 0: training loss 1.1571\n",
      "2025-06-01 14:52:12 [INFO]: epoch 1: training loss 0.6515\n",
      "2025-06-01 14:52:12 [INFO]: epoch 2: training loss 0.5894\n",
      "2025-06-01 14:52:12 [INFO]: epoch 3: training loss 0.6698\n",
      "2025-06-01 14:52:12 [INFO]: epoch 4: training loss 0.5746\n",
      "2025-06-01 14:52:12 [INFO]: epoch 5: training loss 0.5096\n",
      "2025-06-01 14:52:12 [INFO]: epoch 6: training loss 0.5061\n",
      "2025-06-01 14:52:12 [INFO]: epoch 7: training loss 0.5154\n",
      "2025-06-01 14:52:12 [INFO]: epoch 8: training loss 0.5095\n",
      "2025-06-01 14:52:12 [INFO]: epoch 9: training loss 0.5175\n",
      "2025-06-01 14:52:12 [INFO]: epoch 10: training loss 0.4516\n",
      "2025-06-01 14:52:12 [INFO]: epoch 11: training loss 0.4354\n",
      "2025-06-01 14:52:12 [INFO]: epoch 12: training loss 0.4621\n",
      "2025-06-01 14:52:12 [INFO]: epoch 13: training loss 0.4193\n",
      "2025-06-01 14:52:12 [INFO]: epoch 14: training loss 0.4214\n",
      "2025-06-01 14:52:12 [INFO]: epoch 15: training loss 0.4361\n",
      "2025-06-01 14:52:12 [INFO]: epoch 16: training loss 0.4558\n",
      "2025-06-01 14:52:12 [INFO]: epoch 17: training loss 0.4391\n",
      "2025-06-01 14:52:12 [INFO]: epoch 18: training loss 0.3995\n",
      "2025-06-01 14:52:12 [INFO]: epoch 19: training loss 0.4144\n",
      "2025-06-01 14:52:12 [INFO]: epoch 20: training loss 0.3865\n",
      "2025-06-01 14:52:12 [INFO]: epoch 21: training loss 0.4000\n",
      "2025-06-01 14:52:12 [INFO]: epoch 22: training loss 0.3869\n",
      "2025-06-01 14:52:12 [INFO]: epoch 23: training loss 0.4110\n",
      "2025-06-01 14:52:12 [INFO]: epoch 24: training loss 0.4051\n",
      "2025-06-01 14:52:12 [INFO]: epoch 25: training loss 0.3921\n",
      "2025-06-01 14:52:12 [INFO]: epoch 26: training loss 0.3614\n",
      "2025-06-01 14:52:12 [INFO]: epoch 27: training loss 0.3909\n",
      "2025-06-01 14:52:12 [INFO]: epoch 28: training loss 0.3679\n",
      "2025-06-01 14:52:12 [INFO]: epoch 29: training loss 0.3699\n",
      "2025-06-01 14:52:12 [INFO]: epoch 30: training loss 0.3723\n",
      "2025-06-01 14:52:12 [INFO]: epoch 31: training loss 0.3797\n",
      "2025-06-01 14:52:12 [INFO]: epoch 32: training loss 0.3648\n",
      "2025-06-01 14:52:12 [INFO]: epoch 33: training loss 0.3525\n",
      "2025-06-01 14:52:13 [INFO]: epoch 34: training loss 0.3620\n",
      "2025-06-01 14:52:13 [INFO]: epoch 35: training loss 0.3589\n",
      "2025-06-01 14:52:13 [INFO]: epoch 36: training loss 0.3659\n",
      "2025-06-01 14:52:13 [INFO]: epoch 37: training loss 0.3771\n",
      "2025-06-01 14:52:13 [INFO]: epoch 38: training loss 0.3659\n",
      "2025-06-01 14:52:13 [INFO]: epoch 39: training loss 0.3556\n",
      "2025-06-01 14:52:13 [INFO]: epoch 40: training loss 0.3819\n",
      "2025-06-01 14:52:13 [INFO]: epoch 41: training loss 0.3713\n",
      "2025-06-01 14:52:13 [INFO]: epoch 42: training loss 0.3717\n",
      "2025-06-01 14:52:13 [INFO]: epoch 43: training loss 0.3616\n",
      "2025-06-01 14:52:13 [INFO]: epoch 44: training loss 0.3855\n",
      "2025-06-01 14:52:13 [INFO]: epoch 45: training loss 0.3389\n",
      "2025-06-01 14:52:13 [INFO]: epoch 46: training loss 0.3681\n",
      "2025-06-01 14:52:13 [INFO]: epoch 47: training loss 0.3568\n",
      "2025-06-01 14:52:13 [INFO]: epoch 48: training loss 0.3694\n",
      "2025-06-01 14:52:13 [INFO]: epoch 49: training loss 0.3357\n",
      "2025-06-01 14:52:13 [INFO]: epoch 50: training loss 0.3500\n",
      "2025-06-01 14:52:13 [INFO]: epoch 51: training loss 0.3695\n",
      "2025-06-01 14:52:13 [INFO]: epoch 52: training loss 0.3740\n",
      "2025-06-01 14:52:13 [INFO]: epoch 53: training loss 0.3496\n",
      "2025-06-01 14:52:13 [INFO]: epoch 54: training loss 0.3555\n",
      "2025-06-01 14:52:13 [INFO]: epoch 55: training loss 0.3290\n",
      "2025-06-01 14:52:13 [INFO]: epoch 56: training loss 0.3432\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-01 14:52:13 [INFO]: epoch 57: training loss 0.3454\n",
      "2025-06-01 14:52:13 [INFO]: epoch 58: training loss 0.3303\n",
      "2025-06-01 14:52:13 [INFO]: epoch 59: training loss 0.3378\n",
      "2025-06-01 14:52:13 [INFO]: epoch 60: training loss 0.3259\n",
      "2025-06-01 14:52:13 [INFO]: epoch 61: training loss 0.3546\n",
      "2025-06-01 14:52:13 [INFO]: epoch 62: training loss 0.3008\n",
      "2025-06-01 14:52:13 [INFO]: epoch 63: training loss 0.3213\n",
      "2025-06-01 14:52:13 [INFO]: epoch 64: training loss 0.3340\n",
      "2025-06-01 14:52:13 [INFO]: epoch 65: training loss 0.3429\n",
      "2025-06-01 14:52:13 [INFO]: epoch 66: training loss 0.3253\n",
      "2025-06-01 14:52:13 [INFO]: epoch 67: training loss 0.3285\n",
      "2025-06-01 14:52:13 [INFO]: epoch 68: training loss 0.3257\n",
      "2025-06-01 14:52:13 [INFO]: epoch 69: training loss 0.3279\n",
      "2025-06-01 14:52:13 [INFO]: epoch 70: training loss 0.2928\n",
      "2025-06-01 14:52:13 [INFO]: epoch 71: training loss 0.3330\n",
      "2025-06-01 14:52:13 [INFO]: epoch 72: training loss 0.3234\n",
      "2025-06-01 14:52:13 [INFO]: epoch 73: training loss 0.3282\n",
      "2025-06-01 14:52:13 [INFO]: epoch 74: training loss 0.3069\n",
      "2025-06-01 14:52:13 [INFO]: epoch 75: training loss 0.3171\n",
      "2025-06-01 14:52:13 [INFO]: epoch 76: training loss 0.3024\n",
      "2025-06-01 14:52:13 [INFO]: epoch 77: training loss 0.2897\n",
      "2025-06-01 14:52:13 [INFO]: epoch 78: training loss 0.3092\n",
      "2025-06-01 14:52:13 [INFO]: epoch 79: training loss 0.3153\n",
      "2025-06-01 14:52:13 [INFO]: epoch 80: training loss 0.2822\n",
      "2025-06-01 14:52:13 [INFO]: epoch 81: training loss 0.2684\n",
      "2025-06-01 14:52:13 [INFO]: epoch 82: training loss 0.2998\n",
      "2025-06-01 14:52:13 [INFO]: epoch 83: training loss 0.2923\n",
      "2025-06-01 14:52:13 [INFO]: epoch 84: training loss 0.2770\n",
      "2025-06-01 14:52:13 [INFO]: epoch 85: training loss 0.2851\n",
      "2025-06-01 14:52:13 [INFO]: epoch 86: training loss 0.3120\n",
      "2025-06-01 14:52:13 [INFO]: epoch 87: training loss 0.2887\n",
      "2025-06-01 14:52:13 [INFO]: epoch 88: training loss 0.2946\n",
      "2025-06-01 14:52:13 [INFO]: epoch 89: training loss 0.2725\n",
      "2025-06-01 14:52:13 [INFO]: epoch 90: training loss 0.2986\n",
      "2025-06-01 14:52:13 [INFO]: epoch 91: training loss 0.2783\n",
      "2025-06-01 14:52:13 [INFO]: epoch 92: training loss 0.2894\n",
      "2025-06-01 14:52:13 [INFO]: epoch 93: training loss 0.2820\n",
      "2025-06-01 14:52:13 [INFO]: epoch 94: training loss 0.2646\n",
      "2025-06-01 14:52:13 [INFO]: epoch 95: training loss 0.2802\n",
      "2025-06-01 14:52:13 [INFO]: epoch 96: training loss 0.2877\n",
      "2025-06-01 14:52:13 [INFO]: epoch 97: training loss 0.2716\n",
      "2025-06-01 14:52:13 [INFO]: epoch 98: training loss 0.2820\n",
      "2025-06-01 14:52:13 [INFO]: epoch 99: training loss 0.2825\n",
      "2025-06-01 14:52:13 [INFO]: epoch 100: training loss 0.2902\n",
      "2025-06-01 14:52:13 [INFO]: epoch 101: training loss 0.2795\n",
      "2025-06-01 14:52:13 [INFO]: epoch 102: training loss 0.2741\n",
      "2025-06-01 14:52:13 [INFO]: epoch 103: training loss 0.2810\n",
      "2025-06-01 14:52:13 [INFO]: epoch 104: training loss 0.2800\n",
      "2025-06-01 14:52:13 [INFO]: epoch 105: training loss 0.2796\n",
      "2025-06-01 14:52:13 [INFO]: epoch 106: training loss 0.2691\n",
      "2025-06-01 14:52:13 [INFO]: epoch 107: training loss 0.2851\n",
      "2025-06-01 14:52:13 [INFO]: epoch 108: training loss 0.2662\n",
      "2025-06-01 14:52:13 [INFO]: epoch 109: training loss 0.2961\n",
      "2025-06-01 14:52:14 [INFO]: epoch 110: training loss 0.2517\n",
      "2025-06-01 14:52:14 [INFO]: epoch 111: training loss 0.2793\n",
      "2025-06-01 14:52:14 [INFO]: epoch 112: training loss 0.2681\n",
      "2025-06-01 14:52:14 [INFO]: epoch 113: training loss 0.2719\n",
      "2025-06-01 14:52:14 [INFO]: epoch 114: training loss 0.2586\n",
      "2025-06-01 14:52:14 [INFO]: epoch 115: training loss 0.2676\n",
      "2025-06-01 14:52:14 [INFO]: epoch 116: training loss 0.2619\n",
      "2025-06-01 14:52:14 [INFO]: epoch 117: training loss 0.2824\n",
      "2025-06-01 14:52:14 [INFO]: epoch 118: training loss 0.2666\n",
      "2025-06-01 14:52:14 [INFO]: epoch 119: training loss 0.2689\n",
      "2025-06-01 14:52:14 [INFO]: epoch 120: training loss 0.2778\n",
      "2025-06-01 14:52:14 [INFO]: epoch 121: training loss 0.2672\n",
      "2025-06-01 14:52:14 [INFO]: epoch 122: training loss 0.2902\n",
      "2025-06-01 14:52:14 [INFO]: epoch 123: training loss 0.2747\n",
      "2025-06-01 14:52:14 [INFO]: epoch 124: training loss 0.2820\n",
      "2025-06-01 14:52:14 [INFO]: epoch 125: training loss 0.2718\n",
      "2025-06-01 14:52:14 [INFO]: epoch 126: training loss 0.2724\n",
      "2025-06-01 14:52:14 [INFO]: epoch 127: training loss 0.2764\n",
      "2025-06-01 14:52:14 [INFO]: epoch 128: training loss 0.2508\n",
      "2025-06-01 14:52:14 [INFO]: epoch 129: training loss 0.2891\n",
      "2025-06-01 14:52:14 [INFO]: epoch 130: training loss 0.2630\n",
      "2025-06-01 14:52:14 [INFO]: epoch 131: training loss 0.2692\n",
      "2025-06-01 14:52:14 [INFO]: epoch 132: training loss 0.2705\n",
      "2025-06-01 14:52:14 [INFO]: epoch 133: training loss 0.2775\n",
      "2025-06-01 14:52:14 [INFO]: epoch 134: training loss 0.2601\n",
      "2025-06-01 14:52:14 [INFO]: epoch 135: training loss 0.2749\n",
      "2025-06-01 14:52:14 [INFO]: epoch 136: training loss 0.2787\n",
      "2025-06-01 14:52:14 [INFO]: epoch 137: training loss 0.2821\n",
      "2025-06-01 14:52:14 [INFO]: epoch 138: training loss 0.2595\n",
      "2025-06-01 14:52:14 [INFO]: epoch 139: training loss 0.2480\n",
      "2025-06-01 14:52:14 [INFO]: epoch 140: training loss 0.2607\n",
      "2025-06-01 14:52:14 [INFO]: epoch 141: training loss 0.2841\n",
      "2025-06-01 14:52:14 [INFO]: epoch 142: training loss 0.2982\n",
      "2025-06-01 14:52:14 [INFO]: epoch 143: training loss 0.2841\n",
      "2025-06-01 14:52:14 [INFO]: epoch 144: training loss 0.2560\n",
      "2025-06-01 14:52:14 [INFO]: epoch 145: training loss 0.2548\n",
      "2025-06-01 14:52:14 [INFO]: epoch 146: training loss 0.2545\n",
      "2025-06-01 14:52:14 [INFO]: epoch 147: training loss 0.2608\n",
      "2025-06-01 14:52:14 [INFO]: epoch 148: training loss 0.2641\n",
      "2025-06-01 14:52:14 [INFO]: epoch 149: training loss 0.2637\n",
      "2025-06-01 14:52:14 [INFO]: epoch 150: training loss 0.2382\n",
      "2025-06-01 14:52:14 [INFO]: epoch 151: training loss 0.2448\n",
      "2025-06-01 14:52:14 [INFO]: epoch 152: training loss 0.2375\n",
      "2025-06-01 14:52:14 [INFO]: epoch 153: training loss 0.2508\n",
      "2025-06-01 14:52:14 [INFO]: epoch 154: training loss 0.2580\n",
      "2025-06-01 14:52:14 [INFO]: epoch 155: training loss 0.2583\n",
      "2025-06-01 14:52:14 [INFO]: epoch 156: training loss 0.2462\n",
      "2025-06-01 14:52:14 [INFO]: epoch 157: training loss 0.2270\n",
      "2025-06-01 14:52:14 [INFO]: epoch 158: training loss 0.2423\n",
      "2025-06-01 14:52:14 [INFO]: epoch 159: training loss 0.2326\n",
      "2025-06-01 14:52:14 [INFO]: epoch 160: training loss 0.2268\n",
      "2025-06-01 14:52:14 [INFO]: epoch 161: training loss 0.2274\n",
      "2025-06-01 14:52:14 [INFO]: epoch 162: training loss 0.2136\n",
      "2025-06-01 14:52:14 [INFO]: epoch 163: training loss 0.2236\n",
      "2025-06-01 14:52:14 [INFO]: epoch 164: training loss 0.2370\n",
      "2025-06-01 14:52:14 [INFO]: epoch 165: training loss 0.2324\n",
      "2025-06-01 14:52:14 [INFO]: epoch 166: training loss 0.2334\n",
      "2025-06-01 14:52:14 [INFO]: epoch 167: training loss 0.2214\n",
      "2025-06-01 14:52:14 [INFO]: epoch 168: training loss 0.2309\n",
      "2025-06-01 14:52:14 [INFO]: epoch 169: training loss 0.2315\n",
      "2025-06-01 14:52:14 [INFO]: epoch 170: training loss 0.2316\n",
      "2025-06-01 14:52:14 [INFO]: epoch 171: training loss 0.2145\n",
      "2025-06-01 14:52:14 [INFO]: epoch 172: training loss 0.2405\n",
      "2025-06-01 14:52:14 [INFO]: epoch 173: training loss 0.2268\n",
      "2025-06-01 14:52:14 [INFO]: epoch 174: training loss 0.2188\n",
      "2025-06-01 14:52:14 [INFO]: epoch 175: training loss 0.2243\n",
      "2025-06-01 14:52:14 [INFO]: epoch 176: training loss 0.2260\n",
      "2025-06-01 14:52:14 [INFO]: epoch 177: training loss 0.2407\n",
      "2025-06-01 14:52:14 [INFO]: epoch 178: training loss 0.2182\n",
      "2025-06-01 14:52:14 [INFO]: epoch 179: training loss 0.2178\n",
      "2025-06-01 14:52:14 [INFO]: epoch 180: training loss 0.2289\n",
      "2025-06-01 14:52:14 [INFO]: epoch 181: training loss 0.2321\n",
      "2025-06-01 14:52:14 [INFO]: epoch 182: training loss 0.2194\n",
      "2025-06-01 14:52:14 [INFO]: epoch 183: training loss 0.2223\n",
      "2025-06-01 14:52:14 [INFO]: epoch 184: training loss 0.2121\n",
      "2025-06-01 14:52:14 [INFO]: epoch 185: training loss 0.2131\n",
      "2025-06-01 14:52:15 [INFO]: epoch 186: training loss 0.2190\n",
      "2025-06-01 14:52:15 [INFO]: epoch 187: training loss 0.2127\n",
      "2025-06-01 14:52:15 [INFO]: epoch 188: training loss 0.2120\n",
      "2025-06-01 14:52:15 [INFO]: epoch 189: training loss 0.2167\n",
      "2025-06-01 14:52:15 [INFO]: epoch 190: training loss 0.2204\n",
      "2025-06-01 14:52:15 [INFO]: epoch 191: training loss 0.2095\n",
      "2025-06-01 14:52:15 [INFO]: epoch 192: training loss 0.2079\n",
      "2025-06-01 14:52:15 [INFO]: epoch 193: training loss 0.2168\n",
      "2025-06-01 14:52:15 [INFO]: epoch 194: training loss 0.2163\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-01 14:52:15 [INFO]: epoch 195: training loss 0.2270\n",
      "2025-06-01 14:52:15 [INFO]: epoch 196: training loss 0.2188\n",
      "2025-06-01 14:52:15 [INFO]: epoch 197: training loss 0.2031\n",
      "2025-06-01 14:52:15 [INFO]: epoch 198: training loss 0.2129\n",
      "2025-06-01 14:52:15 [INFO]: epoch 199: training loss 0.2202\n",
      "2025-06-01 14:52:15 [INFO]: epoch 200: training loss 0.1979\n",
      "2025-06-01 14:52:15 [INFO]: epoch 201: training loss 0.2220\n",
      "2025-06-01 14:52:15 [INFO]: epoch 202: training loss 0.2450\n",
      "2025-06-01 14:52:15 [INFO]: epoch 203: training loss 0.2207\n",
      "2025-06-01 14:52:15 [INFO]: epoch 204: training loss 0.2216\n",
      "2025-06-01 14:52:15 [INFO]: epoch 205: training loss 0.2029\n",
      "2025-06-01 14:52:15 [INFO]: epoch 206: training loss 0.2101\n",
      "2025-06-01 14:52:15 [INFO]: epoch 207: training loss 0.2116\n",
      "2025-06-01 14:52:15 [INFO]: epoch 208: training loss 0.2092\n",
      "2025-06-01 14:52:15 [INFO]: epoch 209: training loss 0.2004\n",
      "2025-06-01 14:52:15 [INFO]: epoch 210: training loss 0.1922\n",
      "2025-06-01 14:52:15 [INFO]: epoch 211: training loss 0.2075\n",
      "2025-06-01 14:52:15 [INFO]: epoch 212: training loss 0.1982\n",
      "2025-06-01 14:52:15 [INFO]: epoch 213: training loss 0.2024\n",
      "2025-06-01 14:52:15 [INFO]: epoch 214: training loss 0.2010\n",
      "2025-06-01 14:52:15 [INFO]: epoch 215: training loss 0.2110\n",
      "2025-06-01 14:52:15 [INFO]: epoch 216: training loss 0.2007\n",
      "2025-06-01 14:52:15 [INFO]: epoch 217: training loss 0.2056\n",
      "2025-06-01 14:52:15 [INFO]: epoch 218: training loss 0.2105\n",
      "2025-06-01 14:52:15 [INFO]: epoch 219: training loss 0.1886\n",
      "2025-06-01 14:52:15 [INFO]: epoch 220: training loss 0.2061\n",
      "2025-06-01 14:52:15 [INFO]: epoch 221: training loss 0.2111\n",
      "2025-06-01 14:52:15 [INFO]: epoch 222: training loss 0.1935\n",
      "2025-06-01 14:52:15 [INFO]: epoch 223: training loss 0.1821\n",
      "2025-06-01 14:52:15 [INFO]: epoch 224: training loss 0.2010\n",
      "2025-06-01 14:52:15 [INFO]: epoch 225: training loss 0.1978\n",
      "2025-06-01 14:52:15 [INFO]: epoch 226: training loss 0.2012\n",
      "2025-06-01 14:52:15 [INFO]: epoch 227: training loss 0.1917\n",
      "2025-06-01 14:52:15 [INFO]: epoch 228: training loss 0.1932\n",
      "2025-06-01 14:52:15 [INFO]: epoch 229: training loss 0.1987\n",
      "2025-06-01 14:52:15 [INFO]: epoch 230: training loss 0.1907\n",
      "2025-06-01 14:52:15 [INFO]: epoch 231: training loss 0.2072\n",
      "2025-06-01 14:52:15 [INFO]: epoch 232: training loss 0.2148\n",
      "2025-06-01 14:52:15 [INFO]: epoch 233: training loss 0.1925\n",
      "2025-06-01 14:52:15 [INFO]: epoch 234: training loss 0.1910\n",
      "2025-06-01 14:52:15 [INFO]: epoch 235: training loss 0.1845\n",
      "2025-06-01 14:52:15 [INFO]: epoch 236: training loss 0.1898\n",
      "2025-06-01 14:52:15 [INFO]: epoch 237: training loss 0.1849\n",
      "2025-06-01 14:52:15 [INFO]: epoch 238: training loss 0.1855\n",
      "2025-06-01 14:52:15 [INFO]: epoch 239: training loss 0.2001\n",
      "2025-06-01 14:52:15 [INFO]: epoch 240: training loss 0.2124\n",
      "2025-06-01 14:52:15 [INFO]: epoch 241: training loss 0.1949\n",
      "2025-06-01 14:52:15 [INFO]: epoch 242: training loss 0.1974\n",
      "2025-06-01 14:52:15 [INFO]: epoch 243: training loss 0.1972\n",
      "2025-06-01 14:52:15 [INFO]: epoch 244: training loss 0.1967\n",
      "2025-06-01 14:52:15 [INFO]: epoch 245: training loss 0.1991\n",
      "2025-06-01 14:52:15 [INFO]: epoch 246: training loss 0.1943\n",
      "2025-06-01 14:52:15 [INFO]: epoch 247: training loss 0.1897\n",
      "2025-06-01 14:52:15 [INFO]: epoch 248: training loss 0.1837\n",
      "2025-06-01 14:52:15 [INFO]: epoch 249: training loss 0.2092\n",
      "2025-06-01 14:52:15 [INFO]: epoch 250: training loss 0.2140\n",
      "2025-06-01 14:52:15 [INFO]: epoch 251: training loss 0.1961\n",
      "2025-06-01 14:52:15 [INFO]: epoch 252: training loss 0.1820\n",
      "2025-06-01 14:52:15 [INFO]: epoch 253: training loss 0.1942\n",
      "2025-06-01 14:52:15 [INFO]: epoch 254: training loss 0.2027\n",
      "2025-06-01 14:52:15 [INFO]: epoch 255: training loss 0.1920\n",
      "2025-06-01 14:52:15 [INFO]: epoch 256: training loss 0.1707\n",
      "2025-06-01 14:52:15 [INFO]: epoch 257: training loss 0.1768\n",
      "2025-06-01 14:52:15 [INFO]: epoch 258: training loss 0.1843\n",
      "2025-06-01 14:52:15 [INFO]: epoch 259: training loss 0.1742\n",
      "2025-06-01 14:52:15 [INFO]: epoch 260: training loss 0.1805\n",
      "2025-06-01 14:52:16 [INFO]: epoch 261: training loss 0.1719\n",
      "2025-06-01 14:52:16 [INFO]: epoch 262: training loss 0.1717\n",
      "2025-06-01 14:52:16 [INFO]: epoch 263: training loss 0.1773\n",
      "2025-06-01 14:52:16 [INFO]: epoch 264: training loss 0.1766\n",
      "2025-06-01 14:52:16 [INFO]: epoch 265: training loss 0.1749\n",
      "2025-06-01 14:52:16 [INFO]: epoch 266: training loss 0.1719\n",
      "2025-06-01 14:52:16 [INFO]: epoch 267: training loss 0.1799\n",
      "2025-06-01 14:52:16 [INFO]: epoch 268: training loss 0.1816\n",
      "2025-06-01 14:52:16 [INFO]: epoch 269: training loss 0.1699\n",
      "2025-06-01 14:52:16 [INFO]: epoch 270: training loss 0.1822\n",
      "2025-06-01 14:52:16 [INFO]: epoch 271: training loss 0.1867\n",
      "2025-06-01 14:52:16 [INFO]: epoch 272: training loss 0.1739\n",
      "2025-06-01 14:52:16 [INFO]: epoch 273: training loss 0.1746\n",
      "2025-06-01 14:52:16 [INFO]: epoch 274: training loss 0.1684\n",
      "2025-06-01 14:52:16 [INFO]: epoch 275: training loss 0.1780\n",
      "2025-06-01 14:52:16 [INFO]: epoch 276: training loss 0.1768\n",
      "2025-06-01 14:52:16 [INFO]: epoch 277: training loss 0.1779\n",
      "2025-06-01 14:52:16 [INFO]: epoch 278: training loss 0.1667\n",
      "2025-06-01 14:52:16 [INFO]: epoch 279: training loss 0.1646\n",
      "2025-06-01 14:52:16 [INFO]: epoch 280: training loss 0.1795\n",
      "2025-06-01 14:52:16 [INFO]: epoch 281: training loss 0.1726\n",
      "2025-06-01 14:52:16 [INFO]: epoch 282: training loss 0.1630\n",
      "2025-06-01 14:52:16 [INFO]: epoch 283: training loss 0.1665\n",
      "2025-06-01 14:52:16 [INFO]: epoch 284: training loss 0.1711\n",
      "2025-06-01 14:52:16 [INFO]: epoch 285: training loss 0.1663\n",
      "2025-06-01 14:52:16 [INFO]: epoch 286: training loss 0.1691\n",
      "2025-06-01 14:52:16 [INFO]: epoch 287: training loss 0.1758\n",
      "2025-06-01 14:52:16 [INFO]: epoch 288: training loss 0.1799\n",
      "2025-06-01 14:52:16 [INFO]: epoch 289: training loss 0.1632\n",
      "2025-06-01 14:52:16 [INFO]: epoch 290: training loss 0.1606\n",
      "2025-06-01 14:52:16 [INFO]: epoch 291: training loss 0.1632\n",
      "2025-06-01 14:52:16 [INFO]: epoch 292: training loss 0.1739\n",
      "2025-06-01 14:52:16 [INFO]: epoch 293: training loss 0.1697\n",
      "2025-06-01 14:52:16 [INFO]: epoch 294: training loss 0.1649\n",
      "2025-06-01 14:52:16 [INFO]: epoch 295: training loss 0.1599\n",
      "2025-06-01 14:52:16 [INFO]: epoch 296: training loss 0.1610\n",
      "2025-06-01 14:52:16 [INFO]: epoch 297: training loss 0.1585\n",
      "2025-06-01 14:52:16 [INFO]: epoch 298: training loss 0.1659\n",
      "2025-06-01 14:52:16 [INFO]: epoch 299: training loss 0.1571\n",
      "2025-06-01 14:52:16 [INFO]: epoch 300: training loss 0.1633\n",
      "2025-06-01 14:52:16 [INFO]: epoch 301: training loss 0.1667\n",
      "2025-06-01 14:52:16 [INFO]: epoch 302: training loss 0.1580\n",
      "2025-06-01 14:52:16 [INFO]: epoch 303: training loss 0.1608\n",
      "2025-06-01 14:52:16 [INFO]: epoch 304: training loss 0.1632\n",
      "2025-06-01 14:52:16 [INFO]: epoch 305: training loss 0.1455\n",
      "2025-06-01 14:52:16 [INFO]: epoch 306: training loss 0.1602\n",
      "2025-06-01 14:52:16 [INFO]: epoch 307: training loss 0.1545\n",
      "2025-06-01 14:52:16 [INFO]: epoch 308: training loss 0.1510\n",
      "2025-06-01 14:52:16 [INFO]: epoch 309: training loss 0.1544\n",
      "2025-06-01 14:52:16 [INFO]: epoch 310: training loss 0.1546\n",
      "2025-06-01 14:52:16 [INFO]: epoch 311: training loss 0.1675\n",
      "2025-06-01 14:52:16 [INFO]: epoch 312: training loss 0.1476\n",
      "2025-06-01 14:52:16 [INFO]: epoch 313: training loss 0.1505\n",
      "2025-06-01 14:52:16 [INFO]: epoch 314: training loss 0.1470\n",
      "2025-06-01 14:52:16 [INFO]: epoch 315: training loss 0.1518\n",
      "2025-06-01 14:52:16 [INFO]: epoch 316: training loss 0.1492\n",
      "2025-06-01 14:52:16 [INFO]: epoch 317: training loss 0.1465\n",
      "2025-06-01 14:52:16 [INFO]: epoch 318: training loss 0.1460\n",
      "2025-06-01 14:52:16 [INFO]: epoch 319: training loss 0.1414\n",
      "2025-06-01 14:52:16 [INFO]: epoch 320: training loss 0.1488\n",
      "2025-06-01 14:52:16 [INFO]: epoch 321: training loss 0.1512\n",
      "2025-06-01 14:52:16 [INFO]: epoch 322: training loss 0.1543\n",
      "2025-06-01 14:52:16 [INFO]: epoch 323: training loss 0.1337\n",
      "2025-06-01 14:52:16 [INFO]: epoch 324: training loss 0.1499\n",
      "2025-06-01 14:52:16 [INFO]: epoch 325: training loss 0.1476\n",
      "2025-06-01 14:52:16 [INFO]: epoch 326: training loss 0.1445\n",
      "2025-06-01 14:52:16 [INFO]: epoch 327: training loss 0.1400\n",
      "2025-06-01 14:52:16 [INFO]: epoch 328: training loss 0.1675\n",
      "2025-06-01 14:52:16 [INFO]: epoch 329: training loss 0.1491\n",
      "2025-06-01 14:52:16 [INFO]: epoch 330: training loss 0.1479\n",
      "2025-06-01 14:52:16 [INFO]: epoch 331: training loss 0.1395\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-01 14:52:16 [INFO]: epoch 332: training loss 0.1562\n",
      "2025-06-01 14:52:16 [INFO]: epoch 333: training loss 0.1374\n",
      "2025-06-01 14:52:16 [INFO]: epoch 334: training loss 0.1456\n",
      "2025-06-01 14:52:16 [INFO]: epoch 335: training loss 0.1354\n",
      "2025-06-01 14:52:16 [INFO]: epoch 336: training loss 0.1404\n",
      "2025-06-01 14:52:17 [INFO]: epoch 337: training loss 0.1459\n",
      "2025-06-01 14:52:17 [INFO]: epoch 338: training loss 0.1469\n",
      "2025-06-01 14:52:17 [INFO]: epoch 339: training loss 0.1546\n",
      "2025-06-01 14:52:17 [INFO]: epoch 340: training loss 0.1441\n",
      "2025-06-01 14:52:17 [INFO]: epoch 341: training loss 0.1498\n",
      "2025-06-01 14:52:17 [INFO]: epoch 342: training loss 0.1491\n",
      "2025-06-01 14:52:17 [INFO]: epoch 343: training loss 0.1443\n",
      "2025-06-01 14:52:17 [INFO]: epoch 344: training loss 0.1465\n",
      "2025-06-01 14:52:17 [INFO]: epoch 345: training loss 0.1437\n",
      "2025-06-01 14:52:17 [INFO]: epoch 346: training loss 0.1419\n",
      "2025-06-01 14:52:17 [INFO]: epoch 347: training loss 0.1510\n",
      "2025-06-01 14:52:17 [INFO]: epoch 348: training loss 0.1376\n",
      "2025-06-01 14:52:17 [INFO]: epoch 349: training loss 0.1453\n",
      "2025-06-01 14:52:17 [INFO]: epoch 350: training loss 0.1436\n",
      "2025-06-01 14:52:17 [INFO]: epoch 351: training loss 0.1441\n",
      "2025-06-01 14:52:17 [INFO]: epoch 352: training loss 0.1370\n",
      "2025-06-01 14:52:17 [INFO]: epoch 353: training loss 0.1428\n",
      "2025-06-01 14:52:17 [INFO]: epoch 354: training loss 0.1425\n",
      "2025-06-01 14:52:17 [INFO]: epoch 355: training loss 0.1482\n",
      "2025-06-01 14:52:17 [INFO]: epoch 356: training loss 0.1331\n",
      "2025-06-01 14:52:17 [INFO]: epoch 357: training loss 0.1476\n",
      "2025-06-01 14:52:17 [INFO]: epoch 358: training loss 0.1409\n",
      "2025-06-01 14:52:17 [INFO]: epoch 359: training loss 0.1305\n",
      "2025-06-01 14:52:17 [INFO]: epoch 360: training loss 0.1369\n",
      "2025-06-01 14:52:17 [INFO]: epoch 361: training loss 0.1498\n",
      "2025-06-01 14:52:17 [INFO]: epoch 362: training loss 0.1531\n",
      "2025-06-01 14:52:17 [INFO]: epoch 363: training loss 0.1526\n",
      "2025-06-01 14:52:17 [INFO]: epoch 364: training loss 0.1464\n",
      "2025-06-01 14:52:17 [INFO]: epoch 365: training loss 0.1315\n",
      "2025-06-01 14:52:17 [INFO]: epoch 366: training loss 0.1464\n",
      "2025-06-01 14:52:17 [INFO]: epoch 367: training loss 0.1344\n",
      "2025-06-01 14:52:17 [INFO]: epoch 368: training loss 0.1382\n",
      "2025-06-01 14:52:17 [INFO]: epoch 369: training loss 0.1439\n",
      "2025-06-01 14:52:17 [INFO]: epoch 370: training loss 0.1413\n",
      "2025-06-01 14:52:17 [INFO]: epoch 371: training loss 0.1371\n",
      "2025-06-01 14:52:17 [INFO]: epoch 372: training loss 0.1393\n",
      "2025-06-01 14:52:17 [INFO]: epoch 373: training loss 0.1388\n",
      "2025-06-01 14:52:17 [INFO]: epoch 374: training loss 0.1376\n",
      "2025-06-01 14:52:17 [INFO]: epoch 375: training loss 0.1407\n",
      "2025-06-01 14:52:17 [INFO]: epoch 376: training loss 0.1535\n",
      "2025-06-01 14:52:17 [INFO]: epoch 377: training loss 0.1404\n",
      "2025-06-01 14:52:17 [INFO]: epoch 378: training loss 0.1366\n",
      "2025-06-01 14:52:17 [INFO]: epoch 379: training loss 0.1329\n",
      "2025-06-01 14:52:17 [INFO]: epoch 380: training loss 0.1426\n",
      "2025-06-01 14:52:17 [INFO]: epoch 381: training loss 0.1370\n",
      "2025-06-01 14:52:17 [INFO]: epoch 382: training loss 0.1356\n",
      "2025-06-01 14:52:17 [INFO]: epoch 383: training loss 0.1377\n",
      "2025-06-01 14:52:17 [INFO]: epoch 384: training loss 0.1384\n",
      "2025-06-01 14:52:17 [INFO]: epoch 385: training loss 0.1274\n",
      "2025-06-01 14:52:17 [INFO]: epoch 386: training loss 0.1267\n",
      "2025-06-01 14:52:17 [INFO]: epoch 387: training loss 0.1411\n",
      "2025-06-01 14:52:17 [INFO]: epoch 388: training loss 0.1366\n",
      "2025-06-01 14:52:17 [INFO]: epoch 389: training loss 0.1296\n",
      "2025-06-01 14:52:17 [INFO]: epoch 390: training loss 0.1268\n",
      "2025-06-01 14:52:17 [INFO]: epoch 391: training loss 0.1314\n",
      "2025-06-01 14:52:17 [INFO]: epoch 392: training loss 0.1316\n",
      "2025-06-01 14:52:17 [INFO]: epoch 393: training loss 0.1382\n",
      "2025-06-01 14:52:17 [INFO]: epoch 394: training loss 0.1177\n",
      "2025-06-01 14:52:17 [INFO]: epoch 395: training loss 0.1253\n",
      "2025-06-01 14:52:17 [INFO]: epoch 396: training loss 0.1238\n",
      "2025-06-01 14:52:17 [INFO]: epoch 397: training loss 0.1340\n",
      "2025-06-01 14:52:17 [INFO]: epoch 398: training loss 0.1306\n",
      "2025-06-01 14:52:17 [INFO]: epoch 399: training loss 0.1311\n",
      "2025-06-01 14:52:17 [INFO]: epoch 400: training loss 0.1257\n",
      "2025-06-01 14:52:17 [INFO]: epoch 401: training loss 0.1293\n",
      "2025-06-01 14:52:17 [INFO]: epoch 402: training loss 0.1253\n",
      "2025-06-01 14:52:17 [INFO]: epoch 403: training loss 0.1202\n",
      "2025-06-01 14:52:17 [INFO]: epoch 404: training loss 0.1315\n",
      "2025-06-01 14:52:17 [INFO]: epoch 405: training loss 0.1311\n",
      "2025-06-01 14:52:17 [INFO]: epoch 406: training loss 0.1170\n",
      "2025-06-01 14:52:17 [INFO]: epoch 407: training loss 0.1176\n",
      "2025-06-01 14:52:17 [INFO]: epoch 408: training loss 0.1169\n",
      "2025-06-01 14:52:17 [INFO]: epoch 409: training loss 0.1187\n",
      "2025-06-01 14:52:17 [INFO]: epoch 410: training loss 0.1270\n",
      "2025-06-01 14:52:17 [INFO]: epoch 411: training loss 0.1263\n",
      "2025-06-01 14:52:17 [INFO]: epoch 412: training loss 0.1266\n",
      "2025-06-01 14:52:18 [INFO]: epoch 413: training loss 0.1268\n",
      "2025-06-01 14:52:18 [INFO]: epoch 414: training loss 0.1112\n",
      "2025-06-01 14:52:18 [INFO]: epoch 415: training loss 0.1137\n",
      "2025-06-01 14:52:18 [INFO]: epoch 416: training loss 0.1187\n",
      "2025-06-01 14:52:18 [INFO]: epoch 417: training loss 0.1152\n",
      "2025-06-01 14:52:18 [INFO]: epoch 418: training loss 0.1226\n",
      "2025-06-01 14:52:18 [INFO]: epoch 419: training loss 0.1165\n",
      "2025-06-01 14:52:18 [INFO]: epoch 420: training loss 0.1197\n",
      "2025-06-01 14:52:18 [INFO]: epoch 421: training loss 0.1245\n",
      "2025-06-01 14:52:18 [INFO]: epoch 422: training loss 0.1168\n",
      "2025-06-01 14:52:18 [INFO]: epoch 423: training loss 0.1058\n",
      "2025-06-01 14:52:18 [INFO]: epoch 424: training loss 0.1140\n",
      "2025-06-01 14:52:18 [INFO]: epoch 425: training loss 0.1168\n",
      "2025-06-01 14:52:18 [INFO]: epoch 426: training loss 0.1191\n",
      "2025-06-01 14:52:18 [INFO]: epoch 427: training loss 0.1239\n",
      "2025-06-01 14:52:18 [INFO]: epoch 428: training loss 0.1126\n",
      "2025-06-01 14:52:18 [INFO]: epoch 429: training loss 0.1159\n",
      "2025-06-01 14:52:18 [INFO]: epoch 430: training loss 0.1145\n",
      "2025-06-01 14:52:18 [INFO]: epoch 431: training loss 0.1145\n",
      "2025-06-01 14:52:18 [INFO]: epoch 432: training loss 0.1173\n",
      "2025-06-01 14:52:18 [INFO]: epoch 433: training loss 0.1161\n",
      "2025-06-01 14:52:18 [INFO]: epoch 434: training loss 0.1164\n",
      "2025-06-01 14:52:18 [INFO]: epoch 435: training loss 0.1167\n",
      "2025-06-01 14:52:18 [INFO]: epoch 436: training loss 0.1146\n",
      "2025-06-01 14:52:18 [INFO]: epoch 437: training loss 0.1140\n",
      "2025-06-01 14:52:18 [INFO]: epoch 438: training loss 0.1079\n",
      "2025-06-01 14:52:18 [INFO]: epoch 439: training loss 0.1259\n",
      "2025-06-01 14:52:18 [INFO]: epoch 440: training loss 0.1131\n",
      "2025-06-01 14:52:18 [INFO]: epoch 441: training loss 0.1077\n",
      "2025-06-01 14:52:18 [INFO]: epoch 442: training loss 0.1123\n",
      "2025-06-01 14:52:18 [INFO]: epoch 443: training loss 0.1107\n",
      "2025-06-01 14:52:18 [INFO]: epoch 444: training loss 0.1155\n",
      "2025-06-01 14:52:18 [INFO]: epoch 445: training loss 0.1025\n",
      "2025-06-01 14:52:18 [INFO]: epoch 446: training loss 0.1132\n",
      "2025-06-01 14:52:18 [INFO]: epoch 447: training loss 0.1221\n",
      "2025-06-01 14:52:18 [INFO]: epoch 448: training loss 0.1194\n",
      "2025-06-01 14:52:18 [INFO]: epoch 449: training loss 0.1076\n",
      "2025-06-01 14:52:18 [INFO]: epoch 450: training loss 0.1054\n",
      "2025-06-01 14:52:18 [INFO]: epoch 451: training loss 0.1164\n",
      "2025-06-01 14:52:18 [INFO]: epoch 452: training loss 0.1081\n",
      "2025-06-01 14:52:18 [INFO]: epoch 453: training loss 0.1109\n",
      "2025-06-01 14:52:18 [INFO]: epoch 454: training loss 0.1183\n",
      "2025-06-01 14:52:18 [INFO]: epoch 455: training loss 0.1205\n",
      "2025-06-01 14:52:18 [INFO]: epoch 456: training loss 0.1117\n",
      "2025-06-01 14:52:18 [INFO]: epoch 457: training loss 0.1070\n",
      "2025-06-01 14:52:18 [INFO]: epoch 458: training loss 0.1074\n",
      "2025-06-01 14:52:18 [INFO]: epoch 459: training loss 0.1124\n",
      "2025-06-01 14:52:18 [INFO]: epoch 460: training loss 0.1092\n",
      "2025-06-01 14:52:18 [INFO]: epoch 461: training loss 0.1112\n",
      "2025-06-01 14:52:18 [INFO]: epoch 462: training loss 0.1125\n",
      "2025-06-01 14:52:18 [INFO]: epoch 463: training loss 0.1075\n",
      "2025-06-01 14:52:18 [INFO]: epoch 464: training loss 0.1093\n",
      "2025-06-01 14:52:18 [INFO]: epoch 465: training loss 0.0996\n",
      "2025-06-01 14:52:18 [INFO]: epoch 466: training loss 0.1039\n",
      "2025-06-01 14:52:18 [INFO]: epoch 467: training loss 0.0988\n",
      "2025-06-01 14:52:18 [INFO]: epoch 468: training loss 0.1022\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-01 14:52:18 [INFO]: epoch 469: training loss 0.1028\n",
      "2025-06-01 14:52:18 [INFO]: epoch 470: training loss 0.0951\n",
      "2025-06-01 14:52:18 [INFO]: epoch 471: training loss 0.1096\n",
      "2025-06-01 14:52:18 [INFO]: epoch 472: training loss 0.1138\n",
      "2025-06-01 14:52:18 [INFO]: epoch 473: training loss 0.1071\n",
      "2025-06-01 14:52:18 [INFO]: epoch 474: training loss 0.1128\n",
      "2025-06-01 14:52:18 [INFO]: epoch 475: training loss 0.1100\n",
      "2025-06-01 14:52:18 [INFO]: epoch 476: training loss 0.1058\n",
      "2025-06-01 14:52:18 [INFO]: epoch 477: training loss 0.1030\n",
      "2025-06-01 14:52:18 [INFO]: epoch 478: training loss 0.1052\n",
      "2025-06-01 14:52:18 [INFO]: epoch 479: training loss 0.1146\n",
      "2025-06-01 14:52:18 [INFO]: epoch 480: training loss 0.1089\n",
      "2025-06-01 14:52:18 [INFO]: epoch 481: training loss 0.0989\n",
      "2025-06-01 14:52:18 [INFO]: epoch 482: training loss 0.1023\n",
      "2025-06-01 14:52:18 [INFO]: epoch 483: training loss 0.0969\n",
      "2025-06-01 14:52:18 [INFO]: epoch 484: training loss 0.1112\n",
      "2025-06-01 14:52:18 [INFO]: epoch 485: training loss 0.1037\n",
      "2025-06-01 14:52:18 [INFO]: epoch 486: training loss 0.0955\n",
      "2025-06-01 14:52:18 [INFO]: epoch 487: training loss 0.1066\n",
      "2025-06-01 14:52:18 [INFO]: epoch 488: training loss 0.1093\n",
      "2025-06-01 14:52:19 [INFO]: epoch 489: training loss 0.1054\n",
      "2025-06-01 14:52:19 [INFO]: epoch 490: training loss 0.1066\n",
      "2025-06-01 14:52:19 [INFO]: epoch 491: training loss 0.1003\n",
      "2025-06-01 14:52:19 [INFO]: epoch 492: training loss 0.1017\n",
      "2025-06-01 14:52:19 [INFO]: epoch 493: training loss 0.1036\n",
      "2025-06-01 14:52:19 [INFO]: epoch 494: training loss 0.0990\n",
      "2025-06-01 14:52:19 [INFO]: epoch 495: training loss 0.1078\n",
      "2025-06-01 14:52:19 [INFO]: epoch 496: training loss 0.0989\n",
      "2025-06-01 14:52:19 [INFO]: epoch 497: training loss 0.0979\n",
      "2025-06-01 14:52:19 [INFO]: epoch 498: training loss 0.0917\n",
      "2025-06-01 14:52:19 [INFO]: epoch 499: training loss 0.1021\n",
      "2025-06-01 14:52:19 [INFO]: epoch 500: training loss 0.1054\n",
      "2025-06-01 14:52:19 [INFO]: epoch 501: training loss 0.0977\n",
      "2025-06-01 14:52:19 [INFO]: epoch 502: training loss 0.0973\n",
      "2025-06-01 14:52:19 [INFO]: epoch 503: training loss 0.0878\n",
      "2025-06-01 14:52:19 [INFO]: epoch 504: training loss 0.0925\n",
      "2025-06-01 14:52:19 [INFO]: epoch 505: training loss 0.0996\n",
      "2025-06-01 14:52:19 [INFO]: epoch 506: training loss 0.0951\n",
      "2025-06-01 14:52:19 [INFO]: epoch 507: training loss 0.0935\n",
      "2025-06-01 14:52:19 [INFO]: epoch 508: training loss 0.0974\n",
      "2025-06-01 14:52:19 [INFO]: epoch 509: training loss 0.0909\n",
      "2025-06-01 14:52:19 [INFO]: epoch 510: training loss 0.0985\n",
      "2025-06-01 14:52:19 [INFO]: epoch 511: training loss 0.0940\n",
      "2025-06-01 14:52:19 [INFO]: epoch 512: training loss 0.0900\n",
      "2025-06-01 14:52:19 [INFO]: epoch 513: training loss 0.0907\n",
      "2025-06-01 14:52:19 [INFO]: epoch 514: training loss 0.0948\n",
      "2025-06-01 14:52:19 [INFO]: epoch 515: training loss 0.0927\n",
      "2025-06-01 14:52:19 [INFO]: epoch 516: training loss 0.0870\n",
      "2025-06-01 14:52:19 [INFO]: epoch 517: training loss 0.0940\n",
      "2025-06-01 14:52:19 [INFO]: epoch 518: training loss 0.0917\n",
      "2025-06-01 14:52:19 [INFO]: epoch 519: training loss 0.0919\n",
      "2025-06-01 14:52:19 [INFO]: epoch 520: training loss 0.0921\n",
      "2025-06-01 14:52:19 [INFO]: epoch 521: training loss 0.0863\n",
      "2025-06-01 14:52:19 [INFO]: epoch 522: training loss 0.0964\n",
      "2025-06-01 14:52:19 [INFO]: epoch 523: training loss 0.0981\n",
      "2025-06-01 14:52:19 [INFO]: epoch 524: training loss 0.0910\n",
      "2025-06-01 14:52:19 [INFO]: epoch 525: training loss 0.0942\n",
      "2025-06-01 14:52:19 [INFO]: epoch 526: training loss 0.0898\n",
      "2025-06-01 14:52:19 [INFO]: epoch 527: training loss 0.0879\n",
      "2025-06-01 14:52:19 [INFO]: epoch 528: training loss 0.0946\n",
      "2025-06-01 14:52:19 [INFO]: epoch 529: training loss 0.0920\n",
      "2025-06-01 14:52:19 [INFO]: epoch 530: training loss 0.0957\n",
      "2025-06-01 14:52:19 [INFO]: epoch 531: training loss 0.0897\n",
      "2025-06-01 14:52:19 [INFO]: epoch 532: training loss 0.0883\n",
      "2025-06-01 14:52:19 [INFO]: epoch 533: training loss 0.0880\n",
      "2025-06-01 14:52:19 [INFO]: epoch 534: training loss 0.0827\n",
      "2025-06-01 14:52:19 [INFO]: epoch 535: training loss 0.0870\n",
      "2025-06-01 14:52:19 [INFO]: epoch 536: training loss 0.0843\n",
      "2025-06-01 14:52:19 [INFO]: epoch 537: training loss 0.0925\n",
      "2025-06-01 14:52:19 [INFO]: epoch 538: training loss 0.0856\n",
      "2025-06-01 14:52:19 [INFO]: epoch 539: training loss 0.0779\n",
      "2025-06-01 14:52:19 [INFO]: epoch 540: training loss 0.0817\n",
      "2025-06-01 14:52:19 [INFO]: epoch 541: training loss 0.0894\n",
      "2025-06-01 14:52:19 [INFO]: epoch 542: training loss 0.0779\n",
      "2025-06-01 14:52:19 [INFO]: epoch 543: training loss 0.0795\n",
      "2025-06-01 14:52:19 [INFO]: epoch 544: training loss 0.0822\n",
      "2025-06-01 14:52:19 [INFO]: epoch 545: training loss 0.0869\n",
      "2025-06-01 14:52:19 [INFO]: epoch 546: training loss 0.0831\n",
      "2025-06-01 14:52:19 [INFO]: epoch 547: training loss 0.0865\n",
      "2025-06-01 14:52:19 [INFO]: epoch 548: training loss 0.0798\n",
      "2025-06-01 14:52:19 [INFO]: epoch 549: training loss 0.0875\n",
      "2025-06-01 14:52:19 [INFO]: epoch 550: training loss 0.0872\n",
      "2025-06-01 14:52:19 [INFO]: epoch 551: training loss 0.0875\n",
      "2025-06-01 14:52:19 [INFO]: epoch 552: training loss 0.0851\n",
      "2025-06-01 14:52:19 [INFO]: epoch 553: training loss 0.0861\n",
      "2025-06-01 14:52:19 [INFO]: epoch 554: training loss 0.0966\n",
      "2025-06-01 14:52:19 [INFO]: epoch 555: training loss 0.0869\n",
      "2025-06-01 14:52:19 [INFO]: epoch 556: training loss 0.0811\n",
      "2025-06-01 14:52:19 [INFO]: epoch 557: training loss 0.0840\n",
      "2025-06-01 14:52:19 [INFO]: epoch 558: training loss 0.0878\n",
      "2025-06-01 14:52:19 [INFO]: epoch 559: training loss 0.0888\n",
      "2025-06-01 14:52:19 [INFO]: epoch 560: training loss 0.0798\n",
      "2025-06-01 14:52:19 [INFO]: epoch 561: training loss 0.0772\n",
      "2025-06-01 14:52:19 [INFO]: epoch 562: training loss 0.0812\n",
      "2025-06-01 14:52:19 [INFO]: epoch 563: training loss 0.0855\n",
      "2025-06-01 14:52:19 [INFO]: epoch 564: training loss 0.0864\n",
      "2025-06-01 14:52:20 [INFO]: epoch 565: training loss 0.0824\n",
      "2025-06-01 14:52:20 [INFO]: epoch 566: training loss 0.0796\n",
      "2025-06-01 14:52:20 [INFO]: epoch 567: training loss 0.0851\n",
      "2025-06-01 14:52:20 [INFO]: epoch 568: training loss 0.0826\n",
      "2025-06-01 14:52:20 [INFO]: epoch 569: training loss 0.0751\n",
      "2025-06-01 14:52:20 [INFO]: epoch 570: training loss 0.0780\n",
      "2025-06-01 14:52:20 [INFO]: epoch 571: training loss 0.0866\n",
      "2025-06-01 14:52:20 [INFO]: epoch 572: training loss 0.0794\n",
      "2025-06-01 14:52:20 [INFO]: epoch 573: training loss 0.0781\n",
      "2025-06-01 14:52:20 [INFO]: epoch 574: training loss 0.0830\n",
      "2025-06-01 14:52:20 [INFO]: epoch 575: training loss 0.0722\n",
      "2025-06-01 14:52:20 [INFO]: epoch 576: training loss 0.0850\n",
      "2025-06-01 14:52:20 [INFO]: epoch 577: training loss 0.0788\n",
      "2025-06-01 14:52:20 [INFO]: epoch 578: training loss 0.0774\n",
      "2025-06-01 14:52:20 [INFO]: epoch 579: training loss 0.0800\n",
      "2025-06-01 14:52:20 [INFO]: epoch 580: training loss 0.0804\n",
      "2025-06-01 14:52:20 [INFO]: epoch 581: training loss 0.0697\n",
      "2025-06-01 14:52:20 [INFO]: epoch 582: training loss 0.0811\n",
      "2025-06-01 14:52:20 [INFO]: epoch 583: training loss 0.0751\n",
      "2025-06-01 14:52:20 [INFO]: epoch 584: training loss 0.0828\n",
      "2025-06-01 14:52:20 [INFO]: epoch 585: training loss 0.0797\n",
      "2025-06-01 14:52:20 [INFO]: epoch 586: training loss 0.0904\n",
      "2025-06-01 14:52:20 [INFO]: epoch 587: training loss 0.0922\n",
      "2025-06-01 14:52:20 [INFO]: epoch 588: training loss 0.0766\n",
      "2025-06-01 14:52:20 [INFO]: epoch 589: training loss 0.0768\n",
      "2025-06-01 14:52:20 [INFO]: epoch 590: training loss 0.0846\n",
      "2025-06-01 14:52:20 [INFO]: epoch 591: training loss 0.0765\n",
      "2025-06-01 14:52:20 [INFO]: epoch 592: training loss 0.0793\n",
      "2025-06-01 14:52:20 [INFO]: epoch 593: training loss 0.0737\n",
      "2025-06-01 14:52:20 [INFO]: epoch 594: training loss 0.0889\n",
      "2025-06-01 14:52:20 [INFO]: epoch 595: training loss 0.0928\n",
      "2025-06-01 14:52:20 [INFO]: epoch 596: training loss 0.0834\n",
      "2025-06-01 14:52:20 [INFO]: epoch 597: training loss 0.0782\n",
      "2025-06-01 14:52:20 [INFO]: epoch 598: training loss 0.0772\n",
      "2025-06-01 14:52:20 [INFO]: epoch 599: training loss 0.0756\n",
      "2025-06-01 14:52:20 [INFO]: epoch 600: training loss 0.0820\n",
      "2025-06-01 14:52:20 [INFO]: epoch 601: training loss 0.0813\n",
      "2025-06-01 14:52:20 [INFO]: epoch 602: training loss 0.0854\n",
      "2025-06-01 14:52:20 [INFO]: epoch 603: training loss 0.0805\n",
      "2025-06-01 14:52:20 [INFO]: epoch 604: training loss 0.0841\n",
      "2025-06-01 14:52:20 [INFO]: epoch 605: training loss 0.0787\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-01 14:52:20 [INFO]: epoch 606: training loss 0.0945\n",
      "2025-06-01 14:52:20 [INFO]: epoch 607: training loss 0.0931\n",
      "2025-06-01 14:52:20 [INFO]: epoch 608: training loss 0.0783\n",
      "2025-06-01 14:52:20 [INFO]: epoch 609: training loss 0.0729\n",
      "2025-06-01 14:52:20 [INFO]: epoch 610: training loss 0.0806\n",
      "2025-06-01 14:52:20 [INFO]: epoch 611: training loss 0.0848\n",
      "2025-06-01 14:52:20 [INFO]: epoch 612: training loss 0.0824\n",
      "2025-06-01 14:52:20 [INFO]: epoch 613: training loss 0.0770\n",
      "2025-06-01 14:52:20 [INFO]: epoch 614: training loss 0.0751\n",
      "2025-06-01 14:52:20 [INFO]: epoch 615: training loss 0.0813\n",
      "2025-06-01 14:52:20 [INFO]: epoch 616: training loss 0.0818\n",
      "2025-06-01 14:52:20 [INFO]: epoch 617: training loss 0.0857\n",
      "2025-06-01 14:52:20 [INFO]: epoch 618: training loss 0.0805\n",
      "2025-06-01 14:52:20 [INFO]: epoch 619: training loss 0.0746\n",
      "2025-06-01 14:52:20 [INFO]: epoch 620: training loss 0.0666\n",
      "2025-06-01 14:52:20 [INFO]: epoch 621: training loss 0.0757\n",
      "2025-06-01 14:52:20 [INFO]: epoch 622: training loss 0.0704\n",
      "2025-06-01 14:52:20 [INFO]: epoch 623: training loss 0.0764\n",
      "2025-06-01 14:52:20 [INFO]: epoch 624: training loss 0.0733\n",
      "2025-06-01 14:52:20 [INFO]: epoch 625: training loss 0.0764\n",
      "2025-06-01 14:52:20 [INFO]: epoch 626: training loss 0.0761\n",
      "2025-06-01 14:52:20 [INFO]: epoch 627: training loss 0.0661\n",
      "2025-06-01 14:52:20 [INFO]: epoch 628: training loss 0.0723\n",
      "2025-06-01 14:52:20 [INFO]: epoch 629: training loss 0.0815\n",
      "2025-06-01 14:52:20 [INFO]: epoch 630: training loss 0.0765\n",
      "2025-06-01 14:52:20 [INFO]: epoch 631: training loss 0.0755\n",
      "2025-06-01 14:52:20 [INFO]: epoch 632: training loss 0.0726\n",
      "2025-06-01 14:52:20 [INFO]: epoch 633: training loss 0.0739\n",
      "2025-06-01 14:52:20 [INFO]: epoch 634: training loss 0.0764\n",
      "2025-06-01 14:52:20 [INFO]: epoch 635: training loss 0.0782\n",
      "2025-06-01 14:52:20 [INFO]: epoch 636: training loss 0.0639\n",
      "2025-06-01 14:52:20 [INFO]: epoch 637: training loss 0.0712\n",
      "2025-06-01 14:52:20 [INFO]: epoch 638: training loss 0.0699\n",
      "2025-06-01 14:52:20 [INFO]: epoch 639: training loss 0.0677\n",
      "2025-06-01 14:52:20 [INFO]: epoch 640: training loss 0.0689\n",
      "2025-06-01 14:52:20 [INFO]: epoch 641: training loss 0.0763\n",
      "2025-06-01 14:52:21 [INFO]: epoch 642: training loss 0.0747\n",
      "2025-06-01 14:52:21 [INFO]: epoch 643: training loss 0.0685\n",
      "2025-06-01 14:52:21 [INFO]: epoch 644: training loss 0.0684\n",
      "2025-06-01 14:52:21 [INFO]: epoch 645: training loss 0.0734\n",
      "2025-06-01 14:52:21 [INFO]: epoch 646: training loss 0.0719\n",
      "2025-06-01 14:52:21 [INFO]: epoch 647: training loss 0.0702\n",
      "2025-06-01 14:52:21 [INFO]: epoch 648: training loss 0.0685\n",
      "2025-06-01 14:52:21 [INFO]: epoch 649: training loss 0.0716\n",
      "2025-06-01 14:52:21 [INFO]: epoch 650: training loss 0.0707\n",
      "2025-06-01 14:52:21 [INFO]: epoch 651: training loss 0.0681\n",
      "2025-06-01 14:52:21 [INFO]: epoch 652: training loss 0.0729\n",
      "2025-06-01 14:52:21 [INFO]: epoch 653: training loss 0.0753\n",
      "2025-06-01 14:52:21 [INFO]: epoch 654: training loss 0.0663\n",
      "2025-06-01 14:52:21 [INFO]: epoch 655: training loss 0.0740\n",
      "2025-06-01 14:52:21 [INFO]: epoch 656: training loss 0.0719\n",
      "2025-06-01 14:52:21 [INFO]: epoch 657: training loss 0.0728\n",
      "2025-06-01 14:52:21 [INFO]: epoch 658: training loss 0.0745\n",
      "2025-06-01 14:52:21 [INFO]: epoch 659: training loss 0.0693\n",
      "2025-06-01 14:52:21 [INFO]: epoch 660: training loss 0.0729\n",
      "2025-06-01 14:52:21 [INFO]: epoch 661: training loss 0.0813\n",
      "2025-06-01 14:52:21 [INFO]: epoch 662: training loss 0.0753\n",
      "2025-06-01 14:52:21 [INFO]: epoch 663: training loss 0.0707\n",
      "2025-06-01 14:52:21 [INFO]: epoch 664: training loss 0.0698\n",
      "2025-06-01 14:52:21 [INFO]: epoch 665: training loss 0.0731\n",
      "2025-06-01 14:52:21 [INFO]: epoch 666: training loss 0.0789\n",
      "2025-06-01 14:52:21 [INFO]: epoch 667: training loss 0.0758\n",
      "2025-06-01 14:52:21 [INFO]: epoch 668: training loss 0.0707\n",
      "2025-06-01 14:52:21 [INFO]: epoch 669: training loss 0.0651\n",
      "2025-06-01 14:52:21 [INFO]: epoch 670: training loss 0.0635\n",
      "2025-06-01 14:52:21 [INFO]: epoch 671: training loss 0.0734\n",
      "2025-06-01 14:52:21 [INFO]: epoch 672: training loss 0.0696\n",
      "2025-06-01 14:52:21 [INFO]: epoch 673: training loss 0.0642\n",
      "2025-06-01 14:52:21 [INFO]: epoch 674: training loss 0.0629\n",
      "2025-06-01 14:52:21 [INFO]: epoch 675: training loss 0.0626\n",
      "2025-06-01 14:52:21 [INFO]: epoch 676: training loss 0.0747\n",
      "2025-06-01 14:52:21 [INFO]: epoch 677: training loss 0.0680\n",
      "2025-06-01 14:52:21 [INFO]: epoch 678: training loss 0.0680\n",
      "2025-06-01 14:52:21 [INFO]: epoch 679: training loss 0.0701\n",
      "2025-06-01 14:52:21 [INFO]: epoch 680: training loss 0.0648\n",
      "2025-06-01 14:52:21 [INFO]: epoch 681: training loss 0.0646\n",
      "2025-06-01 14:52:21 [INFO]: epoch 682: training loss 0.0650\n",
      "2025-06-01 14:52:21 [INFO]: epoch 683: training loss 0.0666\n",
      "2025-06-01 14:52:21 [INFO]: epoch 684: training loss 0.0619\n",
      "2025-06-01 14:52:21 [INFO]: epoch 685: training loss 0.0599\n",
      "2025-06-01 14:52:21 [INFO]: epoch 686: training loss 0.0603\n",
      "2025-06-01 14:52:21 [INFO]: epoch 687: training loss 0.0670\n",
      "2025-06-01 14:52:21 [INFO]: epoch 688: training loss 0.0666\n",
      "2025-06-01 14:52:21 [INFO]: epoch 689: training loss 0.0654\n",
      "2025-06-01 14:52:21 [INFO]: epoch 690: training loss 0.0639\n",
      "2025-06-01 14:52:21 [INFO]: epoch 691: training loss 0.0660\n",
      "2025-06-01 14:52:21 [INFO]: epoch 692: training loss 0.0632\n",
      "2025-06-01 14:52:21 [INFO]: epoch 693: training loss 0.0620\n",
      "2025-06-01 14:52:21 [INFO]: epoch 694: training loss 0.0657\n",
      "2025-06-01 14:52:21 [INFO]: epoch 695: training loss 0.0614\n",
      "2025-06-01 14:52:21 [INFO]: epoch 696: training loss 0.0650\n",
      "2025-06-01 14:52:21 [INFO]: epoch 697: training loss 0.0659\n",
      "2025-06-01 14:52:21 [INFO]: epoch 698: training loss 0.0625\n",
      "2025-06-01 14:52:21 [INFO]: epoch 699: training loss 0.0649\n",
      "2025-06-01 14:52:21 [INFO]: epoch 700: training loss 0.0625\n",
      "2025-06-01 14:52:21 [INFO]: epoch 701: training loss 0.0583\n",
      "2025-06-01 14:52:21 [INFO]: epoch 702: training loss 0.0623\n",
      "2025-06-01 14:52:21 [INFO]: epoch 703: training loss 0.0649\n",
      "2025-06-01 14:52:21 [INFO]: epoch 704: training loss 0.0610\n",
      "2025-06-01 14:52:21 [INFO]: epoch 705: training loss 0.0577\n",
      "2025-06-01 14:52:21 [INFO]: epoch 706: training loss 0.0589\n",
      "2025-06-01 14:52:21 [INFO]: epoch 707: training loss 0.0576\n",
      "2025-06-01 14:52:21 [INFO]: epoch 708: training loss 0.0585\n",
      "2025-06-01 14:52:21 [INFO]: epoch 709: training loss 0.0606\n",
      "2025-06-01 14:52:21 [INFO]: epoch 710: training loss 0.0619\n",
      "2025-06-01 14:52:21 [INFO]: epoch 711: training loss 0.0591\n",
      "2025-06-01 14:52:21 [INFO]: epoch 712: training loss 0.0590\n",
      "2025-06-01 14:52:21 [INFO]: epoch 713: training loss 0.0602\n",
      "2025-06-01 14:52:21 [INFO]: epoch 714: training loss 0.0594\n",
      "2025-06-01 14:52:21 [INFO]: epoch 715: training loss 0.0630\n",
      "2025-06-01 14:52:22 [INFO]: epoch 716: training loss 0.0566\n",
      "2025-06-01 14:52:22 [INFO]: epoch 717: training loss 0.0611\n",
      "2025-06-01 14:52:22 [INFO]: epoch 718: training loss 0.0678\n",
      "2025-06-01 14:52:22 [INFO]: epoch 719: training loss 0.0639\n",
      "2025-06-01 14:52:22 [INFO]: epoch 720: training loss 0.0619\n",
      "2025-06-01 14:52:22 [INFO]: epoch 721: training loss 0.0649\n",
      "2025-06-01 14:52:22 [INFO]: epoch 722: training loss 0.0700\n",
      "2025-06-01 14:52:22 [INFO]: epoch 723: training loss 0.0648\n",
      "2025-06-01 14:52:22 [INFO]: epoch 724: training loss 0.0625\n",
      "2025-06-01 14:52:22 [INFO]: epoch 725: training loss 0.0576\n",
      "2025-06-01 14:52:22 [INFO]: epoch 726: training loss 0.0593\n",
      "2025-06-01 14:52:22 [INFO]: epoch 727: training loss 0.0609\n",
      "2025-06-01 14:52:22 [INFO]: epoch 728: training loss 0.0616\n",
      "2025-06-01 14:52:22 [INFO]: epoch 729: training loss 0.0588\n",
      "2025-06-01 14:52:22 [INFO]: epoch 730: training loss 0.0597\n",
      "2025-06-01 14:52:22 [INFO]: epoch 731: training loss 0.0588\n",
      "2025-06-01 14:52:22 [INFO]: epoch 732: training loss 0.0616\n",
      "2025-06-01 14:52:22 [INFO]: epoch 733: training loss 0.0618\n",
      "2025-06-01 14:52:22 [INFO]: epoch 734: training loss 0.0551\n",
      "2025-06-01 14:52:22 [INFO]: epoch 735: training loss 0.0591\n",
      "2025-06-01 14:52:22 [INFO]: epoch 736: training loss 0.0609\n",
      "2025-06-01 14:52:22 [INFO]: epoch 737: training loss 0.0587\n",
      "2025-06-01 14:52:22 [INFO]: epoch 738: training loss 0.0568\n",
      "2025-06-01 14:52:22 [INFO]: epoch 739: training loss 0.0577\n",
      "2025-06-01 14:52:22 [INFO]: epoch 740: training loss 0.0589\n",
      "2025-06-01 14:52:22 [INFO]: epoch 741: training loss 0.0622\n",
      "2025-06-01 14:52:22 [INFO]: epoch 742: training loss 0.0585\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-01 14:52:22 [INFO]: epoch 743: training loss 0.0576\n",
      "2025-06-01 14:52:22 [INFO]: epoch 744: training loss 0.0577\n",
      "2025-06-01 14:52:22 [INFO]: epoch 745: training loss 0.0555\n",
      "2025-06-01 14:52:22 [INFO]: epoch 746: training loss 0.0590\n",
      "2025-06-01 14:52:22 [INFO]: epoch 747: training loss 0.0576\n",
      "2025-06-01 14:52:22 [INFO]: epoch 748: training loss 0.0598\n",
      "2025-06-01 14:52:22 [INFO]: epoch 749: training loss 0.0657\n",
      "2025-06-01 14:52:22 [INFO]: epoch 750: training loss 0.0626\n",
      "2025-06-01 14:52:22 [INFO]: epoch 751: training loss 0.0634\n",
      "2025-06-01 14:52:22 [INFO]: epoch 752: training loss 0.0466\n",
      "2025-06-01 14:52:22 [INFO]: epoch 753: training loss 0.0584\n",
      "2025-06-01 14:52:22 [INFO]: epoch 754: training loss 0.0631\n",
      "2025-06-01 14:52:22 [INFO]: epoch 755: training loss 0.0616\n",
      "2025-06-01 14:52:22 [INFO]: epoch 756: training loss 0.0592\n",
      "2025-06-01 14:52:22 [INFO]: epoch 757: training loss 0.0615\n",
      "2025-06-01 14:52:22 [INFO]: epoch 758: training loss 0.0652\n",
      "2025-06-01 14:52:22 [INFO]: epoch 759: training loss 0.0597\n",
      "2025-06-01 14:52:22 [INFO]: epoch 760: training loss 0.0570\n",
      "2025-06-01 14:52:22 [INFO]: epoch 761: training loss 0.0572\n",
      "2025-06-01 14:52:22 [INFO]: epoch 762: training loss 0.0631\n",
      "2025-06-01 14:52:22 [INFO]: epoch 763: training loss 0.0605\n",
      "2025-06-01 14:52:22 [INFO]: epoch 764: training loss 0.0587\n",
      "2025-06-01 14:52:22 [INFO]: epoch 765: training loss 0.0597\n",
      "2025-06-01 14:52:22 [INFO]: epoch 766: training loss 0.0597\n",
      "2025-06-01 14:52:22 [INFO]: epoch 767: training loss 0.0551\n",
      "2025-06-01 14:52:22 [INFO]: epoch 768: training loss 0.0530\n",
      "2025-06-01 14:52:22 [INFO]: epoch 769: training loss 0.0577\n",
      "2025-06-01 14:52:22 [INFO]: epoch 770: training loss 0.0608\n",
      "2025-06-01 14:52:22 [INFO]: epoch 771: training loss 0.0540\n",
      "2025-06-01 14:52:22 [INFO]: epoch 772: training loss 0.0504\n",
      "2025-06-01 14:52:22 [INFO]: epoch 773: training loss 0.0545\n",
      "2025-06-01 14:52:22 [INFO]: epoch 774: training loss 0.0534\n",
      "2025-06-01 14:52:22 [INFO]: epoch 775: training loss 0.0575\n",
      "2025-06-01 14:52:22 [INFO]: epoch 776: training loss 0.0586\n",
      "2025-06-01 14:52:22 [INFO]: epoch 777: training loss 0.0534\n",
      "2025-06-01 14:52:22 [INFO]: epoch 778: training loss 0.0549\n",
      "2025-06-01 14:52:22 [INFO]: epoch 779: training loss 0.0609\n",
      "2025-06-01 14:52:22 [INFO]: epoch 780: training loss 0.0569\n",
      "2025-06-01 14:52:22 [INFO]: epoch 781: training loss 0.0543\n",
      "2025-06-01 14:52:22 [INFO]: epoch 782: training loss 0.0528\n",
      "2025-06-01 14:52:22 [INFO]: epoch 783: training loss 0.0614\n",
      "2025-06-01 14:52:22 [INFO]: epoch 784: training loss 0.0562\n",
      "2025-06-01 14:52:22 [INFO]: epoch 785: training loss 0.0531\n",
      "2025-06-01 14:52:22 [INFO]: epoch 786: training loss 0.0527\n",
      "2025-06-01 14:52:22 [INFO]: epoch 787: training loss 0.0587\n",
      "2025-06-01 14:52:22 [INFO]: epoch 788: training loss 0.0541\n",
      "2025-06-01 14:52:22 [INFO]: epoch 789: training loss 0.0524\n",
      "2025-06-01 14:52:22 [INFO]: epoch 790: training loss 0.0500\n",
      "2025-06-01 14:52:22 [INFO]: epoch 791: training loss 0.0564\n",
      "2025-06-01 14:52:22 [INFO]: epoch 792: training loss 0.0567\n",
      "2025-06-01 14:52:22 [INFO]: epoch 793: training loss 0.0539\n",
      "2025-06-01 14:52:23 [INFO]: epoch 794: training loss 0.0518\n",
      "2025-06-01 14:52:23 [INFO]: epoch 795: training loss 0.0544\n",
      "2025-06-01 14:52:23 [INFO]: epoch 796: training loss 0.0555\n",
      "2025-06-01 14:52:23 [INFO]: epoch 797: training loss 0.0512\n",
      "2025-06-01 14:52:23 [INFO]: epoch 798: training loss 0.0515\n",
      "2025-06-01 14:52:23 [INFO]: epoch 799: training loss 0.0542\n",
      "2025-06-01 14:52:23 [INFO]: Finished training.\n",
      "2025-06-01 14:52:23 [WARNING]: 🚨DeprecationWarning: The method impute is deprecated. Please use `predict` instead.\n",
      " 67%|████████████████████████████████████████████████████████                            | 4/6 [00:42<00:21, 10.61s/it]2025-06-01 14:52:23 [INFO]: No given device, using default device: cuda\n",
      "2025-06-01 14:52:23 [WARNING]: saving_path not given. Model files and tensorboard file will not be saved.\n",
      "2025-06-01 14:52:23 [INFO]: Model initialized successfully with the number of trainable parameters: 597,780\n",
      "2025-06-01 14:52:23 [INFO]: epoch 0: training loss 1.1265\n",
      "2025-06-01 14:52:23 [INFO]: epoch 1: training loss 0.8047\n",
      "2025-06-01 14:52:23 [INFO]: epoch 2: training loss 0.6335\n",
      "2025-06-01 14:52:23 [INFO]: epoch 3: training loss 0.6293\n",
      "2025-06-01 14:52:23 [INFO]: epoch 4: training loss 0.5773\n",
      "2025-06-01 14:52:23 [INFO]: epoch 5: training loss 0.5613\n",
      "2025-06-01 14:52:23 [INFO]: epoch 6: training loss 0.4836\n",
      "2025-06-01 14:52:23 [INFO]: epoch 7: training loss 0.4491\n",
      "2025-06-01 14:52:23 [INFO]: epoch 8: training loss 0.4115\n",
      "2025-06-01 14:52:23 [INFO]: epoch 9: training loss 0.3721\n",
      "2025-06-01 14:52:23 [INFO]: epoch 10: training loss 0.4099\n",
      "2025-06-01 14:52:23 [INFO]: epoch 11: training loss 0.4231\n",
      "2025-06-01 14:52:23 [INFO]: epoch 12: training loss 0.4432\n",
      "2025-06-01 14:52:23 [INFO]: epoch 13: training loss 0.4096\n",
      "2025-06-01 14:52:23 [INFO]: epoch 14: training loss 0.4047\n",
      "2025-06-01 14:52:23 [INFO]: epoch 15: training loss 0.3866\n",
      "2025-06-01 14:52:23 [INFO]: epoch 16: training loss 0.3502\n",
      "2025-06-01 14:52:23 [INFO]: epoch 17: training loss 0.3763\n",
      "2025-06-01 14:52:23 [INFO]: epoch 18: training loss 0.3657\n",
      "2025-06-01 14:52:23 [INFO]: epoch 19: training loss 0.3248\n",
      "2025-06-01 14:52:23 [INFO]: epoch 20: training loss 0.3326\n",
      "2025-06-01 14:52:23 [INFO]: epoch 21: training loss 0.3819\n",
      "2025-06-01 14:52:23 [INFO]: epoch 22: training loss 0.3411\n",
      "2025-06-01 14:52:23 [INFO]: epoch 23: training loss 0.3590\n",
      "2025-06-01 14:52:23 [INFO]: epoch 24: training loss 0.3307\n",
      "2025-06-01 14:52:23 [INFO]: epoch 25: training loss 0.3266\n",
      "2025-06-01 14:52:23 [INFO]: epoch 26: training loss 0.3236\n",
      "2025-06-01 14:52:23 [INFO]: epoch 27: training loss 0.3345\n",
      "2025-06-01 14:52:23 [INFO]: epoch 28: training loss 0.3113\n",
      "2025-06-01 14:52:23 [INFO]: epoch 29: training loss 0.2972\n",
      "2025-06-01 14:52:23 [INFO]: epoch 30: training loss 0.3364\n",
      "2025-06-01 14:52:23 [INFO]: epoch 31: training loss 0.3027\n",
      "2025-06-01 14:52:23 [INFO]: epoch 32: training loss 0.2968\n",
      "2025-06-01 14:52:23 [INFO]: epoch 33: training loss 0.2961\n",
      "2025-06-01 14:52:23 [INFO]: epoch 34: training loss 0.2750\n",
      "2025-06-01 14:52:23 [INFO]: epoch 35: training loss 0.3198\n",
      "2025-06-01 14:52:23 [INFO]: epoch 36: training loss 0.3181\n",
      "2025-06-01 14:52:23 [INFO]: epoch 37: training loss 0.2921\n",
      "2025-06-01 14:52:23 [INFO]: epoch 38: training loss 0.2667\n",
      "2025-06-01 14:52:23 [INFO]: epoch 39: training loss 0.2917\n",
      "2025-06-01 14:52:23 [INFO]: epoch 40: training loss 0.2805\n",
      "2025-06-01 14:52:23 [INFO]: epoch 41: training loss 0.2921\n",
      "2025-06-01 14:52:23 [INFO]: epoch 42: training loss 0.3147\n",
      "2025-06-01 14:52:23 [INFO]: epoch 43: training loss 0.2980\n",
      "2025-06-01 14:52:23 [INFO]: epoch 44: training loss 0.2754\n",
      "2025-06-01 14:52:23 [INFO]: epoch 45: training loss 0.2659\n",
      "2025-06-01 14:52:23 [INFO]: epoch 46: training loss 0.2590\n",
      "2025-06-01 14:52:23 [INFO]: epoch 47: training loss 0.2589\n",
      "2025-06-01 14:52:23 [INFO]: epoch 48: training loss 0.2619\n",
      "2025-06-01 14:52:23 [INFO]: epoch 49: training loss 0.2857\n",
      "2025-06-01 14:52:23 [INFO]: epoch 50: training loss 0.2496\n",
      "2025-06-01 14:52:23 [INFO]: epoch 51: training loss 0.2535\n",
      "2025-06-01 14:52:23 [INFO]: epoch 52: training loss 0.2632\n",
      "2025-06-01 14:52:23 [INFO]: epoch 53: training loss 0.2364\n",
      "2025-06-01 14:52:23 [INFO]: epoch 54: training loss 0.2490\n",
      "2025-06-01 14:52:23 [INFO]: epoch 55: training loss 0.2510\n",
      "2025-06-01 14:52:23 [INFO]: epoch 56: training loss 0.2555\n",
      "2025-06-01 14:52:23 [INFO]: epoch 57: training loss 0.2411\n",
      "2025-06-01 14:52:23 [INFO]: epoch 58: training loss 0.2503\n",
      "2025-06-01 14:52:23 [INFO]: epoch 59: training loss 0.2460\n",
      "2025-06-01 14:52:23 [INFO]: epoch 60: training loss 0.2443\n",
      "2025-06-01 14:52:23 [INFO]: epoch 61: training loss 0.2599\n",
      "2025-06-01 14:52:24 [INFO]: epoch 62: training loss 0.2581\n",
      "2025-06-01 14:52:24 [INFO]: epoch 63: training loss 0.2417\n",
      "2025-06-01 14:52:24 [INFO]: epoch 64: training loss 0.2290\n",
      "2025-06-01 14:52:24 [INFO]: epoch 65: training loss 0.2277\n",
      "2025-06-01 14:52:24 [INFO]: epoch 66: training loss 0.2313\n",
      "2025-06-01 14:52:24 [INFO]: epoch 67: training loss 0.2593\n",
      "2025-06-01 14:52:24 [INFO]: epoch 68: training loss 0.2261\n",
      "2025-06-01 14:52:24 [INFO]: epoch 69: training loss 0.2119\n",
      "2025-06-01 14:52:24 [INFO]: epoch 70: training loss 0.2371\n",
      "2025-06-01 14:52:24 [INFO]: epoch 71: training loss 0.2175\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-01 14:52:24 [INFO]: epoch 72: training loss 0.2113\n",
      "2025-06-01 14:52:24 [INFO]: epoch 73: training loss 0.2398\n",
      "2025-06-01 14:52:24 [INFO]: epoch 74: training loss 0.2003\n",
      "2025-06-01 14:52:24 [INFO]: epoch 75: training loss 0.2140\n",
      "2025-06-01 14:52:24 [INFO]: epoch 76: training loss 0.2062\n",
      "2025-06-01 14:52:24 [INFO]: epoch 77: training loss 0.2147\n",
      "2025-06-01 14:52:24 [INFO]: epoch 78: training loss 0.2119\n",
      "2025-06-01 14:52:24 [INFO]: epoch 79: training loss 0.2082\n",
      "2025-06-01 14:52:24 [INFO]: epoch 80: training loss 0.2012\n",
      "2025-06-01 14:52:24 [INFO]: epoch 81: training loss 0.2159\n",
      "2025-06-01 14:52:24 [INFO]: epoch 82: training loss 0.2169\n",
      "2025-06-01 14:52:24 [INFO]: epoch 83: training loss 0.2060\n",
      "2025-06-01 14:52:24 [INFO]: epoch 84: training loss 0.2149\n",
      "2025-06-01 14:52:24 [INFO]: epoch 85: training loss 0.2044\n",
      "2025-06-01 14:52:24 [INFO]: epoch 86: training loss 0.1937\n",
      "2025-06-01 14:52:24 [INFO]: epoch 87: training loss 0.1976\n",
      "2025-06-01 14:52:24 [INFO]: epoch 88: training loss 0.2100\n",
      "2025-06-01 14:52:24 [INFO]: epoch 89: training loss 0.1947\n",
      "2025-06-01 14:52:24 [INFO]: epoch 90: training loss 0.1995\n",
      "2025-06-01 14:52:24 [INFO]: epoch 91: training loss 0.1862\n",
      "2025-06-01 14:52:24 [INFO]: epoch 92: training loss 0.2020\n",
      "2025-06-01 14:52:24 [INFO]: epoch 93: training loss 0.1895\n",
      "2025-06-01 14:52:24 [INFO]: epoch 94: training loss 0.1868\n",
      "2025-06-01 14:52:24 [INFO]: epoch 95: training loss 0.1963\n",
      "2025-06-01 14:52:24 [INFO]: epoch 96: training loss 0.1807\n",
      "2025-06-01 14:52:24 [INFO]: epoch 97: training loss 0.2045\n",
      "2025-06-01 14:52:24 [INFO]: epoch 98: training loss 0.1884\n",
      "2025-06-01 14:52:24 [INFO]: epoch 99: training loss 0.1907\n",
      "2025-06-01 14:52:24 [INFO]: epoch 100: training loss 0.1930\n",
      "2025-06-01 14:52:24 [INFO]: epoch 101: training loss 0.2046\n",
      "2025-06-01 14:52:24 [INFO]: epoch 102: training loss 0.1942\n",
      "2025-06-01 14:52:24 [INFO]: epoch 103: training loss 0.1824\n",
      "2025-06-01 14:52:24 [INFO]: epoch 104: training loss 0.1781\n",
      "2025-06-01 14:52:24 [INFO]: epoch 105: training loss 0.1842\n",
      "2025-06-01 14:52:24 [INFO]: epoch 106: training loss 0.1947\n",
      "2025-06-01 14:52:24 [INFO]: epoch 107: training loss 0.1809\n",
      "2025-06-01 14:52:24 [INFO]: epoch 108: training loss 0.1849\n",
      "2025-06-01 14:52:24 [INFO]: epoch 109: training loss 0.1741\n",
      "2025-06-01 14:52:24 [INFO]: epoch 110: training loss 0.1883\n",
      "2025-06-01 14:52:24 [INFO]: epoch 111: training loss 0.1908\n",
      "2025-06-01 14:52:24 [INFO]: epoch 112: training loss 0.1839\n",
      "2025-06-01 14:52:24 [INFO]: epoch 113: training loss 0.1856\n",
      "2025-06-01 14:52:24 [INFO]: epoch 114: training loss 0.1715\n",
      "2025-06-01 14:52:24 [INFO]: epoch 115: training loss 0.1785\n",
      "2025-06-01 14:52:24 [INFO]: epoch 116: training loss 0.1716\n",
      "2025-06-01 14:52:24 [INFO]: epoch 117: training loss 0.1716\n",
      "2025-06-01 14:52:24 [INFO]: epoch 118: training loss 0.1997\n",
      "2025-06-01 14:52:24 [INFO]: epoch 119: training loss 0.1816\n",
      "2025-06-01 14:52:24 [INFO]: epoch 120: training loss 0.1713\n",
      "2025-06-01 14:52:24 [INFO]: epoch 121: training loss 0.1811\n",
      "2025-06-01 14:52:24 [INFO]: epoch 122: training loss 0.1776\n",
      "2025-06-01 14:52:24 [INFO]: epoch 123: training loss 0.1787\n",
      "2025-06-01 14:52:24 [INFO]: epoch 124: training loss 0.1583\n",
      "2025-06-01 14:52:24 [INFO]: epoch 125: training loss 0.1654\n",
      "2025-06-01 14:52:24 [INFO]: epoch 126: training loss 0.1828\n",
      "2025-06-01 14:52:24 [INFO]: epoch 127: training loss 0.1794\n",
      "2025-06-01 14:52:24 [INFO]: epoch 128: training loss 0.1676\n",
      "2025-06-01 14:52:24 [INFO]: epoch 129: training loss 0.1516\n",
      "2025-06-01 14:52:24 [INFO]: epoch 130: training loss 0.1788\n",
      "2025-06-01 14:52:24 [INFO]: epoch 131: training loss 0.1635\n",
      "2025-06-01 14:52:24 [INFO]: epoch 132: training loss 0.1686\n",
      "2025-06-01 14:52:24 [INFO]: epoch 133: training loss 0.1780\n",
      "2025-06-01 14:52:24 [INFO]: epoch 134: training loss 0.1554\n",
      "2025-06-01 14:52:24 [INFO]: epoch 135: training loss 0.1570\n",
      "2025-06-01 14:52:24 [INFO]: epoch 136: training loss 0.1615\n",
      "2025-06-01 14:52:24 [INFO]: epoch 137: training loss 0.1561\n",
      "2025-06-01 14:52:24 [INFO]: epoch 138: training loss 0.1520\n",
      "2025-06-01 14:52:25 [INFO]: epoch 139: training loss 0.1583\n",
      "2025-06-01 14:52:25 [INFO]: epoch 140: training loss 0.1430\n",
      "2025-06-01 14:52:25 [INFO]: epoch 141: training loss 0.1507\n",
      "2025-06-01 14:52:25 [INFO]: epoch 142: training loss 0.1701\n",
      "2025-06-01 14:52:25 [INFO]: epoch 143: training loss 0.1469\n",
      "2025-06-01 14:52:25 [INFO]: epoch 144: training loss 0.1461\n",
      "2025-06-01 14:52:25 [INFO]: epoch 145: training loss 0.1436\n",
      "2025-06-01 14:52:25 [INFO]: epoch 146: training loss 0.1583\n",
      "2025-06-01 14:52:25 [INFO]: epoch 147: training loss 0.1498\n",
      "2025-06-01 14:52:25 [INFO]: epoch 148: training loss 0.1427\n",
      "2025-06-01 14:52:25 [INFO]: epoch 149: training loss 0.1571\n",
      "2025-06-01 14:52:25 [INFO]: epoch 150: training loss 0.1675\n",
      "2025-06-01 14:52:25 [INFO]: epoch 151: training loss 0.1495\n",
      "2025-06-01 14:52:25 [INFO]: epoch 152: training loss 0.1472\n",
      "2025-06-01 14:52:25 [INFO]: epoch 153: training loss 0.1473\n",
      "2025-06-01 14:52:25 [INFO]: epoch 154: training loss 0.1492\n",
      "2025-06-01 14:52:25 [INFO]: epoch 155: training loss 0.1437\n",
      "2025-06-01 14:52:25 [INFO]: epoch 156: training loss 0.1566\n",
      "2025-06-01 14:52:25 [INFO]: epoch 157: training loss 0.1569\n",
      "2025-06-01 14:52:25 [INFO]: epoch 158: training loss 0.1389\n",
      "2025-06-01 14:52:25 [INFO]: epoch 159: training loss 0.1512\n",
      "2025-06-01 14:52:25 [INFO]: epoch 160: training loss 0.1300\n",
      "2025-06-01 14:52:25 [INFO]: epoch 161: training loss 0.1441\n",
      "2025-06-01 14:52:25 [INFO]: epoch 162: training loss 0.1338\n",
      "2025-06-01 14:52:25 [INFO]: epoch 163: training loss 0.1353\n",
      "2025-06-01 14:52:25 [INFO]: epoch 164: training loss 0.1281\n",
      "2025-06-01 14:52:25 [INFO]: epoch 165: training loss 0.1444\n",
      "2025-06-01 14:52:25 [INFO]: epoch 166: training loss 0.1309\n",
      "2025-06-01 14:52:25 [INFO]: epoch 167: training loss 0.1380\n",
      "2025-06-01 14:52:25 [INFO]: epoch 168: training loss 0.1392\n",
      "2025-06-01 14:52:25 [INFO]: epoch 169: training loss 0.1320\n",
      "2025-06-01 14:52:25 [INFO]: epoch 170: training loss 0.1321\n",
      "2025-06-01 14:52:25 [INFO]: epoch 171: training loss 0.1427\n",
      "2025-06-01 14:52:25 [INFO]: epoch 172: training loss 0.1319\n",
      "2025-06-01 14:52:25 [INFO]: epoch 173: training loss 0.1374\n",
      "2025-06-01 14:52:25 [INFO]: epoch 174: training loss 0.1374\n",
      "2025-06-01 14:52:25 [INFO]: epoch 175: training loss 0.1292\n",
      "2025-06-01 14:52:25 [INFO]: epoch 176: training loss 0.1276\n",
      "2025-06-01 14:52:25 [INFO]: epoch 177: training loss 0.1394\n",
      "2025-06-01 14:52:25 [INFO]: epoch 178: training loss 0.1393\n",
      "2025-06-01 14:52:25 [INFO]: epoch 179: training loss 0.1370\n",
      "2025-06-01 14:52:25 [INFO]: epoch 180: training loss 0.1274\n",
      "2025-06-01 14:52:25 [INFO]: epoch 181: training loss 0.1380\n",
      "2025-06-01 14:52:25 [INFO]: epoch 182: training loss 0.1281\n",
      "2025-06-01 14:52:25 [INFO]: epoch 183: training loss 0.1388\n",
      "2025-06-01 14:52:25 [INFO]: epoch 184: training loss 0.1343\n",
      "2025-06-01 14:52:25 [INFO]: epoch 185: training loss 0.1183\n",
      "2025-06-01 14:52:25 [INFO]: epoch 186: training loss 0.1206\n",
      "2025-06-01 14:52:25 [INFO]: epoch 187: training loss 0.1334\n",
      "2025-06-01 14:52:25 [INFO]: epoch 188: training loss 0.1309\n",
      "2025-06-01 14:52:25 [INFO]: epoch 189: training loss 0.1195\n",
      "2025-06-01 14:52:25 [INFO]: epoch 190: training loss 0.1282\n",
      "2025-06-01 14:52:25 [INFO]: epoch 191: training loss 0.1290\n",
      "2025-06-01 14:52:25 [INFO]: epoch 192: training loss 0.1233\n",
      "2025-06-01 14:52:25 [INFO]: epoch 193: training loss 0.1207\n",
      "2025-06-01 14:52:25 [INFO]: epoch 194: training loss 0.1110\n",
      "2025-06-01 14:52:25 [INFO]: epoch 195: training loss 0.1136\n",
      "2025-06-01 14:52:25 [INFO]: epoch 196: training loss 0.1224\n",
      "2025-06-01 14:52:25 [INFO]: epoch 197: training loss 0.1231\n",
      "2025-06-01 14:52:25 [INFO]: epoch 198: training loss 0.1118\n",
      "2025-06-01 14:52:25 [INFO]: epoch 199: training loss 0.1148\n",
      "2025-06-01 14:52:25 [INFO]: epoch 200: training loss 0.1107\n",
      "2025-06-01 14:52:25 [INFO]: epoch 201: training loss 0.1196\n",
      "2025-06-01 14:52:25 [INFO]: epoch 202: training loss 0.1195\n",
      "2025-06-01 14:52:25 [INFO]: epoch 203: training loss 0.1143\n",
      "2025-06-01 14:52:25 [INFO]: epoch 204: training loss 0.1207\n",
      "2025-06-01 14:52:25 [INFO]: epoch 205: training loss 0.1321\n",
      "2025-06-01 14:52:25 [INFO]: epoch 206: training loss 0.1180\n",
      "2025-06-01 14:52:25 [INFO]: epoch 207: training loss 0.1098\n",
      "2025-06-01 14:52:25 [INFO]: epoch 208: training loss 0.1243\n",
      "2025-06-01 14:52:25 [INFO]: epoch 209: training loss 0.1213\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-01 14:52:25 [INFO]: epoch 210: training loss 0.1206\n",
      "2025-06-01 14:52:25 [INFO]: epoch 211: training loss 0.1045\n",
      "2025-06-01 14:52:25 [INFO]: epoch 212: training loss 0.1187\n",
      "2025-06-01 14:52:25 [INFO]: epoch 213: training loss 0.1200\n",
      "2025-06-01 14:52:25 [INFO]: epoch 214: training loss 0.1045\n",
      "2025-06-01 14:52:25 [INFO]: epoch 215: training loss 0.1055\n",
      "2025-06-01 14:52:26 [INFO]: epoch 216: training loss 0.1150\n",
      "2025-06-01 14:52:26 [INFO]: epoch 217: training loss 0.1138\n",
      "2025-06-01 14:52:26 [INFO]: epoch 218: training loss 0.1003\n",
      "2025-06-01 14:52:26 [INFO]: epoch 219: training loss 0.1128\n",
      "2025-06-01 14:52:26 [INFO]: epoch 220: training loss 0.1035\n",
      "2025-06-01 14:52:26 [INFO]: epoch 221: training loss 0.1039\n",
      "2025-06-01 14:52:26 [INFO]: epoch 222: training loss 0.0974\n",
      "2025-06-01 14:52:26 [INFO]: epoch 223: training loss 0.1035\n",
      "2025-06-01 14:52:26 [INFO]: epoch 224: training loss 0.1115\n",
      "2025-06-01 14:52:26 [INFO]: epoch 225: training loss 0.1155\n",
      "2025-06-01 14:52:26 [INFO]: epoch 226: training loss 0.1085\n",
      "2025-06-01 14:52:26 [INFO]: epoch 227: training loss 0.1078\n",
      "2025-06-01 14:52:26 [INFO]: epoch 228: training loss 0.0984\n",
      "2025-06-01 14:52:26 [INFO]: epoch 229: training loss 0.1223\n",
      "2025-06-01 14:52:26 [INFO]: epoch 230: training loss 0.0998\n",
      "2025-06-01 14:52:26 [INFO]: epoch 231: training loss 0.0967\n",
      "2025-06-01 14:52:26 [INFO]: epoch 232: training loss 0.1047\n",
      "2025-06-01 14:52:26 [INFO]: epoch 233: training loss 0.1140\n",
      "2025-06-01 14:52:26 [INFO]: epoch 234: training loss 0.0956\n",
      "2025-06-01 14:52:26 [INFO]: epoch 235: training loss 0.1135\n",
      "2025-06-01 14:52:26 [INFO]: epoch 236: training loss 0.1025\n",
      "2025-06-01 14:52:26 [INFO]: epoch 237: training loss 0.1035\n",
      "2025-06-01 14:52:26 [INFO]: epoch 238: training loss 0.1124\n",
      "2025-06-01 14:52:26 [INFO]: epoch 239: training loss 0.1030\n",
      "2025-06-01 14:52:26 [INFO]: epoch 240: training loss 0.0946\n",
      "2025-06-01 14:52:26 [INFO]: epoch 241: training loss 0.1165\n",
      "2025-06-01 14:52:26 [INFO]: epoch 242: training loss 0.1153\n",
      "2025-06-01 14:52:26 [INFO]: epoch 243: training loss 0.0933\n",
      "2025-06-01 14:52:26 [INFO]: epoch 244: training loss 0.0988\n",
      "2025-06-01 14:52:26 [INFO]: epoch 245: training loss 0.0921\n",
      "2025-06-01 14:52:26 [INFO]: epoch 246: training loss 0.0968\n",
      "2025-06-01 14:52:26 [INFO]: epoch 247: training loss 0.0976\n",
      "2025-06-01 14:52:26 [INFO]: epoch 248: training loss 0.0924\n",
      "2025-06-01 14:52:26 [INFO]: epoch 249: training loss 0.0867\n",
      "2025-06-01 14:52:26 [INFO]: epoch 250: training loss 0.0914\n",
      "2025-06-01 14:52:26 [INFO]: epoch 251: training loss 0.0898\n",
      "2025-06-01 14:52:26 [INFO]: epoch 252: training loss 0.0961\n",
      "2025-06-01 14:52:26 [INFO]: epoch 253: training loss 0.0990\n",
      "2025-06-01 14:52:26 [INFO]: epoch 254: training loss 0.0924\n",
      "2025-06-01 14:52:26 [INFO]: epoch 255: training loss 0.0892\n",
      "2025-06-01 14:52:26 [INFO]: epoch 256: training loss 0.0977\n",
      "2025-06-01 14:52:26 [INFO]: epoch 257: training loss 0.1134\n",
      "2025-06-01 14:52:26 [INFO]: epoch 258: training loss 0.0839\n",
      "2025-06-01 14:52:26 [INFO]: epoch 259: training loss 0.0866\n",
      "2025-06-01 14:52:26 [INFO]: epoch 260: training loss 0.1109\n",
      "2025-06-01 14:52:26 [INFO]: epoch 261: training loss 0.1121\n",
      "2025-06-01 14:52:26 [INFO]: epoch 262: training loss 0.0834\n",
      "2025-06-01 14:52:26 [INFO]: epoch 263: training loss 0.0880\n",
      "2025-06-01 14:52:26 [INFO]: epoch 264: training loss 0.0885\n",
      "2025-06-01 14:52:26 [INFO]: epoch 265: training loss 0.0906\n",
      "2025-06-01 14:52:26 [INFO]: epoch 266: training loss 0.0868\n",
      "2025-06-01 14:52:26 [INFO]: epoch 267: training loss 0.0852\n",
      "2025-06-01 14:52:26 [INFO]: epoch 268: training loss 0.0822\n",
      "2025-06-01 14:52:26 [INFO]: epoch 269: training loss 0.0858\n",
      "2025-06-01 14:52:26 [INFO]: epoch 270: training loss 0.1042\n",
      "2025-06-01 14:52:26 [INFO]: epoch 271: training loss 0.0962\n",
      "2025-06-01 14:52:26 [INFO]: epoch 272: training loss 0.0808\n",
      "2025-06-01 14:52:26 [INFO]: epoch 273: training loss 0.0811\n",
      "2025-06-01 14:52:26 [INFO]: epoch 274: training loss 0.0762\n",
      "2025-06-01 14:52:26 [INFO]: epoch 275: training loss 0.0821\n",
      "2025-06-01 14:52:26 [INFO]: epoch 276: training loss 0.0808\n",
      "2025-06-01 14:52:26 [INFO]: epoch 277: training loss 0.0781\n",
      "2025-06-01 14:52:26 [INFO]: epoch 278: training loss 0.0770\n",
      "2025-06-01 14:52:26 [INFO]: epoch 279: training loss 0.0681\n",
      "2025-06-01 14:52:26 [INFO]: epoch 280: training loss 0.0801\n",
      "2025-06-01 14:52:26 [INFO]: epoch 281: training loss 0.0800\n",
      "2025-06-01 14:52:26 [INFO]: epoch 282: training loss 0.0711\n",
      "2025-06-01 14:52:26 [INFO]: epoch 283: training loss 0.0739\n",
      "2025-06-01 14:52:26 [INFO]: epoch 284: training loss 0.0701\n",
      "2025-06-01 14:52:26 [INFO]: epoch 285: training loss 0.0800\n",
      "2025-06-01 14:52:26 [INFO]: epoch 286: training loss 0.0762\n",
      "2025-06-01 14:52:26 [INFO]: epoch 287: training loss 0.0764\n",
      "2025-06-01 14:52:26 [INFO]: epoch 288: training loss 0.0700\n",
      "2025-06-01 14:52:26 [INFO]: epoch 289: training loss 0.0914\n",
      "2025-06-01 14:52:26 [INFO]: epoch 290: training loss 0.0832\n",
      "2025-06-01 14:52:26 [INFO]: epoch 291: training loss 0.0790\n",
      "2025-06-01 14:52:26 [INFO]: epoch 292: training loss 0.0746\n",
      "2025-06-01 14:52:27 [INFO]: epoch 293: training loss 0.0649\n",
      "2025-06-01 14:52:27 [INFO]: epoch 294: training loss 0.0734\n",
      "2025-06-01 14:52:27 [INFO]: epoch 295: training loss 0.0817\n",
      "2025-06-01 14:52:27 [INFO]: epoch 296: training loss 0.0844\n",
      "2025-06-01 14:52:27 [INFO]: epoch 297: training loss 0.0716\n",
      "2025-06-01 14:52:27 [INFO]: epoch 298: training loss 0.0644\n",
      "2025-06-01 14:52:27 [INFO]: epoch 299: training loss 0.0740\n",
      "2025-06-01 14:52:27 [INFO]: epoch 300: training loss 0.0710\n",
      "2025-06-01 14:52:27 [INFO]: epoch 301: training loss 0.0752\n",
      "2025-06-01 14:52:27 [INFO]: epoch 302: training loss 0.0661\n",
      "2025-06-01 14:52:27 [INFO]: epoch 303: training loss 0.0679\n",
      "2025-06-01 14:52:27 [INFO]: epoch 304: training loss 0.0671\n",
      "2025-06-01 14:52:27 [INFO]: epoch 305: training loss 0.0634\n",
      "2025-06-01 14:52:27 [INFO]: epoch 306: training loss 0.0614\n",
      "2025-06-01 14:52:27 [INFO]: epoch 307: training loss 0.0601\n",
      "2025-06-01 14:52:27 [INFO]: epoch 308: training loss 0.0656\n",
      "2025-06-01 14:52:27 [INFO]: epoch 309: training loss 0.0534\n",
      "2025-06-01 14:52:27 [INFO]: epoch 310: training loss 0.0659\n",
      "2025-06-01 14:52:27 [INFO]: epoch 311: training loss 0.0581\n",
      "2025-06-01 14:52:27 [INFO]: epoch 312: training loss 0.0531\n",
      "2025-06-01 14:52:27 [INFO]: epoch 313: training loss 0.0564\n",
      "2025-06-01 14:52:27 [INFO]: epoch 314: training loss 0.0515\n",
      "2025-06-01 14:52:27 [INFO]: epoch 315: training loss 0.0566\n",
      "2025-06-01 14:52:27 [INFO]: epoch 316: training loss 0.0572\n",
      "2025-06-01 14:52:27 [INFO]: epoch 317: training loss 0.0553\n",
      "2025-06-01 14:52:27 [INFO]: epoch 318: training loss 0.0596\n",
      "2025-06-01 14:52:27 [INFO]: epoch 319: training loss 0.0623\n",
      "2025-06-01 14:52:27 [INFO]: epoch 320: training loss 0.0506\n",
      "2025-06-01 14:52:27 [INFO]: epoch 321: training loss 0.0607\n",
      "2025-06-01 14:52:27 [INFO]: epoch 322: training loss 0.0540\n",
      "2025-06-01 14:52:27 [INFO]: epoch 323: training loss 0.0538\n",
      "2025-06-01 14:52:27 [INFO]: epoch 324: training loss 0.0558\n",
      "2025-06-01 14:52:27 [INFO]: epoch 325: training loss 0.0508\n",
      "2025-06-01 14:52:27 [INFO]: epoch 326: training loss 0.0522\n",
      "2025-06-01 14:52:27 [INFO]: epoch 327: training loss 0.0632\n",
      "2025-06-01 14:52:27 [INFO]: epoch 328: training loss 0.0477\n",
      "2025-06-01 14:52:27 [INFO]: epoch 329: training loss 0.0554\n",
      "2025-06-01 14:52:27 [INFO]: epoch 330: training loss 0.0546\n",
      "2025-06-01 14:52:27 [INFO]: epoch 331: training loss 0.0504\n",
      "2025-06-01 14:52:27 [INFO]: epoch 332: training loss 0.0653\n",
      "2025-06-01 14:52:27 [INFO]: epoch 333: training loss 0.0620\n",
      "2025-06-01 14:52:27 [INFO]: epoch 334: training loss 0.0452\n",
      "2025-06-01 14:52:27 [INFO]: epoch 335: training loss 0.0514\n",
      "2025-06-01 14:52:27 [INFO]: epoch 336: training loss 0.0555\n",
      "2025-06-01 14:52:27 [INFO]: epoch 337: training loss 0.0467\n",
      "2025-06-01 14:52:27 [INFO]: epoch 338: training loss 0.0509\n",
      "2025-06-01 14:52:27 [INFO]: epoch 339: training loss 0.0504\n",
      "2025-06-01 14:52:27 [INFO]: epoch 340: training loss 0.0475\n",
      "2025-06-01 14:52:27 [INFO]: epoch 341: training loss 0.0554\n",
      "2025-06-01 14:52:27 [INFO]: epoch 342: training loss 0.0571\n",
      "2025-06-01 14:52:27 [INFO]: epoch 343: training loss 0.0434\n",
      "2025-06-01 14:52:27 [INFO]: epoch 344: training loss 0.0526\n",
      "2025-06-01 14:52:27 [INFO]: epoch 345: training loss 0.0480\n",
      "2025-06-01 14:52:27 [INFO]: epoch 346: training loss 0.0433\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-01 14:52:27 [INFO]: epoch 347: training loss 0.0507\n",
      "2025-06-01 14:52:27 [INFO]: epoch 348: training loss 0.0504\n",
      "2025-06-01 14:52:27 [INFO]: epoch 349: training loss 0.0428\n",
      "2025-06-01 14:52:27 [INFO]: epoch 350: training loss 0.0434\n",
      "2025-06-01 14:52:27 [INFO]: epoch 351: training loss 0.0457\n",
      "2025-06-01 14:52:27 [INFO]: epoch 352: training loss 0.0452\n",
      "2025-06-01 14:52:27 [INFO]: epoch 353: training loss 0.0444\n",
      "2025-06-01 14:52:27 [INFO]: epoch 354: training loss 0.0455\n",
      "2025-06-01 14:52:27 [INFO]: epoch 355: training loss 0.0504\n",
      "2025-06-01 14:52:27 [INFO]: epoch 356: training loss 0.0422\n",
      "2025-06-01 14:52:27 [INFO]: epoch 357: training loss 0.0467\n",
      "2025-06-01 14:52:27 [INFO]: epoch 358: training loss 0.0371\n",
      "2025-06-01 14:52:27 [INFO]: epoch 359: training loss 0.0378\n",
      "2025-06-01 14:52:27 [INFO]: epoch 360: training loss 0.0413\n",
      "2025-06-01 14:52:27 [INFO]: epoch 361: training loss 0.0415\n",
      "2025-06-01 14:52:27 [INFO]: epoch 362: training loss 0.0401\n",
      "2025-06-01 14:52:27 [INFO]: epoch 363: training loss 0.0387\n",
      "2025-06-01 14:52:27 [INFO]: epoch 364: training loss 0.0481\n",
      "2025-06-01 14:52:27 [INFO]: epoch 365: training loss 0.0431\n",
      "2025-06-01 14:52:27 [INFO]: epoch 366: training loss 0.0396\n",
      "2025-06-01 14:52:27 [INFO]: epoch 367: training loss 0.0410\n",
      "2025-06-01 14:52:27 [INFO]: epoch 368: training loss 0.0378\n",
      "2025-06-01 14:52:27 [INFO]: epoch 369: training loss 0.0347\n",
      "2025-06-01 14:52:28 [INFO]: epoch 370: training loss 0.0437\n",
      "2025-06-01 14:52:28 [INFO]: epoch 371: training loss 0.0501\n",
      "2025-06-01 14:52:28 [INFO]: epoch 372: training loss 0.0398\n",
      "2025-06-01 14:52:28 [INFO]: epoch 373: training loss 0.0389\n",
      "2025-06-01 14:52:28 [INFO]: epoch 374: training loss 0.0455\n",
      "2025-06-01 14:52:28 [INFO]: epoch 375: training loss 0.0381\n",
      "2025-06-01 14:52:28 [INFO]: epoch 376: training loss 0.0380\n",
      "2025-06-01 14:52:28 [INFO]: epoch 377: training loss 0.0418\n",
      "2025-06-01 14:52:28 [INFO]: epoch 378: training loss 0.0509\n",
      "2025-06-01 14:52:28 [INFO]: epoch 379: training loss 0.0428\n",
      "2025-06-01 14:52:28 [INFO]: epoch 380: training loss 0.0416\n",
      "2025-06-01 14:52:28 [INFO]: epoch 381: training loss 0.0551\n",
      "2025-06-01 14:52:28 [INFO]: epoch 382: training loss 0.0373\n",
      "2025-06-01 14:52:28 [INFO]: epoch 383: training loss 0.0325\n",
      "2025-06-01 14:52:28 [INFO]: epoch 384: training loss 0.0398\n",
      "2025-06-01 14:52:28 [INFO]: epoch 385: training loss 0.0342\n",
      "2025-06-01 14:52:28 [INFO]: epoch 386: training loss 0.0301\n",
      "2025-06-01 14:52:28 [INFO]: epoch 387: training loss 0.0301\n",
      "2025-06-01 14:52:28 [INFO]: epoch 388: training loss 0.0365\n",
      "2025-06-01 14:52:28 [INFO]: epoch 389: training loss 0.0403\n",
      "2025-06-01 14:52:28 [INFO]: epoch 390: training loss 0.0303\n",
      "2025-06-01 14:52:28 [INFO]: epoch 391: training loss 0.0369\n",
      "2025-06-01 14:52:28 [INFO]: epoch 392: training loss 0.0387\n",
      "2025-06-01 14:52:28 [INFO]: epoch 393: training loss 0.0362\n",
      "2025-06-01 14:52:28 [INFO]: epoch 394: training loss 0.0447\n",
      "2025-06-01 14:52:28 [INFO]: epoch 395: training loss 0.0332\n",
      "2025-06-01 14:52:28 [INFO]: epoch 396: training loss 0.0343\n",
      "2025-06-01 14:52:28 [INFO]: epoch 397: training loss 0.0450\n",
      "2025-06-01 14:52:28 [INFO]: epoch 398: training loss 0.0391\n",
      "2025-06-01 14:52:28 [INFO]: epoch 399: training loss 0.0439\n",
      "2025-06-01 14:52:28 [INFO]: epoch 400: training loss 0.0499\n",
      "2025-06-01 14:52:28 [INFO]: epoch 401: training loss 0.0435\n",
      "2025-06-01 14:52:28 [INFO]: epoch 402: training loss 0.0414\n",
      "2025-06-01 14:52:28 [INFO]: epoch 403: training loss 0.0568\n",
      "2025-06-01 14:52:28 [INFO]: epoch 404: training loss 0.0296\n",
      "2025-06-01 14:52:28 [INFO]: epoch 405: training loss 0.0329\n",
      "2025-06-01 14:52:28 [INFO]: epoch 406: training loss 0.0410\n",
      "2025-06-01 14:52:28 [INFO]: epoch 407: training loss 0.0380\n",
      "2025-06-01 14:52:28 [INFO]: epoch 408: training loss 0.0309\n",
      "2025-06-01 14:52:28 [INFO]: epoch 409: training loss 0.0413\n",
      "2025-06-01 14:52:28 [INFO]: epoch 410: training loss 0.0307\n",
      "2025-06-01 14:52:28 [INFO]: epoch 411: training loss 0.0344\n",
      "2025-06-01 14:52:28 [INFO]: epoch 412: training loss 0.0388\n",
      "2025-06-01 14:52:28 [INFO]: epoch 413: training loss 0.0347\n",
      "2025-06-01 14:52:28 [INFO]: epoch 414: training loss 0.0330\n",
      "2025-06-01 14:52:28 [INFO]: epoch 415: training loss 0.0356\n",
      "2025-06-01 14:52:28 [INFO]: epoch 416: training loss 0.0350\n",
      "2025-06-01 14:52:28 [INFO]: epoch 417: training loss 0.0332\n",
      "2025-06-01 14:52:28 [INFO]: epoch 418: training loss 0.0374\n",
      "2025-06-01 14:52:28 [INFO]: epoch 419: training loss 0.0355\n",
      "2025-06-01 14:52:28 [INFO]: epoch 420: training loss 0.0385\n",
      "2025-06-01 14:52:28 [INFO]: epoch 421: training loss 0.0305\n",
      "2025-06-01 14:52:28 [INFO]: epoch 422: training loss 0.0341\n",
      "2025-06-01 14:52:28 [INFO]: epoch 423: training loss 0.0330\n",
      "2025-06-01 14:52:28 [INFO]: epoch 424: training loss 0.0307\n",
      "2025-06-01 14:52:28 [INFO]: epoch 425: training loss 0.0288\n",
      "2025-06-01 14:52:28 [INFO]: epoch 426: training loss 0.0311\n",
      "2025-06-01 14:52:28 [INFO]: epoch 427: training loss 0.0332\n",
      "2025-06-01 14:52:28 [INFO]: epoch 428: training loss 0.0312\n",
      "2025-06-01 14:52:28 [INFO]: epoch 429: training loss 0.0301\n",
      "2025-06-01 14:52:28 [INFO]: epoch 430: training loss 0.0320\n",
      "2025-06-01 14:52:28 [INFO]: epoch 431: training loss 0.0290\n",
      "2025-06-01 14:52:28 [INFO]: epoch 432: training loss 0.0306\n",
      "2025-06-01 14:52:28 [INFO]: epoch 433: training loss 0.0321\n",
      "2025-06-01 14:52:28 [INFO]: epoch 434: training loss 0.0370\n",
      "2025-06-01 14:52:28 [INFO]: epoch 435: training loss 0.0262\n",
      "2025-06-01 14:52:28 [INFO]: epoch 436: training loss 0.0270\n",
      "2025-06-01 14:52:28 [INFO]: epoch 437: training loss 0.0295\n",
      "2025-06-01 14:52:28 [INFO]: epoch 438: training loss 0.0262\n",
      "2025-06-01 14:52:28 [INFO]: epoch 439: training loss 0.0274\n",
      "2025-06-01 14:52:28 [INFO]: epoch 440: training loss 0.0306\n",
      "2025-06-01 14:52:28 [INFO]: epoch 441: training loss 0.0254\n",
      "2025-06-01 14:52:28 [INFO]: epoch 442: training loss 0.0291\n",
      "2025-06-01 14:52:28 [INFO]: epoch 443: training loss 0.0306\n",
      "2025-06-01 14:52:28 [INFO]: epoch 444: training loss 0.0374\n",
      "2025-06-01 14:52:28 [INFO]: epoch 445: training loss 0.0341\n",
      "2025-06-01 14:52:28 [INFO]: epoch 446: training loss 0.0283\n",
      "2025-06-01 14:52:29 [INFO]: epoch 447: training loss 0.0371\n",
      "2025-06-01 14:52:29 [INFO]: epoch 448: training loss 0.0313\n",
      "2025-06-01 14:52:29 [INFO]: epoch 449: training loss 0.0368\n",
      "2025-06-01 14:52:29 [INFO]: epoch 450: training loss 0.0325\n",
      "2025-06-01 14:52:29 [INFO]: epoch 451: training loss 0.0230\n",
      "2025-06-01 14:52:29 [INFO]: epoch 452: training loss 0.0310\n",
      "2025-06-01 14:52:29 [INFO]: epoch 453: training loss 0.0299\n",
      "2025-06-01 14:52:29 [INFO]: epoch 454: training loss 0.0322\n",
      "2025-06-01 14:52:29 [INFO]: epoch 455: training loss 0.0325\n",
      "2025-06-01 14:52:29 [INFO]: epoch 456: training loss 0.0285\n",
      "2025-06-01 14:52:29 [INFO]: epoch 457: training loss 0.0283\n",
      "2025-06-01 14:52:29 [INFO]: epoch 458: training loss 0.0320\n",
      "2025-06-01 14:52:29 [INFO]: epoch 459: training loss 0.0281\n",
      "2025-06-01 14:52:29 [INFO]: epoch 460: training loss 0.0349\n",
      "2025-06-01 14:52:29 [INFO]: epoch 461: training loss 0.0325\n",
      "2025-06-01 14:52:29 [INFO]: epoch 462: training loss 0.0283\n",
      "2025-06-01 14:52:29 [INFO]: epoch 463: training loss 0.0284\n",
      "2025-06-01 14:52:29 [INFO]: epoch 464: training loss 0.0240\n",
      "2025-06-01 14:52:29 [INFO]: epoch 465: training loss 0.0305\n",
      "2025-06-01 14:52:29 [INFO]: epoch 466: training loss 0.0274\n",
      "2025-06-01 14:52:29 [INFO]: epoch 467: training loss 0.0307\n",
      "2025-06-01 14:52:29 [INFO]: epoch 468: training loss 0.0275\n",
      "2025-06-01 14:52:29 [INFO]: epoch 469: training loss 0.0312\n",
      "2025-06-01 14:52:29 [INFO]: epoch 470: training loss 0.0283\n",
      "2025-06-01 14:52:29 [INFO]: epoch 471: training loss 0.0310\n",
      "2025-06-01 14:52:29 [INFO]: epoch 472: training loss 0.0254\n",
      "2025-06-01 14:52:29 [INFO]: epoch 473: training loss 0.0321\n",
      "2025-06-01 14:52:29 [INFO]: epoch 474: training loss 0.0311\n",
      "2025-06-01 14:52:29 [INFO]: epoch 475: training loss 0.0356\n",
      "2025-06-01 14:52:29 [INFO]: epoch 476: training loss 0.0304\n",
      "2025-06-01 14:52:29 [INFO]: epoch 477: training loss 0.0276\n",
      "2025-06-01 14:52:29 [INFO]: epoch 478: training loss 0.0337\n",
      "2025-06-01 14:52:29 [INFO]: epoch 479: training loss 0.0292\n",
      "2025-06-01 14:52:29 [INFO]: epoch 480: training loss 0.0326\n",
      "2025-06-01 14:52:29 [INFO]: epoch 481: training loss 0.0267\n",
      "2025-06-01 14:52:29 [INFO]: epoch 482: training loss 0.0304\n",
      "2025-06-01 14:52:29 [INFO]: epoch 483: training loss 0.0292\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-01 14:52:29 [INFO]: epoch 484: training loss 0.0260\n",
      "2025-06-01 14:52:29 [INFO]: epoch 485: training loss 0.0271\n",
      "2025-06-01 14:52:29 [INFO]: epoch 486: training loss 0.0262\n",
      "2025-06-01 14:52:29 [INFO]: epoch 487: training loss 0.0279\n",
      "2025-06-01 14:52:29 [INFO]: epoch 488: training loss 0.0263\n",
      "2025-06-01 14:52:29 [INFO]: epoch 489: training loss 0.0295\n",
      "2025-06-01 14:52:29 [INFO]: epoch 490: training loss 0.0258\n",
      "2025-06-01 14:52:29 [INFO]: epoch 491: training loss 0.0271\n",
      "2025-06-01 14:52:29 [INFO]: epoch 492: training loss 0.0293\n",
      "2025-06-01 14:52:29 [INFO]: epoch 493: training loss 0.0236\n",
      "2025-06-01 14:52:29 [INFO]: epoch 494: training loss 0.0226\n",
      "2025-06-01 14:52:29 [INFO]: epoch 495: training loss 0.0248\n",
      "2025-06-01 14:52:29 [INFO]: epoch 496: training loss 0.0253\n",
      "2025-06-01 14:52:29 [INFO]: epoch 497: training loss 0.0239\n",
      "2025-06-01 14:52:29 [INFO]: epoch 498: training loss 0.0244\n",
      "2025-06-01 14:52:29 [INFO]: epoch 499: training loss 0.0278\n",
      "2025-06-01 14:52:29 [INFO]: epoch 500: training loss 0.0238\n",
      "2025-06-01 14:52:29 [INFO]: epoch 501: training loss 0.0241\n",
      "2025-06-01 14:52:29 [INFO]: epoch 502: training loss 0.0271\n",
      "2025-06-01 14:52:29 [INFO]: epoch 503: training loss 0.0251\n",
      "2025-06-01 14:52:29 [INFO]: epoch 504: training loss 0.0241\n",
      "2025-06-01 14:52:29 [INFO]: epoch 505: training loss 0.0234\n",
      "2025-06-01 14:52:29 [INFO]: epoch 506: training loss 0.0282\n",
      "2025-06-01 14:52:29 [INFO]: epoch 507: training loss 0.0272\n",
      "2025-06-01 14:52:29 [INFO]: epoch 508: training loss 0.0221\n",
      "2025-06-01 14:52:29 [INFO]: epoch 509: training loss 0.0264\n",
      "2025-06-01 14:52:29 [INFO]: epoch 510: training loss 0.0256\n",
      "2025-06-01 14:52:29 [INFO]: epoch 511: training loss 0.0258\n",
      "2025-06-01 14:52:29 [INFO]: epoch 512: training loss 0.0280\n",
      "2025-06-01 14:52:29 [INFO]: epoch 513: training loss 0.0276\n",
      "2025-06-01 14:52:29 [INFO]: epoch 514: training loss 0.0256\n",
      "2025-06-01 14:52:29 [INFO]: epoch 515: training loss 0.0270\n",
      "2025-06-01 14:52:29 [INFO]: epoch 516: training loss 0.0224\n",
      "2025-06-01 14:52:29 [INFO]: epoch 517: training loss 0.0229\n",
      "2025-06-01 14:52:29 [INFO]: epoch 518: training loss 0.0234\n",
      "2025-06-01 14:52:29 [INFO]: epoch 519: training loss 0.0264\n",
      "2025-06-01 14:52:29 [INFO]: epoch 520: training loss 0.0240\n",
      "2025-06-01 14:52:29 [INFO]: epoch 521: training loss 0.0307\n",
      "2025-06-01 14:52:29 [INFO]: epoch 522: training loss 0.0226\n",
      "2025-06-01 14:52:30 [INFO]: epoch 523: training loss 0.0305\n",
      "2025-06-01 14:52:30 [INFO]: epoch 524: training loss 0.0269\n",
      "2025-06-01 14:52:30 [INFO]: epoch 525: training loss 0.0244\n",
      "2025-06-01 14:52:30 [INFO]: epoch 526: training loss 0.0247\n",
      "2025-06-01 14:52:30 [INFO]: epoch 527: training loss 0.0299\n",
      "2025-06-01 14:52:30 [INFO]: epoch 528: training loss 0.0244\n",
      "2025-06-01 14:52:30 [INFO]: epoch 529: training loss 0.0268\n",
      "2025-06-01 14:52:30 [INFO]: epoch 530: training loss 0.0233\n",
      "2025-06-01 14:52:30 [INFO]: epoch 531: training loss 0.0227\n",
      "2025-06-01 14:52:30 [INFO]: epoch 532: training loss 0.0195\n",
      "2025-06-01 14:52:30 [INFO]: epoch 533: training loss 0.0290\n",
      "2025-06-01 14:52:30 [INFO]: epoch 534: training loss 0.0231\n",
      "2025-06-01 14:52:30 [INFO]: epoch 535: training loss 0.0285\n",
      "2025-06-01 14:52:30 [INFO]: epoch 536: training loss 0.0357\n",
      "2025-06-01 14:52:30 [INFO]: epoch 537: training loss 0.0241\n",
      "2025-06-01 14:52:30 [INFO]: epoch 538: training loss 0.0309\n",
      "2025-06-01 14:52:30 [INFO]: epoch 539: training loss 0.0338\n",
      "2025-06-01 14:52:30 [INFO]: epoch 540: training loss 0.0289\n",
      "2025-06-01 14:52:30 [INFO]: epoch 541: training loss 0.0336\n",
      "2025-06-01 14:52:30 [INFO]: epoch 542: training loss 0.0300\n",
      "2025-06-01 14:52:30 [INFO]: epoch 543: training loss 0.0275\n",
      "2025-06-01 14:52:30 [INFO]: epoch 544: training loss 0.0315\n",
      "2025-06-01 14:52:30 [INFO]: epoch 545: training loss 0.0332\n",
      "2025-06-01 14:52:30 [INFO]: epoch 546: training loss 0.0279\n",
      "2025-06-01 14:52:30 [INFO]: epoch 547: training loss 0.0380\n",
      "2025-06-01 14:52:30 [INFO]: epoch 548: training loss 0.0296\n",
      "2025-06-01 14:52:30 [INFO]: epoch 549: training loss 0.0279\n",
      "2025-06-01 14:52:30 [INFO]: epoch 550: training loss 0.0372\n",
      "2025-06-01 14:52:30 [INFO]: epoch 551: training loss 0.0283\n",
      "2025-06-01 14:52:30 [INFO]: epoch 552: training loss 0.0295\n",
      "2025-06-01 14:52:30 [INFO]: epoch 553: training loss 0.0391\n",
      "2025-06-01 14:52:30 [INFO]: epoch 554: training loss 0.0229\n",
      "2025-06-01 14:52:30 [INFO]: epoch 555: training loss 0.0240\n",
      "2025-06-01 14:52:30 [INFO]: epoch 556: training loss 0.0326\n",
      "2025-06-01 14:52:30 [INFO]: epoch 557: training loss 0.0356\n",
      "2025-06-01 14:52:30 [INFO]: epoch 558: training loss 0.0230\n",
      "2025-06-01 14:52:30 [INFO]: epoch 559: training loss 0.0312\n",
      "2025-06-01 14:52:30 [INFO]: epoch 560: training loss 0.0323\n",
      "2025-06-01 14:52:30 [INFO]: epoch 561: training loss 0.0275\n",
      "2025-06-01 14:52:30 [INFO]: epoch 562: training loss 0.0295\n",
      "2025-06-01 14:52:30 [INFO]: epoch 563: training loss 0.0241\n",
      "2025-06-01 14:52:30 [INFO]: epoch 564: training loss 0.0260\n",
      "2025-06-01 14:52:30 [INFO]: epoch 565: training loss 0.0255\n",
      "2025-06-01 14:52:30 [INFO]: epoch 566: training loss 0.0237\n",
      "2025-06-01 14:52:30 [INFO]: epoch 567: training loss 0.0218\n",
      "2025-06-01 14:52:30 [INFO]: epoch 568: training loss 0.0271\n",
      "2025-06-01 14:52:30 [INFO]: epoch 569: training loss 0.0242\n",
      "2025-06-01 14:52:30 [INFO]: epoch 570: training loss 0.0216\n",
      "2025-06-01 14:52:30 [INFO]: epoch 571: training loss 0.0287\n",
      "2025-06-01 14:52:30 [INFO]: epoch 572: training loss 0.0241\n",
      "2025-06-01 14:52:30 [INFO]: epoch 573: training loss 0.0280\n",
      "2025-06-01 14:52:30 [INFO]: epoch 574: training loss 0.0356\n",
      "2025-06-01 14:52:30 [INFO]: epoch 575: training loss 0.0246\n",
      "2025-06-01 14:52:30 [INFO]: epoch 576: training loss 0.0237\n",
      "2025-06-01 14:52:30 [INFO]: epoch 577: training loss 0.0302\n",
      "2025-06-01 14:52:30 [INFO]: epoch 578: training loss 0.0243\n",
      "2025-06-01 14:52:30 [INFO]: epoch 579: training loss 0.0256\n",
      "2025-06-01 14:52:30 [INFO]: epoch 580: training loss 0.0319\n",
      "2025-06-01 14:52:30 [INFO]: epoch 581: training loss 0.0339\n",
      "2025-06-01 14:52:30 [INFO]: epoch 582: training loss 0.0248\n",
      "2025-06-01 14:52:30 [INFO]: epoch 583: training loss 0.0296\n",
      "2025-06-01 14:52:30 [INFO]: epoch 584: training loss 0.0301\n",
      "2025-06-01 14:52:30 [INFO]: epoch 585: training loss 0.0351\n",
      "2025-06-01 14:52:30 [INFO]: epoch 586: training loss 0.0269\n",
      "2025-06-01 14:52:30 [INFO]: epoch 587: training loss 0.0277\n",
      "2025-06-01 14:52:30 [INFO]: epoch 588: training loss 0.0276\n",
      "2025-06-01 14:52:30 [INFO]: epoch 589: training loss 0.0326\n",
      "2025-06-01 14:52:30 [INFO]: epoch 590: training loss 0.0262\n",
      "2025-06-01 14:52:30 [INFO]: epoch 591: training loss 0.0233\n",
      "2025-06-01 14:52:30 [INFO]: epoch 592: training loss 0.0242\n",
      "2025-06-01 14:52:30 [INFO]: epoch 593: training loss 0.0234\n",
      "2025-06-01 14:52:30 [INFO]: epoch 594: training loss 0.0229\n",
      "2025-06-01 14:52:30 [INFO]: epoch 595: training loss 0.0250\n",
      "2025-06-01 14:52:30 [INFO]: epoch 596: training loss 0.0257\n",
      "2025-06-01 14:52:30 [INFO]: epoch 597: training loss 0.0247\n",
      "2025-06-01 14:52:30 [INFO]: epoch 598: training loss 0.0211\n",
      "2025-06-01 14:52:31 [INFO]: epoch 599: training loss 0.0262\n",
      "2025-06-01 14:52:31 [INFO]: epoch 600: training loss 0.0292\n",
      "2025-06-01 14:52:31 [INFO]: epoch 601: training loss 0.0256\n",
      "2025-06-01 14:52:31 [INFO]: epoch 602: training loss 0.0231\n",
      "2025-06-01 14:52:31 [INFO]: epoch 603: training loss 0.0269\n",
      "2025-06-01 14:52:31 [INFO]: epoch 604: training loss 0.0240\n",
      "2025-06-01 14:52:31 [INFO]: epoch 605: training loss 0.0244\n",
      "2025-06-01 14:52:31 [INFO]: epoch 606: training loss 0.0244\n",
      "2025-06-01 14:52:31 [INFO]: epoch 607: training loss 0.0188\n",
      "2025-06-01 14:52:31 [INFO]: epoch 608: training loss 0.0228\n",
      "2025-06-01 14:52:31 [INFO]: epoch 609: training loss 0.0268\n",
      "2025-06-01 14:52:31 [INFO]: epoch 610: training loss 0.0268\n",
      "2025-06-01 14:52:31 [INFO]: epoch 611: training loss 0.0246\n",
      "2025-06-01 14:52:31 [INFO]: epoch 612: training loss 0.0267\n",
      "2025-06-01 14:52:31 [INFO]: epoch 613: training loss 0.0224\n",
      "2025-06-01 14:52:31 [INFO]: epoch 614: training loss 0.0290\n",
      "2025-06-01 14:52:31 [INFO]: epoch 615: training loss 0.0296\n",
      "2025-06-01 14:52:31 [INFO]: epoch 616: training loss 0.0206\n",
      "2025-06-01 14:52:31 [INFO]: epoch 617: training loss 0.0268\n",
      "2025-06-01 14:52:31 [INFO]: epoch 618: training loss 0.0298\n",
      "2025-06-01 14:52:31 [INFO]: epoch 619: training loss 0.0236\n",
      "2025-06-01 14:52:31 [INFO]: epoch 620: training loss 0.0282\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-01 14:52:31 [INFO]: epoch 621: training loss 0.0285\n",
      "2025-06-01 14:52:31 [INFO]: epoch 622: training loss 0.0198\n",
      "2025-06-01 14:52:31 [INFO]: epoch 623: training loss 0.0282\n",
      "2025-06-01 14:52:31 [INFO]: epoch 624: training loss 0.0249\n",
      "2025-06-01 14:52:31 [INFO]: epoch 625: training loss 0.0227\n",
      "2025-06-01 14:52:31 [INFO]: epoch 626: training loss 0.0240\n",
      "2025-06-01 14:52:31 [INFO]: epoch 627: training loss 0.0247\n",
      "2025-06-01 14:52:31 [INFO]: epoch 628: training loss 0.0257\n",
      "2025-06-01 14:52:31 [INFO]: epoch 629: training loss 0.0264\n",
      "2025-06-01 14:52:31 [INFO]: epoch 630: training loss 0.0232\n",
      "2025-06-01 14:52:31 [INFO]: epoch 631: training loss 0.0220\n",
      "2025-06-01 14:52:31 [INFO]: epoch 632: training loss 0.0282\n",
      "2025-06-01 14:52:31 [INFO]: epoch 633: training loss 0.0236\n",
      "2025-06-01 14:52:31 [INFO]: epoch 634: training loss 0.0204\n",
      "2025-06-01 14:52:31 [INFO]: epoch 635: training loss 0.0239\n",
      "2025-06-01 14:52:31 [INFO]: epoch 636: training loss 0.0268\n",
      "2025-06-01 14:52:31 [INFO]: epoch 637: training loss 0.0238\n",
      "2025-06-01 14:52:31 [INFO]: epoch 638: training loss 0.0260\n",
      "2025-06-01 14:52:31 [INFO]: epoch 639: training loss 0.0219\n",
      "2025-06-01 14:52:31 [INFO]: epoch 640: training loss 0.0266\n",
      "2025-06-01 14:52:31 [INFO]: epoch 641: training loss 0.0239\n",
      "2025-06-01 14:52:31 [INFO]: epoch 642: training loss 0.0211\n",
      "2025-06-01 14:52:31 [INFO]: epoch 643: training loss 0.0230\n",
      "2025-06-01 14:52:31 [INFO]: epoch 644: training loss 0.0241\n",
      "2025-06-01 14:52:31 [INFO]: epoch 645: training loss 0.0212\n",
      "2025-06-01 14:52:31 [INFO]: epoch 646: training loss 0.0274\n",
      "2025-06-01 14:52:31 [INFO]: epoch 647: training loss 0.0348\n",
      "2025-06-01 14:52:31 [INFO]: epoch 648: training loss 0.0195\n",
      "2025-06-01 14:52:31 [INFO]: epoch 649: training loss 0.0220\n",
      "2025-06-01 14:52:31 [INFO]: epoch 650: training loss 0.0243\n",
      "2025-06-01 14:52:31 [INFO]: epoch 651: training loss 0.0234\n",
      "2025-06-01 14:52:31 [INFO]: epoch 652: training loss 0.0224\n",
      "2025-06-01 14:52:31 [INFO]: epoch 653: training loss 0.0226\n",
      "2025-06-01 14:52:31 [INFO]: epoch 654: training loss 0.0254\n",
      "2025-06-01 14:52:31 [INFO]: epoch 655: training loss 0.0286\n",
      "2025-06-01 14:52:31 [INFO]: epoch 656: training loss 0.0257\n",
      "2025-06-01 14:52:31 [INFO]: epoch 657: training loss 0.0232\n",
      "2025-06-01 14:52:31 [INFO]: epoch 658: training loss 0.0269\n",
      "2025-06-01 14:52:31 [INFO]: epoch 659: training loss 0.0247\n",
      "2025-06-01 14:52:31 [INFO]: epoch 660: training loss 0.0200\n",
      "2025-06-01 14:52:31 [INFO]: epoch 661: training loss 0.0310\n",
      "2025-06-01 14:52:31 [INFO]: epoch 662: training loss 0.0233\n",
      "2025-06-01 14:52:31 [INFO]: epoch 663: training loss 0.0244\n",
      "2025-06-01 14:52:31 [INFO]: epoch 664: training loss 0.0319\n",
      "2025-06-01 14:52:31 [INFO]: epoch 665: training loss 0.0277\n",
      "2025-06-01 14:52:31 [INFO]: epoch 666: training loss 0.0217\n",
      "2025-06-01 14:52:31 [INFO]: epoch 667: training loss 0.0267\n",
      "2025-06-01 14:52:31 [INFO]: epoch 668: training loss 0.0220\n",
      "2025-06-01 14:52:31 [INFO]: epoch 669: training loss 0.0253\n",
      "2025-06-01 14:52:31 [INFO]: epoch 670: training loss 0.0274\n",
      "2025-06-01 14:52:31 [INFO]: epoch 671: training loss 0.0250\n",
      "2025-06-01 14:52:31 [INFO]: epoch 672: training loss 0.0241\n",
      "2025-06-01 14:52:31 [INFO]: epoch 673: training loss 0.0286\n",
      "2025-06-01 14:52:31 [INFO]: epoch 674: training loss 0.0189\n",
      "2025-06-01 14:52:32 [INFO]: epoch 675: training loss 0.0250\n",
      "2025-06-01 14:52:32 [INFO]: epoch 676: training loss 0.0269\n",
      "2025-06-01 14:52:32 [INFO]: epoch 677: training loss 0.0224\n",
      "2025-06-01 14:52:32 [INFO]: epoch 678: training loss 0.0252\n",
      "2025-06-01 14:52:32 [INFO]: epoch 679: training loss 0.0256\n",
      "2025-06-01 14:52:32 [INFO]: epoch 680: training loss 0.0261\n",
      "2025-06-01 14:52:32 [INFO]: epoch 681: training loss 0.0246\n",
      "2025-06-01 14:52:32 [INFO]: epoch 682: training loss 0.0235\n",
      "2025-06-01 14:52:32 [INFO]: epoch 683: training loss 0.0233\n",
      "2025-06-01 14:52:32 [INFO]: epoch 684: training loss 0.0259\n",
      "2025-06-01 14:52:32 [INFO]: epoch 685: training loss 0.0299\n",
      "2025-06-01 14:52:32 [INFO]: epoch 686: training loss 0.0227\n",
      "2025-06-01 14:52:32 [INFO]: epoch 687: training loss 0.0212\n",
      "2025-06-01 14:52:32 [INFO]: epoch 688: training loss 0.0254\n",
      "2025-06-01 14:52:32 [INFO]: epoch 689: training loss 0.0219\n",
      "2025-06-01 14:52:32 [INFO]: epoch 690: training loss 0.0204\n",
      "2025-06-01 14:52:32 [INFO]: epoch 691: training loss 0.0220\n",
      "2025-06-01 14:52:32 [INFO]: epoch 692: training loss 0.0235\n",
      "2025-06-01 14:52:32 [INFO]: epoch 693: training loss 0.0245\n",
      "2025-06-01 14:52:32 [INFO]: epoch 694: training loss 0.0203\n",
      "2025-06-01 14:52:32 [INFO]: epoch 695: training loss 0.0211\n",
      "2025-06-01 14:52:32 [INFO]: epoch 696: training loss 0.0248\n",
      "2025-06-01 14:52:32 [INFO]: epoch 697: training loss 0.0240\n",
      "2025-06-01 14:52:32 [INFO]: epoch 698: training loss 0.0182\n",
      "2025-06-01 14:52:32 [INFO]: epoch 699: training loss 0.0212\n",
      "2025-06-01 14:52:32 [INFO]: epoch 700: training loss 0.0215\n",
      "2025-06-01 14:52:32 [INFO]: epoch 701: training loss 0.0268\n",
      "2025-06-01 14:52:32 [INFO]: epoch 702: training loss 0.0202\n",
      "2025-06-01 14:52:32 [INFO]: epoch 703: training loss 0.0177\n",
      "2025-06-01 14:52:32 [INFO]: epoch 704: training loss 0.0202\n",
      "2025-06-01 14:52:32 [INFO]: epoch 705: training loss 0.0230\n",
      "2025-06-01 14:52:32 [INFO]: epoch 706: training loss 0.0205\n",
      "2025-06-01 14:52:32 [INFO]: epoch 707: training loss 0.0236\n",
      "2025-06-01 14:52:32 [INFO]: epoch 708: training loss 0.0235\n",
      "2025-06-01 14:52:32 [INFO]: epoch 709: training loss 0.0203\n",
      "2025-06-01 14:52:32 [INFO]: epoch 710: training loss 0.0200\n",
      "2025-06-01 14:52:32 [INFO]: epoch 711: training loss 0.0230\n",
      "2025-06-01 14:52:32 [INFO]: epoch 712: training loss 0.0221\n",
      "2025-06-01 14:52:32 [INFO]: epoch 713: training loss 0.0256\n",
      "2025-06-01 14:52:32 [INFO]: epoch 714: training loss 0.0226\n",
      "2025-06-01 14:52:32 [INFO]: epoch 715: training loss 0.0171\n",
      "2025-06-01 14:52:32 [INFO]: epoch 716: training loss 0.0227\n",
      "2025-06-01 14:52:32 [INFO]: epoch 717: training loss 0.0237\n",
      "2025-06-01 14:52:32 [INFO]: epoch 718: training loss 0.0209\n",
      "2025-06-01 14:52:32 [INFO]: epoch 719: training loss 0.0224\n",
      "2025-06-01 14:52:32 [INFO]: epoch 720: training loss 0.0219\n",
      "2025-06-01 14:52:32 [INFO]: epoch 721: training loss 0.0220\n",
      "2025-06-01 14:52:32 [INFO]: epoch 722: training loss 0.0193\n",
      "2025-06-01 14:52:32 [INFO]: epoch 723: training loss 0.0211\n",
      "2025-06-01 14:52:32 [INFO]: epoch 724: training loss 0.0222\n",
      "2025-06-01 14:52:32 [INFO]: epoch 725: training loss 0.0218\n",
      "2025-06-01 14:52:32 [INFO]: epoch 726: training loss 0.0204\n",
      "2025-06-01 14:52:32 [INFO]: epoch 727: training loss 0.0203\n",
      "2025-06-01 14:52:32 [INFO]: epoch 728: training loss 0.0210\n",
      "2025-06-01 14:52:32 [INFO]: epoch 729: training loss 0.0241\n",
      "2025-06-01 14:52:32 [INFO]: epoch 730: training loss 0.0211\n",
      "2025-06-01 14:52:32 [INFO]: epoch 731: training loss 0.0253\n",
      "2025-06-01 14:52:32 [INFO]: epoch 732: training loss 0.0206\n",
      "2025-06-01 14:52:32 [INFO]: epoch 733: training loss 0.0251\n",
      "2025-06-01 14:52:32 [INFO]: epoch 734: training loss 0.0259\n",
      "2025-06-01 14:52:32 [INFO]: epoch 735: training loss 0.0264\n",
      "2025-06-01 14:52:32 [INFO]: epoch 736: training loss 0.0255\n",
      "2025-06-01 14:52:32 [INFO]: epoch 737: training loss 0.0241\n",
      "2025-06-01 14:52:32 [INFO]: epoch 738: training loss 0.0259\n",
      "2025-06-01 14:52:32 [INFO]: epoch 739: training loss 0.0202\n",
      "2025-06-01 14:52:32 [INFO]: epoch 740: training loss 0.0248\n",
      "2025-06-01 14:52:32 [INFO]: epoch 741: training loss 0.0214\n",
      "2025-06-01 14:52:32 [INFO]: epoch 742: training loss 0.0226\n",
      "2025-06-01 14:52:32 [INFO]: epoch 743: training loss 0.0264\n",
      "2025-06-01 14:52:32 [INFO]: epoch 744: training loss 0.0246\n",
      "2025-06-01 14:52:32 [INFO]: epoch 745: training loss 0.0246\n",
      "2025-06-01 14:52:32 [INFO]: epoch 746: training loss 0.0217\n",
      "2025-06-01 14:52:32 [INFO]: epoch 747: training loss 0.0235\n",
      "2025-06-01 14:52:32 [INFO]: epoch 748: training loss 0.0254\n",
      "2025-06-01 14:52:32 [INFO]: epoch 749: training loss 0.0242\n",
      "2025-06-01 14:52:32 [INFO]: epoch 750: training loss 0.0252\n",
      "2025-06-01 14:52:33 [INFO]: epoch 751: training loss 0.0261\n",
      "2025-06-01 14:52:33 [INFO]: epoch 752: training loss 0.0240\n",
      "2025-06-01 14:52:33 [INFO]: epoch 753: training loss 0.0339\n",
      "2025-06-01 14:52:33 [INFO]: epoch 754: training loss 0.0351\n",
      "2025-06-01 14:52:33 [INFO]: epoch 755: training loss 0.0218\n",
      "2025-06-01 14:52:33 [INFO]: epoch 756: training loss 0.0330\n",
      "2025-06-01 14:52:33 [INFO]: epoch 757: training loss 0.0309\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-01 14:52:33 [INFO]: epoch 758: training loss 0.0187\n",
      "2025-06-01 14:52:33 [INFO]: epoch 759: training loss 0.0242\n",
      "2025-06-01 14:52:33 [INFO]: epoch 760: training loss 0.0270\n",
      "2025-06-01 14:52:33 [INFO]: epoch 761: training loss 0.0207\n",
      "2025-06-01 14:52:33 [INFO]: epoch 762: training loss 0.0226\n",
      "2025-06-01 14:52:33 [INFO]: epoch 763: training loss 0.0262\n",
      "2025-06-01 14:52:33 [INFO]: epoch 764: training loss 0.0250\n",
      "2025-06-01 14:52:33 [INFO]: epoch 765: training loss 0.0214\n",
      "2025-06-01 14:52:33 [INFO]: epoch 766: training loss 0.0238\n",
      "2025-06-01 14:52:33 [INFO]: epoch 767: training loss 0.0207\n",
      "2025-06-01 14:52:33 [INFO]: epoch 768: training loss 0.0210\n",
      "2025-06-01 14:52:33 [INFO]: epoch 769: training loss 0.0214\n",
      "2025-06-01 14:52:33 [INFO]: epoch 770: training loss 0.0199\n",
      "2025-06-01 14:52:33 [INFO]: epoch 771: training loss 0.0180\n",
      "2025-06-01 14:52:33 [INFO]: epoch 772: training loss 0.0210\n",
      "2025-06-01 14:52:33 [INFO]: epoch 773: training loss 0.0209\n",
      "2025-06-01 14:52:33 [INFO]: epoch 774: training loss 0.0223\n",
      "2025-06-01 14:52:33 [INFO]: epoch 775: training loss 0.0210\n",
      "2025-06-01 14:52:33 [INFO]: epoch 776: training loss 0.0192\n",
      "2025-06-01 14:52:33 [INFO]: epoch 777: training loss 0.0200\n",
      "2025-06-01 14:52:33 [INFO]: epoch 778: training loss 0.0251\n",
      "2025-06-01 14:52:33 [INFO]: epoch 779: training loss 0.0239\n",
      "2025-06-01 14:52:33 [INFO]: epoch 780: training loss 0.0231\n",
      "2025-06-01 14:52:33 [INFO]: epoch 781: training loss 0.0270\n",
      "2025-06-01 14:52:33 [INFO]: epoch 782: training loss 0.0217\n",
      "2025-06-01 14:52:33 [INFO]: epoch 783: training loss 0.0262\n",
      "2025-06-01 14:52:33 [INFO]: epoch 784: training loss 0.0234\n",
      "2025-06-01 14:52:33 [INFO]: epoch 785: training loss 0.0206\n",
      "2025-06-01 14:52:33 [INFO]: epoch 786: training loss 0.0258\n",
      "2025-06-01 14:52:33 [INFO]: epoch 787: training loss 0.0259\n",
      "2025-06-01 14:52:33 [INFO]: epoch 788: training loss 0.0194\n",
      "2025-06-01 14:52:33 [INFO]: epoch 789: training loss 0.0243\n",
      "2025-06-01 14:52:33 [INFO]: epoch 790: training loss 0.0263\n",
      "2025-06-01 14:52:33 [INFO]: epoch 791: training loss 0.0237\n",
      "2025-06-01 14:52:33 [INFO]: epoch 792: training loss 0.0231\n",
      "2025-06-01 14:52:33 [INFO]: epoch 793: training loss 0.0200\n",
      "2025-06-01 14:52:33 [INFO]: epoch 794: training loss 0.0201\n",
      "2025-06-01 14:52:33 [INFO]: epoch 795: training loss 0.0225\n",
      "2025-06-01 14:52:33 [INFO]: epoch 796: training loss 0.0194\n",
      "2025-06-01 14:52:33 [INFO]: epoch 797: training loss 0.0220\n",
      "2025-06-01 14:52:33 [INFO]: epoch 798: training loss 0.0188\n",
      "2025-06-01 14:52:33 [INFO]: epoch 799: training loss 0.0288\n",
      "2025-06-01 14:52:33 [INFO]: Finished training.\n",
      "2025-06-01 14:52:33 [WARNING]: 🚨DeprecationWarning: The method impute is deprecated. Please use `predict` instead.\n",
      " 83%|██████████████████████████████████████████████████████████████████████              | 5/6 [00:53<00:10, 10.59s/it]2025-06-01 14:52:33 [INFO]: No given device, using default device: cuda\n",
      "2025-06-01 14:52:33 [WARNING]: saving_path not given. Model files and tensorboard file will not be saved.\n",
      "2025-06-01 14:52:33 [INFO]: Model initialized successfully with the number of trainable parameters: 597,780\n",
      "2025-06-01 14:52:33 [INFO]: epoch 0: training loss 0.9469\n",
      "2025-06-01 14:52:33 [INFO]: epoch 1: training loss 0.7352\n",
      "2025-06-01 14:52:33 [INFO]: epoch 2: training loss 0.5735\n",
      "2025-06-01 14:52:33 [INFO]: epoch 3: training loss 0.5271\n",
      "2025-06-01 14:52:33 [INFO]: epoch 4: training loss 0.5088\n",
      "2025-06-01 14:52:33 [INFO]: epoch 5: training loss 0.3787\n",
      "2025-06-01 14:52:33 [INFO]: epoch 6: training loss 0.4670\n",
      "2025-06-01 14:52:33 [INFO]: epoch 7: training loss 0.4234\n",
      "2025-06-01 14:52:33 [INFO]: epoch 8: training loss 0.4290\n",
      "2025-06-01 14:52:33 [INFO]: epoch 9: training loss 0.3765\n",
      "2025-06-01 14:52:33 [INFO]: epoch 10: training loss 0.3253\n",
      "2025-06-01 14:52:33 [INFO]: epoch 11: training loss 0.2809\n",
      "2025-06-01 14:52:33 [INFO]: epoch 12: training loss 0.2949\n",
      "2025-06-01 14:52:33 [INFO]: epoch 13: training loss 0.2725\n",
      "2025-06-01 14:52:33 [INFO]: epoch 14: training loss 0.2646\n",
      "2025-06-01 14:52:33 [INFO]: epoch 15: training loss 0.2609\n",
      "2025-06-01 14:52:33 [INFO]: epoch 16: training loss 0.2645\n",
      "2025-06-01 14:52:33 [INFO]: epoch 17: training loss 0.2403\n",
      "2025-06-01 14:52:33 [INFO]: epoch 18: training loss 0.1975\n",
      "2025-06-01 14:52:33 [INFO]: epoch 19: training loss 0.2195\n",
      "2025-06-01 14:52:34 [INFO]: epoch 20: training loss 0.2256\n",
      "2025-06-01 14:52:34 [INFO]: epoch 21: training loss 0.2695\n",
      "2025-06-01 14:52:34 [INFO]: epoch 22: training loss 0.2757\n",
      "2025-06-01 14:52:34 [INFO]: epoch 23: training loss 0.2168\n",
      "2025-06-01 14:52:34 [INFO]: epoch 24: training loss 0.1958\n",
      "2025-06-01 14:52:34 [INFO]: epoch 25: training loss 0.1718\n",
      "2025-06-01 14:52:34 [INFO]: epoch 26: training loss 0.1984\n",
      "2025-06-01 14:52:34 [INFO]: epoch 27: training loss 0.2276\n",
      "2025-06-01 14:52:34 [INFO]: epoch 28: training loss 0.2185\n",
      "2025-06-01 14:52:34 [INFO]: epoch 29: training loss 0.2163\n",
      "2025-06-01 14:52:34 [INFO]: epoch 30: training loss 0.1821\n",
      "2025-06-01 14:52:34 [INFO]: epoch 31: training loss 0.2179\n",
      "2025-06-01 14:52:34 [INFO]: epoch 32: training loss 0.2111\n",
      "2025-06-01 14:52:34 [INFO]: epoch 33: training loss 0.1772\n",
      "2025-06-01 14:52:34 [INFO]: epoch 34: training loss 0.1658\n",
      "2025-06-01 14:52:34 [INFO]: epoch 35: training loss 0.2016\n",
      "2025-06-01 14:52:34 [INFO]: epoch 36: training loss 0.1980\n",
      "2025-06-01 14:52:34 [INFO]: epoch 37: training loss 0.1794\n",
      "2025-06-01 14:52:34 [INFO]: epoch 38: training loss 0.1729\n",
      "2025-06-01 14:52:34 [INFO]: epoch 39: training loss 0.1925\n",
      "2025-06-01 14:52:34 [INFO]: epoch 40: training loss 0.1943\n",
      "2025-06-01 14:52:34 [INFO]: epoch 41: training loss 0.1873\n",
      "2025-06-01 14:52:34 [INFO]: epoch 42: training loss 0.1934\n",
      "2025-06-01 14:52:34 [INFO]: epoch 43: training loss 0.1728\n",
      "2025-06-01 14:52:34 [INFO]: epoch 44: training loss 0.1726\n",
      "2025-06-01 14:52:34 [INFO]: epoch 45: training loss 0.2080\n",
      "2025-06-01 14:52:34 [INFO]: epoch 46: training loss 0.1632\n",
      "2025-06-01 14:52:34 [INFO]: epoch 47: training loss 0.1756\n",
      "2025-06-01 14:52:34 [INFO]: epoch 48: training loss 0.1902\n",
      "2025-06-01 14:52:34 [INFO]: epoch 49: training loss 0.1810\n",
      "2025-06-01 14:52:34 [INFO]: epoch 50: training loss 0.1744\n",
      "2025-06-01 14:52:34 [INFO]: epoch 51: training loss 0.1479\n",
      "2025-06-01 14:52:34 [INFO]: epoch 52: training loss 0.1666\n",
      "2025-06-01 14:52:34 [INFO]: epoch 53: training loss 0.1642\n",
      "2025-06-01 14:52:34 [INFO]: epoch 54: training loss 0.1777\n",
      "2025-06-01 14:52:34 [INFO]: epoch 55: training loss 0.1668\n",
      "2025-06-01 14:52:34 [INFO]: epoch 56: training loss 0.1491\n",
      "2025-06-01 14:52:34 [INFO]: epoch 57: training loss 0.1450\n",
      "2025-06-01 14:52:34 [INFO]: epoch 58: training loss 0.1445\n",
      "2025-06-01 14:52:34 [INFO]: epoch 59: training loss 0.1477\n",
      "2025-06-01 14:52:34 [INFO]: epoch 60: training loss 0.1516\n",
      "2025-06-01 14:52:34 [INFO]: epoch 61: training loss 0.1732\n",
      "2025-06-01 14:52:34 [INFO]: epoch 62: training loss 0.1574\n",
      "2025-06-01 14:52:34 [INFO]: epoch 63: training loss 0.1477\n",
      "2025-06-01 14:52:34 [INFO]: epoch 64: training loss 0.1749\n",
      "2025-06-01 14:52:34 [INFO]: epoch 65: training loss 0.1529\n",
      "2025-06-01 14:52:34 [INFO]: epoch 66: training loss 0.1624\n",
      "2025-06-01 14:52:34 [INFO]: epoch 67: training loss 0.1488\n",
      "2025-06-01 14:52:34 [INFO]: epoch 68: training loss 0.1572\n",
      "2025-06-01 14:52:34 [INFO]: epoch 69: training loss 0.1576\n",
      "2025-06-01 14:52:34 [INFO]: epoch 70: training loss 0.1720\n",
      "2025-06-01 14:52:34 [INFO]: epoch 71: training loss 0.1615\n",
      "2025-06-01 14:52:34 [INFO]: epoch 72: training loss 0.1451\n",
      "2025-06-01 14:52:34 [INFO]: epoch 73: training loss 0.1443\n",
      "2025-06-01 14:52:34 [INFO]: epoch 74: training loss 0.1392\n",
      "2025-06-01 14:52:34 [INFO]: epoch 75: training loss 0.1508\n",
      "2025-06-01 14:52:34 [INFO]: epoch 76: training loss 0.1403\n",
      "2025-06-01 14:52:34 [INFO]: epoch 77: training loss 0.1325\n",
      "2025-06-01 14:52:34 [INFO]: epoch 78: training loss 0.1372\n",
      "2025-06-01 14:52:34 [INFO]: epoch 79: training loss 0.1266\n",
      "2025-06-01 14:52:34 [INFO]: epoch 80: training loss 0.1351\n",
      "2025-06-01 14:52:34 [INFO]: epoch 81: training loss 0.1472\n",
      "2025-06-01 14:52:34 [INFO]: epoch 82: training loss 0.1415\n",
      "2025-06-01 14:52:34 [INFO]: epoch 83: training loss 0.1241\n",
      "2025-06-01 14:52:34 [INFO]: epoch 84: training loss 0.1312\n",
      "2025-06-01 14:52:34 [INFO]: epoch 85: training loss 0.1437\n",
      "2025-06-01 14:52:34 [INFO]: epoch 86: training loss 0.1431\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-01 14:52:34 [INFO]: epoch 87: training loss 0.1353\n",
      "2025-06-01 14:52:34 [INFO]: epoch 88: training loss 0.1320\n",
      "2025-06-01 14:52:34 [INFO]: epoch 89: training loss 0.1367\n",
      "2025-06-01 14:52:34 [INFO]: epoch 90: training loss 0.1308\n",
      "2025-06-01 14:52:34 [INFO]: epoch 91: training loss 0.1278\n",
      "2025-06-01 14:52:34 [INFO]: epoch 92: training loss 0.1189\n",
      "2025-06-01 14:52:34 [INFO]: epoch 93: training loss 0.1335\n",
      "2025-06-01 14:52:34 [INFO]: epoch 94: training loss 0.1253\n",
      "2025-06-01 14:52:34 [INFO]: epoch 95: training loss 0.1256\n",
      "2025-06-01 14:52:35 [INFO]: epoch 96: training loss 0.1235\n",
      "2025-06-01 14:52:35 [INFO]: epoch 97: training loss 0.1177\n",
      "2025-06-01 14:52:35 [INFO]: epoch 98: training loss 0.1310\n",
      "2025-06-01 14:52:35 [INFO]: epoch 99: training loss 0.1268\n",
      "2025-06-01 14:52:35 [INFO]: epoch 100: training loss 0.1234\n",
      "2025-06-01 14:52:35 [INFO]: epoch 101: training loss 0.1181\n",
      "2025-06-01 14:52:35 [INFO]: epoch 102: training loss 0.1217\n",
      "2025-06-01 14:52:35 [INFO]: epoch 103: training loss 0.1246\n",
      "2025-06-01 14:52:35 [INFO]: epoch 104: training loss 0.1308\n",
      "2025-06-01 14:52:35 [INFO]: epoch 105: training loss 0.1360\n",
      "2025-06-01 14:52:35 [INFO]: epoch 106: training loss 0.1255\n",
      "2025-06-01 14:52:35 [INFO]: epoch 107: training loss 0.1169\n",
      "2025-06-01 14:52:35 [INFO]: epoch 108: training loss 0.1309\n",
      "2025-06-01 14:52:35 [INFO]: epoch 109: training loss 0.1254\n",
      "2025-06-01 14:52:35 [INFO]: epoch 110: training loss 0.1610\n",
      "2025-06-01 14:52:35 [INFO]: epoch 111: training loss 0.1233\n",
      "2025-06-01 14:52:35 [INFO]: epoch 112: training loss 0.1191\n",
      "2025-06-01 14:52:35 [INFO]: epoch 113: training loss 0.1326\n",
      "2025-06-01 14:52:35 [INFO]: epoch 114: training loss 0.1319\n",
      "2025-06-01 14:52:35 [INFO]: epoch 115: training loss 0.1191\n",
      "2025-06-01 14:52:35 [INFO]: epoch 116: training loss 0.1066\n",
      "2025-06-01 14:52:35 [INFO]: epoch 117: training loss 0.1117\n",
      "2025-06-01 14:52:35 [INFO]: epoch 118: training loss 0.1091\n",
      "2025-06-01 14:52:35 [INFO]: epoch 119: training loss 0.1229\n",
      "2025-06-01 14:52:35 [INFO]: epoch 120: training loss 0.1267\n",
      "2025-06-01 14:52:35 [INFO]: epoch 121: training loss 0.1063\n",
      "2025-06-01 14:52:35 [INFO]: epoch 122: training loss 0.1090\n",
      "2025-06-01 14:52:35 [INFO]: epoch 123: training loss 0.1238\n",
      "2025-06-01 14:52:35 [INFO]: epoch 124: training loss 0.1143\n",
      "2025-06-01 14:52:35 [INFO]: epoch 125: training loss 0.1153\n",
      "2025-06-01 14:52:35 [INFO]: epoch 126: training loss 0.1089\n",
      "2025-06-01 14:52:35 [INFO]: epoch 127: training loss 0.1037\n",
      "2025-06-01 14:52:35 [INFO]: epoch 128: training loss 0.1208\n",
      "2025-06-01 14:52:35 [INFO]: epoch 129: training loss 0.1130\n",
      "2025-06-01 14:52:35 [INFO]: epoch 130: training loss 0.1116\n",
      "2025-06-01 14:52:35 [INFO]: epoch 131: training loss 0.1165\n",
      "2025-06-01 14:52:35 [INFO]: epoch 132: training loss 0.1210\n",
      "2025-06-01 14:52:35 [INFO]: epoch 133: training loss 0.0974\n",
      "2025-06-01 14:52:35 [INFO]: epoch 134: training loss 0.0978\n",
      "2025-06-01 14:52:35 [INFO]: epoch 135: training loss 0.0981\n",
      "2025-06-01 14:52:35 [INFO]: epoch 136: training loss 0.0996\n",
      "2025-06-01 14:52:35 [INFO]: epoch 137: training loss 0.1090\n",
      "2025-06-01 14:52:35 [INFO]: epoch 138: training loss 0.0959\n",
      "2025-06-01 14:52:35 [INFO]: epoch 139: training loss 0.0947\n",
      "2025-06-01 14:52:35 [INFO]: epoch 140: training loss 0.1188\n",
      "2025-06-01 14:52:35 [INFO]: epoch 141: training loss 0.1167\n",
      "2025-06-01 14:52:35 [INFO]: epoch 142: training loss 0.0881\n",
      "2025-06-01 14:52:35 [INFO]: epoch 143: training loss 0.0956\n",
      "2025-06-01 14:52:35 [INFO]: epoch 144: training loss 0.1046\n",
      "2025-06-01 14:52:35 [INFO]: epoch 145: training loss 0.1145\n",
      "2025-06-01 14:52:35 [INFO]: epoch 146: training loss 0.0983\n",
      "2025-06-01 14:52:35 [INFO]: epoch 147: training loss 0.0989\n",
      "2025-06-01 14:52:35 [INFO]: epoch 148: training loss 0.0853\n",
      "2025-06-01 14:52:35 [INFO]: epoch 149: training loss 0.0998\n",
      "2025-06-01 14:52:35 [INFO]: epoch 150: training loss 0.0972\n",
      "2025-06-01 14:52:35 [INFO]: epoch 151: training loss 0.0898\n",
      "2025-06-01 14:52:35 [INFO]: epoch 152: training loss 0.0825\n",
      "2025-06-01 14:52:35 [INFO]: epoch 153: training loss 0.0868\n",
      "2025-06-01 14:52:35 [INFO]: epoch 154: training loss 0.0948\n",
      "2025-06-01 14:52:35 [INFO]: epoch 155: training loss 0.0796\n",
      "2025-06-01 14:52:35 [INFO]: epoch 156: training loss 0.0949\n",
      "2025-06-01 14:52:35 [INFO]: epoch 157: training loss 0.0949\n",
      "2025-06-01 14:52:35 [INFO]: epoch 158: training loss 0.0916\n",
      "2025-06-01 14:52:35 [INFO]: epoch 159: training loss 0.0868\n",
      "2025-06-01 14:52:35 [INFO]: epoch 160: training loss 0.0940\n",
      "2025-06-01 14:52:35 [INFO]: epoch 161: training loss 0.0951\n",
      "2025-06-01 14:52:35 [INFO]: epoch 162: training loss 0.1005\n",
      "2025-06-01 14:52:35 [INFO]: epoch 163: training loss 0.0994\n",
      "2025-06-01 14:52:35 [INFO]: epoch 164: training loss 0.0930\n",
      "2025-06-01 14:52:35 [INFO]: epoch 165: training loss 0.0982\n",
      "2025-06-01 14:52:35 [INFO]: epoch 166: training loss 0.1052\n",
      "2025-06-01 14:52:35 [INFO]: epoch 167: training loss 0.1005\n",
      "2025-06-01 14:52:35 [INFO]: epoch 168: training loss 0.0942\n",
      "2025-06-01 14:52:35 [INFO]: epoch 169: training loss 0.0877\n",
      "2025-06-01 14:52:35 [INFO]: epoch 170: training loss 0.1068\n",
      "2025-06-01 14:52:35 [INFO]: epoch 171: training loss 0.1003\n",
      "2025-06-01 14:52:35 [INFO]: epoch 172: training loss 0.1001\n",
      "2025-06-01 14:52:36 [INFO]: epoch 173: training loss 0.1089\n",
      "2025-06-01 14:52:36 [INFO]: epoch 174: training loss 0.1065\n",
      "2025-06-01 14:52:36 [INFO]: epoch 175: training loss 0.0735\n",
      "2025-06-01 14:52:36 [INFO]: epoch 176: training loss 0.0884\n",
      "2025-06-01 14:52:36 [INFO]: epoch 177: training loss 0.0957\n",
      "2025-06-01 14:52:36 [INFO]: epoch 178: training loss 0.0894\n",
      "2025-06-01 14:52:36 [INFO]: epoch 179: training loss 0.0876\n",
      "2025-06-01 14:52:36 [INFO]: epoch 180: training loss 0.0855\n",
      "2025-06-01 14:52:36 [INFO]: epoch 181: training loss 0.0753\n",
      "2025-06-01 14:52:36 [INFO]: epoch 182: training loss 0.0889\n",
      "2025-06-01 14:52:36 [INFO]: epoch 183: training loss 0.0864\n",
      "2025-06-01 14:52:36 [INFO]: epoch 184: training loss 0.0873\n",
      "2025-06-01 14:52:36 [INFO]: epoch 185: training loss 0.0840\n",
      "2025-06-01 14:52:36 [INFO]: epoch 186: training loss 0.0792\n",
      "2025-06-01 14:52:36 [INFO]: epoch 187: training loss 0.0909\n",
      "2025-06-01 14:52:36 [INFO]: epoch 188: training loss 0.0809\n",
      "2025-06-01 14:52:36 [INFO]: epoch 189: training loss 0.0792\n",
      "2025-06-01 14:52:36 [INFO]: epoch 190: training loss 0.0739\n",
      "2025-06-01 14:52:36 [INFO]: epoch 191: training loss 0.0777\n",
      "2025-06-01 14:52:36 [INFO]: epoch 192: training loss 0.0843\n",
      "2025-06-01 14:52:36 [INFO]: epoch 193: training loss 0.0779\n",
      "2025-06-01 14:52:36 [INFO]: epoch 194: training loss 0.0913\n",
      "2025-06-01 14:52:36 [INFO]: epoch 195: training loss 0.0788\n",
      "2025-06-01 14:52:36 [INFO]: epoch 196: training loss 0.0780\n",
      "2025-06-01 14:52:36 [INFO]: epoch 197: training loss 0.0899\n",
      "2025-06-01 14:52:36 [INFO]: epoch 198: training loss 0.0731\n",
      "2025-06-01 14:52:36 [INFO]: epoch 199: training loss 0.0861\n",
      "2025-06-01 14:52:36 [INFO]: epoch 200: training loss 0.0869\n",
      "2025-06-01 14:52:36 [INFO]: epoch 201: training loss 0.0785\n",
      "2025-06-01 14:52:36 [INFO]: epoch 202: training loss 0.0761\n",
      "2025-06-01 14:52:36 [INFO]: epoch 203: training loss 0.0848\n",
      "2025-06-01 14:52:36 [INFO]: epoch 204: training loss 0.0754\n",
      "2025-06-01 14:52:36 [INFO]: epoch 205: training loss 0.0670\n",
      "2025-06-01 14:52:36 [INFO]: epoch 206: training loss 0.0833\n",
      "2025-06-01 14:52:36 [INFO]: epoch 207: training loss 0.0751\n",
      "2025-06-01 14:52:36 [INFO]: epoch 208: training loss 0.0781\n",
      "2025-06-01 14:52:36 [INFO]: epoch 209: training loss 0.0834\n",
      "2025-06-01 14:52:36 [INFO]: epoch 210: training loss 0.0758\n",
      "2025-06-01 14:52:36 [INFO]: epoch 211: training loss 0.0696\n",
      "2025-06-01 14:52:36 [INFO]: epoch 212: training loss 0.0785\n",
      "2025-06-01 14:52:36 [INFO]: epoch 213: training loss 0.0812\n",
      "2025-06-01 14:52:36 [INFO]: epoch 214: training loss 0.0698\n",
      "2025-06-01 14:52:36 [INFO]: epoch 215: training loss 0.0751\n",
      "2025-06-01 14:52:36 [INFO]: epoch 216: training loss 0.0888\n",
      "2025-06-01 14:52:36 [INFO]: epoch 217: training loss 0.0833\n",
      "2025-06-01 14:52:36 [INFO]: epoch 218: training loss 0.0676\n",
      "2025-06-01 14:52:36 [INFO]: epoch 219: training loss 0.0750\n",
      "2025-06-01 14:52:36 [INFO]: epoch 220: training loss 0.0931\n",
      "2025-06-01 14:52:36 [INFO]: epoch 221: training loss 0.0672\n",
      "2025-06-01 14:52:36 [INFO]: epoch 222: training loss 0.0666\n",
      "2025-06-01 14:52:36 [INFO]: epoch 223: training loss 0.0853\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-01 14:52:36 [INFO]: epoch 224: training loss 0.0759\n",
      "2025-06-01 14:52:36 [INFO]: epoch 225: training loss 0.0720\n",
      "2025-06-01 14:52:36 [INFO]: epoch 226: training loss 0.0652\n",
      "2025-06-01 14:52:36 [INFO]: epoch 227: training loss 0.0800\n",
      "2025-06-01 14:52:36 [INFO]: epoch 228: training loss 0.0685\n",
      "2025-06-01 14:52:36 [INFO]: epoch 229: training loss 0.0749\n",
      "2025-06-01 14:52:36 [INFO]: epoch 230: training loss 0.0660\n",
      "2025-06-01 14:52:36 [INFO]: epoch 231: training loss 0.0727\n",
      "2025-06-01 14:52:36 [INFO]: epoch 232: training loss 0.0707\n",
      "2025-06-01 14:52:36 [INFO]: epoch 233: training loss 0.0706\n",
      "2025-06-01 14:52:36 [INFO]: epoch 234: training loss 0.0666\n",
      "2025-06-01 14:52:36 [INFO]: epoch 235: training loss 0.0642\n",
      "2025-06-01 14:52:36 [INFO]: epoch 236: training loss 0.0655\n",
      "2025-06-01 14:52:36 [INFO]: epoch 237: training loss 0.0594\n",
      "2025-06-01 14:52:36 [INFO]: epoch 238: training loss 0.0676\n",
      "2025-06-01 14:52:36 [INFO]: epoch 239: training loss 0.0672\n",
      "2025-06-01 14:52:36 [INFO]: epoch 240: training loss 0.0585\n",
      "2025-06-01 14:52:36 [INFO]: epoch 241: training loss 0.0618\n",
      "2025-06-01 14:52:36 [INFO]: epoch 242: training loss 0.0554\n",
      "2025-06-01 14:52:36 [INFO]: epoch 243: training loss 0.0650\n",
      "2025-06-01 14:52:36 [INFO]: epoch 244: training loss 0.0627\n",
      "2025-06-01 14:52:36 [INFO]: epoch 245: training loss 0.0584\n",
      "2025-06-01 14:52:36 [INFO]: epoch 246: training loss 0.0563\n",
      "2025-06-01 14:52:36 [INFO]: epoch 247: training loss 0.0641\n",
      "2025-06-01 14:52:36 [INFO]: epoch 248: training loss 0.0672\n",
      "2025-06-01 14:52:37 [INFO]: epoch 249: training loss 0.0642\n",
      "2025-06-01 14:52:37 [INFO]: epoch 250: training loss 0.0651\n",
      "2025-06-01 14:52:37 [INFO]: epoch 251: training loss 0.0669\n",
      "2025-06-01 14:52:37 [INFO]: epoch 252: training loss 0.0569\n",
      "2025-06-01 14:52:37 [INFO]: epoch 253: training loss 0.0537\n",
      "2025-06-01 14:52:37 [INFO]: epoch 254: training loss 0.0522\n",
      "2025-06-01 14:52:37 [INFO]: epoch 255: training loss 0.0568\n",
      "2025-06-01 14:52:37 [INFO]: epoch 256: training loss 0.0709\n",
      "2025-06-01 14:52:37 [INFO]: epoch 257: training loss 0.0579\n",
      "2025-06-01 14:52:37 [INFO]: epoch 258: training loss 0.0576\n",
      "2025-06-01 14:52:37 [INFO]: epoch 259: training loss 0.0640\n",
      "2025-06-01 14:52:37 [INFO]: epoch 260: training loss 0.0700\n",
      "2025-06-01 14:52:37 [INFO]: epoch 261: training loss 0.0635\n",
      "2025-06-01 14:52:37 [INFO]: epoch 262: training loss 0.0520\n",
      "2025-06-01 14:52:37 [INFO]: epoch 263: training loss 0.0589\n",
      "2025-06-01 14:52:37 [INFO]: epoch 264: training loss 0.0561\n",
      "2025-06-01 14:52:37 [INFO]: epoch 265: training loss 0.0604\n",
      "2025-06-01 14:52:37 [INFO]: epoch 266: training loss 0.0626\n",
      "2025-06-01 14:52:37 [INFO]: epoch 267: training loss 0.0568\n",
      "2025-06-01 14:52:37 [INFO]: epoch 268: training loss 0.0635\n",
      "2025-06-01 14:52:37 [INFO]: epoch 269: training loss 0.0667\n",
      "2025-06-01 14:52:37 [INFO]: epoch 270: training loss 0.0522\n",
      "2025-06-01 14:52:37 [INFO]: epoch 271: training loss 0.0591\n",
      "2025-06-01 14:52:37 [INFO]: epoch 272: training loss 0.0588\n",
      "2025-06-01 14:52:37 [INFO]: epoch 273: training loss 0.0555\n",
      "2025-06-01 14:52:37 [INFO]: epoch 274: training loss 0.0525\n",
      "2025-06-01 14:52:37 [INFO]: epoch 275: training loss 0.0575\n",
      "2025-06-01 14:52:37 [INFO]: epoch 276: training loss 0.0530\n",
      "2025-06-01 14:52:37 [INFO]: epoch 277: training loss 0.0659\n",
      "2025-06-01 14:52:37 [INFO]: epoch 278: training loss 0.0544\n",
      "2025-06-01 14:52:37 [INFO]: epoch 279: training loss 0.0574\n",
      "2025-06-01 14:52:37 [INFO]: epoch 280: training loss 0.0548\n",
      "2025-06-01 14:52:37 [INFO]: epoch 281: training loss 0.0522\n",
      "2025-06-01 14:52:37 [INFO]: epoch 282: training loss 0.0580\n",
      "2025-06-01 14:52:37 [INFO]: epoch 283: training loss 0.0531\n",
      "2025-06-01 14:52:37 [INFO]: epoch 284: training loss 0.0563\n",
      "2025-06-01 14:52:37 [INFO]: epoch 285: training loss 0.0491\n",
      "2025-06-01 14:52:37 [INFO]: epoch 286: training loss 0.0562\n",
      "2025-06-01 14:52:37 [INFO]: epoch 287: training loss 0.0563\n",
      "2025-06-01 14:52:37 [INFO]: epoch 288: training loss 0.0574\n",
      "2025-06-01 14:52:37 [INFO]: epoch 289: training loss 0.0542\n",
      "2025-06-01 14:52:37 [INFO]: epoch 290: training loss 0.0493\n",
      "2025-06-01 14:52:37 [INFO]: epoch 291: training loss 0.0506\n",
      "2025-06-01 14:52:37 [INFO]: epoch 292: training loss 0.0654\n",
      "2025-06-01 14:52:37 [INFO]: epoch 293: training loss 0.0557\n",
      "2025-06-01 14:52:37 [INFO]: epoch 294: training loss 0.0510\n",
      "2025-06-01 14:52:37 [INFO]: epoch 295: training loss 0.0474\n",
      "2025-06-01 14:52:37 [INFO]: epoch 296: training loss 0.0564\n",
      "2025-06-01 14:52:37 [INFO]: epoch 297: training loss 0.0616\n",
      "2025-06-01 14:52:37 [INFO]: epoch 298: training loss 0.0539\n",
      "2025-06-01 14:52:37 [INFO]: epoch 299: training loss 0.0557\n",
      "2025-06-01 14:52:37 [INFO]: epoch 300: training loss 0.0496\n",
      "2025-06-01 14:52:37 [INFO]: epoch 301: training loss 0.0528\n",
      "2025-06-01 14:52:37 [INFO]: epoch 302: training loss 0.0534\n",
      "2025-06-01 14:52:37 [INFO]: epoch 303: training loss 0.0586\n",
      "2025-06-01 14:52:37 [INFO]: epoch 304: training loss 0.0525\n",
      "2025-06-01 14:52:37 [INFO]: epoch 305: training loss 0.0480\n",
      "2025-06-01 14:52:37 [INFO]: epoch 306: training loss 0.0512\n",
      "2025-06-01 14:52:37 [INFO]: epoch 307: training loss 0.0492\n",
      "2025-06-01 14:52:37 [INFO]: epoch 308: training loss 0.0472\n",
      "2025-06-01 14:52:37 [INFO]: epoch 309: training loss 0.0472\n",
      "2025-06-01 14:52:37 [INFO]: epoch 310: training loss 0.0442\n",
      "2025-06-01 14:52:37 [INFO]: epoch 311: training loss 0.0523\n",
      "2025-06-01 14:52:37 [INFO]: epoch 312: training loss 0.0453\n",
      "2025-06-01 14:52:37 [INFO]: epoch 313: training loss 0.0494\n",
      "2025-06-01 14:52:37 [INFO]: epoch 314: training loss 0.0452\n",
      "2025-06-01 14:52:37 [INFO]: epoch 315: training loss 0.0465\n",
      "2025-06-01 14:52:37 [INFO]: epoch 316: training loss 0.0479\n",
      "2025-06-01 14:52:37 [INFO]: epoch 317: training loss 0.0485\n",
      "2025-06-01 14:52:37 [INFO]: epoch 318: training loss 0.0447\n",
      "2025-06-01 14:52:37 [INFO]: epoch 319: training loss 0.0497\n",
      "2025-06-01 14:52:37 [INFO]: epoch 320: training loss 0.0424\n",
      "2025-06-01 14:52:37 [INFO]: epoch 321: training loss 0.0552\n",
      "2025-06-01 14:52:37 [INFO]: epoch 322: training loss 0.0513\n",
      "2025-06-01 14:52:37 [INFO]: epoch 323: training loss 0.0481\n",
      "2025-06-01 14:52:37 [INFO]: epoch 324: training loss 0.0492\n",
      "2025-06-01 14:52:38 [INFO]: epoch 325: training loss 0.0517\n",
      "2025-06-01 14:52:38 [INFO]: epoch 326: training loss 0.0516\n",
      "2025-06-01 14:52:38 [INFO]: epoch 327: training loss 0.0493\n",
      "2025-06-01 14:52:38 [INFO]: epoch 328: training loss 0.0477\n",
      "2025-06-01 14:52:38 [INFO]: epoch 329: training loss 0.0450\n",
      "2025-06-01 14:52:38 [INFO]: epoch 330: training loss 0.0435\n",
      "2025-06-01 14:52:38 [INFO]: epoch 331: training loss 0.0386\n",
      "2025-06-01 14:52:38 [INFO]: epoch 332: training loss 0.0369\n",
      "2025-06-01 14:52:38 [INFO]: epoch 333: training loss 0.0421\n",
      "2025-06-01 14:52:38 [INFO]: epoch 334: training loss 0.0470\n",
      "2025-06-01 14:52:38 [INFO]: epoch 335: training loss 0.0464\n",
      "2025-06-01 14:52:38 [INFO]: epoch 336: training loss 0.0411\n",
      "2025-06-01 14:52:38 [INFO]: epoch 337: training loss 0.0452\n",
      "2025-06-01 14:52:38 [INFO]: epoch 338: training loss 0.0447\n",
      "2025-06-01 14:52:38 [INFO]: epoch 339: training loss 0.0440\n",
      "2025-06-01 14:52:38 [INFO]: epoch 340: training loss 0.0428\n",
      "2025-06-01 14:52:38 [INFO]: epoch 341: training loss 0.0441\n",
      "2025-06-01 14:52:38 [INFO]: epoch 342: training loss 0.0439\n",
      "2025-06-01 14:52:38 [INFO]: epoch 343: training loss 0.0407\n",
      "2025-06-01 14:52:38 [INFO]: epoch 344: training loss 0.0395\n",
      "2025-06-01 14:52:38 [INFO]: epoch 345: training loss 0.0415\n",
      "2025-06-01 14:52:38 [INFO]: epoch 346: training loss 0.0532\n",
      "2025-06-01 14:52:38 [INFO]: epoch 347: training loss 0.0462\n",
      "2025-06-01 14:52:38 [INFO]: epoch 348: training loss 0.0450\n",
      "2025-06-01 14:52:38 [INFO]: epoch 349: training loss 0.0422\n",
      "2025-06-01 14:52:38 [INFO]: epoch 350: training loss 0.0409\n",
      "2025-06-01 14:52:38 [INFO]: epoch 351: training loss 0.0423\n",
      "2025-06-01 14:52:38 [INFO]: epoch 352: training loss 0.0414\n",
      "2025-06-01 14:52:38 [INFO]: epoch 353: training loss 0.0396\n",
      "2025-06-01 14:52:38 [INFO]: epoch 354: training loss 0.0428\n",
      "2025-06-01 14:52:38 [INFO]: epoch 355: training loss 0.0418\n",
      "2025-06-01 14:52:38 [INFO]: epoch 356: training loss 0.0368\n",
      "2025-06-01 14:52:38 [INFO]: epoch 357: training loss 0.0450\n",
      "2025-06-01 14:52:38 [INFO]: epoch 358: training loss 0.0413\n",
      "2025-06-01 14:52:38 [INFO]: epoch 359: training loss 0.0417\n",
      "2025-06-01 14:52:38 [INFO]: epoch 360: training loss 0.0417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-01 14:52:38 [INFO]: epoch 361: training loss 0.0413\n",
      "2025-06-01 14:52:38 [INFO]: epoch 362: training loss 0.0392\n",
      "2025-06-01 14:52:38 [INFO]: epoch 363: training loss 0.0412\n",
      "2025-06-01 14:52:38 [INFO]: epoch 364: training loss 0.0486\n",
      "2025-06-01 14:52:38 [INFO]: epoch 365: training loss 0.0367\n",
      "2025-06-01 14:52:38 [INFO]: epoch 366: training loss 0.0427\n",
      "2025-06-01 14:52:38 [INFO]: epoch 367: training loss 0.0457\n",
      "2025-06-01 14:52:38 [INFO]: epoch 368: training loss 0.0368\n",
      "2025-06-01 14:52:38 [INFO]: epoch 369: training loss 0.0411\n",
      "2025-06-01 14:52:38 [INFO]: epoch 370: training loss 0.0471\n",
      "2025-06-01 14:52:38 [INFO]: epoch 371: training loss 0.0433\n",
      "2025-06-01 14:52:38 [INFO]: epoch 372: training loss 0.0337\n",
      "2025-06-01 14:52:38 [INFO]: epoch 373: training loss 0.0425\n",
      "2025-06-01 14:52:38 [INFO]: epoch 374: training loss 0.0431\n",
      "2025-06-01 14:52:38 [INFO]: epoch 375: training loss 0.0342\n",
      "2025-06-01 14:52:38 [INFO]: epoch 376: training loss 0.0378\n",
      "2025-06-01 14:52:38 [INFO]: epoch 377: training loss 0.0365\n",
      "2025-06-01 14:52:38 [INFO]: epoch 378: training loss 0.0356\n",
      "2025-06-01 14:52:38 [INFO]: epoch 379: training loss 0.0355\n",
      "2025-06-01 14:52:38 [INFO]: epoch 380: training loss 0.0345\n",
      "2025-06-01 14:52:38 [INFO]: epoch 381: training loss 0.0339\n",
      "2025-06-01 14:52:38 [INFO]: epoch 382: training loss 0.0397\n",
      "2025-06-01 14:52:38 [INFO]: epoch 383: training loss 0.0337\n",
      "2025-06-01 14:52:38 [INFO]: epoch 384: training loss 0.0349\n",
      "2025-06-01 14:52:38 [INFO]: epoch 385: training loss 0.0354\n",
      "2025-06-01 14:52:38 [INFO]: epoch 386: training loss 0.0323\n",
      "2025-06-01 14:52:38 [INFO]: epoch 387: training loss 0.0362\n",
      "2025-06-01 14:52:38 [INFO]: epoch 388: training loss 0.0321\n",
      "2025-06-01 14:52:38 [INFO]: epoch 389: training loss 0.0318\n",
      "2025-06-01 14:52:38 [INFO]: epoch 390: training loss 0.0359\n",
      "2025-06-01 14:52:38 [INFO]: epoch 391: training loss 0.0348\n",
      "2025-06-01 14:52:38 [INFO]: epoch 392: training loss 0.0334\n",
      "2025-06-01 14:52:38 [INFO]: epoch 393: training loss 0.0344\n",
      "2025-06-01 14:52:38 [INFO]: epoch 394: training loss 0.0311\n",
      "2025-06-01 14:52:38 [INFO]: epoch 395: training loss 0.0344\n",
      "2025-06-01 14:52:38 [INFO]: epoch 396: training loss 0.0305\n",
      "2025-06-01 14:52:38 [INFO]: epoch 397: training loss 0.0349\n",
      "2025-06-01 14:52:38 [INFO]: epoch 398: training loss 0.0348\n",
      "2025-06-01 14:52:38 [INFO]: epoch 399: training loss 0.0315\n",
      "2025-06-01 14:52:38 [INFO]: epoch 400: training loss 0.0376\n",
      "2025-06-01 14:52:39 [INFO]: epoch 401: training loss 0.0399\n",
      "2025-06-01 14:52:39 [INFO]: epoch 402: training loss 0.0312\n",
      "2025-06-01 14:52:39 [INFO]: epoch 403: training loss 0.0324\n",
      "2025-06-01 14:52:39 [INFO]: epoch 404: training loss 0.0377\n",
      "2025-06-01 14:52:39 [INFO]: epoch 405: training loss 0.0366\n",
      "2025-06-01 14:52:39 [INFO]: epoch 406: training loss 0.0365\n",
      "2025-06-01 14:52:39 [INFO]: epoch 407: training loss 0.0352\n",
      "2025-06-01 14:52:39 [INFO]: epoch 408: training loss 0.0334\n",
      "2025-06-01 14:52:39 [INFO]: epoch 409: training loss 0.0409\n",
      "2025-06-01 14:52:39 [INFO]: epoch 410: training loss 0.0375\n",
      "2025-06-01 14:52:39 [INFO]: epoch 411: training loss 0.0339\n",
      "2025-06-01 14:52:39 [INFO]: epoch 412: training loss 0.0345\n",
      "2025-06-01 14:52:39 [INFO]: epoch 413: training loss 0.0308\n",
      "2025-06-01 14:52:39 [INFO]: epoch 414: training loss 0.0323\n",
      "2025-06-01 14:52:39 [INFO]: epoch 415: training loss 0.0399\n",
      "2025-06-01 14:52:39 [INFO]: epoch 416: training loss 0.0365\n",
      "2025-06-01 14:52:39 [INFO]: epoch 417: training loss 0.0355\n",
      "2025-06-01 14:52:39 [INFO]: epoch 418: training loss 0.0305\n",
      "2025-06-01 14:52:39 [INFO]: epoch 419: training loss 0.0302\n",
      "2025-06-01 14:52:39 [INFO]: epoch 420: training loss 0.0380\n",
      "2025-06-01 14:52:39 [INFO]: epoch 421: training loss 0.0382\n",
      "2025-06-01 14:52:39 [INFO]: epoch 422: training loss 0.0296\n",
      "2025-06-01 14:52:39 [INFO]: epoch 423: training loss 0.0296\n",
      "2025-06-01 14:52:39 [INFO]: epoch 424: training loss 0.0370\n",
      "2025-06-01 14:52:39 [INFO]: epoch 425: training loss 0.0322\n",
      "2025-06-01 14:52:39 [INFO]: epoch 426: training loss 0.0343\n",
      "2025-06-01 14:52:39 [INFO]: epoch 427: training loss 0.0285\n",
      "2025-06-01 14:52:39 [INFO]: epoch 428: training loss 0.0341\n",
      "2025-06-01 14:52:39 [INFO]: epoch 429: training loss 0.0379\n",
      "2025-06-01 14:52:39 [INFO]: epoch 430: training loss 0.0291\n",
      "2025-06-01 14:52:39 [INFO]: epoch 431: training loss 0.0270\n",
      "2025-06-01 14:52:39 [INFO]: epoch 432: training loss 0.0302\n",
      "2025-06-01 14:52:39 [INFO]: epoch 433: training loss 0.0247\n",
      "2025-06-01 14:52:39 [INFO]: epoch 434: training loss 0.0289\n",
      "2025-06-01 14:52:39 [INFO]: epoch 435: training loss 0.0380\n",
      "2025-06-01 14:52:39 [INFO]: epoch 436: training loss 0.0304\n",
      "2025-06-01 14:52:39 [INFO]: epoch 437: training loss 0.0275\n",
      "2025-06-01 14:52:39 [INFO]: epoch 438: training loss 0.0289\n",
      "2025-06-01 14:52:39 [INFO]: epoch 439: training loss 0.0353\n",
      "2025-06-01 14:52:39 [INFO]: epoch 440: training loss 0.0288\n",
      "2025-06-01 14:52:39 [INFO]: epoch 441: training loss 0.0292\n",
      "2025-06-01 14:52:39 [INFO]: epoch 442: training loss 0.0318\n",
      "2025-06-01 14:52:39 [INFO]: epoch 443: training loss 0.0314\n",
      "2025-06-01 14:52:39 [INFO]: epoch 444: training loss 0.0283\n",
      "2025-06-01 14:52:39 [INFO]: epoch 445: training loss 0.0275\n",
      "2025-06-01 14:52:39 [INFO]: epoch 446: training loss 0.0245\n",
      "2025-06-01 14:52:39 [INFO]: epoch 447: training loss 0.0297\n",
      "2025-06-01 14:52:39 [INFO]: epoch 448: training loss 0.0281\n",
      "2025-06-01 14:52:39 [INFO]: epoch 449: training loss 0.0248\n",
      "2025-06-01 14:52:39 [INFO]: epoch 450: training loss 0.0233\n",
      "2025-06-01 14:52:39 [INFO]: epoch 451: training loss 0.0289\n",
      "2025-06-01 14:52:39 [INFO]: epoch 452: training loss 0.0317\n",
      "2025-06-01 14:52:39 [INFO]: epoch 453: training loss 0.0267\n",
      "2025-06-01 14:52:39 [INFO]: epoch 454: training loss 0.0261\n",
      "2025-06-01 14:52:39 [INFO]: epoch 455: training loss 0.0267\n",
      "2025-06-01 14:52:39 [INFO]: epoch 456: training loss 0.0281\n",
      "2025-06-01 14:52:39 [INFO]: epoch 457: training loss 0.0265\n",
      "2025-06-01 14:52:39 [INFO]: epoch 458: training loss 0.0267\n",
      "2025-06-01 14:52:39 [INFO]: epoch 459: training loss 0.0255\n",
      "2025-06-01 14:52:39 [INFO]: epoch 460: training loss 0.0285\n",
      "2025-06-01 14:52:39 [INFO]: epoch 461: training loss 0.0266\n",
      "2025-06-01 14:52:39 [INFO]: epoch 462: training loss 0.0298\n",
      "2025-06-01 14:52:39 [INFO]: epoch 463: training loss 0.0277\n",
      "2025-06-01 14:52:39 [INFO]: epoch 464: training loss 0.0276\n",
      "2025-06-01 14:52:39 [INFO]: epoch 465: training loss 0.0262\n",
      "2025-06-01 14:52:39 [INFO]: epoch 466: training loss 0.0318\n",
      "2025-06-01 14:52:39 [INFO]: epoch 467: training loss 0.0252\n",
      "2025-06-01 14:52:39 [INFO]: epoch 468: training loss 0.0298\n",
      "2025-06-01 14:52:39 [INFO]: epoch 469: training loss 0.0301\n",
      "2025-06-01 14:52:39 [INFO]: epoch 470: training loss 0.0266\n",
      "2025-06-01 14:52:39 [INFO]: epoch 471: training loss 0.0274\n",
      "2025-06-01 14:52:39 [INFO]: epoch 472: training loss 0.0290\n",
      "2025-06-01 14:52:39 [INFO]: epoch 473: training loss 0.0293\n",
      "2025-06-01 14:52:39 [INFO]: epoch 474: training loss 0.0310\n",
      "2025-06-01 14:52:39 [INFO]: epoch 475: training loss 0.0304\n",
      "2025-06-01 14:52:39 [INFO]: epoch 476: training loss 0.0248\n",
      "2025-06-01 14:52:40 [INFO]: epoch 477: training loss 0.0273\n",
      "2025-06-01 14:52:40 [INFO]: epoch 478: training loss 0.0294\n",
      "2025-06-01 14:52:40 [INFO]: epoch 479: training loss 0.0256\n",
      "2025-06-01 14:52:40 [INFO]: epoch 480: training loss 0.0244\n",
      "2025-06-01 14:52:40 [INFO]: epoch 481: training loss 0.0221\n",
      "2025-06-01 14:52:40 [INFO]: epoch 482: training loss 0.0250\n",
      "2025-06-01 14:52:40 [INFO]: epoch 483: training loss 0.0264\n",
      "2025-06-01 14:52:40 [INFO]: epoch 484: training loss 0.0313\n",
      "2025-06-01 14:52:40 [INFO]: epoch 485: training loss 0.0225\n",
      "2025-06-01 14:52:40 [INFO]: epoch 486: training loss 0.0254\n",
      "2025-06-01 14:52:40 [INFO]: epoch 487: training loss 0.0256\n",
      "2025-06-01 14:52:40 [INFO]: epoch 488: training loss 0.0280\n",
      "2025-06-01 14:52:40 [INFO]: epoch 489: training loss 0.0315\n",
      "2025-06-01 14:52:40 [INFO]: epoch 490: training loss 0.0316\n",
      "2025-06-01 14:52:40 [INFO]: epoch 491: training loss 0.0330\n",
      "2025-06-01 14:52:40 [INFO]: epoch 492: training loss 0.0295\n",
      "2025-06-01 14:52:40 [INFO]: epoch 493: training loss 0.0330\n",
      "2025-06-01 14:52:40 [INFO]: epoch 494: training loss 0.0334\n",
      "2025-06-01 14:52:40 [INFO]: epoch 495: training loss 0.0273\n",
      "2025-06-01 14:52:40 [INFO]: epoch 496: training loss 0.0316\n",
      "2025-06-01 14:52:40 [INFO]: epoch 497: training loss 0.0339\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-01 14:52:40 [INFO]: epoch 498: training loss 0.0291\n",
      "2025-06-01 14:52:40 [INFO]: epoch 499: training loss 0.0271\n",
      "2025-06-01 14:52:40 [INFO]: epoch 500: training loss 0.0406\n",
      "2025-06-01 14:52:40 [INFO]: epoch 501: training loss 0.0334\n",
      "2025-06-01 14:52:40 [INFO]: epoch 502: training loss 0.0269\n",
      "2025-06-01 14:52:40 [INFO]: epoch 503: training loss 0.0306\n",
      "2025-06-01 14:52:40 [INFO]: epoch 504: training loss 0.0308\n",
      "2025-06-01 14:52:40 [INFO]: epoch 505: training loss 0.0295\n",
      "2025-06-01 14:52:40 [INFO]: epoch 506: training loss 0.0289\n",
      "2025-06-01 14:52:40 [INFO]: epoch 507: training loss 0.0313\n",
      "2025-06-01 14:52:40 [INFO]: epoch 508: training loss 0.0384\n",
      "2025-06-01 14:52:40 [INFO]: epoch 509: training loss 0.0286\n",
      "2025-06-01 14:52:40 [INFO]: epoch 510: training loss 0.0297\n",
      "2025-06-01 14:52:40 [INFO]: epoch 511: training loss 0.0276\n",
      "2025-06-01 14:52:40 [INFO]: epoch 512: training loss 0.0310\n",
      "2025-06-01 14:52:40 [INFO]: epoch 513: training loss 0.0307\n",
      "2025-06-01 14:52:40 [INFO]: epoch 514: training loss 0.0326\n",
      "2025-06-01 14:52:40 [INFO]: epoch 515: training loss 0.0260\n",
      "2025-06-01 14:52:40 [INFO]: epoch 516: training loss 0.0270\n",
      "2025-06-01 14:52:40 [INFO]: epoch 517: training loss 0.0323\n",
      "2025-06-01 14:52:40 [INFO]: epoch 518: training loss 0.0252\n",
      "2025-06-01 14:52:40 [INFO]: epoch 519: training loss 0.0273\n",
      "2025-06-01 14:52:40 [INFO]: epoch 520: training loss 0.0282\n",
      "2025-06-01 14:52:40 [INFO]: epoch 521: training loss 0.0269\n",
      "2025-06-01 14:52:40 [INFO]: epoch 522: training loss 0.0228\n",
      "2025-06-01 14:52:40 [INFO]: epoch 523: training loss 0.0297\n",
      "2025-06-01 14:52:40 [INFO]: epoch 524: training loss 0.0289\n",
      "2025-06-01 14:52:40 [INFO]: epoch 525: training loss 0.0280\n",
      "2025-06-01 14:52:40 [INFO]: epoch 526: training loss 0.0288\n",
      "2025-06-01 14:52:40 [INFO]: epoch 527: training loss 0.0243\n",
      "2025-06-01 14:52:40 [INFO]: epoch 528: training loss 0.0258\n",
      "2025-06-01 14:52:40 [INFO]: epoch 529: training loss 0.0362\n",
      "2025-06-01 14:52:40 [INFO]: epoch 530: training loss 0.0308\n",
      "2025-06-01 14:52:40 [INFO]: epoch 531: training loss 0.0218\n",
      "2025-06-01 14:52:40 [INFO]: epoch 532: training loss 0.0308\n",
      "2025-06-01 14:52:40 [INFO]: epoch 533: training loss 0.0359\n",
      "2025-06-01 14:52:40 [INFO]: epoch 534: training loss 0.0320\n",
      "2025-06-01 14:52:40 [INFO]: epoch 535: training loss 0.0303\n",
      "2025-06-01 14:52:40 [INFO]: epoch 536: training loss 0.0280\n",
      "2025-06-01 14:52:40 [INFO]: epoch 537: training loss 0.0241\n",
      "2025-06-01 14:52:40 [INFO]: epoch 538: training loss 0.0365\n",
      "2025-06-01 14:52:40 [INFO]: epoch 539: training loss 0.0311\n",
      "2025-06-01 14:52:40 [INFO]: epoch 540: training loss 0.0244\n",
      "2025-06-01 14:52:40 [INFO]: epoch 541: training loss 0.0267\n",
      "2025-06-01 14:52:40 [INFO]: epoch 542: training loss 0.0258\n",
      "2025-06-01 14:52:40 [INFO]: epoch 543: training loss 0.0253\n",
      "2025-06-01 14:52:40 [INFO]: epoch 544: training loss 0.0208\n",
      "2025-06-01 14:52:40 [INFO]: epoch 545: training loss 0.0296\n",
      "2025-06-01 14:52:40 [INFO]: epoch 546: training loss 0.0268\n",
      "2025-06-01 14:52:40 [INFO]: epoch 547: training loss 0.0217\n",
      "2025-06-01 14:52:40 [INFO]: epoch 548: training loss 0.0224\n",
      "2025-06-01 14:52:40 [INFO]: epoch 549: training loss 0.0231\n",
      "2025-06-01 14:52:40 [INFO]: epoch 550: training loss 0.0286\n",
      "2025-06-01 14:52:41 [INFO]: epoch 551: training loss 0.0277\n",
      "2025-06-01 14:52:41 [INFO]: epoch 552: training loss 0.0276\n",
      "2025-06-01 14:52:41 [INFO]: epoch 553: training loss 0.0250\n",
      "2025-06-01 14:52:41 [INFO]: epoch 554: training loss 0.0228\n",
      "2025-06-01 14:52:41 [INFO]: epoch 555: training loss 0.0285\n",
      "2025-06-01 14:52:41 [INFO]: epoch 556: training loss 0.0285\n",
      "2025-06-01 14:52:41 [INFO]: epoch 557: training loss 0.0268\n",
      "2025-06-01 14:52:41 [INFO]: epoch 558: training loss 0.0250\n",
      "2025-06-01 14:52:41 [INFO]: epoch 559: training loss 0.0276\n",
      "2025-06-01 14:52:41 [INFO]: epoch 560: training loss 0.0241\n",
      "2025-06-01 14:52:41 [INFO]: epoch 561: training loss 0.0316\n",
      "2025-06-01 14:52:41 [INFO]: epoch 562: training loss 0.0303\n",
      "2025-06-01 14:52:41 [INFO]: epoch 563: training loss 0.0251\n",
      "2025-06-01 14:52:41 [INFO]: epoch 564: training loss 0.0289\n",
      "2025-06-01 14:52:41 [INFO]: epoch 565: training loss 0.0307\n",
      "2025-06-01 14:52:41 [INFO]: epoch 566: training loss 0.0324\n",
      "2025-06-01 14:52:41 [INFO]: epoch 567: training loss 0.0255\n",
      "2025-06-01 14:52:41 [INFO]: epoch 568: training loss 0.0263\n",
      "2025-06-01 14:52:41 [INFO]: epoch 569: training loss 0.0286\n",
      "2025-06-01 14:52:41 [INFO]: epoch 570: training loss 0.0296\n",
      "2025-06-01 14:52:41 [INFO]: epoch 571: training loss 0.0248\n",
      "2025-06-01 14:52:41 [INFO]: epoch 572: training loss 0.0237\n",
      "2025-06-01 14:52:41 [INFO]: epoch 573: training loss 0.0233\n",
      "2025-06-01 14:52:41 [INFO]: epoch 574: training loss 0.0229\n",
      "2025-06-01 14:52:41 [INFO]: epoch 575: training loss 0.0223\n",
      "2025-06-01 14:52:41 [INFO]: epoch 576: training loss 0.0250\n",
      "2025-06-01 14:52:41 [INFO]: epoch 577: training loss 0.0231\n",
      "2025-06-01 14:52:41 [INFO]: epoch 578: training loss 0.0248\n",
      "2025-06-01 14:52:41 [INFO]: epoch 579: training loss 0.0266\n",
      "2025-06-01 14:52:41 [INFO]: epoch 580: training loss 0.0212\n",
      "2025-06-01 14:52:41 [INFO]: epoch 581: training loss 0.0288\n",
      "2025-06-01 14:52:41 [INFO]: epoch 582: training loss 0.0308\n",
      "2025-06-01 14:52:41 [INFO]: epoch 583: training loss 0.0223\n",
      "2025-06-01 14:52:41 [INFO]: epoch 584: training loss 0.0249\n",
      "2025-06-01 14:52:41 [INFO]: epoch 585: training loss 0.0382\n",
      "2025-06-01 14:52:41 [INFO]: epoch 586: training loss 0.0318\n",
      "2025-06-01 14:52:41 [INFO]: epoch 587: training loss 0.0257\n",
      "2025-06-01 14:52:41 [INFO]: epoch 588: training loss 0.0318\n",
      "2025-06-01 14:52:41 [INFO]: epoch 589: training loss 0.0310\n",
      "2025-06-01 14:52:41 [INFO]: epoch 590: training loss 0.0208\n",
      "2025-06-01 14:52:41 [INFO]: epoch 591: training loss 0.0287\n",
      "2025-06-01 14:52:41 [INFO]: epoch 592: training loss 0.0327\n",
      "2025-06-01 14:52:41 [INFO]: epoch 593: training loss 0.0276\n",
      "2025-06-01 14:52:41 [INFO]: epoch 594: training loss 0.0239\n",
      "2025-06-01 14:52:41 [INFO]: epoch 595: training loss 0.0299\n",
      "2025-06-01 14:52:41 [INFO]: epoch 596: training loss 0.0267\n",
      "2025-06-01 14:52:41 [INFO]: epoch 597: training loss 0.0265\n",
      "2025-06-01 14:52:41 [INFO]: epoch 598: training loss 0.0285\n",
      "2025-06-01 14:52:41 [INFO]: epoch 599: training loss 0.0290\n",
      "2025-06-01 14:52:41 [INFO]: epoch 600: training loss 0.0244\n",
      "2025-06-01 14:52:41 [INFO]: epoch 601: training loss 0.0296\n",
      "2025-06-01 14:52:41 [INFO]: epoch 602: training loss 0.0248\n",
      "2025-06-01 14:52:41 [INFO]: epoch 603: training loss 0.0258\n",
      "2025-06-01 14:52:41 [INFO]: epoch 604: training loss 0.0261\n",
      "2025-06-01 14:52:41 [INFO]: epoch 605: training loss 0.0285\n",
      "2025-06-01 14:52:41 [INFO]: epoch 606: training loss 0.0236\n",
      "2025-06-01 14:52:41 [INFO]: epoch 607: training loss 0.0261\n",
      "2025-06-01 14:52:41 [INFO]: epoch 608: training loss 0.0255\n",
      "2025-06-01 14:52:41 [INFO]: epoch 609: training loss 0.0302\n",
      "2025-06-01 14:52:41 [INFO]: epoch 610: training loss 0.0273\n",
      "2025-06-01 14:52:41 [INFO]: epoch 611: training loss 0.0303\n",
      "2025-06-01 14:52:41 [INFO]: epoch 612: training loss 0.0316\n",
      "2025-06-01 14:52:41 [INFO]: epoch 613: training loss 0.0235\n",
      "2025-06-01 14:52:41 [INFO]: epoch 614: training loss 0.0254\n",
      "2025-06-01 14:52:41 [INFO]: epoch 615: training loss 0.0272\n",
      "2025-06-01 14:52:41 [INFO]: epoch 616: training loss 0.0253\n",
      "2025-06-01 14:52:41 [INFO]: epoch 617: training loss 0.0245\n",
      "2025-06-01 14:52:41 [INFO]: epoch 618: training loss 0.0294\n",
      "2025-06-01 14:52:41 [INFO]: epoch 619: training loss 0.0254\n",
      "2025-06-01 14:52:41 [INFO]: epoch 620: training loss 0.0211\n",
      "2025-06-01 14:52:41 [INFO]: epoch 621: training loss 0.0249\n",
      "2025-06-01 14:52:41 [INFO]: epoch 622: training loss 0.0236\n",
      "2025-06-01 14:52:41 [INFO]: epoch 623: training loss 0.0224\n",
      "2025-06-01 14:52:41 [INFO]: epoch 624: training loss 0.0257\n",
      "2025-06-01 14:52:42 [INFO]: epoch 625: training loss 0.0258\n",
      "2025-06-01 14:52:42 [INFO]: epoch 626: training loss 0.0208\n",
      "2025-06-01 14:52:42 [INFO]: epoch 627: training loss 0.0212\n",
      "2025-06-01 14:52:42 [INFO]: epoch 628: training loss 0.0221\n",
      "2025-06-01 14:52:42 [INFO]: epoch 629: training loss 0.0235\n",
      "2025-06-01 14:52:42 [INFO]: epoch 630: training loss 0.0245\n",
      "2025-06-01 14:52:42 [INFO]: epoch 631: training loss 0.0239\n",
      "2025-06-01 14:52:42 [INFO]: epoch 632: training loss 0.0227\n",
      "2025-06-01 14:52:42 [INFO]: epoch 633: training loss 0.0262\n",
      "2025-06-01 14:52:42 [INFO]: epoch 634: training loss 0.0213\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-01 14:52:42 [INFO]: epoch 635: training loss 0.0213\n",
      "2025-06-01 14:52:42 [INFO]: epoch 636: training loss 0.0232\n",
      "2025-06-01 14:52:42 [INFO]: epoch 637: training loss 0.0217\n",
      "2025-06-01 14:52:42 [INFO]: epoch 638: training loss 0.0243\n",
      "2025-06-01 14:52:42 [INFO]: epoch 639: training loss 0.0255\n",
      "2025-06-01 14:52:42 [INFO]: epoch 640: training loss 0.0232\n",
      "2025-06-01 14:52:42 [INFO]: epoch 641: training loss 0.0215\n",
      "2025-06-01 14:52:42 [INFO]: epoch 642: training loss 0.0244\n",
      "2025-06-01 14:52:42 [INFO]: epoch 643: training loss 0.0277\n",
      "2025-06-01 14:52:42 [INFO]: epoch 644: training loss 0.0245\n",
      "2025-06-01 14:52:42 [INFO]: epoch 645: training loss 0.0201\n",
      "2025-06-01 14:52:42 [INFO]: epoch 646: training loss 0.0233\n",
      "2025-06-01 14:52:42 [INFO]: epoch 647: training loss 0.0283\n",
      "2025-06-01 14:52:42 [INFO]: epoch 648: training loss 0.0279\n",
      "2025-06-01 14:52:42 [INFO]: epoch 649: training loss 0.0232\n",
      "2025-06-01 14:52:42 [INFO]: epoch 650: training loss 0.0187\n",
      "2025-06-01 14:52:42 [INFO]: epoch 651: training loss 0.0255\n",
      "2025-06-01 14:52:42 [INFO]: epoch 652: training loss 0.0295\n",
      "2025-06-01 14:52:42 [INFO]: epoch 653: training loss 0.0229\n",
      "2025-06-01 14:52:42 [INFO]: epoch 654: training loss 0.0233\n",
      "2025-06-01 14:52:42 [INFO]: epoch 655: training loss 0.0260\n",
      "2025-06-01 14:52:42 [INFO]: epoch 656: training loss 0.0261\n",
      "2025-06-01 14:52:42 [INFO]: epoch 657: training loss 0.0275\n",
      "2025-06-01 14:52:42 [INFO]: epoch 658: training loss 0.0225\n",
      "2025-06-01 14:52:42 [INFO]: epoch 659: training loss 0.0231\n",
      "2025-06-01 14:52:42 [INFO]: epoch 660: training loss 0.0275\n",
      "2025-06-01 14:52:42 [INFO]: epoch 661: training loss 0.0262\n",
      "2025-06-01 14:52:42 [INFO]: epoch 662: training loss 0.0271\n",
      "2025-06-01 14:52:42 [INFO]: epoch 663: training loss 0.0235\n",
      "2025-06-01 14:52:42 [INFO]: epoch 664: training loss 0.0276\n",
      "2025-06-01 14:52:42 [INFO]: epoch 665: training loss 0.0228\n",
      "2025-06-01 14:52:42 [INFO]: epoch 666: training loss 0.0234\n",
      "2025-06-01 14:52:42 [INFO]: epoch 667: training loss 0.0261\n",
      "2025-06-01 14:52:42 [INFO]: epoch 668: training loss 0.0225\n",
      "2025-06-01 14:52:42 [INFO]: epoch 669: training loss 0.0243\n",
      "2025-06-01 14:52:42 [INFO]: epoch 670: training loss 0.0284\n",
      "2025-06-01 14:52:42 [INFO]: epoch 671: training loss 0.0230\n",
      "2025-06-01 14:52:42 [INFO]: epoch 672: training loss 0.0275\n",
      "2025-06-01 14:52:42 [INFO]: epoch 673: training loss 0.0246\n",
      "2025-06-01 14:52:42 [INFO]: epoch 674: training loss 0.0194\n",
      "2025-06-01 14:52:42 [INFO]: epoch 675: training loss 0.0203\n",
      "2025-06-01 14:52:42 [INFO]: epoch 676: training loss 0.0206\n",
      "2025-06-01 14:52:42 [INFO]: epoch 677: training loss 0.0201\n",
      "2025-06-01 14:52:42 [INFO]: epoch 678: training loss 0.0227\n",
      "2025-06-01 14:52:42 [INFO]: epoch 679: training loss 0.0197\n",
      "2025-06-01 14:52:42 [INFO]: epoch 680: training loss 0.0192\n",
      "2025-06-01 14:52:42 [INFO]: epoch 681: training loss 0.0236\n",
      "2025-06-01 14:52:42 [INFO]: epoch 682: training loss 0.0220\n",
      "2025-06-01 14:52:42 [INFO]: epoch 683: training loss 0.0186\n",
      "2025-06-01 14:52:42 [INFO]: epoch 684: training loss 0.0203\n",
      "2025-06-01 14:52:42 [INFO]: epoch 685: training loss 0.0200\n",
      "2025-06-01 14:52:42 [INFO]: epoch 686: training loss 0.0220\n",
      "2025-06-01 14:52:42 [INFO]: epoch 687: training loss 0.0208\n",
      "2025-06-01 14:52:42 [INFO]: epoch 688: training loss 0.0217\n",
      "2025-06-01 14:52:42 [INFO]: epoch 689: training loss 0.0211\n",
      "2025-06-01 14:52:42 [INFO]: epoch 690: training loss 0.0214\n",
      "2025-06-01 14:52:42 [INFO]: epoch 691: training loss 0.0208\n",
      "2025-06-01 14:52:42 [INFO]: epoch 692: training loss 0.0246\n",
      "2025-06-01 14:52:42 [INFO]: epoch 693: training loss 0.0240\n",
      "2025-06-01 14:52:42 [INFO]: epoch 694: training loss 0.0224\n",
      "2025-06-01 14:52:42 [INFO]: epoch 695: training loss 0.0221\n",
      "2025-06-01 14:52:42 [INFO]: epoch 696: training loss 0.0224\n",
      "2025-06-01 14:52:42 [INFO]: epoch 697: training loss 0.0235\n",
      "2025-06-01 14:52:42 [INFO]: epoch 698: training loss 0.0213\n",
      "2025-06-01 14:52:42 [INFO]: epoch 699: training loss 0.0223\n",
      "2025-06-01 14:52:42 [INFO]: epoch 700: training loss 0.0231\n",
      "2025-06-01 14:52:42 [INFO]: epoch 701: training loss 0.0212\n",
      "2025-06-01 14:52:43 [INFO]: epoch 702: training loss 0.0275\n",
      "2025-06-01 14:52:43 [INFO]: epoch 703: training loss 0.0224\n",
      "2025-06-01 14:52:43 [INFO]: epoch 704: training loss 0.0204\n",
      "2025-06-01 14:52:43 [INFO]: epoch 705: training loss 0.0200\n",
      "2025-06-01 14:52:43 [INFO]: epoch 706: training loss 0.0214\n",
      "2025-06-01 14:52:43 [INFO]: epoch 707: training loss 0.0238\n",
      "2025-06-01 14:52:43 [INFO]: epoch 708: training loss 0.0206\n",
      "2025-06-01 14:52:43 [INFO]: epoch 709: training loss 0.0207\n",
      "2025-06-01 14:52:43 [INFO]: epoch 710: training loss 0.0224\n",
      "2025-06-01 14:52:43 [INFO]: epoch 711: training loss 0.0211\n",
      "2025-06-01 14:52:43 [INFO]: epoch 712: training loss 0.0201\n",
      "2025-06-01 14:52:43 [INFO]: epoch 713: training loss 0.0196\n",
      "2025-06-01 14:52:43 [INFO]: epoch 714: training loss 0.0261\n",
      "2025-06-01 14:52:43 [INFO]: epoch 715: training loss 0.0224\n",
      "2025-06-01 14:52:43 [INFO]: epoch 716: training loss 0.0219\n",
      "2025-06-01 14:52:43 [INFO]: epoch 717: training loss 0.0197\n",
      "2025-06-01 14:52:43 [INFO]: epoch 718: training loss 0.0198\n",
      "2025-06-01 14:52:43 [INFO]: epoch 719: training loss 0.0205\n",
      "2025-06-01 14:52:43 [INFO]: epoch 720: training loss 0.0202\n",
      "2025-06-01 14:52:43 [INFO]: epoch 721: training loss 0.0206\n",
      "2025-06-01 14:52:43 [INFO]: epoch 722: training loss 0.0237\n",
      "2025-06-01 14:52:43 [INFO]: epoch 723: training loss 0.0200\n",
      "2025-06-01 14:52:43 [INFO]: epoch 724: training loss 0.0230\n",
      "2025-06-01 14:52:43 [INFO]: epoch 725: training loss 0.0207\n",
      "2025-06-01 14:52:43 [INFO]: epoch 726: training loss 0.0237\n",
      "2025-06-01 14:52:43 [INFO]: epoch 727: training loss 0.0266\n",
      "2025-06-01 14:52:43 [INFO]: epoch 728: training loss 0.0214\n",
      "2025-06-01 14:52:43 [INFO]: epoch 729: training loss 0.0212\n",
      "2025-06-01 14:52:43 [INFO]: epoch 730: training loss 0.0206\n",
      "2025-06-01 14:52:43 [INFO]: epoch 731: training loss 0.0202\n",
      "2025-06-01 14:52:43 [INFO]: epoch 732: training loss 0.0228\n",
      "2025-06-01 14:52:43 [INFO]: epoch 733: training loss 0.0209\n",
      "2025-06-01 14:52:43 [INFO]: epoch 734: training loss 0.0206\n",
      "2025-06-01 14:52:43 [INFO]: epoch 735: training loss 0.0225\n",
      "2025-06-01 14:52:43 [INFO]: epoch 736: training loss 0.0200\n",
      "2025-06-01 14:52:43 [INFO]: epoch 737: training loss 0.0164\n",
      "2025-06-01 14:52:43 [INFO]: epoch 738: training loss 0.0213\n",
      "2025-06-01 14:52:43 [INFO]: epoch 739: training loss 0.0224\n",
      "2025-06-01 14:52:43 [INFO]: epoch 740: training loss 0.0195\n",
      "2025-06-01 14:52:43 [INFO]: epoch 741: training loss 0.0206\n",
      "2025-06-01 14:52:43 [INFO]: epoch 742: training loss 0.0204\n",
      "2025-06-01 14:52:43 [INFO]: epoch 743: training loss 0.0221\n",
      "2025-06-01 14:52:43 [INFO]: epoch 744: training loss 0.0205\n",
      "2025-06-01 14:52:43 [INFO]: epoch 745: training loss 0.0202\n",
      "2025-06-01 14:52:43 [INFO]: epoch 746: training loss 0.0221\n",
      "2025-06-01 14:52:43 [INFO]: epoch 747: training loss 0.0214\n",
      "2025-06-01 14:52:43 [INFO]: epoch 748: training loss 0.0212\n",
      "2025-06-01 14:52:43 [INFO]: epoch 749: training loss 0.0229\n",
      "2025-06-01 14:52:43 [INFO]: epoch 750: training loss 0.0231\n",
      "2025-06-01 14:52:43 [INFO]: epoch 751: training loss 0.0219\n",
      "2025-06-01 14:52:43 [INFO]: epoch 752: training loss 0.0217\n",
      "2025-06-01 14:52:43 [INFO]: epoch 753: training loss 0.0224\n",
      "2025-06-01 14:52:43 [INFO]: epoch 754: training loss 0.0224\n",
      "2025-06-01 14:52:43 [INFO]: epoch 755: training loss 0.0205\n",
      "2025-06-01 14:52:43 [INFO]: epoch 756: training loss 0.0213\n",
      "2025-06-01 14:52:43 [INFO]: epoch 757: training loss 0.0211\n",
      "2025-06-01 14:52:43 [INFO]: epoch 758: training loss 0.0201\n",
      "2025-06-01 14:52:43 [INFO]: epoch 759: training loss 0.0194\n",
      "2025-06-01 14:52:43 [INFO]: epoch 760: training loss 0.0212\n",
      "2025-06-01 14:52:43 [INFO]: epoch 761: training loss 0.0222\n",
      "2025-06-01 14:52:43 [INFO]: epoch 762: training loss 0.0192\n",
      "2025-06-01 14:52:43 [INFO]: epoch 763: training loss 0.0196\n",
      "2025-06-01 14:52:43 [INFO]: epoch 764: training loss 0.0199\n",
      "2025-06-01 14:52:43 [INFO]: epoch 765: training loss 0.0195\n",
      "2025-06-01 14:52:43 [INFO]: epoch 766: training loss 0.0184\n",
      "2025-06-01 14:52:43 [INFO]: epoch 767: training loss 0.0207\n",
      "2025-06-01 14:52:43 [INFO]: epoch 768: training loss 0.0222\n",
      "2025-06-01 14:52:43 [INFO]: epoch 769: training loss 0.0240\n",
      "2025-06-01 14:52:43 [INFO]: epoch 770: training loss 0.0237\n",
      "2025-06-01 14:52:43 [INFO]: epoch 771: training loss 0.0239\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-01 14:52:43 [INFO]: epoch 772: training loss 0.0200\n",
      "2025-06-01 14:52:43 [INFO]: epoch 773: training loss 0.0256\n",
      "2025-06-01 14:52:43 [INFO]: epoch 774: training loss 0.0216\n",
      "2025-06-01 14:52:43 [INFO]: epoch 775: training loss 0.0194\n",
      "2025-06-01 14:52:43 [INFO]: epoch 776: training loss 0.0269\n",
      "2025-06-01 14:52:44 [INFO]: epoch 777: training loss 0.0226\n",
      "2025-06-01 14:52:44 [INFO]: epoch 778: training loss 0.0184\n",
      "2025-06-01 14:52:44 [INFO]: epoch 779: training loss 0.0242\n",
      "2025-06-01 14:52:44 [INFO]: epoch 780: training loss 0.0242\n",
      "2025-06-01 14:52:44 [INFO]: epoch 781: training loss 0.0219\n",
      "2025-06-01 14:52:44 [INFO]: epoch 782: training loss 0.0234\n",
      "2025-06-01 14:52:44 [INFO]: epoch 783: training loss 0.0229\n",
      "2025-06-01 14:52:44 [INFO]: epoch 784: training loss 0.0225\n",
      "2025-06-01 14:52:44 [INFO]: epoch 785: training loss 0.0257\n",
      "2025-06-01 14:52:44 [INFO]: epoch 786: training loss 0.0198\n",
      "2025-06-01 14:52:44 [INFO]: epoch 787: training loss 0.0217\n",
      "2025-06-01 14:52:44 [INFO]: epoch 788: training loss 0.0234\n",
      "2025-06-01 14:52:44 [INFO]: epoch 789: training loss 0.0213\n",
      "2025-06-01 14:52:44 [INFO]: epoch 790: training loss 0.0192\n",
      "2025-06-01 14:52:44 [INFO]: epoch 791: training loss 0.0190\n",
      "2025-06-01 14:52:44 [INFO]: epoch 792: training loss 0.0216\n",
      "2025-06-01 14:52:44 [INFO]: epoch 793: training loss 0.0212\n",
      "2025-06-01 14:52:44 [INFO]: epoch 794: training loss 0.0209\n",
      "2025-06-01 14:52:44 [INFO]: epoch 795: training loss 0.0198\n",
      "2025-06-01 14:52:44 [INFO]: epoch 796: training loss 0.0238\n",
      "2025-06-01 14:52:44 [INFO]: epoch 797: training loss 0.0223\n",
      "2025-06-01 14:52:44 [INFO]: epoch 798: training loss 0.0206\n",
      "2025-06-01 14:52:44 [INFO]: epoch 799: training loss 0.0253\n",
      "2025-06-01 14:52:44 [INFO]: Finished training.\n",
      "2025-06-01 14:52:44 [WARNING]: 🚨DeprecationWarning: The method impute is deprecated. Please use `predict` instead.\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 6/6 [01:03<00:00, 10.61s/it]\n"
     ]
    }
   ],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "imputation_list_3 = []\n",
    "imputation_list_4 = []\n",
    "\n",
    "for model in range(3,5):\n",
    "    \n",
    "    if model ==3:\n",
    "        epoch = 400\n",
    "    else:\n",
    "        epoch = 800\n",
    "        \n",
    "    for i in tqdm( range(6) ):             # 运行5次计算标准差\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        saits = SAITS(n_steps=30, n_features=6, n_layers=2, d_model=128, d_inner=64, n_heads=4, d_k=64, d_v=64, dropout=0.1, batch_size=10, epochs = epoch)\n",
    "\n",
    "        dataset = {\"X\": X_missed_2}\n",
    "\n",
    "        saits.fit(dataset)               \n",
    "        imputation = saits.impute(dataset)  \n",
    "        \n",
    "        if model ==3:\n",
    "            imputation_list_3.append(imputation)\n",
    "        else:\n",
    "            imputation_list_4.append(imputation)\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "9d868c19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "6\n",
      "(6, 30, 6)\n",
      "(6, 30, 6)\n"
     ]
    }
   ],
   "source": [
    "print(len(imputation_list_3))\n",
    "print(len(imputation_list_4))\n",
    "\n",
    "imp_3 = np.array(imputation_list_3).reshape((6,30,6)) \n",
    "print(imp_3.shape)\n",
    "\n",
    "imp_4 = np.array(imputation_list_4).reshape((6,30,6)) \n",
    "print(imp_4.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2ca2f8ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30, 6)\n",
      "(30, 6)\n"
     ]
    }
   ],
   "source": [
    "res_3 = torch.from_numpy(imp_3)\n",
    "\n",
    "res3_005 = torch.quantile( res_3, 0.05, dim=0 ).cpu().numpy()\n",
    "res3_05 = torch.quantile( res_3, 0.5, dim=0 ).cpu().numpy()\n",
    "res3_095 = torch.quantile( res_3, 0.95, dim=0 ).cpu().numpy()\n",
    "res3_x= torch.quantile( res_3, 0.6, dim=0 ).cpu().numpy()\n",
    "print(res3_005.shape)\n",
    "\n",
    "\n",
    "\n",
    "res_4 = torch.from_numpy(imp_4)\n",
    "\n",
    "res4_005 = torch.quantile( res_4, 0.05, dim=0 ).cpu().numpy()\n",
    "res4_05 = torch.quantile( res_4, 0.5, dim=0 ).cpu().numpy()\n",
    "res4_095 = torch.quantile( res_4, 0.95, dim=0 ).cpu().numpy()\n",
    "res4_x= torch.quantile( res_4, 0.6, dim=0 ).cpu().numpy()\n",
    "print(res4_005.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "81da71b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAl8AAADRCAYAAADymtecAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAA9hAAAPYQGoP6dpAACAUUlEQVR4nO2deXhU1fnHP/fOmn0hgRDWoEBABGQTXBAkarBuuK+tWm0RW6HuVGp/KqhVq9hWxWq1VuuGgiAVRHYFIawCAmGHbJA9M9lmu/f3x5mZbJNkErJBzud55iG5y7nnXnLPvOc93/d9FV3XdSQSiUQikUgkbYLa3h2QSCQSiUQi6UxI40sikUgkEomkDZHGl0QikUgkEkkbIo0viUQikUgkkjZEGl8SiUQikUgkbYg0viQSiUQikUjaEGl8SSQSiUQikbQh0viSSCQSiUQiaUOM7d2BlkLTNLKzs4mIiEBRlPbujkQiaSF0Xcdut5OYmIiqnp7zRTk+SSRnJs0dn84Y4ys7O5tevXq1dzckEkkrkZGRQc+ePdu7G81Cjk8SyZlNU8enM8b4ioiIAMQDiIyMbOfeSCSSlsJms9GrVy//O346IscnieTMpLnj0xljfPlc+ZGRkXJwk0jOQE7n5To5PkkkZzZNHZ9OTwGFRCKRSCQSyWmKNL4kEolEIpFI2pAmGV9lZWU8+OCDzJo1ixkzZuBwOOo9LjY2FkVRUBSFhQsXBrVv8eLFTJ8+nalTp7JixYpm3pJE0sLouvhIJBKJ5PRA09q7Bw2i6Hrw3yq//OUvmTJlClOmTOE///kPO3bs4NVXX61z3N///ncSEhKIiYkBYOLEiRgMhgb37du3j1/+8pds2rQJXdcZNWoUX3/9NT169AiqbzabjaioKEpKSqSmQtKynNwDzjLoNbq9e9I0dB3sJyCye3v35JQ4E97tM+EeJJLTBmcZHF4LfS8Ea1SrXqq573bQnq/s7Gzmz5/P5MmTAZg8eTLz5s3DbrfXOM7j8bBkyRKGDh1KSkoKKSkpfsOroX1z584lNTUVRVFQVZVx48bx1ltvBX0jEkmrUZIBtqz27kXTKc2FjDSoLGnvnkgkEknbUZIF9hyoKGrvntRL0MbXmjVriIuLw2q1AhAfH4/ZbCYtLa3GccuXL2f9+vUkJydzxRVXkJubG9S+VatW0adPH//v/fv3Z+3atc2+MYmkRXBVCiPGYQe3s7170zQqi6GiQPRdIpFIOgO6DgUHoSy/Q499QRtfWVlZxMbG1tgWERFBdnZ2jW2TJ0/Gbrezdu1asrKyuPrqq9G8a68N7avdfqC2q+NwOLDZbDU+EkmLU1EEjlJwV4KztL170zTKC0X/nWXt3ROJRCJpG8oLwZYNqkH83EEJ2vhSFMXv9fLhdDoxmUwBjx0/fjyrV6/m4MGDbNy4sdF9tduvr20fL7zwAlFRUf6PzB4taRUqisDjBLfj9DK+dF243V2VHXr2J5FIJC2KLVNMOEO7QFlehw2WCtr4SkxMpKSkpnaktLSUxMTEes+Jj4/n5ptvJiMjo9F9tdv31Uqqj5kzZ1JSUuL/BLqGRHLKlJ4EoxnQTy8PkrNUaL1MIWIAkkgkkjMdjxvyD4A1AkyhYtXCVd7evQpI0MbXhAkTyMzMxOkUuhffkuCYMWMaPM9oNDJ8+PBG902aNIn9+/f79x08eJCJEyfW267FYvFni5ZZoyWtgsclvEfmcECBytNoabuiGIqOwuZ3ofgYaJ727pFEIpG0LqUnoTQPQmLFxNNV0WE9/03yfKWmpvpF8MuXL2fatGlYrVZeeeUV0tPTAVi0aBF79+4FID09ncjISAYOHNjovuq5vdxuN2lpadx///0tdJsSSTOoKBYvriUcTNbTy4NUWSJmgBVFkH+oww5AEolE0mKUZIDuBqMFDCbQ3B127GtSbcd58+bx5JNPsmnTJgoLC3nxxRcB+OSTT+jbty8DBw4kLS2Nu+66i/HjxzNx4kSee+45//kN7Rs2bBj33HMPjz76KE6nk9dee42EhIQWuk2JpBlUFIkIR6NVfCpLhFvbcBqURC3NFQORxyX+dZZCSHR790oikUhaB1cF5B8Ea3TVNoUOu2LRpCSrHRmZxFDS4hxdD9k7oEs/ofeqLIEhN3R8I0bzwI5PYflMMetLGArX/xPiB7Z3z5rFmfBunwn3IJG0KR6XGMtM1saPBSg4BOlLIaavmDhvfheSrxTj38DJrdbNVk+yKpF0KjRNhCtbwsTvRqs33cRpILqvLIGC/SJCUzEI3VoHdb1LJBJJQPL2waGVwedXLDwCiio+2/4jzi86JtJNeFyt29dmII0viSQQlcXCiDFHiN9VA+ja6ZFuorJY6L10XURqVtqg6Eh790oikUiCx3ZCGFA5Oxs/tqIYSo6L9BLH1sOJnULvVZLZYUX30viSSAJRUSReWlNIze2O08D4qigRkY4gPF+6Brl7ZcSjRCI5PfC4oSwXDBbI2SGMqIawZYuxWXPDT5+KiadqFJnu3dL4kkhOH8oLhVhTUaq2GS1Qnt9uXQqakgzxUQ3CBY8ORcc75AAkkUgkdXDYRH6uyESxZJiRBs568nVpmjCyjFbY8TE47WLSrBrFJLqypEOOfadB2JZE0sbouphpmUJrbjdaobxIvOxq685blu3OYe6KAxzJLyMpLowZKf1JHdK98RPdDuGm9zhFqLWiiI+tc0c8lpWV8fjjjxMTE0NpaSl/+ctfsFgsdY57/vnn0TQNVVUpLy/nueeeQ/Ea4IsXL2blypU4HA5uvPFGUlJS2vo2JJLOQWWx0NgarRDdU4jps3dAn3E1J8QgUgDZc0Q+w6wtoHrHPdUALhfYOmaBben5kkhq47BDZRFYImpuN1rBXQ6u1hXdL9udw9SPtpF+wo7DrZF+ws7Uj7axbHdO4ydXFAudhK6LJUcfJdmnR7BAK/HAAw+QkpLC7NmzGTFiBDNnzqxzzJIlSzhy5AizZs3ij3/8I9nZ2Xz++ecA7Nu3j9mzZzN37lzefPNNHn/8cbKystr6NiSSzkFlCeh4jSgjRHaHEz8F1q7asqC8AHbOB3Qx6QSv+F7xJl7N7XBlhqTxJZHUpqIInBVgCqu53RgCLkerGzFzVxxAQYw9UDUGvb7yQOMnV5ZA4SHxs2+GqBjEcmlZQSv0tuOTnZ3N/PnzmTxZhJtPnjyZefPmYbfXXIrYu3dvjW0hISH+kmdz584lNTUVRVFQVZVx48bx1ltvtd1NSCSdCfsJMFXzTFsihScrY3PNJUSPSwQXHV4LjhIxQa6Orgv9q9Pe4coMSeNLIqlNRZEQqddeWjQYRfbkVhbdH8kvo/YcTdfhcF4QRp/9BBR79V4+VAPoHhEB1AlZs2YNcXFxWK1iYI6Pj8dsNpOWllbjuGuuuYbFixfzxRdfUFRURH5+PnfddRcAq1atok+fPv5j+/fv76/2IZFI6sFV2fRz3E4xUaw9+Y3sIca3zC1C+gFCaH/0B8j5CVQjOgrbshw8ubSI1763Ca+ZPcdbraRjBUtJzZdEUhtbVs3EfllbhbcrabxwQ7VyuomkuDD25dhqaBsUdPrFhzVwFsJCy9gEmlNECflPNoh9eftExGN1w6wTkJWVRWxsbI1tERER/vq0PgYOHMjHH3/M7bffziWXXMKXX35JSEhIwDYCnV8dh8OBw+Hw/26zdcws2xJJq+EohUOroOdosWwY9Hk2Ie0oy4cDyyFxOHTpL8atqJ4s23GUuQvdHCl2kxShMcOVSf8CFx/v9vDxT2Xsy3P7m5qY1JXh3YDi46LdiG4tfpvNRXq+JJLqOMvFS28O9/5eBlv/Dbu/FN4wg0lEQrYiE/tY6ohKdRSmTxrQ8InOMji527tOWe3VVhTxe3FGh4z6aW0URfF7vXw4nU5MJlOdYysrK1m8eDGHDh3itttuw+12B2yjvvN9vPDCC0RFRfk/vXr1aqG7kUhOEwoOQOHhptfErSwRAUMHv4N9S2DNi/C/R2DHxyzbV8jUzV1Jz3fgcGvsK9SYWnof534zkKdXlLAvz43VqJAQISaYC/d4PW8lWR2uzJA0viSS6lQUCc+WxWt87V/mdVnbhIvbZBXizlYUb/53xRYAVFc5BkVcR3c5GBnfyDUriyF/v/i5dkSQrokkhKdDktgWJjEx0a/d8lFaWkpiYmKNbWvWrGHLli2kpKSwZs0atm/fzquvvhqwDbvdXuf86sycOZOSkhL/JyMjowXvSCLp4DhK4eQeEX3dWI6u2lQUC21twSEwmMWEt7IYDnzL3O0eFDR0fHpWFV3TiL7gNi7vb+XfN3Xh5KyevJgaDcDCPRVirLZlNt0IbGWk8SWRVKei0Ls0ZxQergPLhbtb08SynTFEuMRdFa1y+be+XIktoi+65uFtXuTAnCvBfhLFZGH2C7MbPrk0T7jXAy0rqgahl+iEnq8JEyaQmZmJ0ynKlPiWC8eMGVPjuC+++IL+/fsDwtiaPXs269atA2DSpEns37/ff+zBgweZOHFivde0WCxERkbW+EgknYaCg8LYie4l/m1KkJI9RyRY9TjFOKwaRd4uYwiHPQnotcwWRVUJ79aLb3/djV+NDCfSqnL1oBAMKuw64eJQkSbyHpbmiuStHQRpfEkk1bGfELMtgL2LqnLNKEDuPm+6CUereJDcbg+vrDwMQN+cFVw2ayGqqjI6TohLl9r7NtxARpqI/lEDSDkVgxjM8oOImDzDSExMJDU11S+QX758OdOmTcNqtfLKK6+Qnp4OwPDhw9m+fbv/PEVR/Aba1KlTWbFiBQBut5u0tDTuv//+Nr4TieQ0wFkGuT9DSIyIUnTYxWpBMLgqxepDiddTXE0+8b/0CiryM9F9YnsvChpnqSdqbIsNNXBJkpAJLNznEiL+omMi6rGDII0vicSH2wn2kyK/V0mmiKJRDFWaqdw9IgLS7WwV4+vxN+fjiUxEd5TxztC9ENMbgKdun4SueXDGD2bFks8Dn6x5hPEFNfVePny1KXN+avF+nw7MmzePzz77jNmzZ7Nz507mzJkDwCeffMKuXbsAuPfee+natSsvvvgi7777LgcPHuSJJ54AYNiwYdxzzz08+uijPPzww7z22mskJCS02/1IJB2WgoNCNxsW5x139ODT3Dhswnjz1aZVFDyazlPfFnHVv/Mo+OFjFFVF0YUBpqCjozLd/FUdKciUc0SwzMI9lWLsKzrUoTz/MtpRIvHh03tFdIft/xEual9tR9UolhqLjwN6i+f6Kiix8+UBN0oYXFC4mAEvfe3fN3xgEpH2r7FHncUrn68m5aqb6zZQWQJ5e0Xfauu9wGuQKUIE2wkjHuPi4nj33XfrbN+6dav/Z1VVefbZZ+tt45577mmVvkkkZwy+oB9rdNUk0BwiPFmJwwOPTdWpLBHjsC0bDEZO2j3c/mk+qw4J4fy9XXZzhekfvOm5hsNKb/pF6EzvmU5q5t6qWrzea1x3Tii/X1zEj8ednLBrJHSwgCPp+ZJIfFQUimW7wkPCQ+QrzwPebPFe3ZdqFKLQFuT+Vz5FCYsB+0nmXRVTM9UFcOMo4QXbG30h5eUBkgWW5onyGkpDRpWs8SiRSFqRwsNQmg+hcVXbLBEiyXMw4055kZjgam5+OO5hxN9zWHWokjCzwn9vjeMf18RwtXkLS5O/Jf2ZFJbe1YPUJDNcOF0YXq4KvwesZ5SR0T3N6DosSneKdjtQmSFpfEkkPkpzhbG1a763fmM1x7CiiBQOuXuFYVTWcgW2dx08ztYykUPqNscCom78W51jHrtjMnp5MUpoDK+++H91G8lMq6rn6EWvHZGpqGIG2gkjHiUSSSvjLIcTuyAkqmaCanM4OMqC033Zc8CWyZqjLia8k0u2zcOgribSHkzg9uFhVSL8UfeD0QLh3YQXv8tZcNGMOgbYlHNEfd6Fe53C+MtLb4Ubbx7S+JJIQCzF2XJE7bDCw2A013WRqwaRykExCgPG7QjcVhOZ+o+vUcxWTPn7mfPbGwK65kOtFpKtIk/NZ8cDJFs9vlH863X1Z9vcnPNaDhPePoGmeY0w1dvv4hZKe9DBMkZLJJJ2pPCQmJSGxtfc7lt+bGzC6iwXnqn8A/x1fTkeDa5KDiHtwQQGdzMLg0rzQI9R0GuUOCe8K4TEipWI+GS46A9gDgW3MMB8xteqww5KKlxw8mdxnQ6ANL4kEhAvb2UR7P9WvOSBIgZVozC47Dni5T5F42PZ7hwumPMtmRZRtmZK+C7U0fXrih69/kIAbF3PY9fmDVU73I4qIb2iYHdo/OL9XPbmulh7xMGKg95Eg4oqhKcnd59SvwFhoG59v+k5fCQSyZmHqwJO7BZLjLXLsgFYwoTXvVakYg0cNijJ5EReIUsPiLQwL18ZQ7jF257HIUq8nXsjhHkNPIMJYvtWLWnGDxQGmEkYYMnxRgbGG3F54JsDbjFudRDZRZOMr7KyMh588EFmzZrFjBkzapTPqH1cbGwsiqKgKAoLFy4EoKSkhJtuuonIyEjOO+88Nm7cGNR5EkmrU1EIx34U+WWMlsDH+GZwhYdPOd3Est05TP1oG9k2F4qigK7zufUGlu3Oqfecy8YOxVx8FEU1MHveB1U7SnOFx04x4Pbo3PpxPjtyXP7d8zbZa/b/VI2v3HRYNhNWz4HdC06tLYlEcvpTcEiMnWFdA++3RAqvVmVx/W1UlkDBQf67Q3i9xvY2k9zVK6PQdWG4JY6EXufXNPAivKWLNG8Or7gBcNHDosSax1G19LjHAYVHTk/j64EHHiAlJYXZs2czYsQIZs6cGfC49957j7fffpvvvvuO7777jmuuuQaAv/zlL1x//fWsXr2aXr16ce2111JWVtboeRJJa7Fsdw6pc9cx8LX9pP50IcvcI0E1UOnS+fmkk0U/l/PPTXYKyjxe3ZcudF9wShGPc1ccoEZkoqKgKPD6yobzcKUmC23YJvNINN8s8vhG8DjRVSO/X1zIN+kVhJgU3r+xCwCL91aQbXNXXavgoHDfN4esHfDVVFGzzeMStSQlEknnxVUhJnSWyMBeLxCeKFd5w6XZygvRC4/w/nbhqb97ZHjVPo9DSEH6XwZRPWqeF94NrFHCePMR118cq2lMGSyCl5YecFJZfBJKTzbnLlucoI2v7Oxs5s+fz+TJkwGYPHky8+bNw26vaUV6PB6WLFnC0KFDSUlJISUlBYNBRGClpKRw2223MXLkSD766COKiorYs2dPo+dJJK2Bz/u074Qdhwf2eRKZWvk7en/ak9CnjzPktRyu+zCP3y4s5DcLvIOGaoCiwyLXV/WXvYkczisFamq7dB0O5zVs0M365ZXozgqI6sG7f/+L2JghPMiv/FDGvE2lKAp8fGscd48K56K+Fjwa/Guzz0uniGSDzZn9HV4LX9wtBlqDWXjSTuwSRphEIqkf+8kWj5DuMBQeFvcXFl//Mb5ciaW5gffrOpRksvXn/fyc68FqVLhlaFjVPl2DHiMh8TyxtFkdkxVi+kBFrfF44GSwRjKqm06PSAOlTp0VBytEcFIHIGjja82aNcTFxfmLy8bHx2M2m0lLq3kjy5cvZ/369SQnJ3PFFVeQm1v1sC+99FL/z76SGz179mz0vEA4HA5sNluNj0TSFIT3qRreOmHu4beg6xBpURjRQ2S7X7innPQ8b/Z4jwtKT4jw6WaiBBDrKwr0iw8gpq9G19goenrE0uS/thaJgSljM/N3O3h8aTEAr/0ihuu8rvap54vZ4ztppXg0XRiP5fliiaAe/N7AWUtJnbuOZbtyYNcX8MW9YMsSWf4NJvEsSnOh6GgznoBE0knQdTj2A+z9WuiizqTJiqtS3JM1ovHcgeZwoREN5HV3lUPePv69WRhQU84JITrEa564HaKsW+8LhJEViMgegFZTU2YOh0FXo6pwndf7tXCvE3J2Nd/z34IEbXxlZWURGxtbY1tERIS/TpqPyZMnY7fbWbt2LVlZWVx99dVVyyPV2L9/PxMmTKB79+5NOs/HCy+8QFRUlP/Tq1evYG9FIgHgSH5dL5OiqoR17UnurJ4U/18vtv6+O1cPCkHX4ZV1tirdVEkmVNqaNZB+vW4rlZp3oPJlavauaE6fNKDR86deMRyAE/FjyUzfzobte7lrgfBkPXRBBNMvqqojeMOQMLqEqmSUeFiaXiEMJk0TRW8D4PMGpp+w43BrpJ+wM/W/21i24H0hiDWFVA2yBhPoHji2IWBbEokEoQ2t9GZuP7RaBPXYshs/73Sg8LBYxqtP61UdSwQ4igPn2qosoTJrNx/vrLXkqGvi03ssdOlXpe+qTXg3b/u1nDD9JkJYN6YMFAFUi/c5cOfu7xC6r6CNL0VR/F4vH06nE5PJFPDY8ePHs3r1ag4ePFhHWA/wxhtv8PLLLzf5PB8zZ86kpKTE/8nIaKHweUmnISkurE5JCgWNs425xIcbhBAeeOISYcz8Z1spOXaPOCf/gNA6NFF073Z7eOSTzSgGA6FF+xnUxYDFqJKcEMG8O0eSOqTxkjV3pF4IZYUoJgsXv5fBLe4/o/YbxzWDQnj1qpgax1pNin8gm7eptNGIx7krDqAgUprh/VdB43XHVUK3Ub10ke/nrC1NegYSSaei0iY8O1E9RMmwkgzY940I8Dnd07UUHRHLfsFUzDCFCE9ZoHxflSV8vXYbRRViiXDS2V5bw+0UqSMShkGX/jXyGNbAEg6RPesK+g0mGHI945PMxIQo5JfrrN+xv0MkWw3a+EpMTKSkpOaaamlpKYmJifWeEx8fz80331zHMPr222+59NJLSUpKatJ51bFYLP6lS99HImkKk/uHg6L4k5EqaKJOmPWbGsdd2NfKhX0sOD0wd71dhDvbssUg0kTR/fTXP8UZ1QvdWc5HXT9m6eNXkj57Mkunjw/K8AJYvuckhAkvtK4aUbv0oeuUp7jnphQMat0cYb8ZI4yvb9IrOFbsDRzwBQ3U4kh+GXqtbToqh7TugUuD6EDWtjpGrEQi8eKwiwmPahRayZi+wljI2AT7/icmcg2lYOiouB1CQG8Or/+YksyqKEQQz8B+ou5xtiz+/YP4vv/liDAxjukaoEHSJRDRDaIaWd2K7i2uVXss6jUGU/xZXD3QKyHZXSqWStuZoI2vCRMmkJmZidMp8m/4lhvHjBnT4HlGo5Hhw4f7f//55585fvw41157bZPOk0hamu0/iYLKqrsSi6qRrGYyL+QtUs11i08/7vV+zdtop8Spipe8+FiTZq77j2WzJEO4v8cXLGTErNXN6rfPO+VDUVUUNN52/yLg8QPiTUw624quw7s+4b2vxmMtAnkDdU1DK8oUmrfaqAah+SrLa9a9SCRnPBVF1PmqtUYJT47TDgeWw6GVwstzOlFZIjx6ptDA+wuPwMpn4OdqKaMsEcIgqy7X0HVytn/HsgNCB+tfcnQ7wBQG3YcKr1ZoTdlTHcK7gjms7mqEosLQm/1Rjwv3OtAz299b3yTPV2pqKmvXrgWEQH7atGlYrVZeeeUV0tNF2v5Fixaxd6+YVaenpxMZGcnAgQMBOHToEO+88w6XXXYZR48e5aeffuKtt95q9DyJpDXYlCkyHV+Yv4D0p8ezNOIFUg2bAx57VXIIg7uasDl03k4rE16g4uNNWna8668LUazhGAoP884dg8XstxnU55067OlW7zm/9Xq/3t1ciktToSQ7oOt9cKSjpodL11BUlezVHzP89Rzm/mCrypgP3gAEZ1WGfYlEUpPSE3VqtQIiLUNkD4hIEN6vUwjgaRcqS8S7X19exPRvhHG2f5nIAwYiHYXDXjPlhMPOhwuWoelwQR8zA+JNXm+ZDoOvBdUsygc1VpTbGgXhCYGjSuOTufyi0YSY4HiJxo6Na5tzxy1Kk/J8zZs3j88++4zZs2ezc+dO5syZA8Ann3zCrl3Ci5CWlsb555/PVVddxZIlS3juuecAyMnJYcKECbz++uskJSWRlJTE8OHD/SL++s6TSFqDYzl5lIeLfDG/7XNCvLi9z/cL4GujqgqPjRfer7k/2HG4dSE2DbLG45tfrOBkaF90XeM56ydYL5/V7L4nxYVRexhS0OhnqD9/zbWDQ+kWrnLC7mFxugs0Z52lx4ISOwu2ZgFgdNmxqDqDDJnM8fydi9hOpVvnD0uKmPjOSQ4XeGeuqgHQITOw0SqRdGpcFcLYqM87BGKf5j6l1DXtQnkRtdPl+CnJgqytYLCCxw1b3vMaambxbzVDU68o5t+rhfPm7pHhwvPucULXc0RqiZCYurm9AqEoIhrSXRlwd+jIW0jt7/V+fb9L/N+0I00yvuLi4nj33XeZNWsWr776KmazWEPdunUrN954IwBz5szBZrOxZMkSHnnkEVRv0rXu3buTkZGBrus1PrfcckuD50kkrcE/vlyNohpQi45x8S+fEhsHXAHo9YYh3z48jJ5RBnLsHj78ySlmb4VHGtVr2MsqeHm10DP0zf6O22f965T6PiOlvxDC+/Kz+rRqliX1nmM2Ktw7Sni/3t5c4Y14rKl7uGX2hxARj16az5qzPiP9+atY2uPf3BGxhW/v7cq8KbGEmRXWHXEw9PUc3k2zV3Uia+sp3ZNEckZSaRO1BM0NGF8gPMhBTuQ6DGUnhYg+EOlLRSS0wSQ8YyWZsNc7PhnMNaI9N6/9lr0nHYSY4OahYaJ0mykMRt0jnl/c2fVfpzbhCcKYDVS/MaoH140fCsDCnSXC29iOSAtH0ilZsU9olAaVb4OzJ4mNg67xlqQInD7CbFT4gzeNw8s/lKF53JC3p96lR1++rKHPrUSP6IruKOXD849C10Gn1PfUId2Zd+cIkhMiRKRknJV5ke+TqmxqUPh+/5hwFAW+O1jJwUKthufrg/+t44AqBK2/c31Izwe8ZYP6XiSy7wO/PT+CndO7c0mShTKnzv0LCtmS6QAMoq12nklKJB0Ohx00lzA4GkJRhRD9dBHeuyqEbCGQR680VyR+VgxicqYaxBJr+jdismqJEPfqEh6q998Xk9HrzwklyuQBFBh2mzjOYBJC+mAJiYGwLqJObwCuuvU+wgaOo+AXc+n/t2Mij2EDJd1aE2l8STod9rIKCs1CH3VHzM9VO8K7QuIIMWOrh/vHhBNtVdmf72ZRuhvy9geMeKyeL0v3uuYVSzh7xr/VIveQOqQ7S6ePF5GSj6SQemmKGOACJG/1kRRrInWAcLv/c0uF3/gqsZfxf98cRFFUumWv4bGZ/1fl0Uq6BFD83sB+XUysur8b1w0WM9H/bCsT0Z+uMhH1KJFIqqgsod6lORDShR/mwurZkL9fCPBPBypLxLgXyKO3/9u6BqdvUrv1fTCGinxc5QVUlpfzyQoxbtw9MkyclzgC+lwgvGMRCcKbFSyqCrH9Anu+gLTKHsRd9xSm+D64dFXkMfxoW7sYYNL4knQ65i1cjWIOQS8t4Obb7q65c8AV3iKugQ2wCIvKtHFi+e4v35eh5x8ImLCvdr4sAAWd11cfapmbqI6iwDlT4KwUoVmrHtpdi9+OEaU53t/uwJF7EDxubp3zIXpEN/SyQj4d8bPwdvlIukSIhbUqb6CqKvzmfNHOpz+V4dINYsZ+XNZ5lEhqUHoysCC94JAwulbNhpztItIxd+/po/uqtIlxprZHr7wQjn5f5fXyoSjiORQfg4PfiglueQGLvviEknIXvaJULu2tC0H+8NuFhyy0C/QaU3+9yPoITxDXCqD9mrtDB3QUb45Cn3yjsZq6rYE0viSnP5U2OLQm6DX8xVuPANC7ZBvGsb+puXPQ1WJA0QIvPQI8dEEkFiNsynTz/YFCyE+vsV/TNPafsAWISFQard3YbCK6wZjfQHQvMZDXs/z4i+QQekQayC/XWbCjkE8XLGKPJnL13e/4iKTp/6t5QkgUdDu3TiDCZWdbiQ9TySvT+O6gN0JSJluVSKrwuOouzRUcgh9eg9VzhNGlKKJ0DgjD5HSp/1hRSECP3sHvhPc90DKragBUof2qKIaSTP793rsA/Gq4BVU1wNBbRP7E2CQxEY6sP49ovYTFibQUtZ+lx82REr1Ov4OpqdsaSONLcnpTdAzSl0HWZiH6bkR35HZ7OO6JBuC6sJ11w5dj+kC3cxqs/dUtwsA93lw0f1lXDsd/9O/btu8ww37/FlqAgSmY2o2nRPdzYfgdYinAHfg5GA0K948JJ2TAOP4U8zxPbrGgqAYiCvcy64knAodz970I0GsYdEaDwm3DxL18tN2recvZcfpoViSS1saX2d4UKsYpv9G1o8roMpi9uihVpK6xt4/+qMnYc+ouOTpsonySotafFsJoEZGMexaTdWgPy78X3vJfDbNAjxHCa9VjBJydIvRbzUE1QHSSyMGoa8IALjwMxcdIigg0MuutOy7Xg7HNryiRtAQeF+TshOxt4gWLSxYvWF46JA6v97RPv/sRJTQa3VHOb1JHBz5owOViVqpp9bq8H7k4kg+LzmX7hbfTf3Uv+u1Zi7Ushx2FBpSIvugeN4rBiIKOjtKk2o3NJiRGLBPm7oO9i8UgF2AGevaoi+k6ciouXUfxZvi3xw5iWcUgUgO1e9alsH6ueObGqvbuPC+Mv22w89WeCuzXhBNRXiiSt8bL/HwSCQ6b8AKpBvj+r2JJ0WAEQ0hd40Q1imWynJ1w1iRxXEfFWSbupbbY/uAqMekzCl3p9iwna49U0jvayFmxRpJijURaVTBaWJYfzx+PRNPzD19itmdxMH41Zw8YD/3GC0/7qWY7iEwQz73wsEgjlHAuRPdmRpiBqZ/s9I/HAoVrk9u+Qk4H/h+WSOqhokjoi/L3Q1g8hESL7SHRcGKXEFxaA79MH6/dDfSiS+EOIib/KXD7g66BdX8VS49q4ASCByNHETflN+iahgsh3ESJQDGDKT+dv0V9jHLzR7y+IZfDeWX0iw9j+qQBQZcQajbxA+GsCWDLFMuAirHOQPahchVoml/3oCiKX/eQOiRA4dqEcyE0zpvFvsr4GtXTzIA4I/vz3SzY6+JXQ1VRZFsaXxKJt8izLipAOOxi4qLW85WrGEB3CAlDZYmI2OuoVJYIQXtItT66yuHgCkABReXrPeXc8N88XLUWEOLCVBKHXkjJ+N+hm8QY5Iruw9TSe5ln6Elq92Et08fwbtBjFITGiGLc3oTWqTEwz2Dk9ZUHOJRbhtNRCUYzr3/2LfdeOgSzqe1MIml8SU4fdG9i04yNUFYI0X1qeGIIixO6r7x06BXYq7XXboIISDHurD/DfNwAiOsPefuAwMbX3IorUfRqnjGvByncXcT2u1TMlwj9U+ro5ObebfOwhEPCUOg3QeTWsed4i2kLoSmKyhGtW80C2TSiezCHQs+RIndPNRRF4c7zwnj6uxI+2lHBr4aGQ+YWkZ9HIunslBWAahLLjOjCwKrF1kwHD/+viEiLysJbwjAWHBLFoTu68aV7anrnDq3xJpO1siy9ghu9htfonmZUBQ4Vuskv08gv0zAl34xJE5UzAFBUFOD1TSWkjm2hPhpM0Dtw6cPUId39k8wVabv49Wf7qeg6hDsffZbPX3+2hTrQOFLzJTk9cDtEIdoDy4Wuq8tZNQ2vkz+LrMph8eLn6uUrvKze8jN6RAK6x83vxjbgZlYU6H+ZsEjqEa4f0bqh1zJgFEXBHdIF8yUzmnOHLUdcf4g9S0RARvaAiESRW6z3BZA0niSLHaVWOECjerS+F4t/a2nh7hguzll5qJJsm0eK7iUSEJKFslwxccnZKcaRakuNFS6NJ5cWcf6bJ1h3xMGSfRVsyPQIL1lHF92X16pV6a6EA9+CorDykIMpH+bh9MANQ0JZ/0ACG6clkDerByVPJ7Ljd/GExveoMry86LSP6D1lzLlcmyiuu8kyig/nL2qza0vjS3J6kLtHGF+hscKgqK6Z0NywaR6se0kMBI6SOqVzAN75Rog7Q/N/pveNLzR8veSrvXULA0c9JqknUagpLhcGTPPqNbYoRosoRhsWDynPQOrzcMHvhIi1/xXMGN/Dr0MDgtOj9ZsgZvG10lj062Liwj4WdB0+2e2C4gwolUW2JZ0cp10szWluKD5aw0v0w9FKhr+ew1/W2vBoEB8mvoa/2ucUy3fZ29up00Gg66JWZXWx/dEfoLKYdRk6V3+QR6Vb5+pBIXx8SxdMikdM2DSNSLOHYd0UzjacbPrkrxWZ+4c76VK8F0U18KfVBWSeLGiT60rjS9LxcVUKEbk1WuSBqU1eunB5V5bAhr+J0hR5+0Sm5WpsyXECME7bATGNZE3uPlQcU0/KiRnWb9BR/QZYmwjqm0JMkojcLMmAgsNCJ5cwBAZdTeqkS2tmyE+IYN6dIxvWo0X1EuHfAXKI3XmeN+rxJ4d4Xsc3tNZdSSSnB75Ix8JjorahaqTUofH7RYWMf/sk+/PddI8wsOiX8bw9RSwxLtrrQNc8kPOTP/t7h8NZKsZZn/HlcUP6UjYcc3Hlv/OpcOmkDrAy/454zAbdm+hQ9y5TmmHIDcyYPLzpk79WRFVVFj1zD9hPQmQ3Uv/vI1LnrmPgrKWtmgFfar4kHZ+SDFH3LLZv4P3Z2wBd1P+y58D2/8Cg6+DEbjhrIigK6ceycUQkogAPDAhceqIGqkEkLS18p86SAbpOqrKBeSEeXrf+lsN2te0E9cFiMArtV0WxyBLd7RwhPPXeR3XdQ1CYrELAmr+/zvO46dxQHvq6kB05LnafdDMkIw0GX9vCNySRnEY4bIAmyo8pCt8ddPCbBQUcLRLL9veOCuOvv4glOkSlzKlhNSocLnSzO1fj3Pz93mhCa/veQyB8RmVYnPj9+AY2789h8ocllDl1Us62suCueCwGwOMRRpc5DM69SeQh7HIWqcC8mBxeX3mgbYORGqBnty7MvmoAT60qpDTqbPadsAGKPwP+vDtHNG28DAJpfEk6NpomvFj1RQppmhB5e6NsMJiFl8yyGswhIvIuqgdvLFyLokZiKDjM6GnPBXft5F/Aln/VTLGg60JzphpJTZ1C6uirW+xWW5zo3uIerNGnHroNYunxp0/EgKpU/V90CTNw5cAQFu2p4L8/VfLCeVL3JenkVHh1UTk7efmHMh7/VuiK+kQbeOeGLlzWv6pQdJhZ5bL+Vr7eW8GifU7O7XPIO2nq1j59b4jKErGMqBpB19m+agGXf1CCzaFzSZKFRb+MJ8SkepcaXUJreuvHENWzRjNNnvy1AXdOvog5axdQgQFfItbqGfBbur9y2VHSsSk9USWkD0ThQTEbM5jE76pR/Jy5SST8O7ETNI01B4QA/1zHdugdZEhNz1HCW1R96dFdKd7GC2fAyA4e1acoQiPXEoYXiFIf5vCAOjjf0uN/dznRTu4FpyyyLemk6LqQPFSWUGYv5rk1os7gA2PD2f2HxBqGl4/rBotlvK/2OoWEIm9fm3Y5aMoL/JPgnzev5bI3DlBcqXNBHwtL7u5KqFmtmqCGxMBNH9QxvDoyHmPd/5vWyoAvjS9Jx6bgkPiyN9V9KQCvOFWrGcZtMInfj6yDnxdRcGwPJVYxa/lVfBMGNaNFLFuCd0CpFP+O+CVc9IeWM2pOF8LioOvggBGgVyWHEmVVyCjR+P6QTVQbkEg6I65yMSEsOsZnO8uxO3TO7mLkH9fEEm4JPGZcNSgERYGt2S4yip2iakY9kdbthqaBvUps/+Dz71FQrjOmp5lv7uladW+uCjEGXzVX6ERPI/rFhdXJgN9awQCd7NtDclpRUSyML5++oDa6BhlpiCXHWq+MwQxoLNuZwWX/3INisoDHhWX89Kb1YeCVom1XhVhuG3Q1TPpzzTQXnQWjBXqPBrQ6tR6tJoWbzhUD1Ic7KmuUXJJIOhWVNjFe5Kfzzy1COH//mHBUtZ6SO0DXcAMX9hE5BRfvc4vobmfbp15oEGepiOI0hZG2ZTtr9+VjMsCXd8UTZfWaEm4noMPo+yD5ynbtbnOYkdLfv9QIrRsMII0vScel+JjQGFijAu8vOiYKvKqmuvsUhWXaOKZWTqNQ80bmqEam/RjetOiV3uMgNF4YG33HwxUv1J+ctTPQ71LhVWwg6nH+z04qj6S1dc8kwLLdOW0SqSVpAIcdXKX89PM+NmW6MRng7pENjBm6Dh431w4S3v2v9jpEsujK4rbpb7D4MtubQnn5rX8DcPuwUHpGefWfmlt8el8AY6e1Xz9PgdQh3ZseCd5MpPEl6Zi4nd70EpH1F2nN3i6MIrVu5miAuZVXilQQ1aYxPvFk0FjCYejNombi5L9ARNcm3sgZRlx/UfzWU9f4urivhV5RBmwOnSWr1ssi223Mst05TP1oG+kn7Djcmj9SSxpgbUxFMRRl8M9NNgCmnBNK1/AAY5Sui+TR7grwOLk2WUwi1xx1UVyYD4VH2rDTQVBZArrGoYwTLNggxtBHx3snxpomxuwu/WDsVBFhfZqSOqQ7S6ePJ332ZJZOH99qUZhNMr7Kysp48MEHmTVrFjNmzMDhcNR7XGxsrLdmnMLChQv9+xYvXsz06dOZOnUqK1asqHFeQ/sknYySDFFLMLS+JUcdMjeLn+sxzo5o3dBr/Yk3Szx5/m/hypcg7uymnXcmEhIraj1CHU2Kqirc4cv5tblQpKWQtBlzVxxAAX/6yuqRWpI2pOwkZbmHRN474Ddjanm9qhtdikF4k7sPo38MDO5qwq3BN/s74NJ9WT4YTLw6799oOlw5wMKQBLP3fiqFwTXkJug2pP4Js8RPk1JNPPDAA0yZMoUpU6bwn//8h5kzZ/Lqq6/WOe69997j7bffJiYmBoCJE4Voed++fcyePZtNmzah6zqjRo3i66+/pkePHg3uk3QydF18cavGmvXDqmPLhtKTgZccvfTkBAe1mqUsmiWePI1ncS2O0SyiRQ8sF17HWvXq7jwvjNezz2HzuNsZMPcA/brmMiOlf4cLKz8TOZJfRm2JdmtFaknqwe2A8iI+X7ENm0PnrC5GJvbz5uvSdfA4xHtjMEPfCTBwsojkLjwMJ3dz3SAze3JdLNrn4vasbcKj1BECezQNSk+QV+rm/f8Jo/CxS6KqIhst4cLw6jpIVCCRNErQ/6vZ2dnMnz+fyZMnAzB58mTmzZuH3W6vcZzH42HJkiUMHTqUlJQUUlJSMBjEAD137lxSU1NRFAVVVRk3bhxvvfVWo/sknYzSk8LzVV96CfAuOXoC5/7yEvLT58Lw0jtoFvrTlb4XCyMsQPb/jJjRdJ3yFMa4Pjg1VS59tSFxoSp6LW9ke5Zt6ZRU2qDkOP/8UZSouX+0V2jv83SpJjj7cqEdHfHLqjEuth90O8e/9PjNfieOnJ9FqbSOgMMGzjLe+HwlFU4Po3oYuaSfVdyXwQijfi3S2nQd1DGMxdOAoJ/SmjVriIuLw2oVVnx8fDxms5m0tJrC2uXLl7N+/XqSk5O54ooryM2tKvGyatUq+vTp4/+9f//+rF27ttF9gXA4HNhsthofyRlCwWFwOWrWD6tN5mZqhKXUYtcJJ998u5rchXPo48nAonhaVTzZqYhIEIW7A2i65lZcCbrm9zbKpa+2o/hkNopvhuFFTjbaGIeNnVs2sjHDhVH1Cu09LjFRPOtSSH0RzrsjcAR38i8YlWgmMUKl1KmzememqJXaEagsodxewj8+/RaAx8ZHoWhuQBPZ68PjIaqHKEMmCYqgja+srCxiY2NrbIuIiCA7O7vGtsmTJ2O321m7di1ZWVlcffXVaN5BunYb1c9vaF8gXnjhBaKiovyfXr3kf/oZQaUNCg6IWVR9+DxjhvqXHJ9YWoSuw1XmHayNnU365PRWFU92KkJjRbkidOFV1NxCbOuq4IjWVVQaqIZc+mp95q/cRFlIV3Rdo4d+wu/ttRYdkn/zbYnDxj+/ETnurjsnlG7hqjC+EobCeb9seFyLG4jabSDXJIs0Nl/tqYTjG9ui141TWcK///cjBfZKkmJUrj8nRHi+E4YJzZrbIcaE+mQikjoEbXwpiuL3evlwOp2YTHW/ABVFYfz48axevZqDBw+ycePGgG1UP7+hfYGYOXMmJSUl/k9GRhNnCGX5IipF0rEoPi6iakKi6z8me0dViYsArDpYwdL0SowqPH9FtPj2jx/UGr3tnBhMIu2GYhCJZzW3qMYREkuSqQillvJIof2XvoIJFlqzZo0/SMj3GT16dI026gskam+e+VJ84XfNWc/6Ry/i/Zv6oWseKmPO4tNPPmrn3nUeynMO8eHGkwD8dky4EKKbQ+G8uxoXoSsKDPwF1w3y5vtKd6Id39TaXQ4Kjy2HVz8RQXAPXxiBUXOIsmWj7oXyfOENj+7TcCOSGgRtpiYmJlJSUnP9ubS0lMTExHrPiY+P5+abb/YbRrXbsNvt/vMb2hcIi8WCxWIJtvt1yUsXM5KzJjS/DUnL4nGJshrmsDrekxpkemsHBhjMNE3n8aXFAEwdE8bZcWbh8o8f2Aod7sQkDoNzroeoRDCFCgM3JIYZtr5M/a6iTtRdey99BRMstGrVKj7//HO/F3316tW43VUpNeoLJGpv/r1kHaWRfdA1D68kLIeuLzKxK3R7/ztyI/oz59tD3Hpbe/eyE+Bx8/knH2Jz6PSLNXBpkhF0Fwy5EcKDTFGTcC4TzutPxGdbybFrbN70I+df72rQy9/qaB4WLl7KoZxiYkMU7hlhEbquUfeCJUJEpfce1zkTT58CQXu+JkyYQGZmJk6nE8C/JDhmzJgGzzMajQwfPhyASZMmsX9/Vfj5wYMH/QNYQ/taB10k8XSUtuI1JE2iJFMsKTYktC8vFJFB9bi3P9tZztYsJxEWhT+lxAjDy2iFmNOrzEWHJ7SLMMDMYcJL2XscDL6W1EmXiiSF3SMwKML80j0elBN72q2rwQYL3Xfffdx0002MHTuWsWPHcvjwYaZMmQI0HEjUnmiaxovf/AxAzxNruGTmV/59L96dgq55sCeM4vNPpfer1XHY+OeiHwCv0F5zQXwy9GvC95iiYDn3Gq7sLwyZRZuPi8judkSvKOal94WX93fnhxBm1OHsFOg+THi9wuJPuzJCHYGgja/ExERSU1P9Ivjly5czbdo0rFYrr7zyCunp6QAsWrSIvXv3ApCenk5kZCQDBwqvQ/X8XW63m7S0NO6///5G97Ua5YXt/oct8aLrUHAQUBqe5eXsEMtcAVJMONw6Ty0vBuCJ8RF0jbSIY+MHSC1CSxMSKwrm9hoLg6+DvhdAWBegKknhoReuIqIoHcVgYMbHm/zaz7Ym2GCh3r17+3/WNI29e/cyZMgQoOFAokC0VUDQG1+soDKyN7rbxdy+GyCm6h4uHT2EbmWHAZi99GCrXF9Sxa7tm/nxYKEQ2g8zinJcI+9uevRf4giuHSF0el/tbX/d17pV37F573GsRvjdGAtE94ZzbhB1Hp1lQutVX+1dSb006a9i3rx5fPbZZ8yePZudO3cyZ84cAD755BN27doFQFpaGueffz5XXXUVS5Ys4bnnnvOfP2zYMO655x4effRRHn74YV577TUSEhIa3ddquMqEF6WjFTDtjJQXinJB4Q14vQAyvQWbAyw5vrXRzpFCN90jVGZcHCU0SYoqxK6SlsVgFHUv+4xrUET81gNXorudVHQdyp9eeKkNO1hFsMFC1fnxxx8ZN26c//eGAokC0RYBQZqm8bc1Igt6v5MrGf3Y4jrHPP+rSeiaB1v30Xw5/9OWuCjYcuSYGYB33hapka5NtpAQocI5UyCiGfntVJUrr7sJkwH25nnYv3FZC/e0abz8+hsA3D3cTHx0GIy8RwQ8qUbhAes2pF37d7rSJHdAXFwc7777bp3tW7du9f88Z84cv1EWiHvuuadZ+1oFg1l4viqKGo5CkbQ+5QXgKhfhyvVRaYP89IBC++IKjedWCc3gsymRhFm9mZd1Teq9WosgslhfNDyZ8z5fxQ6tDx/ldGd6QTFdu0S3ft+q0ZRgIR8LFy7k+uuvr9OOL5AoOTmZjRs3csEFFwQ8f+bMmTz88MP+3202W4sbYC9++D9ckT3RXQ7+MWh3wNJXKWPOJf7fK8mP7M8z/9vLDTed4kXz98OJneJLtzljZl668GzH9jvFjnQsysvL+XChSMPwm1EWcX/9L2t2e1HJ45lw1lt8t7+MRcvX8thvW6qnwbNsdw5/+d8uDg9/mO69b2F0zGJIPktIOeLOhp7n+73dkqbTubOhmcNFEdSWXnrc9SWsfbll2zzTKcurky29Djk/CVF+gCXEv6wtobBcY3BXI3ePihRt6R7h+YqTxld78v6TvwTbSZTwOH75xOw2v35zgoU2bdrE2LFjA+6rHUgUCIvFQmRkZI1PS+J2e3h3k4iqG5z7Lef84at6j33hl5ei6xq2hDEs+OIUvF9l+ZCZJnSZzmamDik8DHlnVtmpZbtzmPjiciJ/+zF97vs77qSLhRi9gQTQjaIauS5FGPaLtmQKB0Eb4qsTeqTQiWI0Y47vw7Om6SzTx4kky2dfJg2vU6RzG1+KApYwkVeqpfQoFSWw4s+Q9k/weFqmzTMdTYOSLFGioiGyt4l/a0VCZhS7mfuDEE+/eEUkRp9HQ3OLAbBrckv3WNIEYiLD+e1Y4ZXZG3Mx365Y1abXb2qw0O7duxk8eDBqA1qd6oFE7cGf//UVWmR3dEcZb47MhJCoeo+9bOxQ4ksPAfDM180MfHA74fgm4X3W9eYZX5omjIhTMd46GD4j5aTDgGI0Q2wfplU+wLKinqfc9jU33ELIgHEcnvQqA55bT+rcdW1WKcJXJ9Tv3VZUFHReP9oDepzXvtGXZwid2/gCIRwuzRWel5Yg7W0hRHTYoaiDVaXvqFQWi/IVDRlfznI4+TOodb1jf1peTKVbZ3xfM1cNDq/yoGluCO8GITGt029J0Mz85VVEFO1HMRiZ8XFam4rvgw0W8vHVV1/5oxx9NBRI1JIs251D6tx1DJy1NOCX7bLdOVzx2lr+e1ik2Ukq20nStC8abXfOnRPRdY2S7ufz1YL5Te9Yzk9QeBCie4nJj7MZUeLOUpH3ymETXrQzgLkrvJUbfBNCn5Hy06lr4na7utN1ylOY4vrg1JQ2LdUVsE4oCocLna1+7c6CNL5MIWJAsGWdelvlBbD5X+Jn3VPlqZE0THmhKM5qbCBi5sRP3jpiNWdc27Ic/Ge7mEW/PDkKpfp+XYOEc1ujx5Jm8Pa0K9FdDiq6nsuIWQvqNTBag2CChXysXr2aSy+9tMa2hgKJWgqfF2XfCTsOt8Y+75ft4/9eySffbuD/Pv2eqR9tI/2kHRQFXdc5GjuOZfuLG237inHDiLML79fD6/WmPfuiY5CzHcK6Cp2s0SzGuqbiLKtKynuGGF9H8ut68HQUDrdASca5O3TQ9XYp1ZUUF1YnqELWCW1ZpPEFYI2C/ANCT3Qq/PgGVBSKvFIAJ3Y1fLxEUJYnZo71CbjtJ2DHJ2J/tSVHXdf5wxJRRuiO4SGM6R1SUzemqKLQq6RDcMGwgfQzi2+lIo8Vh1trs9m8L1ho1qxZvPrqq5jNIo/S1q1bufHGG2scu3LlSv9+H3PmzMFms7FkyRIeeeSRBpckm4t/qacWn++rZObqIv69w5euQhwlMu0H/2V846XnA6CZwoJ/9g67WG7Uqao6YbSK5cOmei+dZWJSao0SORbbKfVIS9K3S2jdYubo9AtzQOERMba5KprV9pES6oyJbVWq656xPWtc21cytL2TJZ9JSOMLxLJUeYH4km8u9pOw7QPxs08QfrL9EkueNmia8DrWV0S7ohh+eBUcJVVGrZevfq5g3REHVqPC85dH1hTi65r4SOOrQ2GMTxKzee/ALgtvVxFoqQcAXUe359b5kvfuCvrLeG2mq2bRbYQZV++z1zyQsRnsORBZLQrZaBUGhas8qOv6cZaKi1oihbe7srhp53dA+lnKULxeSPAaKShMn9BHeN1Vs0hEmn9AeBAd9uDSdFTaSApz1C3V1Ubep2MHxRI7HhcWVSc5IYJ5d46UdUJbEJl5EoQrXddEBfnoZoaDr58rxPb+ZHMK5O0VL1oQIflBU1EMhYcgcUTLttteOEpELUdrAMGwq1w8V/tJMFlr3K/DrfPYUhEB9OjF4fSOMdf0evnE9jLSsUNxvLCi3WbzHZ2kuDDST9hrfN0qCiQnRLL0gQmkvpFGem5Fnf3BfhkfyS+r++yBvdklbNt3mBHJtdI/5KVD7h4xJlb39BmtYrLqLGs8SKY6FUVCNmAKFQZdWX7Q6SqW7c5h7ooDHMkvIykujBkp/Ukd0owcWi3M5r1HIKwvZpcdNTSafvFhTJ80oMpI8biFkVleKDxhtixx7+Zwce+1JpS4ykUeNaOVGWPPYup35ejeyYqCjq4rbeJ9Wrr9GJj60PPEWn74z6tnxndNB0MaXz5CooVR0+O8pmfrLc6Enz4Ry1w+QbhqEEuQtuyGc1c1lbI8Ea4dP6h+b9HpRHmhENPXTkboccGPb4oBy2ipE+H4xo92DhW4SYgw8MTFPpF9tQFCc3vLCvVt9VuQBE99BobUksCMlP5M/Wibf4nHv9STMgAs4cy4fFDg/UF+GQd69gAoKlPe/YkEfTUh3fqSY3OSFGthxlknSO0ZUXc8NJiEUeEsBboFd3O6Lgw23yRKMUDpCVF9ohF8WjhfvVDfcum8O0e0qwF2Ir+IPHN3FOAZ99+5ffaGugcZjBAWJz5x/YUBassSnjD7CfEcQ6KFMVZ6EtBFXsKEc0mNSGBetxwe+GAjusFMpLuIv9x9WZt4n45WhoAJLrPulYZXKyGXHX1Yo4UHpjk5v374qxiIjNUKfSsG4bbPamHRvcMuXuDmRBt1RMoKhM1U/QXXNNjyL6GZM5rrRDjml3l4dmUxAHMujyLcqtaNgtTcYhCTZYU6FDNS+vuXGkFqSaqTOqS7qIuZEIHFqNZZ6mlsf2MEevYAxpIMFKOZk6YEjhRUCD1YbjlTf4xiWX49FSeUJqabcFeK432eHkuEWGkIQmfr08JVL9TeEZaqX/hoGYrJglKcya33Tm/8BEUR3q6Ec2HwteLTa7TwKpaegMhEGDgZzpoEEVX/5708QpPX4+T3bWJ4/bBjH3pEV3SPm1+Pa3/v4pmK/GbyoXpL0RQdhS5nBX9e4WHY9QVQywBQDeBBZIMefHXL9bOsQHiKHHYIr5vR+rRC18GWKYozV9+263M4/qOYYQdIVPjMihJKKnWGdzfxq/NCvGL9WsaXospIxw6Iz4B4feUBDueV1V2m6eSkDuneoDensf2NtR3o2V8+eDJjnv6KfJe5mhZP8aZMgNS+ATwfilFIIILFWSYMMJ+8wBopJrpl+RDZ8P0ETHvQAZaql+8vhshIRpZvQB3+QdNOVg3C2IpMhO7DxSpJWHzA/Fn94sI4boMstW3ekf8s3wzEYi3cT8/rnm6Ta3ZGpPFVndBYKD4uEglag8xIve4VcFeAsdYSoG9amduContNg4oCIXY9EzxfDpvQQ5gjqrbtXwr7lwljKsBAtDfXxVubRELVV6+KxaBQ10DzlxWSyVU7IqdiQEhOjfqevZ0QUGpGHzaYMsFkbVq6CWepSNRq8EaRGszC61XeuPHVOzaE/SdL/YYhiEhns1YZ/PVbmK17D1Me0RMFeKTfsVNrzGQFU/3VFob3S2DNDje2kPqPaUk2HrNBRCxDXbsh9rE2uWZnRC47VscSIQyCYJceT+6BPYuEoVBf6HlLGl/OUiHINJiEB6w5HFoNe5fA8TRR5sNXmb49CuWWF4KjrEq7dmyD14uo1FzCrcaj/yvCo8G1g0OY2M9cpR+pjq+skKzpKJEERVJcWN00F7pGv3B34BOMVq9B5QjuAs6yuvICk1UsPTZCrCu/RkShL1rWrpm59clXg7t+C/PS56tRFBVL7s+M+83cVr3WRUPPBkCL6E758Z2teq3S8kpKLELHd3OXg616rc6ONL6qo6gi0WfhoXqNkRoZqN/azrLKIWAIbCigGoRx01xDqTbOUpGk0Jcao6kGk9sBJZliaTUzDQ58C7sXwq758PNCOLoecvcKo6gtjLHyQu+A7F3u3fK+MJzqMbyW76/gm/QKjCq8NDlGeLcUtY4Y3x/pKI0viSQoauvBdE28W7/nU7ESUBujtUrHFQyVtrrvqSVSVBdxNOzF33ZMjJ+h7hIsRpVB3SPpUXkYgI0M5PKHXmzTigmaprE5VzyoFH0jJA5v1euNSE5Cd1agGIys/7YZ1QmawIdL16OYQ9DLi5ly4x2teq3OTudddjz6A2RthbNTIKqXf9RZlh/H3JUujpQtJSk+vEZIc52oG3ckU3mIeaZ/kmr+qe41FCN4nJC9A/pPOvU+O0qFcWIJ92aLLq+pl2qMSps4J6pnlYHjcYllzMoSEQKtaaLNyB4QmyQiNZtyjWDRdSjJEGHnAPu/Fc/KFBIwusbt0XnkfyK1xO/GRTAgzigCGgIJ6jW36L8sKySRBEV1Pdih3DIq8o9SsO6/FJy7C6KOwvjHakY9Gi1iMucsCy5dRHm+MNhKssSkNCJByA3K8kUEdz0pK+av3IQrsie6x8WX3T9k8KPLvHvGc/uTL7GBc9gfei4jHnqLbmcN4WhBeZ1UFC2dpmLB6s1oEd3Q3U6eGFvPxLsFUVUVU0UhbnMPftx9kMta8VqLNx8AetGteCfGsXNa8UqSzmt8HfwOsrfD0e9FmoPe41jGBUzdGCPyqaD7Q5pvSrZidZaw4KgBMKDjE6WqKLrGq+WTAxtfqgHcmiiN0yLGlx2xJBfiXbKzN80wctiEgVPds2QwiY9P46brwsNWdATy94uCvTFJEN1bPCejOXDbTcVZKqI2LeHCi5e52Rv0EDis+b0tpew+6SI2VOXpSVGAN9Y+QK1HdA26D22ZfkoknYTqerAXn3yQmQd+ZE6RgduHHsSw8S244KGqyY6iivcsGO2pxyUmfkYrbPg7eBxw2bPCO63rUJonJnoBmPv1FrD2Jf7kJgb/6d819n384uM8NOfvLCrpQ3F4X4pPiLJL1VNRAC2epmLesm1g6E2XvC30fubvzWqjqcSYXOQBu4tat6D1fpsBIuES0x4ZKd7KdN5lRx3AGyVXkgW7v2BuWikKWjXjSjB/XyUfHrZQphmhljJCV1TSXd1InJPJ5f86yR++LuS9zaXYKjWvHkkRBaFbgvJ8YfwYTMK704i7vg6VQRQcUxShfYvpA7H9xPPJ2Sl0YrsXwIndzet7bXz5vUzhcHgNaK4qMW4tsm1u/vRdMQB/nhRFTKhBDPxqrdxe/nuQZYUkklPhwSeeISbUSHq+hy/2ekRh7a3viffOh6JCpb3xxryRjgePn+CRT3aTefy4eOdBTL5KjgcsNXT8RD6ZBmEgPRD5PUTWjfb721O/J9asCz1YtaoJuq7z2w/SmPrBJlEhwHv8qaapqHQ4OeAUEZs3WdNE/q42oE+0mDAfpfUiHncdPI4nMhFd17h/WAtNsiX10nmNLxDf2waTEHwbQziiJaAHeiS6jjlvL6qrrjBd1zRcBZnk2D18d6CSuevt/PrLAn63qLDqoJMtYLBoGpQX1Sw+7Qxi4KtOWV69eqqAqKpYUujST3i+HDYxCHvqEeE2BZ+uzOMUQQAodbxeOTY3M74u5KyXsskt1RgQZ+SBsRFV/wcB0lD4ywrJSEeJpNlExMQx444rAZi9xo6mGIUmdNf8qvfPaBEpEhrDa3z9+R//5dX1Zdz4mQ33rq+E18waKVJWVBTVOe1P7y1BMVlQi45xz30P1dt8OeYakZDgrXtpMIrxPUBFhf0n7M3Sif19/gqUkCj08mKmXzOuyec3l3N6CyOv0Nx6EY/v/U8kiTUVHmbAjX9qtetIBJ3b+KqOotBDPyGEptU3ozOoq4X97z7Cm/dcDN5itmKfhqKqvNFrOT9OS+DdG2K5f4zQLiz8uZxKl+71rGU2LSFhIJyl4C6vSlJoMDdNyO9xCX2FqZlZ8Q1GCOsi9GGOAALcpmLLFNFOxzeI5dNqRqHP6Or3Ujavr7dT6dYZ19vCl3fGYzIo9QvtoZrYXhpfEsmp8NAfZxNpVdl90s2idLfwNKcvFYE64C2wXdx4gWxnGbrmYe32/QBsynQzZ/kJ4U03hggdanl+jVM0TWNdljDyUpyrUYffXG/zgSI1FXQSlGJitcDBQx4dBkx7m5lvfcHi7RlVQVRz1zVYaPyzTUcA6Fe0kdDUtsuBNW5wXwBcET3QSvMbPriZfH9QfJ8kV+5q9SACiTS+/Lg9OsXrP0FRVb9r3V8k9YpzQVFqZpg2QLKaxTzrP5gStouxvS38enQE866LpWeUgVKnzncHKsSApbmEx+hUcJYKw8fkNb6MVjFgBTt7c9i95zexdFJ1jCFV4vxTwVkmDEdzKBxY7o94zLa5mb64kKSXsvxG1wV9LCz/dVfWP9CNIQleV7iue71eAZYcNbe4x+g+p9ZHiaSTE90rmd9fLXRTz60qRjeYAQV2zhdSDaNVTAhdjUwsHXaO5hSSVVB13HNry9m0eqnI7K4a66T3eevLlSLLurOcZy9q2FsfsGoCCv93xySe/2VKrQmzMMR0twt3dG8+ORbCQ5/tZN8Jm8js79WEBTLAfOWEAH4Ts6VNNVEXDU9G1zwollB2rvykxdt3utzkGboAMCVqX4u3L6lLk4yvsrIyHnzwQWbNmsWMGTNwOBrO8fLpp58yYcIE/+9r1qwR7uBqn9GjR9doPzY21r9v4cKFTbubU+CFNSVsX7+O0q+f5yw9A4viCVi+I3VId5ZOH0/6M5exdPAqUg1basysVFVhyjnCu/Tl7nJvmSFNiPtPBUepV+fkfeFNoWLG2NjA5z/fJgyn2oVcm4KiiLIip+r5Ki8UBljhUbCfQFNMPLG0iH4vZfG3DXYcbrigj4Xvft2VH6Z247L+IVXLCrpelZ4iEJpbFNOWYlGJ5NQwmJjx2CzCzArbs118k14hPNSaG/L2irHE5Wjcq19ewPc7RSLSsb1M3DYsFI8Gd35eSOnmT8XSY0lWjZxh764VXrK+ed+TcNfbDTbfUNmlOvu6RzLvzpF8d98Azsv7Bl3zeFup0ovVpwmrXk7olnv/ENwzbCHCQ62oZcLj9f2mrS3e/hcrN6FYI9Adpdxx9eUt3r6kLk36hnrggQeYMmUKU6ZM4T//+Q8zZ87k1VcDJ7nLzs5mzpw5dOnSxb9t1apVfP755/Tq1QuA1atX43ZX6Yfee+893n77bWJiRIqAiRMnNvmGmsPG4w6eWSm8OXMH7+HOyGfg8tkw5pr6TzKaYdQ9cOwHEb1Tzai5YUgof99gZ/HeCly6gklRRELWU8FRS99lsgoNl8MuBPKNUWlDRAieorNTNYvrngoVhaC74dAK0HX+tKqUl9YKg+7CPhb+LyWKSWdb6+g4AO+So6FuYlUfsqyQRNJixA0czbRJ/Xh56SGeW1nClQNDhBcpb59I06O5Gza+NA0qClm3TYx/F/c1M3NiDN8fdXCw0MMj7/3A24MmiTGsvAAiE/lp/1EKQ3uhAI/FbwgqoruhqgmB9yWw8F9vMOCpb3B6aul4dThwsq6e1l9OqKIZ5YRagAgqsAHbs1s+s//89XuARGIKf8Y68YkWb19Sl6C/ibOzs5k/fz6TJ08GYPLkycybNw+7PbDo+8UXX+SBBx6ose2+++7jpptuYuzYsYwdO5bDhw8zZcoUADweD0uWLGHo0KGkpKSQkpKCwVDPF2wLYndo3PlZPh4Nbhsayh3DrMK71GNE4ycnDoceo8QAU837dVFfC13DVYoqNFYfqhT7TjXisTy/plheNYqcX8FGPJbl1xtN2CTMYVCaL3JsNZeSLLF0mbuXT3a7eH61MLzeuT6W76d2I6W6p6s6uu5dcqzn78JfVkgmV5VIWoSwrjz8wD1YjbApw8nKg5VigpO7zyt5UBpON+EqB1cF3+8QnqSL+5iIMXv4z01dUBT455YKFn/yrvB6eSd1f/7PdyiqAUvuHq56aG6r3l6/+PBA4gVcHp0rHnmdvCIxNm3Zc4jyiJ4APHaq5YSaSY8IMe4d8rR8Td+f88V4foH6c+vkdZTUIWjja82aNcTFxWG1Cg9PfHw8ZrOZtLS0Ose+88473HnnnYSG1hR39+7d2/+zpmns3buXIUOGALB8+XLWr19PcnIyV1xxBbm5uQ32x+FwYLPZanyaw4yvCzlU4KZ3tIE3r4lCURWxpNctCO9JeFcYfI2IqPE4/ZsNqsJ1g6stPaoGKDoMblez+ojmEUt1xgB6rdoesYDna1CWe2p6Lx+mULHU2dylR2e5GGQzt7Ilo5J7FwiP4+OXRHLfmIjARpcfPXA5If9uX1khKbaXSFoEg5GEEVfym4tEyZnnVpWIiZ+zTCRJNhgaLrDtLONkbi77s4pQFLiotwHQmNhH5+GLhMf+vv8e4uTB7VB8nPJKB9ttYpy6Xl0LfS9q1duroxfzblcUhXTT2Yz60wJ+/fpi7vpwF4qiorgqKJr4Qqv2qT6Su0cDkNtAHcjmcCwnD0eEaPOe/uUt2rakfoI2vrKysoiNrZnJOCIiguzsmkLJgwcPYrPZGDNmTIPt/fjjj4wbVxWqO3nyZOx2O2vXriUrK4urr766wVDgF154gaioKP/Ht5TZFL782cF7W8pQFPjw5liiQ736rMTzgksmqhqg9/nQfZj44q/m/bphiDC+vtpTjgdVzOxO7mpyHwFvqHY1sb0Po6VOlFDg8+3ebPjNjHSsjskqShwFKjkSDBVFYM8hZ99mrvvERqVb5xfJITx/RXTj5/pyezWk95JlhSSSliUykcd/dRVmA6w74mDdUZd4F/PTvYE/DURdO0v5wRvlOKSrgZgQAwz8BZjDmTPRwrkJJvLKdO57+Uv0ogz+8v5ClNAY9PJinrr6nFa/tbqaMKEXeyDhMNhOooTHszLHQLkmFDq60crUbwobjIhsLUYOEJ638rAeInq9hXjn6+9RVANKcQajb5GFtNuKoI0vRVH8Xi8fTqcTk6kq467H4+HNN99kxowZjba3cOFC/5Jj9WuMHz+e1atXc/DgQTZu3Fjv+TNnzqSkpMT/ychovEBrdTKLKrl/sdAqPHlJJOP7mKsKZPc+P/iGIhKh30RQa3q/Jp5lJSZEJbdU44djHuG9ytnRpD768UU61hbLG0PEwNfYEmClTXicjC1gfCkqoDXf81VRSOWRTUz5KI8sm8agriY+vjUOg9qQx4sqw7Y+rxcI4ysiAUKim9c3iURSl7B4eoy4gntHi+Siz63yvvt56VUFtl316JCcZazzGl8X9zYKz/k518Gkp7F07cd/bwjHbICVnmGc/6WZD46KJa/elemEX/18a98ZUC2IavZklk4fT+qQBJ6Y8Xu2vnATFncpureQN+CPnGxuktZTYcIIkThaCYslc/M3Ldbuqj3CkDyrbCecdWmLtStpmKCNr8TEREpKaqYYKC0tJTGxygW6YcMG5s2bR5cuXYiOjmbatGn88MMPREdH12lv06ZNjB07NuC14uPjufnmmxs0qCwWC5GRkTU+waJpGne/v5eiCp2RPcz836RI73KW93H0GBl0W4TGQrdzhLdMq/J+mQwK1wwS7vMv91SK9pubHb52pKMPnxeqsRIfDjugCcOyJVBNQkPWDPTCI/zmH9+xKdNNTIjK4l/GE2kNol+6DtRTTsh/jAYJsqyQRNKiqCrED+DJ28ZjUGHFwUoOFmlCdK+aGi6wXVHM9zu8xlcfI3QbIsax8G4w4Y+ce8Hl3HvjRLpOeYqTxm6gqOi6Tkbs+Szb07D0pLXpEhUB1sg6Ughdh8N5p5i3sRn07NYFvVwko12z5rsWaVPTNLI8wqj+RZhMMdGWBP1tPGHCBDIzM3E6hXfHt9xYfXlx9OjR7Nmzhx07drBjxw6effZZRo0axY4dO2q0tXv3bgYPHozagDFgNBoZPnx4E24leF577TVW7isi1AT/vTUOs8Er4tbcQpSeeF7wjSmKKMPTc0wd7Zdv6XHB7nI0jwa5zRTd16frMoaI5cjGRPcVhaLINwSfF6whTCFQmtv0tlyV/PXll/hwWxkGFebfEcfZcUHUKvMJ6evL7eVDUaGr1HtJJC1ORCJ9RlzKxH4i6OeLPS6vfvOkt8B24DGo5ORxfjoshPQX9zVDwpCqnUYzjPwVR86ZCrqG4p38KoqCQvt4l2oTMIGrAv3i20eUHuoSXseth08x4tzLNxt2oITFoLsc3HvZsBZpUxIcTfJ8paamsnbtWkAI5KdNm4bVauWVV14hPT0dq9VK3759/R+fQL9v37412vrqq6/qLDkuWrSIvXv3ApCenk5kZCQDB7a8dkfXdbZs2QLAa5PDGBjnNUpUo0iGGj8QrFFNazQyUZTf6XV+De3XZf1DCDcrZNk8pGVrkH+gecZP7UhHH6pXo9aQ50vXhaFkChHHfjcL1r/efM0WCO2Ysyzo8kbLdueQOncdZz+9gleVWwkZMI7XfhHDpLODDADQvXUyG8rd5S8rJGs6SiQtTmgXiO3HTRecDcD8XRWAd0xDF5rS2rgdbNiyA03X6Rej0iPaIjxftThSGVZHx6nTPt6l2gRM4KrD9EkD2qU/Xb1D5r6K6FNua9nuHB7/33EAVDR+7DPtlNuUBE+T1qHmzZvHZ599xuzZs9m5cydz5swB4JNPPmHXruDF5KtXr+bSS2uuLaelpXH++edz1VVXsWTJEp577rmmdC1oFEXh448/ZulDw7h/pEUYS6o3b5SiQu8Lmt6oNVIYXz1GijxYXu+X1aRwlW/pca93ppjfxNlcQ5GO4oagooGM865yYWiZQqHwENhyIGsrrPhz89NfGEOr2m2EZbtzmPrRNvadsOPWFUxxfeg65SnOHnVxcNfypZcwmGjwz1WK7SWS1kNVIbYf110xAVWBbdkuDhd4xNIjauCxwFnG91vFGHNxbwNE9hAyDR+ucrBlkxTq8Gee99Ge3qXqNJTAtT04q6t4JlmGU7u+b1yuEUjwxf52CSTorDQpyWpcXBzvvvtune1btwbOuHv33Xdz991319m+cuXKOtvmzJnjN+ZaG0VRSB3SBXIyxQbVVOU5aYreqzoxfYQx0+cCOLzGm4ld4YYhoXz6Uzlf/lzJS5dZULK3QtcmGAjOUrG0WF8iVV+ZofqotIlBLrSLV/CvCy9YeSF8/1cYeCUMvq5pGeF9JZgqS4CGo0znrjiAIq4KgKKqKGj8zXElky1BlFzSvVq1QEW0qyPLCkkkrUtkIl3PGsaEs8NZdaCUL/Y6eTwhHQZPCRzx6Czl+60iuer4vmaRF9FVCeV54KwQHvTQOGZc3I2p3xT5vUrt7V2qTUMJXNua4UndWbXdhd3a45Ta8Y/LAQIJOsq9nul08tqOepXHy+MS3pXmGl+RiRASI4yvatqvyQNDCDEpHCny8NMJremie4cv0tHr+fK4RaFuHyarSN/gcddzvq3Ku5e5BX+We1/Or71fw5oXoPRk0/qlGhsOMfdyJL+s1pwWdFQOe7o1fg1dF/1VTTSo9QJhfMUny7JCEklrERIDUYncdIlIATF/t0OMTeX5UFlcJ+q60lZI2j4xVl2cZIGug0VusIge0D8FzpkCg68ldfwFHcq71JEZP7w/AFpkAqVHm1+yLuC43E6BBJ2VTm58KVUibs0l3OKRzbT6TSFCeK8aoe/FYiDSPISZVVIHiBQRX/7saLro3umLdPRG+R1aAatmC4MLqopd16e/qigGFFG4tizPa8ggppdGi/gUHIQVz8CxDTVylTWIOTQo0X1SXBjUXlJAo596ovFr6B5v+o+GIhx9ehNFzKwlEknr4A0uuj51IqoCW7JcHClyCYMqQMRj2qaNON0a3cIUzu4WIZJSW8Kh7wUiQjwszh+BHSjdg6QuQ/v3QXeWo6gGflj2RbPbSYoLqzPWd5Sl3s5C5za+lOqpC3Toc+GptRfdS3iVBl8rPGFuB+i6P+rxy71Or0aiCTjs+L0+ug5H14sM8xmbxDajteGIR19m++ztXg9YLc+QahD7nWWQ9g6k/VMYYXnpwljT6vGomUKFwddImosZKf2p7rUSS5Aq000LhVFZH759hnq8XrruNTpLxT2M/BVcOKPBvkgkklMkojtd+wzgkgHRAHyx2wEFh7wpb2oaX9+v3wCIFBNKwhAhU4hJEh40SbNQVRVTRSEAG/ccaXY7M1L6V0UR0PGWejsDnXeNxmDyRtgoVaLu5i45+ohIgNA4cJfDBb8XHipnGb8YGILJAHvzPOw9XsCgouMQ07vx9kAYQL5IR1sm2LKEt+noD9D/Cq/+Sg9sBLkqhefLFArZW9l43MW3R5w8fHEkEZZqdreiCE+WxwnHN8Cx9eLZ+D6WSAiLh/B4Ea3Ue6xosyxPLGta68+xNjjOhO7xoBgMmHBzdvcYpifbSd15CCorxbJpoIz1vtQStZOq6rrop8cplhiHXA8X/QG6nBXc85RIJM0nJBri+nPj+CGsTv+BL/Y4eKzggHgfqxtfmofv08Sy2Pi+Zm8UsiLf0xYg1uQmF9hdHESannpIMImkuLquY1E0zkqIZvqkAdLj2IZ0Xs+XJaLqS19zeYtpn6LxZTBB3NlihheZCGPuB9VAtNlFytnepcfdFSLaMBg0j1he9GW2z0gT3itTiDDCio6K7Uo90UYOr9jeVY4r/yg3f27j/1aUMOGfJ8ktDZAV32AWRpUpVPyseA27iiLhsTuyDja/K8T6qkH0r5GIxzcXrEYxiNIVB+6xiiWFy1LhsjnCaHNV1vWA6d6CvWqtwcXjFN423QP9L4O7v4Fr/yEHdImkLenSn+uvuRJFgbRMN8fyykQUdbUJoLu8hPW+5KpJVojoJianES1bl7Az0jdGTMaP03xD6f2lonqMqeAA+/9wtlzqbQc6r/FVHY9LzOi6nH3qbUX2EMaSq0Ikaz1nCugaNwwWL8yXexxwIsi0HL6yHb4cXcc24Nep6RpkeMsvmaxiebE2Dju4nZC7ly92lZNRIoycbVlOLnzrBEcK66kP5luONZhEIkSTVXjGTKHiWWVuFsepxirtWT18t1f0K7lsGwy8wnueAc65BibMFM+9ugGm6+JefVo8j1M8B4dd/Nx7HNwxH27+oGbCRolE0jaEdyVh4GjGDxBpI77YXSEmg9XGgp+2plFa4STSonDu2T3FRCp+oAyIaQGG9IkHoNDSfEN2/SGxdJns+Bm6DW6RfkmahjS+QHhSeo9rmfI7YfFCWFou/rhJ/gX0GM21A40YVNhxwsPh3WnBteXwppkwWqFgv4guNJi8a/WKMMY8biG6rygRhlZ1KkX+Lz1zK3/dUAHAr0eF0zfGwMECNxe8dZKfsmud0xCKIuRXx9YLI8kUAvYT9Yr0S8sryTOKgeLW6FqBBqYQkeLigt+J3D8+A0xzi4+7Uni5NDdEdBearls+gju/FMueEomkfVAUiB/ITZeKlYL5uyuh+JgYn7xjwffffw/ARb2NGLoNEpOs6CClFpIGGTc4CQBXRA80e9Mz3bvdHvJUYThfE7m/RfsmCR5pfPlqBvYc3TLtqapIeaB7oPCwWPYbfS9xiX24pI9YRvty1Zbg2nKWer1ABji+CdDJL4dKly6WBR02OLFTGDLuADUeS0+C7ub7LbvYmu3GalR4cXI06x9I4NwEEyfsHsa/fYJ1h+spihvw/kwi1UVJpvCGOez11nV77+t1qJYw9PIi7rjpproHhHWB5Kth1K+F0eqqAI9DeNuSLoaU/4NfL4ffbYbUF+CsiTVEohKJpJ2I7MENt9yJosCmLA/HjxwUkz23GEu+3yC88hf3NYvJU5ez689VKGkSFw4fiO5xo5hD2Lb84yafv2jdVpSQSHRnObdPvqQVeigJBml8aW7xhd5jRMu1GTdQeLziBgrBuy0bRtzNuRdOpPs9f+eNgW+R+uqqxrMJO+yibx4nZKaxPdtD779kM+ndk3h8/3XHNwhBvruiZg1Ij0t434qO8dcfxPZfjQwjLsxAYqSRdb9N4OK+FmwOncvfO8lXPwcoDwI43DrL0iuY9lUBQ17L5tX15cKwzNgkliFdZcIIDMCCtIMA9CjahvHC3wW+x9gkkfNn+O3QbyKcPw1+vw1u/xzO/y10HSQNLomko6GqJAy5kIsHCs/2FztLxWTTWYau63y/UUwwL06yihQ8sUnt2dszilCrBbVMJNZev6Xpub7mfy9kL5GFewif9EiL9k0SPNL40lzCiOh2bsu1qaoQ1RPOngSDroHuw1iWH8tXPR7CFN8HDGbSc8uZ+tG2hg0wX6Rjzk50h50ZS0upcOlsOObgn2mlIhIw5ydvLrBaEY8OO7gq2L9rK1+ni6XFP1xUFZUYHaLy7a+7cu3gEBxuuOGjPN5NE0ZaXqmHD7aWcsOHecQ9m8Hk93N5a2MpP5908cflxWTbNGH0oXgz3dc1vjRN44gzHICrQnY1rPVIGCrSfPRPEcuLYXHBP2uJRNI+RPfmxl9MAuCLnytFyglnKen79pFXZMNihFGDksRyY7gUc7ckkarwMG7PacKqhZefTorvg1H6HrEcLGkXpPGluaHHeWKpq6VRFBHlkzSeuYcSUQDFG2GpU1XOISAed1WkY8ZGFu5xsu5olT7rj8uKyatQhc4rY7NYmqxe47GyBCpLmLt4O7oOVyWHMDC+ZvRgiEnlizvi+fWocDQd7l9QyHmvZ9NtTiZ3zy9gwc/llDp1EiMN/GZMOOclmnG44eUNlcKr5qvrFkB0/9XaLShhXdBdDqamNFJOSTVA7/MhaQLE9G30sUokkg6AwcQNv3wARYEfM9xkHE4X9RzXrAJgbE8Tlh5DhNC+JfS0Ej89wkUKnqAqhVSjoMROeahIJH5H76brxSQth3wjFAV6nd/qlzlSWNm0cg6+SEfdgyPjJx77Vhz3x4mRnJdoprhS48lvvcbWsR+EkVY94tFhp+DYHv69TZz3yMWBc3EZDQrv3BDLHyeK/TtyXOg6nJdo5ulJUWz+XQIZT/bg7eu78EJqNABvp5WLVBUZG8HsFd3X4j8rdwAQVfATsVcFUSTdFALxAxrOZi+RSDoUiYPGcOEgUWfwyx8PQXkB69atAeDiPmbofh5ENVz/VdJ0BvUQiWpzTU2LePz3kh9QjCZ0ey6X3vDb1uiaJEg6t/Hlq0V2qvm9giApLqxunnZdp2+X0MAn+Apqn9zD334o5nCRh8RIAzMnRPHGtSJS5b0tZfyYpUPRMVFbzWEXWfUByvKZt2AdFS4Y0cPMJf0s9fZNURTmXBHDl3fGM29KLMef7MG2h7rzzGXRjOppQVVFzy/vb2V0TzMVLl1ET2ZuEUufjhJw1tSM7SoU50xQd0KozGgtkZyRmKzcdLMIppm/S4wJ/sz2SRYRmWyuZ4yTNJtRA4RBWxGWCC5H0Oct23EUgB623ajDbm6NrkmCpJMbXy4RNZjYgmL7epiR0h+dKu24ruugKDgO15N2wiFqOub+/D2z1wrD5vkrogm3qIzrY+HeUaIG17TFJXg8bsjZWSW61zQcBcf5x0pRfuLhiyJQghCtXz8klN+eH0Gv6MD6LEVR+NOlUQC8samCguJSyD8Azooaovstew7hiUxE1zUeHBbM05FIWpeysjIefPBBZs2axYwZM3A46n5hrVmzBkVRanxGj66Kgl68eDHTp09n6tSprFixoi2736G54a6pAGzIcLNx3UqOZWRjUGHcsP4Q06ede3dmMnHkIACUsFiOp30d9HmHyoS8ZqJlrwxkamc6t/EFIi1EA+VxWorUId2Zd+cIkhMisBhVYs1iEfKIdQCLly6ve0KlDSpKeHr+TmwOnZE9zNx1XlXR0xdTY4i2quzIcTFvc6WIPnSUe1M/2Pn48y84YffQM8rAzUNbrljqVYNCGN7dRJlTZ+6PFZCZBrq7huh+3tfrAbDkpzPwthda7NoSSXN54IEHSElJYfbs2YwYMYKZM2fWOWbVqlV8/vnn/Pjjj/z44488//zzXHPNNQDs27eP2bNnM3fuXN58800ef/xxsrKy2vo2OiQ9+g3kgqEiQfWMt4VRel53IxHn3SByHkpanIS4GPQykUty7dpVQZ3z0/5jaJEJ6LrGvSNk2o/2pnMbX4pBJFdtI1KHdGfp9PGkz57M1md+QVRROorByMML9lLpqJXstDyfXds28c4W4fV67aoY//IfQHy4gee9GqynVpRxMq8Aig+DsxS9ooRXP18HwEMXRGAytNwMR1EUZnm9X3/bWEHx0V3gKKshut9wTERdjnTvgK7JLXZtiaQ5ZGdnM3/+fCZPngzA5MmTmTdvHna7vcZx9913HzfddBNjx45l7NixHD58mClTpgAwd+5cUlNTURQFVVUZN24cb731VpvfS0flptt/BcCmDDGOXdzHDIOukt6VViTUI/5+Nx8pCOr4D5b9CICp8DBnTZnVav2SBEfnNr4MpjbRewVCVVX+88gN6I5S3F3O5tdPPFO10+NGLyvg4XfXoOlw47mhIl9OLX4zJpwRPcyUVOo8sdwOJ36GimK++3YpuzPthJsV7h/T8jOcKeeEck43EzaHzt832KDggF90n3mygLIwIQL9dc/MFr+2RNJU1qxZQ1xcHFareIfi4+Mxm82kpdVc8u/duyoDu6Zp7N27lyFDRAmrVatW0adP1RJa//79Wbt2bb3XdDgc2Gy2Gp8zmRvvuJuQAePofs/f6f3IAjae/wrLcsLbu1tnNN1CxL/7HdFBHf/9AWGkDXTshu4tmFpJ0iyk8dVzVLtdftiAPlzdQ8wUfzCMZO2GTWKHs5T/LfuWFftLMRvgL14PV20MqsKbXvH9B9sd/LD5Jyg6xquv/wOAX48KJTqk2n+xrou8XLULWTcRVVV4aqLwfs39sQL7kW1CdO+q5K2v1qAYTCjFmaTc9fgpXUciaQmysrKIjY2tsS0iIoLs7Ox6z/nxxx8ZN67KK167jcbOf+GFF4iKivJ/evU6syP+dhcb6DrlKUxxfVCMZnLMPZn6yc7GE0lLmk3/rsK4zVIbj3h0uz3kqiLw6ZqI9FbtlyQ4mmR8BSNarc6nn37KhAkT6rQRGxvrF7QuXLjQv6/NBa2RPSGifZP//e3hOwgpPIBisvDbd9egaRqusiIe+cdXAMy4MIJ+XUz1nn9+bwv3jRYv4YOLi9mxdjHfbtyNqsD0C4WB5C9WrXm8Bpgu8oj5fm8GNw8NZUCckcIKnTdXHITCI1BZwre7hQdsYPk2SL6yWW1LJC2Joih+r5cPp9OJyVT/e7Vw4UL/kmOgNho7f+bMmZSUlPg/GRkZp3AHHZ+5Kw4AOorqy2OoNpzHUHLKDO8n8nXZQ7o3euzi77eihEShOyu4I3V8a3dNEgRNMr6CEa36yM7OZs6cOXW2v/fee7z99tt89913fPfdd+0naA2JEbUC2xlVVXlnWiq6y0Fl1yEM++OXJL+yG/vkF0k49wKeujS60TZeSI0mNlTlQNQYrt1zCb0fWcCAqf8gPXxUldGFLhLJmqwiJ5jB+8Wheeo3wnyeMs0jktF6vB9dw6Aq/NHr/frr+jLKjm6lvCTfX0j75sif67YnkbQDiYmJlJSU1NhWWlpKYmL9HoNNmzYxdmxVAffabdjt9gbPt1gsREZG1vicyRzJL4NayXQazGMoOWXGnyeSV+sRCdgPNVwveP73uwFvSaGUR1u9b5LGCdr4Cla06uPFF1/kgQceqLHN4/GwZMkShg4dSkpKCikpKRgMIqlmmwtar3oNLv1T67XfBC4anszgMCFSt+kheFAxxfXBcuUf2aCe1+j5cWEGfnVDCl2nPAUxvVGMZiojejO1/Dcscw0ThpbJKopio4KiihQbphBRvkhRRb1GzV1ljGlusc2bEgPVJI41mPwG2e3DQkmKNZJXpvPPRT/y/jcbUSxh6OXF3Hnj9a381CSS4JgwYQKZmZk4nWKJ37dcOGbMmIDH7969m8GDB6NWy8o+adIk9u/f7//94MGDTJzY/pO3jkKgPIaKAv3iWy7SWlKTjHIjuqahqAZS/32owSXeHSdEGaKRyJJCHYWgja9gRasA77zzDnfeeSehoTWT6y1fvpz169eTnJzMFVdcQW5uVUb2Nhe0msOEQdJB0GL6gK7783EpqoqCxuuVjSzdeb1Te3pOAV2rcvsr3vMdVwtDK+B/tQKqUXjCjD7jTKkyzoxWr6csRPyuGsW/BgugYFI0Zl4iZvQvry3k8y3HAUgs3o754oda5sFIJKdIYmIiqamp/vFk+fLlTJs2DavVyiuvvEJ6ek0NzFdffVVjyRGoIYVwu92kpaVx//33t80NnAbUzmOoKGJomj5pQLv260xl2e4cpn283f99keUMq7dWcJGt1F9S6LYeJ9u0n5L6Cdr4Cla0evDgQWw2W8BZ5eTJk7Hb7axdu5asrCyuvvpqNE0L2H5nE7QeLSivE5ato3LY0xWcZdU+5eJfV0XVx13JES1BGE2Bzg8GxeD1hlUzxBQDAf9EVKPwgqkGfjXcSq/hF8KNr3M0RiSkHBxR3nAhbYmkjZk3bx6fffYZs2fPZufOnX5JxCeffMKuXbtqHLt69WouvfTSGtuGDRvGPffcw6OPPsrDDz/Ma6+9RkKCLBbto3Yew+SECObdOZLUIfIZtQZzVxwQnsZq1q5CYI3dv//3A4rRjF6az2U3yAlDRyHob8hgRKsej4c333yTl19+ucF2xo8fz+rVq0lOTmbjxo1ccMEFzRK0Pvzww/7fbTbbaW2AJcWFkX7CXqP+o4JGP0sJ9BztXRb0RioqKoTGCt1aSAyYwkja4CS9zIhezfmvoNHPkFv3Yg0SZF4eRQWjhVWu81CvuB+TrqEoCrquszLiFyzbnUPqkMaFoBJJWxAXF8e7775bZ/vWrVvrbFu5cmXANu65554W79eZROqQ7vKdbyOO5JfVrRUM7D9RdwXom+1HQO1Nom0n6nmvtEn/JI0TtPEVjGh1w4YNzJs3j/feew8QBpTT6SQ6Opri4uIa58bHx3PzzTf7o4CaI2i1WOqvV3i6MSOlP1M/2uZ31ysIz9X0cz2QdAuExQdOWFiWB5UlzBgbxdSV7mrn6+J801dVBluLozC38koUqtpXqs3A5EAskUgkLU+gyTqAR1e4+Hev8NWc39IlSuR4PFRqgkiYYE6XSW87EEF/IwcjWh09ejR79uxhx44d7Nixg2effZZRo0axY8eOgG0ajUaGDx8OSEFrHbd99wjm3T6M1IvOF4ZN/oGaxas9big8LITx/SaSOiml1vmRzLsinNSoDLFM6Ssi3pLoGkc8XdGpvdwpo5wkEomktaijsau2LyN8EKOe/IRnPv2eCS+tRIvsjq7rDBgoq410JJrk+fKJVi+77LI6otWrr76agQMH0rdvX/85PoG+b9uiRYsYMGAAgwYNIj09ncjISAYOFOGyU6dO5fe//z1PP/20X9D67LPPtujNdnQCu+17QlQPyN4Befug3ACWCCg9CbFJ0Ot8f66ygOefuxC+uBsKDtdMMVEbXfdGO7rF74qC/5X2/awowovmcXpTU+gkqSdJ1xJrGGAyykkikUhaD99k/fWVBzicV0a/+DCmTxrA/o3f8tefVIjqwfs7bP5odQV4pmAi3aUcpMPQJFX0vHnzePLJJ9m0aROFhYW8+OKLgBCt9u3b129I1UdaWhp33XUX48ePZ+LEiTz33HP+fdUFrU6nUwpaqxMSA/0mQEwfyNoOFQXQawwkjmg8YjPubLjrK1hwPxzfKIwng7nK0NJcVaFJqkGI6cFrXGnCjeU1tMSLrEJYnOhPn4uYUTmYqQuOVi13yigniUQiaXUCTbZTh/yKK49lc/nf1qOZQmsK8hUpB+lIKLrezBTnHQybzUZUVBQlJSVndkJDV4UoYh3RvWnr964K+OZR+Hmh10oyiIjF+IHQYwR0OxcShkDcAEABjwPcDnBXej/en40h4phqOZCW7c6pMwOTUU6SluJMeLfPhHuQnD4MmLUUp7tuGTmLUSV99uR26NGZS3PfbZkP4HTDFCI+zTnv6r9D3/HC29VtCHQdJAywQBiMIhdaEMgoJ4lEIuk49AsUPS/lIB2Kzl1Yu7OhqjDsFjjvTkgcXr/hJZFIJJLTFpn0tuMjjS+JRCKRSM4gZNLbjo9cdpRIJBKJ5AxDykE6NtLzJZFIJBKJRNKGnDGeL1/QZpMLbEskkg6N750+nQOz5fgkkZyZNHd8OmOML7vdDnBa13eUSCT1Y7fbiYqKau9uNAs5PkkkZzZNHZ/OmDxfmqaRnZ1NREQESiP5r3xFuDMyMmTOnSYgn1vTkc+s6dR+Zrqu+2u9qurpqZSQ41PrI59b05HPrOm01Ph0xni+VFWlZ8+eTTonMjJS/sE1A/ncmo58Zk2n+jM7XT1ePuT41HbI59Z05DNrOqc6Pp2e00iJRCKRSCSS0xRpfEkkEolEIpG0IZ3S+LJYLPz5z3/GYpEZ3puCfG5NRz6zptPZn1lnv//mIp9b05HPrOm01DM7YwT3EolEIpFIJKcDndLzJZFIJBKJRNJeSONLIpFIJBKJpA2RxpdEIpFIJBJJGyKNL4lEIpFIJJI2pNMZX2VlZTz44IPMmjWLGTNm4HA42rtLHZalS5cyZswYjh496t8mn1/9LFiwgKSkJLp06cL06dNxu92AfGaNsWHDBgYPHkx0dDTTp0/3b++Mz60z3nNzkeNT05DjU/NotfFJ72Tcdddd+oIFC3Rd1/UPPvhA/8Mf/tDOPeqYnDx5Ul+8eLEO6EeOHPFvl88vMMeOHdPvuusufcuWLfqHH36oh4WF6S+//LKu6/KZNYTdbtfnzJmjFxYW6kuWLNGNRqP+3Xff6breOZ9bZ7zn5iDHp6Yhx6fm0ZrjU6cyvrKysnSr1apXVFTouq7rubm5ekhIiG6z2dq5Zx0Tj8dTY3CTz69+1q1bp7tcLv/vjz/+uH7llVfKZ9YIFRUVuqZp/t9Hjhypr1q1qlM+t854z6eCHJ+CR45PzaM1x6dOtey4Zs0a4uLisFqtAMTHx2M2m0lLS2vnnnVMahcJlc+vfi6++GKMxqpSqYmJifTu3Vs+s0awWq3+QtNlZWUkJyczYcKETvncOuM9nwpyfAoeOT41j9YcnzqV8ZWVlUVsbGyNbREREWRnZ7dTj04v5PMLns2bN/PAAw/IZxYkK1as4PLLL8flclFeXt4pn1tnvOeWRD6/4JHjU9NojfGpUxlfiqL4LVUfTqcTk8nUTj06vZDPLzgOHDhAt27dGDp0qHxmQTJkyBDuu+8+Vq5cyWOPPdYpn1tnvOeWRD6/4JDjU9NpjfHJ2PghZw6JiYmUlJTU2FZaWkpiYmI79ej0Qj6/xnG73fzzn//khRdeAOQzC5aEhATuueceVFXlpZde4qKLLup0z03+rZwa8vk1jhyfmkdrjE+dyvM1YcIEMjMzcTqdAH4X4ZgxY9qzW6cN8vk1zssvv8xjjz2G2WwG5DNrKiNGjKBHjx6d8rl1xntuSeTzaxw5Pp0aLTk+dSrjKzExkdTUVNauXQvA8uXLmTZtWh33oUSge2uu+/6Vz69hZs+ezciRIykvL+fw4cO89957lJeXy2fWAJWVlWzdutX/+9KlS3nooYc65d9aZ7znU0GOT01Djk9NpzXHJ0X3/eV2EvLz83nyySfp27cvhYWFvPjii/5ZgKSK0tJSPvzwQ6ZNm8af//xnfve73xEXFyefXz0899xzPP300zW2JScns3fvXvnMGuCnn37i8ssv56yzzuKCCy5g1KhR3HrrrUDnfFc74z03Bzk+NQ05PjWP1hyfOp3xJZFIJBKJRNKedKplR4lEIpFIJJL2RhpfEolEIpFIJG2INL4kEolEIpFI2hBpfEkkEolEIpG0IdL4kkgkEolEImlDpPElkUgkEolE0oZI40sikUgkEomkDZHGl+S0IJh0dDJlnUQiaWuCHXfk+CSpjjS+JB0al8vFiy++yLZt2xo9dseOHbz00kv+elsSiUTSWlQfmx599FFiY2MbPP7o0aM8++yz2Gy2NuqhpCMjjS9Jh6WsrIxbb72VG264gZEjRzZ6/Hnnncdll13GLbfcgt1ub4MeSiSSzohvbJo8eTLff/89H3zwAUVFRQ2ek5SUxN13381tt91GTk5OG/VU0lGRxpekw3L77bdz9913079//6DPOe+887j55pu59957W7FnEomkM+Mbm4YNG8aMGTO49NJLgzqvd+/e/PGPf+Smm27C5XK1ci8lHRlpfEk6JP/617/Izs7m6quvbvK5t912G0ePHuXdd99thZ5JJJLOTKCxyWq1Bn3+hRdeSPfu3XnmmWdao3uS0wRpfEk6HA6Hg6effprbbrut2W3cddddPPvss3J2KZFIWoz6xiZFUZrUzl133cXcuXMpKChoye5JTiOk8SXpcMyfP5/s7Gwuv/zyGtvLysp49NFHueCCCxgzZgxnn302f/nLXwK2cckll5CRkcH8+fPbossSiaQTUN/YVJ1XX32Vrl270rVrV5566ik8Hk+dY8aPH095eTlvv/12a3ZX0oGRxpekw7FgwQIMBgMDBw6ssf3+++/niy++YOXKlaSlpXHrrbfy5JNPsnz58jptDB48GFVVWbJkSVt1WyKRnOHUNzb5eOWVV3j22Wdxu93k5eXx/PPP89BDD9U5Ljo6mh49esjxqRMjjS9Jh+PHH3+kV69emEymGtuXLVvG4MGDCQkJAeD6668HRIqJ2phMJnr06MGqVatavb8SiaRzUN/Y5EPTNPLy8igoKOCNN94A4K233uLw4cN1jk1KSiItLY3S0tJW7bOkYyKNL0mHorS0lBMnThAVFVVn3z/+8Q+efvpp/+8ZGRkAVFRUBGwrNjaWkydP4nA4WqezEomk09DQ2OTj8ccfx2QyoSgK06ZN48Ybb0TX9YCTwNjYWDweD9nZ2a3ZbUkHRRpfkg5FSUkJAKGhoXX23X777QwfPpw333yTm266iS1btgD1Z472tZGXl9dKvZVIJJ2Fhsam+vjFL34BQG5ubp19vnYC7ZOc+UjjS9KhCAsLAwjorVq/fj1Dhw7FarXy+eef8+tf/7rBtnwRSEajseU7KpFIOhUNjU31kZCQABDQWybHp86NNL4kHYro6GjMZnMdHURmZiaTJ0/mhhtu4N577w0qtNtut2MwGBot+yGRSCSNUd/Y1BC+JcWxY8fW2eerwtG1a9eW6aDktEIaX5IOx9ChQ8nKyqqxbf369djt9hoDlW+5sb5lx7y8PAYOHIjZbG69zkokkk5DoLGpIZYuXcpFF10UsDxaXl4e4eHhJCUltWQXJacJ0viSdDguv/xyysrKOHHihH/bgAEDAJg7dy6rVq3is88+48EHHwRg165dvPnmmzUK1trtdk6cOEFKSkrbdl4ikZyxBBqbACwWCwDvvfeef9tnn33G5s2b+e9//xuwrf3793PppZc2OUGr5MxAGl+SDsdtt92Goih+QT2Imo3PPfccdrude++9lz179vD555+TnJzMzp07Oeecc4iMjPQfv3XrVgDuuOOONu+/RCI5Mwk0NoGYFL700ku89tprJCUlcemll7Ju3To2b95M796967Rz9OhRCgsLufPOO9uq65IOhqLXt2YjkbQj119/Pd27d/fnymkqTzzxBFu2bGHlypUt3DOJRNKZOdWxCUTur9dee429e/diMBhasHeS0wVpfEk6JIcPH2bixIls2rTJHzEULOXl5SQnJ/uTskokEklLcSpjEwiN6vDhw3n11VeZNGlSK/RQcjogjS9Jh+Xrr7/miy++4IMPPmjSeU888QSDBw/mV7/6VSv1TCKRdGaaOzYBvPnmm+Tn59dIGC3pfEjjS9KhWbRoEXv27OHJJ58MSpj6xhtvEB4eLg0viUTSqjR1bAL4/PPPOXbsGI899lgr907S0ZHGl6TDk5GRQXFxMeeee26Dx+3evRuLxUL//v3bqGcSiaQzE+zY5Ds2Nzc3YNoJSedDGl8SiUQikUgkbYhMNSGRSCQSiUTShkjjSyKRSCQSiaQNkcaXRCKRSCQSSRsijS+JRCKRSCSSNkQaXxKJRCKRSCRtiDS+JBKJRCKRSNoQaXxJJBKJRCKRtCHS+JJIJBKJRCJpQ/4fHvuYMiOABaYAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 700x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(figsize=(7,4))\n",
    "\n",
    "X_missed = X_missed_2\n",
    "\n",
    "\n",
    "plt.subplot(2,2,1)\n",
    "plt.plot(res4_x[:,3], c='k', label='imp', alpha=1) \n",
    "plt.fill_between( range(0,30), res3_005[:,3], res3_095[:,3], color='#ff7f0e', alpha=0.3)  # alpha是透明程度\n",
    "plt.fill_between( range(0,30), res4_005[:,3], res4_095[:,3], color='#ff7f0e', alpha=0.9)  \n",
    "plt.plot(X_missed[0][:,3].cpu().numpy(), c='#1f77b4', marker='o',alpha=1, markersize='4') # 观测值的蓝色把之前的值覆盖了 生成区域的值还是之前的0.5分位数的\n",
    "plt.xlabel(\"(a)\", fontsize=15)\n",
    "\n",
    "\n",
    "\n",
    "'第一个样本的图'\n",
    "X_missed = X_missed_1\n",
    "\n",
    "plt.subplot(2,2,2)\n",
    "plt.plot(res2_x[:,3], c='k', label='imp', alpha=1) \n",
    "plt.fill_between( range(0,30), res1_005[:,3], res1_095[:,3], color='#ff7f0e', alpha=0.3)  # alpha是透明程度\n",
    "plt.fill_between( range(0,30), res2_005[:,3], res2_095[:,3], color='#ff7f0e', alpha=0.9)  \n",
    "plt.plot(X_missed[0][:,3].cpu().numpy(), c='#1f77b4', marker='o',alpha=1, markersize='4') # 观测值的蓝色把之前的值覆盖了 生成区域的值还是之前的0.5分位数的\n",
    "plt.xlabel(\"(b)\", fontsize=15)\n",
    "\n",
    "plt.savefig('分位数',bbox_inches='tight',dpi=500)\n",
    "plt.savefig('分位数.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e017f7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ceab5b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
